-------------------------------------------------------------------
Mon Apr 26 09:15:38 UTC 2021 - Dirk Müller <dmueller@suse.com>

- update to 1.6.1:
  * Fatal idempotent producer errors are now also fatal to the transactional
   producer. This is a necessary step to maintain data integrity prior to
   librdkafka supporting KIP-360. Applications should check any transactional
   API errors for the is_fatal flag and decommission the transactional producer
   if the flag is set.
  * The consumer error raised by `auto.offset.reset=error` now has error-code
   set to `ERR__AUTO_OFFSET_RESET` to allow an application to differentiate
   between auto offset resets and other consumer errors.
  * Admin API and transactional `send_offsets_to_transaction()` coordinator
   requests, such as TxnOffsetCommitRequest, could in rare cases be sent
   multiple times which could cause a crash.
  * `ssl.ca.location=probe` is now enabled by default on Mac OSX since the
   librdkafka-bundled OpenSSL might not have the same default CA search paths
   as the system or brew installed OpenSSL. Probing scans all known locations.
  * Fatal idempotent producer errors are now also fatal to the transactional
   producer.
  * The transactional producer could crash if the transaction failed while
   `send_offsets_to_transaction()` was called.
  * Group coordinator requests for transactional
   `send_offsets_to_transaction()` calls would leak memory if the
   underlying request was attempted to be sent after the transaction had
   failed.
  * When gradually producing to multiple partitions (resulting in multiple
   underlying AddPartitionsToTxnRequests) sub-sequent partitions could get
   stuck in pending state under certain conditions. These pending partitions
   would not send queued messages to the broker and eventually trigger
   message timeouts, failing the current transaction. This is now fixed.
  * Committing an empty transaction (no messages were produced and no
   offsets were sent) would previously raise a fatal error due to invalid state
   on the transaction coordinator. We now allow empty/no-op transactions to
   be committed.
  * The consumer will now retry indefinitely (or until the assignment is changed)
   to retrieve committed offsets. This fixes the issue where only two retries
   were attempted when outstanding transactions were blocking OffsetFetch
   requests with `ERR_UNSTABLE_OFFSET_COMMIT`. #3265
  * [KIP-429 Incremental rebalancing](https://cwiki.apache.org/confluence/display/KAFKA/KIP-429%3A+Kafka+Consumer+Incremental+Rebalance+Protocol) with sticky
   consumer group partition assignor (KIP-54) (by @mhowlett).
  * [KIP-480 Sticky producer partitioning](https://cwiki.apache.org/confluence/display/KAFKA/KIP-480%3A+Sticky+Partitioner) (`sticky.partitioning.linger.ms`) -
   achieves higher throughput and lower latency through sticky selection
   of random partition (by @abbycriswell).
  * AdminAPI: Add support for `DeleteRecords()`, `DeleteGroups()` and
   `DeleteConsumerGroupOffsets()` (by @gridaphobe)
  * [KIP-447 Producer scalability for exactly once semantics](https://cwiki.apache.org/confluence/display/KAFKA/KIP-447%3A+Producer+scalability+for+exactly+once+semantics) -
   allows a single transactional producer to be used for multiple input
   partitions. Requires Apache Kafka 2.5 or later.
  * Transactional producer fixes and improvements, see **Transactional Producer fixes** below.
  * The [librdkafka.redist](https://www.nuget.org/packages/librdkafka.redist/)
   NuGet package now supports Linux ARM64/Aarch64.
  * Sticky producer partitioning (`sticky.partitioning.linger.ms`) is
   enabled by default (10 milliseconds) which affects the distribution of
   randomly partitioned messages, where previously these messages would be
   evenly distributed over the available partitions they are now partitioned
   to a single partition for the duration of the sticky time
   (10 milliseconds by default) before a new random sticky partition
   is selected.
  * The new KIP-447 transactional producer scalability guarantees are only
   supported on Apache Kafka 2.5 or later, on earlier releases you will
   need to use one producer per input partition for EOS. This limitation
   is not enforced by the producer or broker.
  * Error handling for the transactional producer has been improved, see
   the **Transactional Producer fixes** below for more information.
  * KIP-107, KIP-204: AdminAPI: Added `DeleteRecords()` (by @gridaphobe).
  * KIP-229: AdminAPI: Added `DeleteGroups()` (by @gridaphobe).
  * KIP-496: AdminAPI: Added `DeleteConsumerGroupOffsets()`.
  * KIP-464: AdminAPI: Added support for broker-side default partition count
   and replication factor for `CreateTopics()`.
  * Windows: Added `ssl.ca.certificate.stores` to specify a list of
   Windows Certificate Stores to read CA certificates from, e.g.,
   `CA,Root`. `Root` remains the default store.
  * Use reentrant `rand_r()` on supporting platforms which decreases lock
   contention (@azat).
  * Added `assignor` debug context for troubleshooting consumer partition
   assignments.
  * Updated to OpenSSL v1.1.1i when building dependencies.
  * Update bundled lz4 (used when `./configure --disable-lz4-ext`) to v1.9.3
   which has vast performance improvements.
  * Added `rd_kafka_conf_get_default_topic_conf()` to retrieve the
   default topic configuration object from a global configuration object.
  * Added `conf` debugging context to `debug` - shows set configuration
   properties on client and topic instantiation. Sensitive properties
   are redacted.
  * Added `rd_kafka_queue_yield()` to cancel a blocking queue call.
  * Will now log a warning when multiple ClusterIds are seen, which is an
   indication that the client might be erroneously configured to connect to
   multiple clusters which is not supported.
  * Added `rd_kafka_seek_partitions()` to seek multiple partitions to
   per-partition specific offsets.
  * Fix a use-after-free crash when certain coordinator requests were retried.
  * The C++ `oauthbearer_set_token()` function would call `free()` on
   a `new`-created pointer, possibly leading to crashes or heap corruption (#3194)
  * The consumer assignment and consumer group implementations have been
   decoupled, simplified and made more strict and robust. This will sort out
   a number of edge cases for the consumer where the behaviour was previously
   undefined.
  * Partition fetch state was not set to STOPPED if OffsetCommit failed.
  * The session timeout is now enforced locally also when the coordinator
   connection is down, which was not previously the case.
  * Transaction commit or abort failures on the broker, such as when the
   producer was fenced by a newer instance, were not propagated to the
   application resulting in failed commits seeming successful.
   This was a critical race condition for applications that had a delay after
   producing messages (or sendings offsets) before committing or
   aborting the transaction. This issue has now been fixed and test coverage
   improved.
  * The transactional producer API would return `RD_KAFKA_RESP_ERR__STATE`
   when API calls were attempted after the transaction had failed, we now
   try to return the error that caused the transaction to fail in the first
   place, such as `RD_KAFKA_RESP_ERR__FENCED` when the producer has
   been fenced, or `RD_KAFKA_RESP_ERR__TIMED_OUT` when the transaction
   has timed out.
  * Transactional producer retry count for transactional control protocol
   requests has been increased from 3 to infinite, retriable errors
   are now automatically retried by the producer until success or the
   transaction timeout is exceeded. This fixes the case where
   `rd_kafka_send_offsets_to_transaction()` would fail the current
   transaction into an abortable state when `CONCURRENT_TRANSACTIONS` was
   returned by the broker (which is a transient error) and the 3 retries
   were exhausted.
  * Calling `rd_kafka_topic_new()` with a topic config object with
   `message.timeout.ms` set could sometimes adjust the global `linger.ms`
   property (if not explicitly configured) which was not desired, this is now
   fixed and the auto adjustment is only done based on the
   `default_topic_conf` at producer creation.
  * `rd_kafka_flush()` could previously return `RD_KAFKA_RESP_ERR__TIMED_OUT`
   just as the timeout was reached if the messages had been flushed but
   there were now no more messages. This has been fixed.

-------------------------------------------------------------------
Tue Dec 22 09:26:30 UTC 2020 - Dirk Müller <dmueller@suse.com>

- update to 1.5.3:
 * Fix a use-after-free crash when certain coordinator requests were retried.
 * Consumer would not filter out messages for aborted transactions
   if the messages were compressed (#3020).
 * Consumer destroy without prior `close()` could hang in certain
   cgrp states (@gridaphobe, #3127).
 * Fix possible null dereference in `Message::errstr()` (#3140).
 * The `roundrobin` partition assignment strategy could get stuck in an
   endless loop or generate uneven assignments in case the group members
   had asymmetric subscriptions (e.g., c1 subscribes to t1,t2 while c2
   subscribes to t2,t3).  (#3159)

-------------------------------------------------------------------
Mon Oct 26 08:08:55 UTC 2020 - Dirk Mueller <dmueller@suse.com>

- update to 1.5.2:
  * Security fixes: There was an incorrect call to zlib's inflateGetHeader()
  with unitialized memory pointers that could lead to the GZIP header of a
  fetched message batch to be copied to arbitrary memory. 
  * The default value for the producer configuration property retries has
  been increased from 2 to infinity
  * The default value for the producer configuration property
  request.timeout.ms has been increased from 5 to 30 seconds
  * lots of bugfixes, see https://raw.githubusercontent.com/edenhill/librdkafka/v1.5.2/CHANGELOG.md

-------------------------------------------------------------------
Fri Aug 21 12:36:30 UTC 2020 - Chris Coutinho <chrisbcoutinho@gmail.com>

- Update library v1.3.0 -> 1.5.0
 * Improved broker connection error reporting with more useful information and
   hints on the cause of the problem.
 * Consumer: Propagate errors when subscribing to unavailable topics (#1540)
 * Producer: Add `batch.size` producer configuration property (#638)
 * Add `topic.metadata.propagation.max.ms` to allow newly manually created
   topics to be propagated throughout the cluster before reporting them
   as non-existent. This fixes race issues where CreateTopics() is
   quickly followed by produce().
 * Prefer least idle connection for periodic metadata refreshes, et.al.,
   to allow truly idle connections to time out and to avoid load-balancer-killed
   idle connection errors (#2845)
 * Added `rd_kafka_event_debug_contexts()` to get the debug contexts for
   a debug log line (by @wolfchimneyrock).
 * Added Test scenarios which define the cluster configuration.
 * Added MinGW-w64 builds (@ed-alertedh, #2553)
 * `./configure --enable-XYZ` now requires the XYZ check to pass,
   and `--disable-XYZ` disables the feature altogether (@benesch)
 * Added `rd_kafka_produceva()` which takes an array of produce arguments
   for situations where the existing `rd_kafka_producev()` va-arg approach
   can't be used.
 * Added `rd_kafka_message_broker_id()` to see the broker that a message
   was produced or fetched from, or an error was associated with.
 * Added RTT/delay simulation to mock brokers.
 * Subscribing to non-existent and unauthorized topics will now propagate
   errors `RD_KAFKA_RESP_ERR_UNKNOWN_TOPIC_OR_PART` and
   `RD_KAFKA_RESP_ERR_TOPIC_AUTHORIZATION_FAILED` to the application through
   the standard consumer error (the err field in the message object).
 * Consumer will no longer trigger auto creation of topics,
   `allow.auto.create.topics=true` may be used to re-enable the old deprecated
   functionality.
 * The default consumer pre-fetch queue threshold `queued.max.messages.kbytes`
   has been decreased from 1GB to 64MB to avoid excessive network usage for low
   and medium throughput consumer applications. High throughput consumer
   applications may need to manually set this property to a higher value.
 * The default consumer Fetch wait time has been increased from 100ms to 500ms
   to avoid excessive network usage for low throughput topics.
 * If OpenSSL is linked statically, or `ssl.ca.location=probe` is configured,
   librdkafka will probe known CA certificate paths and automatically use the
   first one found. This should alleviate the need to configure
   `ssl.ca.location` when the statically linked OpenSSL's OPENSSLDIR differs
   from the system's CA certificate path.
 * The heuristics for handling Apache Kafka < 0.10 brokers has been removed to
   improve connection error handling for modern Kafka versions.
   Users on Brokers 0.9.x or older should already be configuring
   `api.version.request=false` and `broker.version.fallback=...` so there
   should be no functional change.
 * The default producer batch accumulation time, `linger.ms`, has been changed
   from 0.5ms to 5ms to improve batch sizes and throughput while reducing
   the per-message protocol overhead.
   Applications that require lower produce latency than 5ms will need to
   manually set `linger.ms` to a lower value.
 * librdkafka's build tooling now requires Python 3.x (python3 interpreter).

- Update install location of doc/license files

-------------------------------------------------------------------
Wed Mar 25 22:10:10 UTC 2020 - R. Tyler Croy <rtyler@brokenco.de>

-  Add v1.3.0 which includes a number of properties and bugfixes over 1.1.0

-------------------------------------------------------------------
Thu Oct 31 10:33:44 UTC 2019 - Guillaume GARDET <guillaume.gardet@opensuse.org>

- lto breaks crc32 detection in configure script, so disable it
  for non x86_64 architectures.
  Reported upstream: https://github.com/edenhill/librdkafka/issues/2426

-------------------------------------------------------------------
Thu Sep  5 11:47:36 UTC 2019 - Thomas Bechtold <tbechtold@suse.com>

- update to 1.1.0:
  * SASL OAUTHBEARER support
  * In-memory SSL certificates (PEM, DER, PKCS#12) support
  * Pluggable broker SSL certificate verification callback
  * ssl.endpoint.identification.algorithm=https (off by default) to validate
    the broker hostname matches the certificate. Requires OpenSSL >= 1.0.2.
  * Improved GSSAPI/Kerberos ticket refresh
  * SSL peer (broker) certificate verification is now enabled by default
    (disable with enable.ssl.certificate.verification=false)
  * %{broker.name} is no longer supported in sasl.kerberos.kinit.cmd since
    kinit refresh is no longer executed per broker, but per client instance.
  * SASL GSSAPI/Kerberos: Don't run kinit refresh for each broker,
    just per client instance.
  * SASL GSSAPI/Kerberos: Changed sasl.kerberos.kinit.cmd to first attempt
    ticket refresh, then acquire.
  * SASL: Proper locking on broker name acquisition.
  * Consumer: max.poll.interval.ms now correctly handles blocking poll calls,
    allowing a longer poll timeout than the max poll interval.
  * configure: Fix libzstd static lib detection
  * rdkafka_performance: Fix for Misleading "All messages delivered!" message

- update to version 1.0.1:
  * Fix consumer stall when broker connection goes down
  * Fix AdminAPI memory leak when broker does not support request
  * Update/fix protocol error response codes
  * Treat ECONNRESET as standard Disconnects

- update to version 1.0.0:
  * This release changes configuration defaults and deprecates a set
    of configuration properties, make sure to read the Upgrade considerations
    section from https://github.com/edenhill/librdkafka/releases/tag/v1.0.0
  * add support for Idempotent Producer, providing exactly-once
    producing and guaranteed ordering of messages.
  * In previous releases librdkafka would maintain open connections to all
    brokers in the cluster and the bootstrap servers.
    With this release librdkafka now connects to a single bootstrap server
    to retrieve the full broker list, and then connects to the brokers
    it needs to communicate with: partition leaders, group coordinators, etc.
    For large scale deployments this greatly reduces the number of connections
    between clients and brokers, and avoids the repeated idle connection closes
    for unused connections.

-------------------------------------------------------------------
Fri Dec 14 14:57:50 UTC 2018 - astieger@suse.com

- fix pkg-config provides warning

-------------------------------------------------------------------
Mon Dec 10 11:08:33 UTC 2018 - Thomas Bechtold <tbechtold@suse.com>

- Don't BuildRequire lz4 on SLE12
- Do not use %make_build which is not available on SLE12

-------------------------------------------------------------------
Thu Dec  6 09:24:41 UTC 2018 - Thomas Bechtold <tbechtold@suse.com>

- Add missing Requires to -devel package

-------------------------------------------------------------------
Thu Nov 29 19:13:40 UTC 2018 - Jan Engelhardt <jengelh@inai.de>

- Update summaries. Remove performance results that do not
  mention their reference.
- Remove duplicate Group/Summary fields.
- Fix rpmlint's "librdkafka.x86_64: W: no-binary"
- Fix shlib package requiring itself. Remove excess Provides.

-------------------------------------------------------------------
Thu Nov 29 10:49:49 UTC 2018 - Thomas Bechtold <tbechtold@suse.com>

- Fix Requires for -devel package

-------------------------------------------------------------------
Thu Nov 29 09:22:19 UTC 2018 - Thomas Bechtold <tbechtold@suse.com>

- Initial packaging (version 0.11.6)
