From cf3f8205c4509966f04e6b77dad7c002db16d9d8 Mon Sep 17 00:00:00 2001
From: falkTX <falktx@gmail.com>
Date: Tue, 11 Sep 2018 22:36:15 +0200
Subject: [PATCH] Stupid attempt at updating waf

---
 waf                                         |   25 +-
 waflib/Build.py                             | 1127 ++++++++++--------
 waflib/ConfigSet.py                         |   86 +-
 waflib/Configure.py                         |  183 ++-
 waflib/Context.py                           |  312 ++---
 waflib/Errors.py                            |   20 +-
 waflib/Logs.py                              |  105 +-
 waflib/Node.py                              |  630 +++++-----
 waflib/Options.py                           |  163 ++-
 waflib/Runner.py                            |  576 +++++++---
 waflib/Scripting.py                         |  420 ++++---
 waflib/Task.py                              | 1135 +++++++++++--------
 waflib/TaskGen.py                           |  352 +++---
 waflib/Tools/__init__.py                    |    2 +-
 waflib/Tools/ar.py                          |    4 +-
 waflib/Tools/asm.py                         |   73 ++
 waflib/Tools/bison.py                       |   49 +
 waflib/Tools/c.py                           |   14 +-
 waflib/Tools/c_aliases.py                   |   33 +-
 waflib/Tools/c_config.py                    |  688 ++++++-----
 waflib/Tools/c_osx.py                       |   54 +-
 waflib/Tools/c_preproc.py                   |  470 ++++----
 waflib/Tools/c_tests.py                     |   25 +-
 waflib/Tools/ccroot.py                      |  140 ++-
 waflib/Tools/clang.py                       |    2 +-
 waflib/Tools/clangxx.py                     |    4 +-
 waflib/Tools/compiler_c.py                  |   24 +-
 waflib/Tools/compiler_cxx.py                |   22 +-
 waflib/Tools/compiler_d.py                  |   85 ++
 waflib/Tools/compiler_fc.py                 |   73 ++
 waflib/Tools/cs.py                          |  211 ++++
 waflib/Tools/cxx.py                         |   14 +-
 waflib/Tools/d.py                           |   97 ++
 waflib/Tools/d_config.py                    |   64 ++
 waflib/Tools/d_scan.py                      |  211 ++++
 waflib/Tools/dbus.py                        |   70 ++
 waflib/Tools/dmd.py                         |   80 ++
 waflib/Tools/errcheck.py                    |   88 +-
 waflib/Tools/fc.py                          |  189 +++
 waflib/Tools/fc_config.py                   |  488 ++++++++
 waflib/Tools/fc_scan.py                     |  114 ++
 waflib/Tools/flex.py                        |   62 +
 waflib/Tools/g95.py                         |   66 ++
 waflib/Tools/gas.py                         |   18 +
 waflib/Tools/gcc.py                         |  113 +-
 waflib/Tools/gdc.py                         |   55 +
 waflib/Tools/gfortran.py                    |   93 ++
 waflib/Tools/glib2.py                       |  489 ++++++++
 waflib/Tools/gnu_dirs.py                    |  131 +++
 waflib/Tools/gxx.py                         |  114 +-
 waflib/Tools/icc.py                         |    9 +-
 waflib/Tools/icpc.py                        |    9 +-
 waflib/Tools/ifort.py                       |  413 +++++++
 waflib/Tools/intltool.py                    |  231 ++++
 waflib/Tools/irixcc.py                      |   54 +-
 waflib/Tools/javaw.py                       |  464 ++++++++
 waflib/Tools/ldc2.py                        |   56 +
 waflib/Tools/lua.py                         |   38 +
 waflib/Tools/md5_tstamp.py                  |   39 +
 waflib/Tools/msvc.py                        |  869 ++++++--------
 waflib/Tools/nasm.py                        |   26 +
 waflib/Tools/nobuild.py                     |   24 +
 waflib/Tools/perl.py                        |  156 +++
 waflib/Tools/python.py                      |  627 ++++++++++
 waflib/Tools/qt5.py                         |  796 +++++++++++++
 waflib/Tools/ruby.py                        |  186 +++
 waflib/Tools/suncc.py                       |   56 +-
 waflib/Tools/suncxx.py                      |   53 +-
 waflib/Tools/tex.py                         |  543 +++++++++
 waflib/Tools/vala.py                        |  355 ++++++
 waflib/Tools/waf_unit_test.py               |  211 +++-
 waflib/Tools/winres.py                      |   78 ++
 waflib/Tools/xlc.py                         |   54 +-
 waflib/Tools/xlcxx.py                       |   54 +-
 waflib/Utils.py                             |  652 +++++++----
 waflib/__init__.py                          |    2 +-
 waflib/ansiterm.py                          |    6 +-
 waflib/extras/batched_cc.py                 |   32 +-
 waflib/extras/biber.py                      |   58 +
 waflib/extras/bjam.py                       |  128 +++
 waflib/extras/blender.py                    |  108 ++
 waflib/extras/boo.py                        |   81 ++
 waflib/extras/boost.py                      |  525 +++++++++
 waflib/extras/build_file_tracker.py         |   18 +-
 waflib/extras/build_logs.py                 |    7 +-
 waflib/extras/buildcopy.py                  |   82 ++
 waflib/extras/c_dumbpreproc.py              |   72 ++
 waflib/extras/c_emscripten.py               |   87 ++
 waflib/extras/c_nec.py                      |   69 +-
 waflib/extras/cabal.py                      |  152 +++
 waflib/extras/cfg_altoptions.py             |  110 ++
 waflib/extras/clang_compilation_database.py |   85 ++
 waflib/extras/codelite.py                   |  875 ++++++++++++++
 waflib/extras/color_gcc.py                  |   39 +
 waflib/extras/color_rvct.py                 |   51 +
 waflib/extras/compat15.py                   |  406 +++++++
 waflib/extras/cppcheck.py                   |  591 ++++++++++
 waflib/extras/cpplint.py                    |  209 ++++
 waflib/extras/cross_gnu.py                  |  227 ++++
 waflib/extras/cython.py                     |  146 +++
 waflib/extras/dcc.py                        |   72 ++
 waflib/extras/distnet.py                    |  430 +++++++
 waflib/extras/doxygen.py                    |  227 ++++
 waflib/extras/dpapi.py                      |   87 ++
 waflib/extras/eclipse.py                    |  431 +++++++
 waflib/extras/erlang.py                     |  110 ++
 waflib/extras/fast_partial.py               |  518 +++++++++
 waflib/extras/fc_bgxlf.py                   |   32 +
 waflib/extras/fc_cray.py                    |   51 +
 waflib/extras/fc_nag.py                     |   61 +
 waflib/extras/fc_nec.py                     |   60 +
 waflib/extras/fc_open64.py                  |   58 +
 waflib/extras/fc_pgfortran.py               |   68 ++
 waflib/extras/fc_solstudio.py               |   62 +
 waflib/extras/fc_xlf.py                     |   63 +
 waflib/extras/file_to_object.py             |  137 +++
 waflib/extras/fluid.py                      |   30 +
 waflib/extras/freeimage.py                  |   74 ++
 waflib/extras/fsb.py                        |   31 +
 waflib/extras/fsc.py                        |   64 ++
 waflib/extras/gccdeps.py                    |  214 ++++
 waflib/extras/gdbus.py                      |   87 ++
 waflib/extras/gob2.py                       |   17 +
 waflib/extras/halide.py                     |  151 +++
 waflib/extras/javatest.py                   |  118 ++
 waflib/extras/kde4.py                       |   93 ++
 waflib/extras/local_rpath.py                |   19 +
 waflib/extras/make.py                       |  142 +++
 waflib/extras/midl.py                       |   69 ++
 waflib/extras/msvcdeps.py                   |  256 +++++
 waflib/extras/msvs.py                       | 1048 +++++++++++++++++
 waflib/extras/netcache_client.py            |  390 +++++++
 waflib/extras/objcopy.py                    |   50 +
 waflib/extras/ocaml.py                      |  348 ++++++
 waflib/extras/package.py                    |   76 ++
 waflib/extras/parallel_debug.py             |  459 ++++++++
 waflib/extras/pch.py                        |  148 +++
 waflib/extras/pep8.py                       |  106 ++
 waflib/extras/pgicc.py                      |   75 ++
 waflib/extras/pgicxx.py                     |   20 +
 waflib/extras/proc.py                       |   54 +
 waflib/extras/protoc.py                     |  243 ++++
 waflib/extras/pyqt5.py                      |  241 ++++
 waflib/extras/pytest.py                     |  225 ++++
 waflib/extras/qnxnto.py                     |   72 ++
 waflib/extras/qt4.py                        |  695 ++++++++++++
 waflib/extras/relocation.py                 |   85 ++
 waflib/extras/remote.py                     |  327 ++++++
 waflib/extras/resx.py                       |   35 +
 waflib/extras/review.py                     |  325 ++++++
 waflib/extras/rst.py                        |  260 +++++
 waflib/extras/run_do_script.py              |  139 +++
 waflib/extras/run_m_script.py               |   88 ++
 waflib/extras/run_py_script.py              |  104 ++
 waflib/extras/run_r_script.py               |   86 ++
 waflib/extras/sas.py                        |   71 ++
 waflib/extras/satellite_assembly.py         |   57 +
 waflib/extras/scala.py                      |  128 +++
 waflib/extras/slow_qt4.py                   |   96 ++
 waflib/extras/softlink_libs.py              |   76 ++
 waflib/extras/stale.py                      |   98 ++
 waflib/extras/stracedeps.py                 |  174 +++
 waflib/extras/swig.py                       |  237 ++++
 waflib/extras/syms.py                       |   84 ++
 waflib/extras/ticgt.py                      |  300 +++++
 waflib/extras/unity.py                      |  108 ++
 waflib/extras/use_config.py                 |  185 +++
 waflib/extras/valadoc.py                    |  140 +++
 waflib/extras/waf_xattr.py                  |  150 +++
 waflib/extras/why.py                        |   78 ++
 waflib/extras/win32_opts.py                 |  170 +++
 waflib/extras/wix.py                        |   87 ++
 waflib/extras/xcode.py                      |  312 -----
 waflib/extras/xcode6.py                     |  467 ++++----
 waflib/fixpy2.py                            |   34 +-
 waflib/processor.py                         |   64 ++
 wscript                                     |    3 -
 177 files changed, 28077 insertions(+), 4455 deletions(-)
 create mode 100644 waflib/Tools/asm.py
 create mode 100644 waflib/Tools/bison.py
 create mode 100644 waflib/Tools/compiler_d.py
 create mode 100644 waflib/Tools/compiler_fc.py
 create mode 100644 waflib/Tools/cs.py
 create mode 100644 waflib/Tools/d.py
 create mode 100644 waflib/Tools/d_config.py
 create mode 100644 waflib/Tools/d_scan.py
 create mode 100644 waflib/Tools/dbus.py
 create mode 100644 waflib/Tools/dmd.py
 create mode 100644 waflib/Tools/fc.py
 create mode 100644 waflib/Tools/fc_config.py
 create mode 100644 waflib/Tools/fc_scan.py
 create mode 100644 waflib/Tools/flex.py
 create mode 100644 waflib/Tools/g95.py
 create mode 100644 waflib/Tools/gas.py
 create mode 100644 waflib/Tools/gdc.py
 create mode 100644 waflib/Tools/gfortran.py
 create mode 100644 waflib/Tools/glib2.py
 create mode 100644 waflib/Tools/gnu_dirs.py
 create mode 100644 waflib/Tools/ifort.py
 create mode 100644 waflib/Tools/intltool.py
 create mode 100644 waflib/Tools/javaw.py
 create mode 100644 waflib/Tools/ldc2.py
 create mode 100644 waflib/Tools/lua.py
 create mode 100644 waflib/Tools/md5_tstamp.py
 create mode 100644 waflib/Tools/nasm.py
 create mode 100644 waflib/Tools/nobuild.py
 create mode 100644 waflib/Tools/perl.py
 create mode 100644 waflib/Tools/python.py
 create mode 100644 waflib/Tools/qt5.py
 create mode 100644 waflib/Tools/ruby.py
 create mode 100644 waflib/Tools/tex.py
 create mode 100644 waflib/Tools/vala.py
 create mode 100644 waflib/Tools/winres.py
 create mode 100644 waflib/extras/biber.py
 create mode 100644 waflib/extras/bjam.py
 create mode 100644 waflib/extras/blender.py
 create mode 100644 waflib/extras/boo.py
 create mode 100644 waflib/extras/boost.py
 create mode 100644 waflib/extras/buildcopy.py
 create mode 100644 waflib/extras/c_dumbpreproc.py
 create mode 100644 waflib/extras/c_emscripten.py
 create mode 100644 waflib/extras/cabal.py
 create mode 100644 waflib/extras/cfg_altoptions.py
 create mode 100644 waflib/extras/clang_compilation_database.py
 create mode 100644 waflib/extras/codelite.py
 create mode 100644 waflib/extras/color_gcc.py
 create mode 100644 waflib/extras/color_rvct.py
 create mode 100644 waflib/extras/compat15.py
 create mode 100644 waflib/extras/cppcheck.py
 create mode 100644 waflib/extras/cpplint.py
 create mode 100644 waflib/extras/cross_gnu.py
 create mode 100644 waflib/extras/cython.py
 create mode 100644 waflib/extras/dcc.py
 create mode 100644 waflib/extras/distnet.py
 create mode 100644 waflib/extras/doxygen.py
 create mode 100644 waflib/extras/dpapi.py
 create mode 100644 waflib/extras/eclipse.py
 create mode 100644 waflib/extras/erlang.py
 create mode 100644 waflib/extras/fast_partial.py
 create mode 100644 waflib/extras/fc_bgxlf.py
 create mode 100644 waflib/extras/fc_cray.py
 create mode 100644 waflib/extras/fc_nag.py
 create mode 100644 waflib/extras/fc_nec.py
 create mode 100644 waflib/extras/fc_open64.py
 create mode 100644 waflib/extras/fc_pgfortran.py
 create mode 100644 waflib/extras/fc_solstudio.py
 create mode 100644 waflib/extras/fc_xlf.py
 create mode 100644 waflib/extras/file_to_object.py
 create mode 100644 waflib/extras/fluid.py
 create mode 100644 waflib/extras/freeimage.py
 create mode 100644 waflib/extras/fsb.py
 create mode 100644 waflib/extras/fsc.py
 create mode 100644 waflib/extras/gccdeps.py
 create mode 100644 waflib/extras/gdbus.py
 create mode 100644 waflib/extras/gob2.py
 create mode 100644 waflib/extras/halide.py
 create mode 100755 waflib/extras/javatest.py
 create mode 100644 waflib/extras/kde4.py
 create mode 100644 waflib/extras/local_rpath.py
 create mode 100644 waflib/extras/make.py
 create mode 100644 waflib/extras/midl.py
 create mode 100644 waflib/extras/msvcdeps.py
 create mode 100644 waflib/extras/msvs.py
 create mode 100644 waflib/extras/netcache_client.py
 create mode 100644 waflib/extras/objcopy.py
 create mode 100644 waflib/extras/ocaml.py
 create mode 100644 waflib/extras/package.py
 create mode 100644 waflib/extras/parallel_debug.py
 create mode 100644 waflib/extras/pch.py
 create mode 100644 waflib/extras/pep8.py
 create mode 100644 waflib/extras/pgicc.py
 create mode 100644 waflib/extras/pgicxx.py
 create mode 100644 waflib/extras/proc.py
 create mode 100644 waflib/extras/protoc.py
 create mode 100644 waflib/extras/pyqt5.py
 create mode 100644 waflib/extras/pytest.py
 create mode 100644 waflib/extras/qnxnto.py
 create mode 100644 waflib/extras/qt4.py
 create mode 100644 waflib/extras/relocation.py
 create mode 100644 waflib/extras/remote.py
 create mode 100644 waflib/extras/resx.py
 create mode 100644 waflib/extras/review.py
 create mode 100644 waflib/extras/rst.py
 create mode 100644 waflib/extras/run_do_script.py
 create mode 100644 waflib/extras/run_m_script.py
 create mode 100644 waflib/extras/run_py_script.py
 create mode 100644 waflib/extras/run_r_script.py
 create mode 100644 waflib/extras/sas.py
 create mode 100644 waflib/extras/satellite_assembly.py
 create mode 100644 waflib/extras/scala.py
 create mode 100644 waflib/extras/slow_qt4.py
 create mode 100644 waflib/extras/softlink_libs.py
 create mode 100644 waflib/extras/stale.py
 create mode 100644 waflib/extras/stracedeps.py
 create mode 100644 waflib/extras/swig.py
 create mode 100644 waflib/extras/syms.py
 create mode 100644 waflib/extras/ticgt.py
 create mode 100644 waflib/extras/unity.py
 create mode 100644 waflib/extras/use_config.py
 create mode 100644 waflib/extras/valadoc.py
 create mode 100644 waflib/extras/waf_xattr.py
 create mode 100644 waflib/extras/why.py
 create mode 100644 waflib/extras/win32_opts.py
 create mode 100644 waflib/extras/wix.py
 delete mode 100644 waflib/extras/xcode.py
 create mode 100755 waflib/processor.py

diff --git a/waf b/waf
index 108d533c..3670c2ed 100755
--- a/waf
+++ b/waf
@@ -1,7 +1,7 @@
 #!/usr/bin/env python
-# encoding: ISO8859-1
-# Thomas Nagy, 2005-2015
-
+# encoding: latin-1
+# Thomas Nagy, 2005-2018
+#
 """
 Redistribution and use in source and binary forms, with or without
 modification, are permitted provided that the following conditions
@@ -32,18 +32,16 @@ POSSIBILITY OF SUCH DAMAGE.
 
 import os, sys, inspect
 
-VERSION="1.8.17"
-REVISION="x"
-GIT="x"
-INSTALL="x"
-C1='x'
-C2='x'
-C3='x'
+VERSION="2.0.11"
+REVISION="4af5f6d4e441ea5db7e393d841ce352c"
+GIT="fe03c6fa470f0bc3c086baafed8c6b4b6d28a245"
+INSTALL=''
+C1='#4'
+C2='#/'
+C3='#&'
 cwd = os.getcwd()
 join = os.path.join
 
-if sys.hexversion<0x206000f:
-	raise ImportError('Python >= 2.6 is required to create the waf file')
 
 WAF='waf'
 def b(x):
@@ -162,7 +160,6 @@ wafdir = find_lib()
 sys.path.insert(0, wafdir)
 
 if __name__ == '__main__':
-	#import waflib.extras.compat15#PRELUDE
+
 	from waflib import Scripting
 	Scripting.waf_entry_point(cwd, VERSION, wafdir)
-
diff --git a/waflib/Build.py b/waflib/Build.py
index 032e15f3..1afcba64 100644
--- a/waflib/Build.py
+++ b/waflib/Build.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 Classes related to the build phase (build, clean, install, step, etc)
@@ -14,14 +14,13 @@
 	import cPickle
 except ImportError:
 	import pickle as cPickle
-from waflib import Runner, TaskGen, Utils, ConfigSet, Task, Logs, Options, Context, Errors
-import waflib.Node
+from waflib import Node, Runner, TaskGen, Utils, ConfigSet, Task, Logs, Options, Context, Errors
 
 CACHE_DIR = 'c4che'
-"""Location of the cache files"""
+"""Name of the cache directory"""
 
 CACHE_SUFFIX = '_cache.py'
-"""Suffix for the cache files"""
+"""ConfigSet cache files for variants are written under :py:attr:´waflib.Build.CACHE_DIR´ in the form ´variant_name´_cache.py"""
 
 INSTALL = 1337
 """Positive value '->' install, see :py:attr:`waflib.Build.BuildContext.is_install`"""
@@ -29,20 +28,19 @@
 UNINSTALL = -1337
 """Negative value '<-' uninstall, see :py:attr:`waflib.Build.BuildContext.is_install`"""
 
-SAVED_ATTRS = 'root node_deps raw_deps task_sigs'.split()
-"""Build class members to save between the runs (root, node_deps, raw_deps, task_sigs)"""
+SAVED_ATTRS = 'root node_sigs task_sigs imp_sigs raw_deps node_deps'.split()
+"""Build class members to save between the runs; these should be all dicts
+except for `root` which represents a :py:class:`waflib.Node.Node` instance
+"""
 
 CFG_FILES = 'cfg_files'
 """Files from the build directory to hash before starting the build (``config.h`` written during the configuration)"""
 
 POST_AT_ONCE = 0
-"""Post mode: all task generators are posted before the build really starts"""
+"""Post mode: all task generators are posted before any task executed"""
 
 POST_LAZY = 1
-"""Post mode: post the task generators group after group"""
-
-POST_BOTH = 2
-"""Post mode: post the task generators at once, then re-check them for each group"""
+"""Post mode: post the task generators group after group, the tasks in the next group are created when the tasks in the previous groups are done"""
 
 PROTOCOL = -1
 if sys.platform == 'cli':
@@ -61,48 +59,65 @@ def __init__(self, **kw):
 		"""Non-zero value when installing or uninstalling file"""
 
 		self.top_dir = kw.get('top_dir', Context.top_dir)
+		"""See :py:attr:`waflib.Context.top_dir`; prefer :py:attr:`waflib.Build.BuildContext.srcnode`"""
+
+		self.out_dir = kw.get('out_dir', Context.out_dir)
+		"""See :py:attr:`waflib.Context.out_dir`; prefer :py:attr:`waflib.Build.BuildContext.bldnode`"""
 
 		self.run_dir = kw.get('run_dir', Context.run_dir)
+		"""See :py:attr:`waflib.Context.run_dir`"""
 
-		self.post_mode = POST_AT_ONCE
-		"""post the task generators at once, group-by-group, or both"""
+		self.launch_dir = Context.launch_dir
+		"""See :py:attr:`waflib.Context.out_dir`; prefer :py:meth:`waflib.Build.BuildContext.launch_node`"""
 
-		# output directory - may be set until the nodes are considered
-		self.out_dir = kw.get('out_dir', Context.out_dir)
+		self.post_mode = POST_LAZY
+		"""Whether to post the task generators at once or group-by-group (default is group-by-group)"""
 
-		self.cache_dir = kw.get('cache_dir', None)
+		self.cache_dir = kw.get('cache_dir')
 		if not self.cache_dir:
 			self.cache_dir = os.path.join(self.out_dir, CACHE_DIR)
 
-		# map names to environments, the '' must be defined
 		self.all_envs = {}
+		"""Map names to :py:class:`waflib.ConfigSet.ConfigSet`, the empty string must map to the default environment"""
 
 		# ======================================= #
 		# cache variables
 
+		self.node_sigs = {}
+		"""Dict mapping build nodes to task identifier (uid), it indicates whether a task created a particular file (persists across builds)"""
+
 		self.task_sigs = {}
-		"""Signatures of the tasks (persists between build executions)"""
+		"""Dict mapping task identifiers (uid) to task signatures (persists across builds)"""
+
+		self.imp_sigs = {}
+		"""Dict mapping task identifiers (uid) to implicit task dependencies used for scanning targets (persists across builds)"""
 
 		self.node_deps = {}
-		"""Dict of node dependencies found by :py:meth:`waflib.Task.Task.scan` (persists between build executions)"""
+		"""Dict mapping task identifiers (uid) to node dependencies found by :py:meth:`waflib.Task.Task.scan` (persists across builds)"""
 
 		self.raw_deps = {}
-		"""Dict of custom data returned by :py:meth:`waflib.Task.Task.scan` (persists between build executions)"""
-
-		# list of folders that are already scanned
-		# so that we do not need to stat them one more time
-		self.cache_dir_contents = {}
+		"""Dict mapping task identifiers (uid) to custom data returned by :py:meth:`waflib.Task.Task.scan` (persists across builds)"""
 
 		self.task_gen_cache_names = {}
 
-		self.launch_dir = Context.launch_dir
-
 		self.jobs = Options.options.jobs
+		"""Amount of jobs to run in parallel"""
+
 		self.targets = Options.options.targets
+		"""List of targets to build (default: \*)"""
+
 		self.keep = Options.options.keep
+		"""Whether the build should continue past errors"""
+
 		self.progress_bar = Options.options.progress_bar
+		"""
+		Level of progress status:
 
-		############ stuff below has not been reviewed
+		0. normal output
+		1. progress bar
+		2. IDE output
+		3. No output at all
+		"""
 
 		# Manual dependencies.
 		self.deps_man = Utils.defaultdict(list)
@@ -118,16 +133,21 @@ def __init__(self, **kw):
 		"""
 		List containing lists of task generators
 		"""
+
 		self.group_names = {}
 		"""
 		Map group names to the group lists. See :py:meth:`waflib.Build.BuildContext.add_group`
 		"""
 
+		for v in SAVED_ATTRS:
+			if not hasattr(self, v):
+				setattr(self, v, {})
+
 	def get_variant_dir(self):
 		"""Getter for the variant_dir attribute"""
 		if not self.variant:
 			return self.out_dir
-		return os.path.join(self.out_dir, self.variant)
+		return os.path.join(self.out_dir, os.path.normpath(self.variant))
 	variant_dir = property(get_variant_dir, None)
 
 	def __call__(self, *k, **kw):
@@ -152,56 +172,22 @@ def build(bld):
 		kw['bld'] = self
 		ret = TaskGen.task_gen(*k, **kw)
 		self.task_gen_cache_names = {} # reset the cache, each time
-		self.add_to_group(ret, group=kw.get('group', None))
+		self.add_to_group(ret, group=kw.get('group'))
 		return ret
 
-	def rule(self, *k, **kw):
+	def __copy__(self):
 		"""
-		Wrapper for creating a task generator using the decorator notation. The following code::
-
-			@bld.rule(
-				target = "foo"
-			)
-			def _(tsk):
-				print("bar")
-
-		is equivalent to::
+		Build contexts cannot be copied
 
-			def bar(tsk):
-				print("bar")
-
-			bld(
-				target = "foo",
-				rule = bar,
-			)
+		:raises: :py:class:`waflib.Errors.WafError`
 		"""
-		def f(rule):
-			ret = self(*k, **kw)
-			ret.rule = rule
-			return ret
-		return f
-
-	def __copy__(self):
-		"""Implemented to prevents copies of build contexts (raises an exception)"""
-		raise Errors.WafError('build contexts are not supposed to be copied')
-
-	def install_files(self, *k, **kw):
-		"""Actual implementation provided by :py:meth:`waflib.Build.InstallContext.install_files`"""
-		pass
-
-	def install_as(self, *k, **kw):
-		"""Actual implementation provided by :py:meth:`waflib.Build.InstallContext.install_as`"""
-		pass
-
-	def symlink_as(self, *k, **kw):
-		"""Actual implementation provided by :py:meth:`waflib.Build.InstallContext.symlink_as`"""
-		pass
+		raise Errors.WafError('build contexts cannot be copied')
 
 	def load_envs(self):
 		"""
 		The configuration command creates files of the form ``build/c4che/NAMEcache.py``. This method
 		creates a :py:class:`waflib.ConfigSet.ConfigSet` instance for each ``NAME`` by reading those
-		files. The config sets are then stored in the dict :py:attr:`waflib.Build.BuildContext.allenvs`.
+		files and stores them in :py:attr:`waflib.Build.BuildContext.allenvs`.
 		"""
 		node = self.root.find_node(self.cache_dir)
 		if not node:
@@ -217,21 +203,16 @@ def load_envs(self):
 			self.all_envs[name] = env
 			for f in env[CFG_FILES]:
 				newnode = self.root.find_resource(f)
-				try:
-					h = Utils.h_file(newnode.abspath())
-				except (IOError, AttributeError):
-					Logs.error('cannot find %r' % f)
-					h = Utils.SIG_NIL
-				newnode.sig = h
+				if not newnode or not newnode.exists():
+					raise Errors.WafError('Missing configuration file %r, reconfigure the project!' % f)
 
 	def init_dirs(self):
 		"""
 		Initialize the project directory and the build directory by creating the nodes
 		:py:attr:`waflib.Build.BuildContext.srcnode` and :py:attr:`waflib.Build.BuildContext.bldnode`
-		corresponding to ``top_dir`` and ``variant_dir`` respectively. The ``bldnode`` directory will be
-		created if it does not exist.
+		corresponding to ``top_dir`` and ``variant_dir`` respectively. The ``bldnode`` directory is
+		created if necessary.
 		"""
-
 		if not (os.path.isabs(self.top_dir) and os.path.isabs(self.out_dir)):
 			raise Errors.WafError('The project was not configured: run "waf configure" first!')
 
@@ -241,12 +222,12 @@ def init_dirs(self):
 
 	def execute(self):
 		"""
-		Restore the data from previous builds and call :py:meth:`waflib.Build.BuildContext.execute_build`. Overrides from :py:func:`waflib.Context.Context.execute`
+		Restore data from previous builds and call :py:meth:`waflib.Build.BuildContext.execute_build`.
+		Overrides from :py:func:`waflib.Context.Context.execute`
 		"""
 		self.restore()
 		if not self.all_envs:
 			self.load_envs()
-
 		self.execute_build()
 
 	def execute_build(self):
@@ -259,7 +240,7 @@ def execute_build(self):
 		* calling :py:meth:`waflib.Build.BuildContext.post_build` to call user build functions
 		"""
 
-		Logs.info("Waf: Entering directory `%s'" % self.variant_dir)
+		Logs.info("Waf: Entering directory `%s'", self.variant_dir)
 		self.recurse([self.run_dir])
 		self.pre_build()
 
@@ -270,65 +251,70 @@ def execute_build(self):
 			self.compile()
 		finally:
 			if self.progress_bar == 1 and sys.stderr.isatty():
-				c = len(self.returned_tasks) or 1
+				c = self.producer.processed or 1
 				m = self.progress_line(c, c, Logs.colors.BLUE, Logs.colors.NORMAL)
 				Logs.info(m, extra={'stream': sys.stderr, 'c1': Logs.colors.cursor_off, 'c2' : Logs.colors.cursor_on})
-			Logs.info("Waf: Leaving directory `%s'" % self.variant_dir)
+			Logs.info("Waf: Leaving directory `%s'", self.variant_dir)
+		try:
+			self.producer.bld = None
+			del self.producer
+		except AttributeError:
+			pass
 		self.post_build()
 
 	def restore(self):
 		"""
-		Load the data from a previous run, sets the attributes listed in :py:const:`waflib.Build.SAVED_ATTRS`
+		Load data from a previous run, sets the attributes listed in :py:const:`waflib.Build.SAVED_ATTRS`
 		"""
 		try:
 			env = ConfigSet.ConfigSet(os.path.join(self.cache_dir, 'build.config.py'))
 		except EnvironmentError:
 			pass
 		else:
-			if env['version'] < Context.HEXVERSION:
-				raise Errors.WafError('Version mismatch! reconfigure the project')
-			for t in env['tools']:
+			if env.version < Context.HEXVERSION:
+				raise Errors.WafError('Project was configured with a different version of Waf, please reconfigure it')
+
+			for t in env.tools:
 				self.setup(**t)
 
 		dbfn = os.path.join(self.variant_dir, Context.DBFILE)
 		try:
 			data = Utils.readf(dbfn, 'rb')
-		except (IOError, EOFError):
+		except (EnvironmentError, EOFError):
 			# handle missing file/empty file
-			Logs.debug('build: Could not load the build cache %s (missing)' % dbfn)
+			Logs.debug('build: Could not load the build cache %s (missing)', dbfn)
 		else:
 			try:
-				waflib.Node.pickle_lock.acquire()
-				waflib.Node.Nod3 = self.node_class
+				Node.pickle_lock.acquire()
+				Node.Nod3 = self.node_class
 				try:
 					data = cPickle.loads(data)
 				except Exception as e:
-					Logs.debug('build: Could not pickle the build cache %s: %r' % (dbfn, e))
+					Logs.debug('build: Could not pickle the build cache %s: %r', dbfn, e)
 				else:
 					for x in SAVED_ATTRS:
-						setattr(self, x, data[x])
+						setattr(self, x, data.get(x, {}))
 			finally:
-				waflib.Node.pickle_lock.release()
+				Node.pickle_lock.release()
 
 		self.init_dirs()
 
 	def store(self):
 		"""
-		Store the data for next runs, sets the attributes listed in :py:const:`waflib.Build.SAVED_ATTRS`. Uses a temporary
+		Store data for next runs, set the attributes listed in :py:const:`waflib.Build.SAVED_ATTRS`. Uses a temporary
 		file to avoid problems on ctrl+c.
 		"""
-
 		data = {}
 		for x in SAVED_ATTRS:
 			data[x] = getattr(self, x)
 		db = os.path.join(self.variant_dir, Context.DBFILE)
 
 		try:
-			waflib.Node.pickle_lock.acquire()
-			waflib.Node.Nod3 = self.node_class
+			Node.pickle_lock.acquire()
+			Node.Nod3 = self.node_class
 			x = cPickle.dumps(data, PROTOCOL)
 		finally:
-			waflib.Node.pickle_lock.release()
+			Node.pickle_lock.release()
 
 		Utils.writef(db + '.tmp', x, m='wb')
 
@@ -346,29 +332,34 @@ def store(self):
 	def compile(self):
 		"""
 		Run the build by creating an instance of :py:class:`waflib.Runner.Parallel`
-		The cache file is not written if the build is up to date (no task executed).
+		The cache file is written when at least a task was executed.
+
+		:raises: :py:class:`waflib.Errors.BuildError` in case the build fails
 		"""
 		Logs.debug('build: compile()')
 
-		# use another object to perform the producer-consumer logic (reduce the complexity)
+		# delegate the producer-consumer logic to another object to reduce the complexity
 		self.producer = Runner.Parallel(self, self.jobs)
 		self.producer.biter = self.get_build_iterator()
-		self.returned_tasks = [] # not part of the API yet
 		try:
 			self.producer.start()
 		except KeyboardInterrupt:
-			self.store()
+			if self.is_dirty():
+				self.store()
 			raise
 		else:
-			if self.producer.dirty:
+			if self.is_dirty():
 				self.store()
 
 		if self.producer.error:
 			raise Errors.BuildError(self.producer.error)
 
+	def is_dirty(self):
+		return self.producer.dirty
+
 	def setup(self, tool, tooldir=None, funs=None):
 		"""
-		Import waf tools, used to import those accessed during the configuration::
+		Import waf tools defined during the configuration::
 
 			def configure(conf):
 				conf.load('glib2')
@@ -383,11 +374,13 @@ def build(bld):
 		:param funs: unused variable
 		"""
 		if isinstance(tool, list):
-			for i in tool: self.setup(i, tooldir)
+			for i in tool:
+				self.setup(i, tooldir)
 			return
 
 		module = Context.load_tool(tool, tooldir)
-		if hasattr(module, "setup"): module.setup(self)
+		if hasattr(module, "setup"):
+			module.setup(self)
 
 	def get_env(self):
 		"""Getter for the env property"""
@@ -412,26 +405,28 @@ def build(bld):
 
 		:param path: file path
 		:type path: string or :py:class:`waflib.Node.Node`
-		:param value: value to depend on
-		:type value: :py:class:`waflib.Node.Node`, string, or function returning a string
+		:param value: value to depend
+		:type value: :py:class:`waflib.Node.Node`, byte object, or function returning a byte object
 		"""
-		if path is None:
-			raise ValueError('Invalid input')
+		if not path:
+			raise ValueError('Invalid input path %r' % path)
 
-		if isinstance(path, waflib.Node.Node):
+		if isinstance(path, Node.Node):
 			node = path
 		elif os.path.isabs(path):
 			node = self.root.find_resource(path)
 		else:
 			node = self.path.find_resource(path)
+		if not node:
+			raise ValueError('Could not find the path %r' % path)
 
 		if isinstance(value, list):
-			self.deps_man[id(node)].extend(value)
+			self.deps_man[node].extend(value)
 		else:
-			self.deps_man[id(node)].append(value)
+			self.deps_man[node].append(value)
 
 	def launch_node(self):
-		"""Returns the launch directory as a :py:class:`waflib.Node.Node` object"""
+		"""Returns the launch directory as a :py:class:`waflib.Node.Node` object (cached)"""
 		try:
 			# private cache
 			return self.p_ln
@@ -441,11 +436,13 @@ def launch_node(self):
 
 	def hash_env_vars(self, env, vars_lst):
 		"""
-		Hash configuration set variables::
+		Hashes configuration set variables::
 
 			def build(bld):
 				bld.hash_env_vars(bld.env, ['CXX', 'CC'])
 
+		This method uses an internal cache.
+
 		:param env: Configuration Set
 		:type env: :py:class:`waflib.ConfigSet.ConfigSet`
 		:param vars_lst: list of variables
@@ -469,21 +466,23 @@ def build(bld):
 				pass
 
 		lst = [env[a] for a in vars_lst]
-		ret = Utils.h_list(lst)
+		cache[idx] = ret = Utils.h_list(lst)
 		Logs.debug('envhash: %s %r', Utils.to_hex(ret), lst)
-
-		cache[idx] = ret
-
 		return ret
 
 	def get_tgen_by_name(self, name):
 		"""
-		Retrieves a task generator from its name or its target name
-		the name must be unique::
+		Fetches a task generator by its name or its target attribute;
+		the name must be unique in a build::
 
 			def build(bld):
 				tg = bld(name='foo')
 				tg == bld.get_tgen_by_name('foo')
+
+		This method use a private internal cache.
+
+		:param name: Task generator name
+		:raises: :py:class:`waflib.Errors.WafError` in case there is no task genenerator by that name
 		"""
 		cache = self.task_gen_cache_names
 		if not cache:
@@ -500,9 +499,12 @@ def build(bld):
 		except KeyError:
 			raise Errors.WafError('Could not find a task generator for the name %r' % name)
 
-	def progress_line(self, state, total, col1, col2):
+	def progress_line(self, idx, total, col1, col2):
 		"""
-		Compute the progress bar used by ``waf -p``
+		Computes a progress bar line displayed when running ``waf -p``
+
+		:returns: progress bar line
+		:rtype: string
 		"""
 		if not sys.stderr.isatty():
 			return ''
@@ -512,16 +514,16 @@ def progress_line(self, state, total, col1, col2):
 		Utils.rot_idx += 1
 		ind = Utils.rot_chr[Utils.rot_idx % 4]
 
-		pc = (100.*state)/total
-		eta = str(self.timer)
-		fs = "[%%%dd/%%%dd][%%s%%2d%%%%%%s][%s][" % (n, n, ind)
-		left = fs % (state, total, col1, pc, col2)
-		right = '][%s%s%s]' % (col1, eta, col2)
+		pc = (100. * idx)/total
+		fs = "[%%%dd/%%d][%%s%%2d%%%%%%s][%s][" % (n, ind)
+		left = fs % (idx, total, col1, pc, col2)
+		right = '][%s%s%s]' % (col1, self.timer, col2)
 
 		cols = Logs.get_term_cols() - len(left) - len(right) + 2*len(col1) + 2*len(col2)
-		if cols < 7: cols = 7
+		if cols < 7:
+			cols = 7
 
-		ratio = ((cols*state)//total) - 1
+		ratio = ((cols * idx)//total) - 1
 
 		bar = ('='*ratio+'>').ljust(cols)
 		msg = Logs.indicator % (left, bar, right)
@@ -530,23 +532,23 @@ def progress_line(self, state, total, col1, col2):
 
 	def declare_chain(self, *k, **kw):
 		"""
-		Wrapper for :py:func:`waflib.TaskGen.declare_chain` provided for convenience
+		Wraps :py:func:`waflib.TaskGen.declare_chain` for convenience
 		"""
 		return TaskGen.declare_chain(*k, **kw)
 
 	def pre_build(self):
-		"""Execute user-defined methods before the build starts, see :py:meth:`waflib.Build.BuildContext.add_pre_fun`"""
+		"""Executes user-defined methods before the build starts, see :py:meth:`waflib.Build.BuildContext.add_pre_fun`"""
 		for m in getattr(self, 'pre_funs', []):
 			m(self)
 
 	def post_build(self):
-		"""Executes the user-defined methods after the build is successful, see :py:meth:`waflib.Build.BuildContext.add_post_fun`"""
+		"""Executes user-defined methods after the build is successful, see :py:meth:`waflib.Build.BuildContext.add_post_fun`"""
 		for m in getattr(self, 'post_funs', []):
 			m(self)
 
 	def add_pre_fun(self, meth):
 		"""
-		Bind a method to execute after the scripts are read and before the build starts::
+		Binds a callback method to execute after the scripts are read and before the build starts::
 
 			def mycallback(bld):
 				print("Hello, world!")
@@ -561,7 +563,7 @@ def build(bld):
 
 	def add_post_fun(self, meth):
 		"""
-		Bind a method to execute immediately after the build is successful::
+		Binds a callback method to execute immediately after the build is successful::
 
 			def call_ldconfig(bld):
 				bld.exec_command('/sbin/ldconfig')
@@ -577,7 +579,7 @@ def build(bld):
 
 	def get_group(self, x):
 		"""
-		Get the group x, or return the current group if x is None
+		Returns the build group named `x`, or the current group if `x` is None
 
 		:param x: name or number or None
 		:type x: string, int or None
@@ -591,14 +593,20 @@ def get_group(self, x):
 		return self.groups[x]
 
 	def add_to_group(self, tgen, group=None):
-		"""add a task or a task generator for the build"""
-		# paranoid
-		assert(isinstance(tgen, TaskGen.task_gen) or isinstance(tgen, Task.TaskBase))
+		"""Adds a task or a task generator to the build; there is no attempt to remove it if it was already added."""
+		assert(isinstance(tgen, TaskGen.task_gen) or isinstance(tgen, Task.Task))
 		tgen.bld = self
 		self.get_group(group).append(tgen)
 
 	def get_group_name(self, g):
-		"""name for the group g (utility)"""
+		"""
+		Returns the name of the input build group
+
+		:param g: build group object or build group index
+		:type g: integer or list
+		:return: name
+		:rtype: string
+		"""
 		if not isinstance(g, list):
 			g = self.groups[g]
 		for x in self.group_names:
@@ -608,7 +616,7 @@ def get_group_name(self, g):
 
 	def get_group_idx(self, tg):
 		"""
-		Index of the group containing the task generator given as argument::
+		Returns the index of the group containing the task generator given as argument::
 
 			def build(bld):
 				tg = bld(name='nada')
@@ -616,27 +624,28 @@ def build(bld):
 
 		:param tg: Task generator object
 		:type tg: :py:class:`waflib.TaskGen.task_gen`
+		:rtype: int
 		"""
 		se = id(tg)
-		for i in range(len(self.groups)):
-			for t in self.groups[i]:
+		for i, tmp in enumerate(self.groups):
+			for t in tmp:
 				if id(t) == se:
 					return i
 		return None
 
 	def add_group(self, name=None, move=True):
 		"""
-		Add a new group of tasks/task generators. By default the new group becomes the default group for new task generators.
+		Adds a new group of tasks/task generators. By default the new group becomes
+		the default group for new task generators (make sure to create build groups in order).
 
 		:param name: name for this group
 		:type name: string
-		:param move: set the group created as default group (True by default)
+		:param move: set this new group as default group (True by default)
 		:type move: bool
+		:raises: :py:class:`waflib.Errors.WafError` if a group by the name given already exists
 		"""
-		#if self.groups and not self.groups[0].tasks:
-		#	error('add_group: an empty group is already present')
 		if name and name in self.group_names:
-			Logs.error('add_group: name %s already present' % name)
+			raise Errors.WafError('add_group: name %s already present', name)
 		g = []
 		self.group_names[name] = g
 		self.groups.append(g)
@@ -645,7 +654,8 @@ def add_group(self, name=None, move=True):
 
 	def set_group(self, idx):
 		"""
-		Set the current group to be idx: now new task generators will be added to this group by default::
+		Sets the build group at position idx as current so that newly added
+		task generators are added to this one by default::
 
 			def build(bld):
 				bld(rule='touch ${TGT}', target='foo.txt')
@@ -659,8 +669,8 @@ def build(bld):
 		"""
 		if isinstance(idx, str):
 			g = self.group_names[idx]
-			for i in range(len(self.groups)):
-				if id(g) == id(self.groups[i]):
+			for i, tmp in enumerate(self.groups):
+				if id(g) == id(tmp):
 					self.current_group = i
 					break
 		else:
@@ -668,8 +678,11 @@ def build(bld):
 
 	def total(self):
 		"""
-		Approximate task count: this value may be inaccurate if task generators are posted lazily (see :py:attr:`waflib.Build.BuildContext.post_mode`).
+		Approximate task count: this value may be inaccurate if task generators
+		are posted lazily (see :py:attr:`waflib.Build.BuildContext.post_mode`).
 		The value :py:attr:`waflib.Runner.Parallel.total` is updated during the task execution.
+
+		:rtype: int
 		"""
 		total = 0
 		for group in self.groups:
@@ -682,9 +695,16 @@ def total(self):
 
 	def get_targets(self):
 		"""
-		Return the task generator corresponding to the 'targets' list, used by :py:meth:`waflib.Build.BuildContext.get_build_iterator`::
+		This method returns a pair containing the index of the last build group to post,
+		and the list of task generator objects corresponding to the target names.
+
+		This is used internally by :py:meth:`waflib.Build.BuildContext.get_build_iterator`
+		to perform partial builds::
 
 			$ waf --targets=myprogram,myshlib
+
+		:return: the minimum build group index, and list of task generators
+		:rtype: tuple
 		"""
 		to_post = []
 		min_grp = 0
@@ -700,7 +720,7 @@ def get_targets(self):
 
 	def get_all_task_gen(self):
 		"""
-		Utility method, returns a list of all task generators - if you need something more complicated, implement your own
+		Returns a list of all task generators for troubleshooting purposes.
 		"""
 		lst = []
 		for g in self.groups:
@@ -709,25 +729,24 @@ def get_all_task_gen(self):
 
 	def post_group(self):
 		"""
-		Post the task generators from the group indexed by self.cur, used by :py:meth:`waflib.Build.BuildContext.get_build_iterator`
+		Post task generators from the group indexed by self.current_group; used internally
+		by :py:meth:`waflib.Build.BuildContext.get_build_iterator`
 		"""
+		def tgpost(tg):
+			try:
+				f = tg.post
+			except AttributeError:
+				pass
+			else:
+				f()
+
 		if self.targets == '*':
-			for tg in self.groups[self.cur]:
-				try:
-					f = tg.post
-				except AttributeError:
-					pass
-				else:
-					f()
+			for tg in self.groups[self.current_group]:
+				tgpost(tg)
 		elif self.targets:
-			if self.cur < self._min_grp:
-				for tg in self.groups[self.cur]:
-					try:
-						f = tg.post
-					except AttributeError:
-						pass
-					else:
-						f()
+			if self.current_group < self._min_grp:
+				for tg in self.groups[self.current_group]:
+					tgpost(tg)
 			else:
 				for tg in self._exact_tg:
 					tg.post()
@@ -737,55 +756,71 @@ def post_group(self):
 				Logs.warn('Building from the build directory, forcing --targets=*')
 				ln = self.srcnode
 			elif not ln.is_child_of(self.srcnode):
-				Logs.warn('CWD %s is not under %s, forcing --targets=* (run distclean?)' % (ln.abspath(), self.srcnode.abspath()))
+				Logs.warn('CWD %s is not under %s, forcing --targets=* (run distclean?)', ln.abspath(), self.srcnode.abspath())
 				ln = self.srcnode
-			for tg in self.groups[self.cur]:
+
+			def is_post(tg, ln):
 				try:
-					f = tg.post
+					p = tg.path
 				except AttributeError:
 					pass
 				else:
-					if tg.path.is_child_of(ln):
-						f()
+					if p.is_child_of(ln):
+						return True
+
+			def is_post_group():
+				for i, g in enumerate(self.groups):
+					if i > self.current_group:
+						for tg in g:
+							if is_post(tg, ln):
+								return True
+
+			if self.post_mode == POST_LAZY and ln != self.srcnode:
+				# partial folder builds require all targets from a previous build group
+				if is_post_group():
+					ln = self.srcnode
+
+			for tg in self.groups[self.current_group]:
+				if is_post(tg, ln):
+					tgpost(tg)
 
 	def get_tasks_group(self, idx):
 		"""
-		Return all the tasks for the group of num idx, used by :py:meth:`waflib.Build.BuildContext.get_build_iterator`
+		Returns all task instances for the build group at position idx,
+		used internally by :py:meth:`waflib.Build.BuildContext.get_build_iterator`
+
+		:rtype: list of :py:class:`waflib.Task.Task`
 		"""
 		tasks = []
 		for tg in self.groups[idx]:
 			try:
 				tasks.extend(tg.tasks)
-			except AttributeError: # not a task generator, can be the case for installation tasks
+			except AttributeError: # not a task generator
 				tasks.append(tg)
 		return tasks
 
 	def get_build_iterator(self):
 		"""
-		Creates a generator object that returns lists of tasks executable in parallel (yield)
+		Creates a Python generator object that returns lists of tasks that may be processed in parallel.
 
-		:return: tasks which can be executed immediatly
-		:rtype: list of :py:class:`waflib.Task.TaskBase`
+		:return: tasks which can be executed immediately
+		:rtype: generator returning lists of :py:class:`waflib.Task.Task`
 		"""
-		self.cur = 0
-
 		if self.targets and self.targets != '*':
 			(self._min_grp, self._exact_tg) = self.get_targets()
 
-		global lazy_post
 		if self.post_mode != POST_LAZY:
-			while self.cur < len(self.groups):
+			for self.current_group, _ in enumerate(self.groups):
 				self.post_group()
-				self.cur += 1
-			self.cur = 0
 
-		while self.cur < len(self.groups):
+		for self.current_group, _ in enumerate(self.groups):
 			# first post the task generators for the group
 			if self.post_mode != POST_AT_ONCE:
 				self.post_group()
 
 			# then extract the tasks
-			tasks = self.get_tasks_group(self.cur)
+			tasks = self.get_tasks_group(self.current_group)
+
 			# if the constraints are set properly (ext_in/ext_out, before/after)
 			# the call to set_file_constraints may be removed (can be a 15% penalty on no-op rebuilds)
 			# (but leave set_file_constraints for the installation step)
@@ -796,141 +831,320 @@ def get_build_iterator(self):
 			Task.set_precedence_constraints(tasks)
 
 			self.cur_tasks = tasks
-			self.cur += 1
-			if not tasks: # return something else the build will stop
-				continue
-			yield tasks
+			if tasks:
+				yield tasks
+
 		while 1:
+			# the build stops once there are no tasks to process
 			yield []
 
-class inst(Task.Task):
+	def install_files(self, dest, files, **kw):
+		"""
+		Creates a task generator to install files on the system::
+
+			def build(bld):
+				bld.install_files('${DATADIR}', self.path.find_resource('wscript'))
+
+		:param dest: path representing the destination directory
+		:type dest: :py:class:`waflib.Node.Node` or string (absolute path)
+		:param files: input files
+		:type files: list of strings or list of :py:class:`waflib.Node.Node`
+		:param env: configuration set to expand *dest*
+		:type env: :py:class:`waflib.ConfigSet.ConfigSet`
+		:param relative_trick: preserve the folder hierarchy when installing whole folders
+		:type relative_trick: bool
+		:param cwd: parent node for searching srcfile, when srcfile is not an instance of :py:class:`waflib.Node.Node`
+		:type cwd: :py:class:`waflib.Node.Node`
+		:param postpone: execute the task immediately to perform the installation (False by default)
+		:type postpone: bool
+		"""
+		assert(dest)
+		tg = self(features='install_task', install_to=dest, install_from=files, **kw)
+		tg.dest = tg.install_to
+		tg.type = 'install_files'
+		if not kw.get('postpone', True):
+			tg.post()
+		return tg
+
+	def install_as(self, dest, srcfile, **kw):
+		"""
+		Creates a task generator to install a file on the system with a different name::
+
+			def build(bld):
+				bld.install_as('${PREFIX}/bin', 'myapp', chmod=Utils.O755)
+
+		:param dest: destination file
+		:type dest: :py:class:`waflib.Node.Node` or string (absolute path)
+		:param srcfile: input file
+		:type srcfile: string or :py:class:`waflib.Node.Node`
+		:param cwd: parent node for searching srcfile, when srcfile is not an instance of :py:class:`waflib.Node.Node`
+		:type cwd: :py:class:`waflib.Node.Node`
+		:param env: configuration set for performing substitutions in dest
+		:type env: :py:class:`waflib.ConfigSet.ConfigSet`
+		:param postpone: execute the task immediately to perform the installation (False by default)
+		:type postpone: bool
+		"""
+		assert(dest)
+		tg = self(features='install_task', install_to=dest, install_from=srcfile, **kw)
+		tg.dest = tg.install_to
+		tg.type = 'install_as'
+		if not kw.get('postpone', True):
+			tg.post()
+		return tg
+
+	def symlink_as(self, dest, src, **kw):
+		"""
+		Creates a task generator to install a symlink::
+
+			def build(bld):
+				bld.symlink_as('${PREFIX}/lib/libfoo.so', 'libfoo.so.1.2.3')
+
+		:param dest: absolute path of the symlink
+		:type dest: :py:class:`waflib.Node.Node` or string (absolute path)
+		:param src: link contents, which is a relative or absolute path which may exist or not
+		:type src: string
+		:param env: configuration set for performing substitutions in dest
+		:type env: :py:class:`waflib.ConfigSet.ConfigSet`
+		:param add: add the task created to a build group - set ``False`` only if the installation task is created after the build has started
+		:type add: bool
+		:param postpone: execute the task immediately to perform the installation
+		:type postpone: bool
+		:param relative_trick: make the symlink relative (default: ``False``)
+		:type relative_trick: bool
+		"""
+		assert(dest)
+		tg = self(features='install_task', install_to=dest, install_from=src, **kw)
+		tg.dest = tg.install_to
+		tg.type = 'symlink_as'
+		tg.link = src
+		# TODO if add: self.add_to_group(tsk)
+		if not kw.get('postpone', True):
+			tg.post()
+		return tg
+
+@TaskGen.feature('install_task')
+@TaskGen.before_method('process_rule', 'process_source')
+def process_install_task(self):
+	"""Creates the installation task for the current task generator; uses :py:func:`waflib.Build.add_install_task` internally."""
+	self.add_install_task(**self.__dict__)
+
+@TaskGen.taskgen_method
+def add_install_task(self, **kw):
+	"""
+	Creates the installation task for the current task generator, and executes it immediately if necessary
+
+	:returns: An installation task
+	:rtype: :py:class:`waflib.Build.inst`
+	"""
+	if not self.bld.is_install:
+		return
+	if not kw['install_to']:
+		return
+
+	if kw['type'] == 'symlink_as' and Utils.is_win32:
+		if kw.get('win32_install'):
+			kw['type'] = 'install_as'
+		else:
+			# just exit
+			return
+
+	tsk = self.install_task = self.create_task('inst')
+	tsk.chmod = kw.get('chmod', Utils.O644)
+	tsk.link = kw.get('link', '') or kw.get('install_from', '')
+	tsk.relative_trick = kw.get('relative_trick', False)
+	tsk.type = kw['type']
+	tsk.install_to = tsk.dest = kw['install_to']
+	tsk.install_from = kw['install_from']
+	tsk.relative_base = kw.get('cwd') or kw.get('relative_base', self.path)
+	tsk.install_user = kw.get('install_user')
+	tsk.install_group = kw.get('install_group')
+	tsk.init_files()
+	if not kw.get('postpone', True):
+		tsk.run_now()
+	return tsk
+
+@TaskGen.taskgen_method
+def add_install_files(self, **kw):
+	"""
+	Creates an installation task for files
+
+	:returns: An installation task
+	:rtype: :py:class:`waflib.Build.inst`
+	"""
+	kw['type'] = 'install_files'
+	return self.add_install_task(**kw)
+
+@TaskGen.taskgen_method
+def add_install_as(self, **kw):
+	"""
+	Creates an installation task for a single file
+
+	:returns: An installation task
+	:rtype: :py:class:`waflib.Build.inst`
+	"""
+	kw['type'] = 'install_as'
+	return self.add_install_task(**kw)
+
+@TaskGen.taskgen_method
+def add_symlink_as(self, **kw):
 	"""
-	Special task used for installing files and symlinks, it behaves both like a task
-	and like a task generator
+	Creates an installation task for a symbolic link
+
+	:returns: An installation task
+	:rtype: :py:class:`waflib.Build.inst`
 	"""
-	color = 'CYAN'
+	kw['type'] = 'symlink_as'
+	return self.add_install_task(**kw)
+
+class inst(Task.Task):
+	"""Task that installs files or symlinks; it is typically executed by :py:class:`waflib.Build.InstallContext` and :py:class:`waflib.Build.UnInstallContext`"""
+	def __str__(self):
+		"""Returns an empty string to disable the standard task display"""
+		return ''
 
 	def uid(self):
-		lst = [self.dest, self.path] + self.source
-		return Utils.h_list(repr(lst))
+		"""Returns a unique identifier for the task"""
+		lst = self.inputs + self.outputs + [self.link, self.generator.path.abspath()]
+		return Utils.h_list(lst)
 
-	def post(self):
+	def init_files(self):
 		"""
-		Same interface as in :py:meth:`waflib.TaskGen.task_gen.post`
+		Initializes the task input and output nodes
 		"""
-		buf = []
-		for x in self.source:
-			if isinstance(x, waflib.Node.Node):
-				y = x
-			else:
-				y = self.path.find_resource(x)
-				if not y:
-					if os.path.isabs(x):
-						y = self.bld.root.make_node(x)
-					else:
-						y = self.path.make_node(x)
-			buf.append(y)
-		self.inputs = buf
+		if self.type == 'symlink_as':
+			inputs = []
+		else:
+			inputs = self.generator.to_nodes(self.install_from)
+			if self.type == 'install_as':
+				assert len(inputs) == 1
+		self.set_inputs(inputs)
+
+		dest = self.get_install_path()
+		outputs = []
+		if self.type == 'symlink_as':
+			if self.relative_trick:
+				self.link = os.path.relpath(self.link, os.path.dirname(dest))
+			outputs.append(self.generator.bld.root.make_node(dest))
+		elif self.type == 'install_as':
+			outputs.append(self.generator.bld.root.make_node(dest))
+		else:
+			for y in inputs:
+				if self.relative_trick:
+					destfile = os.path.join(dest, y.path_from(self.relative_base))
+				else:
+					destfile = os.path.join(dest, y.name)
+				outputs.append(self.generator.bld.root.make_node(destfile))
+		self.set_outputs(outputs)
 
 	def runnable_status(self):
 		"""
 		Installation tasks are always executed, so this method returns either :py:const:`waflib.Task.ASK_LATER` or :py:const:`waflib.Task.RUN_ME`.
 		"""
 		ret = super(inst, self).runnable_status()
-		if ret == Task.SKIP_ME:
+		if ret == Task.SKIP_ME and self.generator.bld.is_install:
 			return Task.RUN_ME
 		return ret
 
-	def __str__(self):
-		"""Return an empty string to disable the display"""
-		return ''
-
-	def run(self):
-		"""The attribute 'exec_task' holds the method to execute"""
-		return self.generator.exec_task()
+	def post_run(self):
+		"""
+		Disables any post-run operations
+		"""
+		pass
 
 	def get_install_path(self, destdir=True):
 		"""
-		Installation path obtained from ``self.dest`` and prefixed by the destdir.
-		The variables such as '${PREFIX}/bin' are substituted.
+		Returns the destination path where files will be installed, pre-pending `destdir`.
+
+		:rtype: string
 		"""
-		dest = Utils.subst_vars(self.dest, self.env)
-		dest = dest.replace('/', os.sep)
+		if isinstance(self.install_to, Node.Node):
+			dest = self.install_to.abspath()
+		else:
+			dest = Utils.subst_vars(self.install_to, self.env)
 		if destdir and Options.options.destdir:
 			dest = os.path.join(Options.options.destdir, os.path.splitdrive(dest)[1].lstrip(os.sep))
 		return dest
 
-	def exec_install_files(self):
+	def copy_fun(self, src, tgt):
 		"""
-		Predefined method for installing files
+		Copies a file from src to tgt, preserving permissions and trying to work
+		around path limitations on Windows platforms. On Unix-like platforms,
+		the owner/group of the target file may be set through install_user/install_group
+
+		:param src: absolute path
+		:type src: string
+		:param tgt: absolute path
+		:type tgt: string
 		"""
-		destpath = self.get_install_path()
-		if not destpath:
-			raise Errors.WafError('unknown installation path %r' % self.generator)
-		for x, y in zip(self.source, self.inputs):
-			if self.relative_trick:
-				destfile = os.path.join(destpath, y.path_from(self.path))
-			else:
-				destfile = os.path.join(destpath, y.name)
-			self.generator.bld.do_install(y.abspath(), destfile, chmod=self.chmod, tsk=self)
+		# override this if you want to strip executables
+		# kw['tsk'].source is the task that created the files in the build
+		if Utils.is_win32 and len(tgt) > 259 and not tgt.startswith('\\\\?\\'):
+			tgt = '\\\\?\\' + tgt
+		shutil.copy2(src, tgt)
+		self.fix_perms(tgt)
 
-	def exec_install_as(self):
+	def rm_empty_dirs(self, tgt):
 		"""
-		Predefined method for installing one file with a given name
+		Removes empty folders recursively when uninstalling.
+
+		:param tgt: absolute path
+		:type tgt: string
 		"""
-		destfile = self.get_install_path()
-		self.generator.bld.do_install(self.inputs[0].abspath(), destfile, chmod=self.chmod, tsk=self)
+		while tgt:
+			tgt = os.path.dirname(tgt)
+			try:
+				os.rmdir(tgt)
+			except OSError:
+				break
 
-	def exec_symlink_as(self):
+	def run(self):
 		"""
-		Predefined method for installing a symlink
+		Performs file or symlink installation
 		"""
-		destfile = self.get_install_path()
-		src = self.link
-		if self.relative_trick:
-			src = os.path.relpath(src, os.path.dirname(destfile))
-		self.generator.bld.do_link(src, destfile, tsk=self)
-
-class InstallContext(BuildContext):
-	'''installs the targets on the system'''
-	cmd = 'install'
-
-	def __init__(self, **kw):
-		super(InstallContext, self).__init__(**kw)
+		is_install = self.generator.bld.is_install
+		if not is_install: # unnecessary?
+			return
 
-		# list of targets to uninstall for removing the empty folders after uninstalling
-		self.uninstall = []
-		self.is_install = INSTALL
+		for x in self.outputs:
+			if is_install == INSTALL:
+				x.parent.mkdir()
+		if self.type == 'symlink_as':
+			fun = is_install == INSTALL and self.do_link or self.do_unlink
+			fun(self.link, self.outputs[0].abspath())
+		else:
+			fun = is_install == INSTALL and self.do_install or self.do_uninstall
+			launch_node = self.generator.bld.launch_node()
+			for x, y in zip(self.inputs, self.outputs):
+				fun(x.abspath(), y.abspath(), x.path_from(launch_node))
 
-	def copy_fun(self, src, tgt, **kw):
-		# override this if you want to strip executables
-		# kw['tsk'].source is the task that created the files in the build
-		if Utils.is_win32 and len(tgt) > 259 and not tgt.startswith('\\\\?\\'):
-			tgt = '\\\\?\\' + tgt
-		shutil.copy2(src, tgt)
-		os.chmod(tgt, kw.get('chmod', Utils.O644))
+	def run_now(self):
+		"""
+		Try executing the installation task right now
 
-	def do_install(self, src, tgt, **kw):
+		:raises: :py:class:`waflib.Errors.TaskNotReady`
 		"""
-		Copy a file from src to tgt with given file permissions. The actual copy is not performed
-		if the source and target file have the same size and the same timestamps. When the copy occurs,
-		the file is first removed and then copied (prevent stale inodes).
+		status = self.runnable_status()
+		if status not in (Task.RUN_ME, Task.SKIP_ME):
+			raise Errors.TaskNotReady('Could not process %r: status %r' % (self, status))
+		self.run()
+		self.hasrun = Task.SUCCESS
 
-		This method is overridden in :py:meth:`waflib.Build.UninstallContext.do_install` to remove the file.
+	def do_install(self, src, tgt, lbl, **kw):
+		"""
+		Copies a file from src to tgt with given file permissions. The actual copy is only performed
+		if the source and target file sizes or timestamps differ. When the copy occurs,
+		the file is always first removed and then copied so as to prevent stale inodes.
 
 		:param src: file name as absolute path
 		:type src: string
 		:param tgt: file destination, as absolute path
 		:type tgt: string
+		:param lbl: file source description
+		:type lbl: string
 		:param chmod: installation mode
 		:type chmod: int
+		:raises: :py:class:`waflib.Errors.WafError` if the file cannot be written
 		"""
-		d, _ = os.path.split(tgt)
-		if not d:
-			raise Errors.WafError('Invalid installation given %r->%r' % (src, tgt))
-		Utils.check_dir(d)
-
-		srclbl = src.replace(self.srcnode.abspath() + os.sep, '')
 		if not Options.options.force:
 			# check if the file is already there to avoid a copy
 			try:
@@ -941,12 +1155,12 @@ def do_install(self, src, tgt, **kw):
 			else:
 				# same size and identical timestamps -> make no copy
 				if st1.st_mtime + 2 >= st2.st_mtime and st1.st_size == st2.st_size:
-					if not self.progress_bar:
-						Logs.info('- install %s (from %s)' % (tgt, srclbl))
+					if not self.generator.bld.progress_bar:
+						Logs.info('- install %s (from %s)', tgt, lbl)
 					return False
 
-		if not self.progress_bar:
-			Logs.info('+ install %s (from %s)' % (tgt, srclbl))
+		if not self.generator.bld.progress_bar:
+			Logs.info('+ install %s (from %s)', tgt, lbl)
 
 		# Give best attempt at making destination overwritable,
 		# like the 'install' utility used by 'make install' does.
@@ -962,190 +1176,66 @@ def do_install(self, src, tgt, **kw):
 			pass
 
 		try:
-			self.copy_fun(src, tgt, **kw)
-		except IOError:
-			try:
-				os.stat(src)
-			except EnvironmentError:
-				Logs.error('File %r does not exist' % src)
-			raise Errors.WafError('Could not install the file %r' % tgt)
+			self.copy_fun(src, tgt)
+		except EnvironmentError as e:
+			if not os.path.exists(src):
+				Logs.error('File %r does not exist', src)
+			elif not os.path.isfile(src):
+				Logs.error('Input %r is not a file', src)
+			raise Errors.WafError('Could not install the file %r' % tgt, e)
 
-	def do_link(self, src, tgt, **kw):
+	def fix_perms(self, tgt):
 		"""
-		Create a symlink from tgt to src.
+		Change the ownership of the file/folder/link pointed by the given path
+		This looks up for `install_user` or `install_group` attributes
+		on the task or on the task generator::
+
+			def build(bld):
+				bld.install_as('${PREFIX}/wscript',
+					'wscript',
+					install_user='nobody', install_group='nogroup')
+				bld.symlink_as('${PREFIX}/wscript_link',
+					Utils.subst_vars('${PREFIX}/wscript', bld.env),
+					install_user='nobody', install_group='nogroup')
+		"""
+		if not Utils.is_win32:
+			user = getattr(self, 'install_user', None) or getattr(self.generator, 'install_user', None)
+			group = getattr(self, 'install_group', None) or getattr(self.generator, 'install_group', None)
+			if user or group:
+				Utils.lchown(tgt, user or -1, group or -1)
+		if not os.path.islink(tgt):
+			os.chmod(tgt, self.chmod)
 
-		This method is overridden in :py:meth:`waflib.Build.UninstallContext.do_link` to remove the symlink.
+	def do_link(self, src, tgt, **kw):
+		"""
+		Creates a symlink from tgt to src.
 
 		:param src: file name as absolute path
 		:type src: string
 		:param tgt: file destination, as absolute path
 		:type tgt: string
 		"""
-		d, _ = os.path.split(tgt)
-		Utils.check_dir(d)
-
-		link = False
-		if not os.path.islink(tgt):
-			link = True
-		elif os.readlink(tgt) != src:
-			link = True
-
-		if link:
-			try: os.remove(tgt)
-			except OSError: pass
-			if not self.progress_bar:
-				Logs.info('+ symlink %s (to %s)' % (tgt, src))
-			os.symlink(src, tgt)
-		else:
-			if not self.progress_bar:
-				Logs.info('- symlink %s (to %s)' % (tgt, src))
-
-	def run_task_now(self, tsk, postpone):
-		"""
-		This method is called by :py:meth:`waflib.Build.InstallContext.install_files`,
-		:py:meth:`waflib.Build.InstallContext.install_as` and :py:meth:`waflib.Build.InstallContext.symlink_as` immediately
-		after the installation task is created. Its role is to force the immediate execution if necessary, that is when
-		``postpone=False`` was given.
-		"""
-		tsk.post()
-		if not postpone:
-			if tsk.runnable_status() == Task.ASK_LATER:
-				raise self.WafError('cannot post the task %r' % tsk)
-			tsk.run()
-			tsk.hasrun = True
-
-	def install_files(self, dest, files, env=None, chmod=Utils.O644, relative_trick=False, cwd=None, add=True, postpone=True, task=None):
-		"""
-		Create a task to install files on the system::
-
-			def build(bld):
-				bld.install_files('${DATADIR}', self.path.find_resource('wscript'))
-
-		:param dest: absolute path of the destination directory
-		:type dest: string
-		:param files: input files
-		:type files: list of strings or list of nodes
-		:param env: configuration set for performing substitutions in dest
-		:type env: Configuration set
-		:param relative_trick: preserve the folder hierarchy when installing whole folders
-		:type relative_trick: bool
-		:param cwd: parent node for searching srcfile, when srcfile is not a :py:class:`waflib.Node.Node`
-		:type cwd: :py:class:`waflib.Node.Node`
-		:param add: add the task created to a build group - set ``False`` only if the installation task is created after the build has started
-		:type add: bool
-		:param postpone: execute the task immediately to perform the installation
-		:type postpone: bool
-		"""
-		assert(dest)
-		tsk = inst(env=env or self.env)
-		tsk.bld = self
-		tsk.path = cwd or self.path
-		tsk.chmod = chmod
-		tsk.task = task
-		if isinstance(files, waflib.Node.Node):
-			tsk.source =  [files]
+		if os.path.islink(tgt) and os.readlink(tgt) == src:
+			if not self.generator.bld.progress_bar:
+				Logs.info('- symlink %s (to %s)', tgt, src)
 		else:
-			tsk.source = Utils.to_list(files)
-		tsk.dest = dest
-		tsk.exec_task = tsk.exec_install_files
-		tsk.relative_trick = relative_trick
-		if add: self.add_to_group(tsk)
-		self.run_task_now(tsk, postpone)
-		return tsk
-
-	def install_as(self, dest, srcfile, env=None, chmod=Utils.O644, cwd=None, add=True, postpone=True, task=None):
-		"""
-		Create a task to install a file on the system with a different name::
-
-			def build(bld):
-				bld.install_as('${PREFIX}/bin', 'myapp', chmod=Utils.O755)
-
-		:param dest: absolute path of the destination file
-		:type dest: string
-		:param srcfile: input file
-		:type srcfile: string or node
-		:param cwd: parent node for searching srcfile, when srcfile is not a :py:class:`waflib.Node.Node`
-		:type cwd: :py:class:`waflib.Node.Node`
-		:param env: configuration set for performing substitutions in dest
-		:type env: Configuration set
-		:param add: add the task created to a build group - set ``False`` only if the installation task is created after the build has started
-		:type add: bool
-		:param postpone: execute the task immediately to perform the installation
-		:type postpone: bool
-		"""
-		assert(dest)
-		tsk = inst(env=env or self.env)
-		tsk.bld = self
-		tsk.path = cwd or self.path
-		tsk.chmod = chmod
-		tsk.source = [srcfile]
-		tsk.task = task
-		tsk.dest = dest
-		tsk.exec_task = tsk.exec_install_as
-		if add: self.add_to_group(tsk)
-		self.run_task_now(tsk, postpone)
-		return tsk
-
-	def symlink_as(self, dest, src, env=None, cwd=None, add=True, postpone=True, relative_trick=False, task=None):
-		"""
-		Create a task to install a symlink::
-
-			def build(bld):
-				bld.symlink_as('${PREFIX}/lib/libfoo.so', 'libfoo.so.1.2.3')
-
-		:param dest: absolute path of the symlink
-		:type dest: string
-		:param src: absolute or relative path of the link
-		:type src: string
-		:param env: configuration set for performing substitutions in dest
-		:type env: Configuration set
-		:param add: add the task created to a build group - set ``False`` only if the installation task is created after the build has started
-		:type add: bool
-		:param postpone: execute the task immediately to perform the installation
-		:type postpone: bool
-		:param relative_trick: make the symlink relative (default: ``False``)
-		:type relative_trick: bool
-		"""
-		if Utils.is_win32:
-			# symlinks *cannot* work on that platform
-			# TODO waf 1.9 - replace by install_as
-			return
-		assert(dest)
-		tsk = inst(env=env or self.env)
-		tsk.bld = self
-		tsk.dest = dest
-		tsk.path = cwd or self.path
-		tsk.source = []
-		tsk.task = task
-		tsk.link = src
-		tsk.relative_trick = relative_trick
-		tsk.exec_task = tsk.exec_symlink_as
-		if add: self.add_to_group(tsk)
-		self.run_task_now(tsk, postpone)
-		return tsk
-
-class UninstallContext(InstallContext):
-	'''removes the targets installed'''
-	cmd = 'uninstall'
-
-	def __init__(self, **kw):
-		super(UninstallContext, self).__init__(**kw)
-		self.is_install = UNINSTALL
-
-	def rm_empty_dirs(self, tgt):
-		while tgt:
-			tgt = os.path.dirname(tgt)
 			try:
-				os.rmdir(tgt)
+				os.remove(tgt)
 			except OSError:
-				break
+				pass
+			if not self.generator.bld.progress_bar:
+				Logs.info('+ symlink %s (to %s)', tgt, src)
+			os.symlink(src, tgt)
+			self.fix_perms(tgt)
 
-	def do_install(self, src, tgt, **kw):
-		"""See :py:meth:`waflib.Build.InstallContext.do_install`"""
-		if not self.progress_bar:
-			Logs.info('- remove %s' % tgt)
+	def do_uninstall(self, src, tgt, lbl, **kw):
+		"""
+		See :py:meth:`waflib.Build.inst.do_install`
+		"""
+		if not self.generator.bld.progress_bar:
+			Logs.info('- remove %s', tgt)
 
-		self.uninstall.append(tgt)
+		#self.uninstall.append(tgt)
 		try:
 			os.remove(tgt)
 		except OSError as e:
@@ -1154,42 +1244,43 @@ def do_install(self, src, tgt, **kw):
 					self.uninstall_error = True
 					Logs.warn('build: some files could not be uninstalled (retry with -vv to list them)')
 				if Logs.verbose > 1:
-					Logs.warn('Could not remove %s (error code %r)' % (e.filename, e.errno))
-
+					Logs.warn('Could not remove %s (error code %r)', e.filename, e.errno)
 		self.rm_empty_dirs(tgt)
 
-	def do_link(self, src, tgt, **kw):
-		"""See :py:meth:`waflib.Build.InstallContext.do_link`"""
+	def do_unlink(self, src, tgt, **kw):
+		"""
+		See :py:meth:`waflib.Build.inst.do_link`
+		"""
 		try:
-			if not self.progress_bar:
-				Logs.info('- remove %s' % tgt)
+			if not self.generator.bld.progress_bar:
+				Logs.info('- remove %s', tgt)
 			os.remove(tgt)
 		except OSError:
 			pass
-
 		self.rm_empty_dirs(tgt)
 
-	def execute(self):
-		"""
-		See :py:func:`waflib.Context.Context.execute`
-		"""
-		try:
-			# do not execute any tasks
-			def runnable_status(self):
-				return Task.SKIP_ME
-			setattr(Task.Task, 'runnable_status_back', Task.Task.runnable_status)
-			setattr(Task.Task, 'runnable_status', runnable_status)
+class InstallContext(BuildContext):
+	'''installs the targets on the system'''
+	cmd = 'install'
 
-			super(UninstallContext, self).execute()
-		finally:
-			setattr(Task.Task, 'runnable_status', Task.Task.runnable_status_back)
+	def __init__(self, **kw):
+		super(InstallContext, self).__init__(**kw)
+		self.is_install = INSTALL
+
+class UninstallContext(InstallContext):
+	'''removes the targets installed'''
+	cmd = 'uninstall'
+
+	def __init__(self, **kw):
+		super(UninstallContext, self).__init__(**kw)
+		self.is_install = UNINSTALL
 
 class CleanContext(BuildContext):
 	'''cleans the project'''
 	cmd = 'clean'
 	def execute(self):
 		"""
-		See :py:func:`waflib.Context.Context.execute`
+		See :py:func:`waflib.Build.BuildContext.execute`.
 		"""
 		self.restore()
 		if not self.all_envs:
@@ -1202,30 +1293,49 @@ def execute(self):
 			self.store()
 
 	def clean(self):
-		"""Remove files from the build directory if possible, and reset the caches"""
+		"""
+		Remove most files from the build directory, and reset all caches.
+
+		Custom lists of files to clean can be declared as `bld.clean_files`.
+		For example, exclude `build/program/myprogram` from getting removed::
+
+			def build(bld):
+				bld.clean_files = bld.bldnode.ant_glob('**',
+					excl='.lock* config.log c4che/* config.h program/myprogram',
+					quiet=True, generator=True)
+		"""
 		Logs.debug('build: clean called')
 
-		if self.bldnode != self.srcnode:
+		if hasattr(self, 'clean_files'):
+			for n in self.clean_files:
+				n.delete()
+		elif self.bldnode != self.srcnode:
 			# would lead to a disaster if top == out
-			lst=[]
-			for e in self.all_envs.values():
-				lst.extend(self.root.find_or_declare(f) for f in e[CFG_FILES])
+			lst = []
+			for env in self.all_envs.values():
+				lst.extend(self.root.find_or_declare(f) for f in env[CFG_FILES])
 			for n in self.bldnode.ant_glob('**/*', excl='.lock* *conf_check_*/** config.log c4che/*', quiet=True):
 				if n in lst:
 					continue
 				n.delete()
 		self.root.children = {}
 
-		for v in 'node_deps task_sigs raw_deps'.split():
+		for v in SAVED_ATTRS:
+			if v == 'root':
+				continue
 			setattr(self, v, {})
 
 class ListContext(BuildContext):
 	'''lists the targets to execute'''
-
 	cmd = 'list'
+
 	def execute(self):
 		"""
-		See :py:func:`waflib.Context.Context.execute`.
+		In addition to printing the name of each build target,
+		a description column will include text for each task
+		generator which has a "description" field set.
+
+		See :py:func:`waflib.Build.BuildContext.execute`.
 		"""
 		self.restore()
 		if not self.all_envs:
@@ -1249,12 +1359,25 @@ def execute(self):
 		try:
 			# force the cache initialization
 			self.get_tgen_by_name('')
-		except Exception:
+		except Errors.WafError:
 			pass
-		lst = list(self.task_gen_cache_names.keys())
-		lst.sort()
-		for k in lst:
-			Logs.pprint('GREEN', k)
+
+		targets = sorted(self.task_gen_cache_names)
+
+		# figure out how much to left-justify, for largest target name
+		line_just = max(len(t) for t in targets) if targets else 0
+
+		for target in targets:
+			tgen = self.task_gen_cache_names[target]
+
+			# Support displaying the description for the target
+			# if it was set on the tgen
+			descript = getattr(tgen, 'description', '')
+			if descript:
+				target = target.ljust(line_just)
+				descript = ': %s' % descript
+
+			Logs.pprint('GREEN', target, label=descript)
 
 class StepContext(BuildContext):
 	'''executes tasks in a step-by-step fashion, for debugging'''
@@ -1266,7 +1389,8 @@ def __init__(self, **kw):
 
 	def compile(self):
 		"""
-		Compile the tasks matching the input/output files given (regular expression matching). Derived from :py:meth:`waflib.Build.BuildContext.compile`::
+		Overrides :py:meth:`waflib.Build.BuildContext.compile` to perform a partial build
+		on tasks matching the input/output pattern given (regular expression matching)::
 
 			$ waf step --files=foo.c,bar.c,in:truc.c,out:bar.o
 			$ waf step --files=in:foo.cpp.1.o # link task only
@@ -1277,7 +1401,7 @@ def compile(self):
 			BuildContext.compile(self)
 			return
 
-		targets = None
+		targets = []
 		if self.targets and self.targets != '*':
 			targets = self.targets.split(',')
 
@@ -1296,25 +1420,32 @@ def compile(self):
 			for pat in self.files.split(','):
 				matcher = self.get_matcher(pat)
 				for tg in g:
-					if isinstance(tg, Task.TaskBase):
+					if isinstance(tg, Task.Task):
 						lst = [tg]
 					else:
 						lst = tg.tasks
 					for tsk in lst:
 						do_exec = False
-						for node in getattr(tsk, 'inputs', []):
+						for node in tsk.inputs:
 							if matcher(node, output=False):
 								do_exec = True
 								break
-						for node in getattr(tsk, 'outputs', []):
+						for node in tsk.outputs:
 							if matcher(node, output=True):
 								do_exec = True
 								break
 						if do_exec:
 							ret = tsk.run()
-							Logs.info('%s -> exit %r' % (str(tsk), ret))
+							Logs.info('%s -> exit %r', tsk, ret)
 
 	def get_matcher(self, pat):
+		"""
+		Converts a step pattern into a function
+
+		:param: pat: pattern of the form in:truc.c,out:bar.o
+		:returns: Python function that uses Node objects as inputs and returns matches
+		:rtype: function
+		"""
 		# this returns a function
 		inn = True
 		out = True
@@ -1335,9 +1466,9 @@ def get_matcher(self, pat):
 			pattern = re.compile(pat)
 
 		def match(node, output):
-			if output == True and not out:
+			if output and not out:
 				return False
-			if output == False and not inn:
+			if not output and not inn:
 				return False
 
 			if anode:
@@ -1346,3 +1477,15 @@ def match(node, output):
 				return pattern.match(node.abspath())
 		return match
 
+class EnvContext(BuildContext):
+	"""Subclass EnvContext to create commands that require configuration data in 'env'"""
+	fun = cmd = None
+	def execute(self):
+		"""
+		See :py:func:`waflib.Build.BuildContext.execute`.
+		"""
+		self.restore()
+		if not self.all_envs:
+			self.load_envs()
+		self.recurse([self.run_dir])
+
diff --git a/waflib/ConfigSet.py b/waflib/ConfigSet.py
index 2fac2166..b300bb56 100644
--- a/waflib/ConfigSet.py
+++ b/waflib/ConfigSet.py
@@ -1,12 +1,12 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 
 ConfigSet: a special dict
 
-The values put in :py:class:`ConfigSet` must be lists
+The values put in :py:class:`ConfigSet` must be serializable (dicts, lists, strings)
 """
 
 import copy, re, os
@@ -15,7 +15,7 @@
 
 class ConfigSet(object):
 	"""
-	A dict that honor serialization and parent relationships. The serialization format
+	A copy-on-write dict with human-readable serialized format. The serialization format
 	is human-readable (python-like) and performed by using eval() and repr().
 	For high performance prefer pickle. Do not store functions as they are not serializable.
 
@@ -39,17 +39,20 @@ def __init__(self, filename=None):
 
 	def __contains__(self, key):
 		"""
-		Enable the *in* syntax::
+		Enables the *in* syntax::
 
 			if 'foo' in env:
 				print(env['foo'])
 		"""
-		if key in self.table: return True
-		try: return self.parent.__contains__(key)
-		except AttributeError: return False # parent may not exist
+		if key in self.table:
+			return True
+		try:
+			return self.parent.__contains__(key)
+		except AttributeError:
+			return False # parent may not exist
 
 	def keys(self):
-		"""Dict interface (unknown purpose)"""
+		"""Dict interface"""
 		keys = set()
 		cur = self
 		while cur:
@@ -59,6 +62,9 @@ def keys(self):
 		keys.sort()
 		return keys
 
+	def __iter__(self):
+		return iter(self.keys())
+
 	def __str__(self):
 		"""Text representation of the ConfigSet (for debugging purposes)"""
 		return "\n".join(["%r %r" % (x, self.__getitem__(x)) for x in self.keys()])
@@ -73,7 +79,7 @@ def configure(conf):
 		"""
 		try:
 			while 1:
-				x = self.table.get(key, None)
+				x = self.table.get(key)
 				if not x is None:
 					return x
 				self = self.parent
@@ -82,13 +88,13 @@ def configure(conf):
 
 	def __setitem__(self, key, value):
 		"""
-		Dictionary interface: get value from key
+		Dictionary interface: set value from key
 		"""
 		self.table[key] = value
 
 	def __delitem__(self, key):
 		"""
-		Dictionary interface: get value from key
+		Dictionary interface: mark the value as missing
 		"""
 		self[key] = []
 
@@ -101,7 +107,7 @@ def configure(conf):
 				conf.env['value']
 		"""
 		if name in self.__slots__:
-			return object.__getattr__(self, name)
+			return object.__getattribute__(self, name)
 		else:
 			return self[name]
 
@@ -152,7 +158,7 @@ def derive(self):
 
 	def detach(self):
 		"""
-		Detach self from its parent (if existing)
+		Detaches this instance from its parent (if present)
 
 		Modifying the parent :py:class:`ConfigSet` will not change the current object
 		Modifying this :py:class:`ConfigSet` will not modify the parent one.
@@ -171,18 +177,19 @@ def detach(self):
 
 	def get_flat(self, key):
 		"""
-		Return a value as a string. If the input is a list, the value returned is space-separated.
+		Returns a value as a string. If the input is a list, the value returned is space-separated.
 
 		:param key: key to use
 		:type key: string
 		"""
 		s = self[key]
-		if isinstance(s, str): return s
+		if isinstance(s, str):
+			return s
 		return ' '.join(s)
 
 	def _get_list_value_for_modification(self, key):
 		"""
-		Return a list value for further modification.
+		Returns a list value for further modification.
 
 		The list may be modified inplace and there is no need to do this afterwards::
 
@@ -191,16 +198,20 @@ def _get_list_value_for_modification(self, key):
 		try:
 			value = self.table[key]
 		except KeyError:
-			try: value = self.parent[key]
-			except AttributeError: value = []
-			if isinstance(value, list):
-				value = value[:]
+			try:
+				value = self.parent[key]
+			except AttributeError:
+				value = []
 			else:
-				value = [value]
+				if isinstance(value, list):
+					# force a copy
+					value = value[:]
+				else:
+					value = [value]
+			self.table[key] = value
 		else:
 			if not isinstance(value, list):
-				value = [value]
-		self.table[key] = value
+				self.table[key] = value = [value]
 		return value
 
 	def append_value(self, var, val):
@@ -232,7 +243,7 @@ def configure(conf):
 
 	def append_unique(self, var, val):
 		"""
-		Append a value to the specified item only if it's not already present::
+		Appends a value to the specified item only if it's not already present::
 
 			def build(bld):
 				bld.env.append_unique('CFLAGS', ['-O2', '-g'])
@@ -249,7 +260,7 @@ def build(bld):
 
 	def get_merged_dict(self):
 		"""
-		Compute the merged dictionary from the fusion of self and all its parent
+		Computes the merged dictionary from the fusion of self and all its parent
 
 		:rtype: a ConfigSet object
 		"""
@@ -257,8 +268,10 @@ def get_merged_dict(self):
 		env = self
 		while 1:
 			table_list.insert(0, env.table)
-			try: env = env.parent
-			except AttributeError: break
+			try:
+				env = env.parent
+			except AttributeError:
+				break
 		merged_table = {}
 		for table in table_list:
 			merged_table.update(table)
@@ -266,7 +279,7 @@ def get_merged_dict(self):
 
 	def store(self, filename):
 		"""
-		Write the :py:class:`ConfigSet` data into a file. See :py:meth:`ConfigSet.load` for reading such files.
+		Serializes the :py:class:`ConfigSet` data to a file. See :py:meth:`ConfigSet.load` for reading such files.
 
 		:param filename: file to use
 		:type filename: string
@@ -293,7 +306,7 @@ def store(self, filename):
 
 	def load(self, filename):
 		"""
-		Retrieve the :py:class:`ConfigSet` data from a file. See :py:meth:`ConfigSet.store` for writing such files
+		Restores contents from a file (current values are not cleared). Files are written using :py:meth:`ConfigSet.store`.
 
 		:param filename: file to use
 		:type filename: string
@@ -303,21 +316,20 @@ def load(self, filename):
 		for m in re_imp.finditer(code):
 			g = m.group
 			tbl[g(2)] = eval(g(3))
-		Logs.debug('env: %s' % str(self.table))
+		Logs.debug('env: %s', self.table)
 
 	def update(self, d):
 		"""
-		Dictionary interface: replace values from another dict
+		Dictionary interface: replace values with the ones from another dict
 
 		:param d: object to use the value from
 		:type d: dict-like object
 		"""
-		for k, v in d.items():
-			self[k] = v
+		self.table.update(d)
 
 	def stash(self):
 		"""
-		Store the object state, to provide a kind of transaction support::
+		Stores the object state to provide transactionality semantics::
 
 			env = ConfigSet()
 			env.stash()
@@ -335,6 +347,12 @@ def stash(self):
 			tbl[x] = copy.deepcopy(tbl[x])
 		self.undo_stack = self.undo_stack + [orig]
 
+	def commit(self):
+		"""
+		Commits transactional changes. See :py:meth:`ConfigSet.stash`
+		"""
+		self.undo_stack.pop(-1)
+
 	def revert(self):
 		"""
 		Reverts the object to a previous state. See :py:meth:`ConfigSet.stash`
diff --git a/waflib/Configure.py b/waflib/Configure.py
index 247173c8..d0a4793a 100644
--- a/waflib/Configure.py
+++ b/waflib/Configure.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 Configuration system
@@ -12,15 +12,9 @@
 * hold configuration routines such as ``find_program``, etc
 """
 
-import os, shlex, sys, time, re, shutil
+import os, re, shlex, shutil, sys, time, traceback
 from waflib import ConfigSet, Utils, Options, Logs, Context, Build, Errors
 
-BREAK    = 'break'
-"""In case of a configuration error, break"""
-
-CONTINUE = 'continue'
-"""In case of a configuration error, continue"""
-
 WAF_CONFIG_LOG = 'config.log'
 """Name of the configuration log file"""
 
@@ -157,7 +151,7 @@ def execute(self):
 		self.msg('Setting out to', self.bldnode.abspath())
 
 		if id(self.srcnode) == id(self.bldnode):
-			Logs.warn('Setting top == out (remember to use "update_outputs")')
+			Logs.warn('Setting top == out')
 		elif id(self.path) != id(self.srcnode):
 			if self.srcnode.is_child_of(self.path):
 				Logs.warn('Are you certain that you do not want to set top="." ?')
@@ -173,8 +167,9 @@ def execute(self):
 		# consider the current path as the root directory (see prepare_impl).
 		# to remove: use 'waf distclean'
 		env = ConfigSet.ConfigSet()
-		env['argv'] = sys.argv
-		env['options'] = Options.options.__dict__
+		env.argv = sys.argv
+		env.options = Options.options.__dict__
+		env.config_cmd = self.cmd
 
 		env.run_dir = Context.run_dir
 		env.top_dir = Context.top_dir
@@ -182,15 +177,15 @@ def execute(self):
 
 		# conf.hash & conf.files hold wscript files paths and hash
 		# (used only by Configure.autoconfig)
-		env['hash'] = self.hash
-		env['files'] = self.files
-		env['environ'] = dict(self.environ)
+		env.hash = self.hash
+		env.files = self.files
+		env.environ = dict(self.environ)
 
-		if not self.env.NO_LOCK_IN_RUN and not getattr(Options.options, 'no_lock_in_run'):
+		if not (self.env.NO_LOCK_IN_RUN or env.environ.get('NO_LOCK_IN_RUN') or getattr(Options.options, 'no_lock_in_run')):
 			env.store(os.path.join(Context.run_dir, Options.lockfile))
-		if not self.env.NO_LOCK_IN_TOP and not getattr(Options.options, 'no_lock_in_top'):
+		if not (self.env.NO_LOCK_IN_TOP or env.environ.get('NO_LOCK_IN_TOP') or getattr(Options.options, 'no_lock_in_top')):
 			env.store(os.path.join(Context.top_dir, Options.lockfile))
-		if not self.env.NO_LOCK_IN_OUT and not getattr(Options.options, 'no_lock_in_out'):
+		if not (self.env.NO_LOCK_IN_OUT or env.environ.get('NO_LOCK_IN_OUT') or getattr(Options.options, 'no_lock_in_out')):
 			env.store(os.path.join(Context.out_dir, Options.lockfile))
 
 	def prepare_env(self, env):
@@ -202,17 +197,17 @@ def prepare_env(self, env):
 		"""
 		if not env.PREFIX:
 			if Options.options.prefix or Utils.is_win32:
-				env.PREFIX = Utils.sane_path(Options.options.prefix)
+				env.PREFIX = Options.options.prefix
 			else:
-				env.PREFIX = ''
+				env.PREFIX = '/'
 		if not env.BINDIR:
 			if Options.options.bindir:
-				env.BINDIR = Utils.sane_path(Options.options.bindir)
+				env.BINDIR = Options.options.bindir
 			else:
 				env.BINDIR = Utils.subst_vars('${PREFIX}/bin', env)
 		if not env.LIBDIR:
 			if Options.options.libdir:
-				env.LIBDIR = Utils.sane_path(Options.options.libdir)
+				env.LIBDIR = Options.options.libdir
 			else:
 				env.LIBDIR = Utils.subst_vars('${PREFIX}/lib%s' % Utils.lib64(), env)
 
@@ -228,38 +223,42 @@ def store(self):
 			tmpenv = self.all_envs[key]
 			tmpenv.store(os.path.join(self.cachedir.abspath(), key + Build.CACHE_SUFFIX))
 
-	def load(self, input, tooldir=None, funs=None, with_sys_path=True):
+	def load(self, tool_list, tooldir=None, funs=None, with_sys_path=True, cache=False):
 		"""
 		Load Waf tools, which will be imported whenever a build is started.
 
-		:param input: waf tools to import
-		:type input: list of string
+		:param tool_list: waf tools to import
+		:type tool_list: list of string
 		:param tooldir: paths for the imports
 		:type tooldir: list of string
 		:param funs: functions to execute from the waf tools
 		:type funs: list of string
+		:param cache: whether to prevent the tool from running twice
+		:type cache: bool
 		"""
 
-		tools = Utils.to_list(input)
-		if tooldir: tooldir = Utils.to_list(tooldir)
+		tools = Utils.to_list(tool_list)
+		if tooldir:
+			tooldir = Utils.to_list(tooldir)
 		for tool in tools:
 			# avoid loading the same tool more than once with the same functions
 			# used by composite projects
 
-			mag = (tool, id(self.env), tooldir, funs)
-			if mag in self.tool_cache:
-				self.to_log('(tool %s is already loaded, skipping)' % tool)
-				continue
-			self.tool_cache.append(mag)
+			if cache:
+				mag = (tool, id(self.env), tooldir, funs)
+				if mag in self.tool_cache:
+					self.to_log('(tool %s is already loaded, skipping)' % tool)
+					continue
+				self.tool_cache.append(mag)
 
 			module = None
 			try:
 				module = Context.load_tool(tool, tooldir, ctx=self, with_sys_path=with_sys_path)
 			except ImportError as e:
-				self.fatal('Could not load the Waf tool %r from %r\n%s' % (tool, sys.path, e))
+				self.fatal('Could not load the Waf tool %r from %r\n%s' % (tool, getattr(e, 'waf_sys_path', sys.path), e))
 			except Exception as e:
 				self.to_log('imp %r (%r & %r)' % (tool, tooldir, funs))
-				self.to_log(Utils.ex_stack())
+				self.to_log(traceback.format_exc())
 				raise
 
 			if funs is not None:
@@ -267,8 +266,10 @@ def load(self, input, tooldir=None, funs=None, with_sys_path=True):
 			else:
 				func = getattr(module, 'configure', None)
 				if func:
-					if type(func) is type(Utils.readf): func(self)
-					else: self.eval_rules(func)
+					if type(func) is type(Utils.readf):
+						func(self)
+					else:
+						self.eval_rules(func)
 
 			self.tools.append({'tool':tool, 'tooldir':tooldir, 'funs':funs})
 
@@ -285,8 +286,7 @@ def post_recurse(self, node):
 
 	def eval_rules(self, rules):
 		"""
-		Execute the configuration tests. The method :py:meth:`waflib.Configure.ConfigurationContext.err_handler`
-		is used to process the eventual exceptions
+		Execute configuration tests provided as list of functions to run
 
 		:param rules: list of configuration method names
 		:type rules: list of string
@@ -294,28 +294,9 @@ def eval_rules(self, rules):
 		self.rules = Utils.to_list(rules)
 		for x in self.rules:
 			f = getattr(self, x)
-			if not f: self.fatal("No such method '%s'." % x)
-			try:
-				f()
-			except Exception as e:
-				ret = self.err_handler(x, e)
-				if ret == BREAK:
-					break
-				elif ret == CONTINUE:
-					continue
-				else:
-					raise
-
-	def err_handler(self, fun, error):
-		"""
-		Error handler for the configuration tests, the default is to let the exception raise
-
-		:param fun: configuration test
-		:type fun: method
-		:param error: exception
-		:type error: exception
-		"""
-		pass
+			if not f:
+				self.fatal('No such configuration function %r' % x)
+			f()
 
 def conf(f):
 	"""
@@ -330,11 +311,7 @@ def configure(conf):
 	:type f: function
 	"""
 	def fun(*k, **kw):
-		mandatory = True
-		if 'mandatory' in kw:
-			mandatory = kw['mandatory']
-			del kw['mandatory']
-
+		mandatory = kw.pop('mandatory', True)
 		try:
 			return f(*k, **kw)
 		except Errors.ConfigurationError:
@@ -347,7 +324,7 @@ def fun(*k, **kw):
 	return f
 
 @conf
-def add_os_flags(self, var, dest=None, dup=True):
+def add_os_flags(self, var, dest=None, dup=False):
 	"""
 	Import operating system environment values into ``conf.env`` dict::
 
@@ -365,7 +342,6 @@ def configure(conf):
 		flags = shlex.split(self.environ[var])
 	except KeyError:
 		return
-	# TODO: in waf 1.9, make dup=False the default
 	if dup or ''.join(flags) not in ''.join(Utils.to_list(self.env[dest or var])):
 		self.env.append_value(dest or var, flags)
 
@@ -377,21 +353,26 @@ def cmd_to_list(self, cmd):
 	:param cmd: command
 	:type cmd: a string or a list of string
 	"""
-	if isinstance(cmd, str) and cmd.find(' '):
-		try:
-			os.stat(cmd)
-		except OSError:
+	if isinstance(cmd, str):
+		if os.path.isfile(cmd):
+			# do not take any risk
+			return [cmd]
+		if os.sep == '/':
 			return shlex.split(cmd)
 		else:
-			return [cmd]
+			try:
+				return shlex.split(cmd, posix=False)
+			except TypeError:
+				# Python 2.5 on windows?
+				return shlex.split(cmd)
 	return cmd
 
 @conf
-def check_waf_version(self, mini='1.7.99', maxi='1.9.0', **kw):
+def check_waf_version(self, mini='1.9.99', maxi='2.1.0', **kw):
 	"""
 	Raise a Configuration error if the Waf version does not strictly match the given bounds::
 
-		conf.check_waf_version(mini='1.8.0', maxi='1.9.0')
+		conf.check_waf_version(mini='1.9.99', maxi='2.1.0')
 
 	:type  mini: number, tuple or string
 	:param mini: Minimum required version
@@ -413,7 +394,7 @@ def find_file(self, filename, path_list=[]):
 
 	:param filename: name of the file to search for
 	:param path_list: list of directories to search
-	:return: the first occurrence filename or '' if filename could not be found
+	:return: the first matching filename; else a configuration exception is raised
 	"""
 	for n in Utils.to_list(filename):
 		for d in Utils.to_list(path_list):
@@ -433,14 +414,17 @@ def find_program(self, filename, **kw):
 
 	:param path_list: paths to use for searching
 	:type param_list: list of string
-	:param var: store the result to conf.env[var], by default use filename.upper()
+	:param var: store the result to conf.env[var] where var defaults to filename.upper() if not provided; the result is stored as a list of strings
 	:type var: string
-	:param ext: list of extensions for the binary (do not add an extension for portability)
-	:type ext: list of string
+	:param value: obtain the program from the value passed exclusively
+	:type value: list or string (list is preferred)
+	:param exts: list of extensions for the binary (do not add an extension for portability)
+	:type exts: list of string
 	:param msg: name to display in the log, by default filename is used
 	:type msg: string
 	:param interpreter: interpreter for the program
 	:type interpreter: ConfigSet variable key
+	:raises: :py:class:`waflib.Errors.ConfigurationError`
 	"""
 
 	exts = kw.get('exts', Utils.is_win32 and '.exe,.com,.bat,.cmd' or ',.sh,.pl,.py')
@@ -462,18 +446,15 @@ def find_program(self, filename, **kw):
 	else:
 		path_list = environ.get('PATH', '').split(os.pathsep)
 
-	if var in environ:
-		filename = environ[var]
-		if os.path.isfile(filename):
-			# typical CC=/usr/bin/gcc waf configure build
-			ret = [filename]
-		else:
-			# case  CC='ccache gcc' waf configure build
-			ret = self.cmd_to_list(filename)
+	if kw.get('value'):
+		# user-provided in command-line options and passed to find_program
+		ret = self.cmd_to_list(kw['value'])
+	elif environ.get(var):
+		# user-provided in the os environment
+		ret = self.cmd_to_list(environ[var])
 	elif self.env[var]:
-		# set by the user in the wscript file
-		ret = self.env[var]
-		ret = self.cmd_to_list(ret)
+		# a default option in the wscript file
+		ret = self.cmd_to_list(self.env[var])
 	else:
 		if not ret:
 			ret = self.find_binary(filename, exts.split(','), path_list)
@@ -483,7 +464,6 @@ def find_program(self, filename, **kw):
 			ret = Utils.get_registry_app_path(Utils.winreg.HKEY_LOCAL_MACHINE, filename)
 		ret = self.cmd_to_list(ret)
 
-
 	if ret:
 		if len(ret) == 1:
 			retmsg = ret[0]
@@ -492,14 +472,14 @@ def find_program(self, filename, **kw):
 	else:
 		retmsg = False
 
-	self.msg("Checking for program '%s'" % msg, retmsg, **kw)
-	if not kw.get('quiet', None):
+	self.msg('Checking for program %r' % msg, retmsg, **kw)
+	if not kw.get('quiet'):
 		self.to_log('find program=%r paths=%r var=%r -> %r' % (filename, path_list, var, ret))
 
 	if not ret:
 		self.fatal(kw.get('errmsg', '') or 'Could not find the program %r' % filename)
 
-	interpreter = kw.get('interpreter', None)
+	interpreter = kw.get('interpreter')
 	if interpreter is None:
 		if not Utils.check_exe(ret[0], env=environ):
 			self.fatal('Program %r is not executable' % ret)
@@ -554,7 +534,6 @@ def options(opt):
 		$ waf configure --confcache
 
 	"""
-
 	lst = [str(v) for (p, v) in kw.items() if p != 'env']
 	h = Utils.h_list(lst)
 	dir = self.bldnode.abspath() + os.sep + (not Utils.is_win32 and '.' or '') + 'conf_check_' + Utils.to_hex(h)
@@ -573,9 +552,7 @@ def options(opt):
 	if cachemode == 1:
 		try:
 			proj = ConfigSet.ConfigSet(os.path.join(dir, 'cache_run_build'))
-		except OSError:
-			pass
-		except IOError:
+		except EnvironmentError:
 			pass
 		else:
 			ret = proj['cache_run_build']
@@ -588,7 +565,8 @@ def options(opt):
 	if not os.path.exists(bdir):
 		os.makedirs(bdir)
 
-	self.test_bld = bld = Build.BuildContext(top_dir=dir, out_dir=bdir)
+	cls_name = kw.get('run_build_cls') or getattr(self, 'run_build_cls', 'build')
+	self.test_bld = bld = Context.create_context(cls_name, top_dir=dir, out_dir=bdir)
 	bld.init_dirs()
 	bld.progress_bar = 0
 	bld.targets = '*'
@@ -597,17 +575,15 @@ def options(opt):
 	bld.all_envs.update(self.all_envs) # not really necessary
 	bld.env = kw['env']
 
-	# OMG huge hack
 	bld.kw = kw
 	bld.conf = self
 	kw['build_fun'](bld)
-
 	ret = -1
 	try:
 		try:
 			bld.compile()
 		except Errors.WafError:
-			ret = 'Test does not build: %s' % Utils.ex_stack()
+			ret = 'Test does not build: %s' % traceback.format_exc()
 			self.fatal(ret)
 		else:
 			ret = getattr(bld, 'retval', 0)
@@ -619,7 +595,6 @@ def options(opt):
 			proj.store(os.path.join(dir, 'cache_run_build'))
 		else:
 			shutil.rmtree(dir)
-
 	return ret
 
 @conf
@@ -635,7 +610,7 @@ def test(self, *k, **kw):
 		kw['env'] = self.env.derive()
 
 	# validate_c for example
-	if kw.get('validate', None):
+	if kw.get('validate'):
 		kw['validate'](kw)
 
 	self.start_msg(kw['msg'], **kw)
@@ -651,7 +626,7 @@ def test(self, *k, **kw):
 	else:
 		kw['success'] = ret
 
-	if kw.get('post_check', None):
+	if kw.get('post_check'):
 		ret = kw['post_check'](kw)
 
 	if ret:
@@ -661,5 +636,3 @@ def test(self, *k, **kw):
 		self.end_msg(self.ret_msg(kw['okmsg'], kw), **kw)
 	return ret
 
-
-
diff --git a/waflib/Context.py b/waflib/Context.py
index b238636a..bb47c921 100644
--- a/waflib/Context.py
+++ b/waflib/Context.py
@@ -1,9 +1,9 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2010 (ita)
+# Thomas Nagy, 2010-2018 (ita)
 
 """
-Classes and functions required for waf commands
+Classes and functions enabling the command system
 """
 
 import os, re, imp, sys
@@ -11,16 +11,16 @@
 import waflib.Node
 
 # the following 3 constants are updated on each new release (do not touch)
-HEXVERSION=0x1081100
+HEXVERSION=0x2000b00
 """Constant updated on new releases"""
 
-WAFVERSION="1.8.17"
+WAFVERSION="2.0.11"
 """Constant updated on new releases"""
 
-WAFREVISION="cd7579a727d1b390bf9cbf111c1b20e811370bc0"
+WAFREVISION="a97f6fb0941091b4966b625f15ec32fa783a8bec"
 """Git revision when the waf version is updated"""
 
-ABI = 98
+ABI = 20
 """Version of the build data cache file format (used in :py:const:`waflib.Context.DBFILE`)"""
 
 DBFILE = '.wafpickle-%s-%d-%d' % (sys.platform, sys.hexversion, ABI)
@@ -41,7 +41,6 @@
 WSCRIPT_FILE = 'wscript'
 """Name of the waf script files"""
 
-
 launch_dir = ''
 """Directory from which waf has been called"""
 run_dir = ''
@@ -53,23 +52,12 @@
 waf_dir = ''
 """Directory containing the waf modules"""
 
-local_repo = ''
-"""Local repository containing additional Waf tools (plugins)"""
-remote_repo = 'https://raw.githubusercontent.com/waf-project/waf/master/'
-"""
-Remote directory containing downloadable waf tools. The missing tools can be downloaded by using::
-
-	$ waf configure --download
-"""
-
-remote_locs = ['waflib/extras', 'waflib/Tools']
-"""
-Remote directories for use with :py:const:`waflib.Context.remote_repo`
-"""
+default_encoding = Utils.console_encoding()
+"""Encoding to use when reading outputs from other processes"""
 
 g_module = None
 """
-Module representing the main wscript file (see :py:const:`waflib.Context.run_dir`)
+Module representing the top-level wscript file (see :py:const:`waflib.Context.run_dir`)
 """
 
 STDOUT = 1
@@ -82,20 +70,20 @@
 are added automatically by a metaclass.
 """
 
-
 def create_context(cmd_name, *k, **kw):
 	"""
-	Create a new :py:class:`waflib.Context.Context` instance corresponding to the given command.
+	Returns a new :py:class:`waflib.Context.Context` instance corresponding to the given command.
 	Used in particular by :py:func:`waflib.Scripting.run_command`
 
-	:param cmd_name: command
+	:param cmd_name: command name
 	:type cmd_name: string
 	:param k: arguments to give to the context class initializer
 	:type k: list
 	:param k: keyword arguments to give to the context class initializer
 	:type k: dict
+	:return: Context object
+	:rtype: :py:class:`waflib.Context.Context`
 	"""
-	global classes
 	for x in classes:
 		if x.cmd == cmd_name:
 			return x(*k, **kw)
@@ -105,14 +93,15 @@ def create_context(cmd_name, *k, **kw):
 
 class store_context(type):
 	"""
-	Metaclass for storing the command classes into the list :py:const:`waflib.Context.classes`
-	Context classes must provide an attribute 'cmd' representing the command to execute
+	Metaclass that registers command classes into the list :py:const:`waflib.Context.classes`
+	Context classes must provide an attribute 'cmd' representing the command name, and a function
+	attribute 'fun' representing the function name that the command uses.
 	"""
-	def __init__(cls, name, bases, dict):
-		super(store_context, cls).__init__(name, bases, dict)
+	def __init__(cls, name, bases, dct):
+		super(store_context, cls).__init__(name, bases, dct)
 		name = cls.__name__
 
-		if name == 'ctx' or name == 'Context':
+		if name in ('ctx', 'Context'):
 			return
 
 		try:
@@ -123,11 +112,10 @@ def __init__(cls, name, bases, dict):
 		if not getattr(cls, 'fun', None):
 			cls.fun = cls.cmd
 
-		global classes
 		classes.insert(0, cls)
 
 ctx = store_context('ctx', (object,), {})
-"""Base class for the :py:class:`waflib.Context.Context` classes"""
+"""Base class for all :py:class:`waflib.Context.Context` classes"""
 
 class Context(ctx):
 	"""
@@ -138,7 +126,7 @@ class Context(ctx):
 		def foo(ctx):
 			print(ctx.__class__.__name__) # waflib.Context.Context
 
-	Subclasses must define the attribute 'cmd':
+	Subclasses must define the class attributes 'cmd' and 'fun':
 
 	:param cmd: command to execute as in ``waf cmd``
 	:type cmd: string
@@ -156,19 +144,18 @@ def foo(ctx):
 
 	tools = {}
 	"""
-	A cache for modules (wscript files) read by :py:meth:`Context.Context.load`
+	A module cache for wscript files; see :py:meth:`Context.Context.load`
 	"""
 
 	def __init__(self, **kw):
 		try:
 			rd = kw['run_dir']
 		except KeyError:
-			global run_dir
 			rd = run_dir
 
 		# binds the context to the nodes in use to avoid a context singleton
-		self.node_class = type("Nod3", (waflib.Node.Node,), {})
-		self.node_class.__module__ = "waflib.Node"
+		self.node_class = type('Nod3', (waflib.Node.Node,), {})
+		self.node_class.__module__ = 'waflib.Node'
 		self.node_class.ctx = self
 
 		self.root = self.node_class('', None)
@@ -179,18 +166,9 @@ def __init__(self, **kw):
 		self.exec_dict = {'ctx':self, 'conf':self, 'bld':self, 'opt':self}
 		self.logger = None
 
-	def __hash__(self):
-		"""
-		Return a hash value for storing context objects in dicts or sets. The value is not persistent.
-
-		:return: hash value
-		:rtype: int
-		"""
-		return id(self)
-
 	def finalize(self):
 		"""
-		Use to free resources such as open files potentially held by the logger
+		Called to free resources such as logger files
 		"""
 		try:
 			logger = self.logger
@@ -202,11 +180,11 @@ def finalize(self):
 
 	def load(self, tool_list, *k, **kw):
 		"""
-		Load a Waf tool as a module, and try calling the function named :py:const:`waflib.Context.Context.fun` from it.
-		A ``tooldir`` value may be provided as a list of module paths.
+		Loads a Waf tool as a module, and try calling the function named :py:const:`waflib.Context.Context.fun`
+		from it.  A ``tooldir`` argument may be provided as a list of module paths.
 
+		:param tool_list: list of Waf tool names to load
 		:type tool_list: list of string or space-separated string
-		:param tool_list: list of Waf tools to use
 		"""
 		tools = Utils.to_list(tool_list)
 		path = Utils.to_list(kw.get('tooldir', ''))
@@ -220,15 +198,16 @@ def load(self, tool_list, *k, **kw):
 
 	def execute(self):
 		"""
-		Execute the command. Redefine this method in subclasses.
+		Here, it calls the function name in the top-level wscript file. Most subclasses
+		redefine this method to provide additional functionality.
 		"""
-		global g_module
 		self.recurse([os.path.dirname(g_module.root_path)])
 
 	def pre_recurse(self, node):
 		"""
-		Method executed immediately before a folder is read by :py:meth:`waflib.Context.Context.recurse`. The node given is set
-		as an attribute ``self.cur_script``, and as the current path ``self.path``
+		Method executed immediately before a folder is read by :py:meth:`waflib.Context.Context.recurse`.
+		The current script is bound as a Node object on ``self.cur_script``, and the current path
+		is bound to ``self.path``
 
 		:param node: script
 		:type node: :py:class:`waflib.Node.Node`
@@ -240,7 +219,7 @@ def pre_recurse(self, node):
 
 	def post_recurse(self, node):
 		"""
-		Restore ``self.cur_script`` and ``self.path`` right after :py:meth:`waflib.Context.Context.recurse` terminates.
+		Restores ``self.cur_script`` and ``self.path`` right after :py:meth:`waflib.Context.Context.recurse` terminates.
 
 		:param node: script
 		:type node: :py:class:`waflib.Node.Node`
@@ -251,10 +230,13 @@ def post_recurse(self, node):
 
 	def recurse(self, dirs, name=None, mandatory=True, once=True, encoding=None):
 		"""
-		Run user code from the supplied list of directories.
+		Runs user-provided functions from the supplied list of directories.
 		The directories can be either absolute, or relative to the directory
-		of the wscript file. The methods :py:meth:`waflib.Context.Context.pre_recurse` and :py:meth:`waflib.Context.Context.post_recurse`
-		are called immediately before and after a script has been executed.
+		of the wscript file
+
+		The methods :py:meth:`waflib.Context.Context.pre_recurse` and
+		:py:meth:`waflib.Context.Context.post_recurse` are called immediately before
+		and after a script has been executed.
 
 		:param dirs: List of directories to visit
 		:type dirs: list of string or space-separated string
@@ -300,7 +282,7 @@ def recurse(self, dirs, name=None, mandatory=True, once=True, encoding=None):
 						if not user_function:
 							if not mandatory:
 								continue
-							raise Errors.WafError('No function %s defined in %s' % (name or self.fun, node.abspath()))
+							raise Errors.WafError('No function %r defined in %s' % (name or self.fun, node.abspath()))
 						user_function(self)
 					finally:
 						self.post_recurse(node)
@@ -313,25 +295,39 @@ def recurse(self, dirs, name=None, mandatory=True, once=True, encoding=None):
 						raise Errors.WafError('Cannot read the folder %r' % d)
 					raise Errors.WafError('No wscript file in directory %s' % d)
 
+	def log_command(self, cmd, kw):
+		if Logs.verbose:
+			fmt = os.environ.get('WAF_CMD_FORMAT')
+			if fmt == 'string':
+				if not isinstance(cmd, str):
+					cmd = Utils.shell_escape(cmd)
+			Logs.debug('runner: %r', cmd)
+			Logs.debug('runner_env: kw=%s', kw)
+
 	def exec_command(self, cmd, **kw):
 		"""
-		Execute a command and return the exit status. If the context has the attribute 'log',
-		capture and log the process stderr/stdout for logging purposes::
+		Runs an external process and returns the exit status::
 
 			def run(tsk):
 				ret = tsk.generator.bld.exec_command('touch foo.txt')
 				return ret
 
-		This method captures the standard/error outputs (Issue 1101), but it does not return the values
-		unlike :py:meth:`waflib.Context.Context.cmd_and_log`
+		If the context has the attribute 'log', then captures and logs the process stderr/stdout.
+		Unlike :py:meth:`waflib.Context.Context.cmd_and_log`, this method does not return the
+		stdout/stderr values captured.
 
 		:param cmd: command argument for subprocess.Popen
+		:type cmd: string or list
 		:param kw: keyword arguments for subprocess.Popen. The parameters input/timeout will be passed to wait/communicate.
+		:type kw: dict
+		:returns: process exit status
+		:rtype: integer
+		:raises: :py:class:`waflib.Errors.WafError` if an invalid executable is specified for a non-shell process
+		:raises: :py:class:`waflib.Errors.WafError` in case of execution failure
 		"""
 		subprocess = Utils.subprocess
 		kw['shell'] = isinstance(cmd, str)
-		Logs.debug('runner: %r' % (cmd,))
-		Logs.debug('runner_env: kw=%s' % kw)
+		self.log_command(cmd, kw)
 
 		if self.logger:
 			self.logger.info(cmd)
@@ -342,40 +338,42 @@ def run(tsk):
 			kw['stderr'] = subprocess.PIPE
 
 		if Logs.verbose and not kw['shell'] and not Utils.check_exe(cmd[0]):
-			raise Errors.WafError("Program %s not found!" % cmd[0])
+			raise Errors.WafError('Program %s not found!' % cmd[0])
 
-		wargs = {}
+		cargs = {}
 		if 'timeout' in kw:
-			if kw['timeout'] is not None:
-				wargs['timeout'] = kw['timeout']
+			if sys.hexversion >= 0x3030000:
+				cargs['timeout'] = kw['timeout']
+				if not 'start_new_session' in kw:
+					kw['start_new_session'] = True
 			del kw['timeout']
 		if 'input' in kw:
 			if kw['input']:
-				wargs['input'] = kw['input']
-				kw['stdin'] = Utils.subprocess.PIPE
+				cargs['input'] = kw['input']
+				kw['stdin'] = subprocess.PIPE
 			del kw['input']
 
+		if 'cwd' in kw:
+			if not isinstance(kw['cwd'], str):
+				kw['cwd'] = kw['cwd'].abspath()
+
+		encoding = kw.pop('decode_as', default_encoding)
+
 		try:
-			if kw['stdout'] or kw['stderr']:
-				p = subprocess.Popen(cmd, **kw)
-				(out, err) = p.communicate(**wargs)
-				ret = p.returncode
-			else:
-				out, err = (None, None)
-				ret = subprocess.Popen(cmd, **kw).wait(**wargs)
+			ret, out, err = Utils.run_process(cmd, kw, cargs)
 		except Exception as e:
 			raise Errors.WafError('Execution failure: %s' % str(e), ex=e)
 
 		if out:
 			if not isinstance(out, str):
-				out = out.decode(sys.stdout.encoding or 'iso8859-1')
+				out = out.decode(encoding, errors='replace')
 			if self.logger:
-				self.logger.debug('out: %s' % out)
+				self.logger.debug('out: %s', out)
 			else:
 				Logs.info(out, extra={'stream':sys.stdout, 'c1': ''})
 		if err:
 			if not isinstance(err, str):
-				err = err.decode(sys.stdout.encoding or 'iso8859-1')
+				err = err.decode(encoding, errors='replace')
 			if self.logger:
 				self.logger.error('err: %s' % err)
 			else:
@@ -385,9 +383,9 @@ def run(tsk):
 
 	def cmd_and_log(self, cmd, **kw):
 		"""
-		Execute a command and return stdout/stderr if the execution is successful.
+		Executes a process and returns stdout/stderr if the execution is successful.
 		An exception is thrown when the exit status is non-0. In that case, both stderr and stdout
-		will be bound to the WafError object::
+		will be bound to the WafError object (configuration tests)::
 
 			def configure(conf):
 				out = conf.cmd_and_log(['echo', 'hello'], output=waflib.Context.STDOUT, quiet=waflib.Context.BOTH)
@@ -395,65 +393,69 @@ def configure(conf):
 				(out, err) = conf.cmd_and_log(cmd, input='\\n'.encode(), output=waflib.Context.STDOUT)
 				try:
 					conf.cmd_and_log(['which', 'someapp'], output=waflib.Context.BOTH)
-				except Exception as e:
+				except Errors.WafError as e:
 					print(e.stdout, e.stderr)
 
 		:param cmd: args for subprocess.Popen
+		:type cmd: list or string
 		:param kw: keyword arguments for subprocess.Popen. The parameters input/timeout will be passed to wait/communicate.
+		:type kw: dict
+		:returns: a tuple containing the contents of stdout and stderr
+		:rtype: string
+		:raises: :py:class:`waflib.Errors.WafError` if an invalid executable is specified for a non-shell process
+		:raises: :py:class:`waflib.Errors.WafError` in case of execution failure; stdout/stderr/returncode are bound to the exception object
 		"""
 		subprocess = Utils.subprocess
 		kw['shell'] = isinstance(cmd, str)
-		Logs.debug('runner: %r' % (cmd,))
-
-		if 'quiet' in kw:
-			quiet = kw['quiet']
-			del kw['quiet']
-		else:
-			quiet = None
+		self.log_command(cmd, kw)
 
-		if 'output' in kw:
-			to_ret = kw['output']
-			del kw['output']
-		else:
-			to_ret = STDOUT
+		quiet = kw.pop('quiet', None)
+		to_ret = kw.pop('output', STDOUT)
 
 		if Logs.verbose and not kw['shell'] and not Utils.check_exe(cmd[0]):
-			raise Errors.WafError("Program %s not found!" % cmd[0])
+			raise Errors.WafError('Program %r not found!' % cmd[0])
 
 		kw['stdout'] = kw['stderr'] = subprocess.PIPE
 		if quiet is None:
 			self.to_log(cmd)
 
-		wargs = {}
+		cargs = {}
 		if 'timeout' in kw:
-			if kw['timeout'] is not None:
-				wargs['timeout'] = kw['timeout']
+			if sys.hexversion >= 0x3030000:
+				cargs['timeout'] = kw['timeout']
+				if not 'start_new_session' in kw:
+					kw['start_new_session'] = True
 			del kw['timeout']
 		if 'input' in kw:
 			if kw['input']:
-				wargs['input'] = kw['input']
-				kw['stdin'] = Utils.subprocess.PIPE
+				cargs['input'] = kw['input']
+				kw['stdin'] = subprocess.PIPE
 			del kw['input']
 
+		if 'cwd' in kw:
+			if not isinstance(kw['cwd'], str):
+				kw['cwd'] = kw['cwd'].abspath()
+
+		encoding = kw.pop('decode_as', default_encoding)
+
 		try:
-			p = subprocess.Popen(cmd, **kw)
-			(out, err) = p.communicate(**wargs)
+			ret, out, err = Utils.run_process(cmd, kw, cargs)
 		except Exception as e:
 			raise Errors.WafError('Execution failure: %s' % str(e), ex=e)
 
 		if not isinstance(out, str):
-			out = out.decode(sys.stdout.encoding or 'iso8859-1')
+			out = out.decode(encoding, errors='replace')
 		if not isinstance(err, str):
-			err = err.decode(sys.stdout.encoding or 'iso8859-1')
+			err = err.decode(encoding, errors='replace')
 
 		if out and quiet != STDOUT and quiet != BOTH:
 			self.to_log('out: %s' % out)
 		if err and quiet != STDERR and quiet != BOTH:
 			self.to_log('err: %s' % err)
 
-		if p.returncode:
-			e = Errors.WafError('Command %r returned %r' % (cmd, p.returncode))
-			e.returncode = p.returncode
+		if ret:
+			e = Errors.WafError('Command %r returned %r' % (cmd, ret))
+			e.returncode = ret
 			e.stderr = err
 			e.stdout = out
 			raise e
@@ -466,7 +468,8 @@ def configure(conf):
 
 	def fatal(self, msg, ex=None):
 		"""
-		Raise a configuration error to interrupt the execution immediately::
+		Prints an error message in red and stops command execution; this is
+		usually used in the configuration section::
 
 			def configure(conf):
 				conf.fatal('a requirement is missing')
@@ -475,24 +478,31 @@ def configure(conf):
 		:type msg: string
 		:param ex: optional exception object
 		:type ex: exception
+		:raises: :py:class:`waflib.Errors.ConfigurationError`
 		"""
 		if self.logger:
 			self.logger.info('from %s: %s' % (self.path.abspath(), msg))
 		try:
-			msg = '%s\n(complete log in %s)' % (msg, self.logger.handlers[0].baseFilename)
-		except Exception:
+			logfile = self.logger.handlers[0].baseFilename
+		except AttributeError:
 			pass
+		else:
+			if os.environ.get('WAF_PRINT_FAILURE_LOG'):
+				# see #1930
+				msg = 'Log from (%s):\n%s\n' % (logfile, Utils.readf(logfile))
+			else:
+				msg = '%s\n(complete log in %s)' % (msg, logfile)
 		raise self.errors.ConfigurationError(msg, ex=ex)
 
 	def to_log(self, msg):
 		"""
-		Log some information to the logger (if present), or to stderr. If the message is empty,
-		it is not printed::
+		Logs information to the logger (if present), or to stderr.
+		Empty messages are not printed::
 
 			def build(bld):
 				bld.to_log('starting the build')
 
-		When in doubt, override this method, or provide a logger on the context class.
+		Provide a logger on the context class or override this method if necessary.
 
 		:param msg: message
 		:type msg: string
@@ -508,7 +518,7 @@ def build(bld):
 
 	def msg(self, *k, **kw):
 		"""
-		Print a configuration message of the form ``msg: result``.
+		Prints a configuration message of the form ``msg: result``.
 		The second part of the message will be in colors. The output
 		can be disabled easly by setting ``in_msg`` to a positive value::
 
@@ -536,7 +546,7 @@ def configure(conf):
 		except KeyError:
 			result = k[1]
 
-		color = kw.get('color', None)
+		color = kw.get('color')
 		if not isinstance(color, str):
 			color = result and 'GREEN' or 'YELLOW'
 
@@ -544,12 +554,12 @@ def configure(conf):
 
 	def start_msg(self, *k, **kw):
 		"""
-		Print the beginning of a 'Checking for xxx' message. See :py:meth:`waflib.Context.Context.msg`
+		Prints the beginning of a 'Checking for xxx' message. See :py:meth:`waflib.Context.Context.msg`
 		"""
-		if kw.get('quiet', None):
+		if kw.get('quiet'):
 			return
 
-		msg = kw.get('msg', None) or k[0]
+		msg = kw.get('msg') or k[0]
 		try:
 			if self.in_msg:
 				self.in_msg += 1
@@ -567,19 +577,19 @@ def start_msg(self, *k, **kw):
 		Logs.pprint('NORMAL', "%s :" % msg.ljust(self.line_just), sep='')
 
 	def end_msg(self, *k, **kw):
-		"""Print the end of a 'Checking for' message. See :py:meth:`waflib.Context.Context.msg`"""
-		if kw.get('quiet', None):
+		"""Prints the end of a 'Checking for' message. See :py:meth:`waflib.Context.Context.msg`"""
+		if kw.get('quiet'):
 			return
 		self.in_msg -= 1
 		if self.in_msg:
 			return
 
-		result = kw.get('result', None) or k[0]
+		result = kw.get('result') or k[0]
 
 		defcolor = 'GREEN'
-		if result == True:
+		if result is True:
 			msg = 'ok'
-		elif result == False:
+		elif not result:
 			msg = 'not found'
 			defcolor = 'YELLOW'
 		else:
@@ -597,7 +607,17 @@ def end_msg(self, *k, **kw):
 		Logs.pprint(color, msg)
 
 	def load_special_tools(self, var, ban=[]):
-		global waf_dir
+		"""
+		Loads third-party extensions modules for certain programming languages
+		by trying to list certain files in the extras/ directory. This method
+		is typically called once for a programming language group, see for
+		example :py:mod:`waflib.Tools.compiler_c`
+
+		:param var: glob expression, for example 'cxx\_\*.py'
+		:type var: string
+		:param ban: list of exact file names to exclude
+		:type ban: list of string
+		"""
 		if os.path.isdir(waf_dir):
 			lst = self.root.find_node(waf_dir).find_node('waflib/extras').ant_glob(var)
 			for x in lst:
@@ -608,12 +628,12 @@ def load_special_tools(self, var, ban=[]):
 			waflibs = PyZipFile(waf_dir)
 			lst = waflibs.namelist()
 			for x in lst:
-				if not re.match("waflib/extras/%s" % var.replace("*", ".*"), var):
+				if not re.match('waflib/extras/%s' % var.replace('*', '.*'), var):
 					continue
 				f = os.path.basename(x)
 				doban = False
 				for b in ban:
-					r = b.replace("*", ".*")
+					r = b.replace('*', '.*')
 					if re.match(r, f):
 						doban = True
 				if not doban:
@@ -622,13 +642,13 @@ def load_special_tools(self, var, ban=[]):
 
 cache_modules = {}
 """
-Dictionary holding already loaded modules, keyed by their absolute path.
+Dictionary holding already loaded modules (wscript), indexed by their absolute path.
 The modules are added automatically by :py:func:`waflib.Context.load_module`
 """
 
 def load_module(path, encoding=None):
 	"""
-	Load a source file as a python module.
+	Loads a wscript file as a python module. This method caches results in :py:attr:`waflib.Context.cache_modules`
 
 	:param path: file path
 	:type path: string
@@ -648,17 +668,17 @@ def load_module(path, encoding=None):
 
 	module_dir = os.path.dirname(path)
 	sys.path.insert(0, module_dir)
-
-	try    : exec(compile(code, path, 'exec'), module.__dict__)
-	finally: sys.path.remove(module_dir)
+	try:
+		exec(compile(code, path, 'exec'), module.__dict__)
+	finally:
+		sys.path.remove(module_dir)
 
 	cache_modules[path] = module
-
 	return module
 
 def load_tool(tool, tooldir=None, ctx=None, with_sys_path=True):
 	"""
-	Import a Waf tool (python module), and store it in the dict :py:const:`waflib.Context.Context.tools`
+	Importx a Waf tool as a python module, and stores it in the dict :py:const:`waflib.Context.Context.tools`
 
 	:type  tool: string
 	:param tool: Name of the tool
@@ -672,14 +692,18 @@ def load_tool(tool, tooldir=None, ctx=None, with_sys_path=True):
 	else:
 		tool = tool.replace('++', 'xx')
 
-	origSysPath = sys.path
-	if not with_sys_path: sys.path = []
+	if not with_sys_path:
+		back_path = sys.path
+		sys.path = []
 	try:
 		if tooldir:
 			assert isinstance(tooldir, list)
 			sys.path = tooldir + sys.path
 			try:
 				__import__(tool)
+			except ImportError as e:
+				e.waf_sys_path = list(sys.path)
+				raise
 			finally:
 				for d in tooldir:
 					sys.path.remove(d)
@@ -687,7 +711,8 @@ def load_tool(tool, tooldir=None, ctx=None, with_sys_path=True):
 			Context.tools[tool] = ret
 			return ret
 		else:
-			if not with_sys_path: sys.path.insert(0, waf_dir)
+			if not with_sys_path:
+				sys.path.insert(0, waf_dir)
 			try:
 				for x in ('waflib.Tools.%s', 'waflib.extras.%s', 'waflib.%s', '%s'):
 					try:
@@ -695,13 +720,18 @@ def load_tool(tool, tooldir=None, ctx=None, with_sys_path=True):
 						break
 					except ImportError:
 						x = None
-				if x is None: # raise an exception
+				else: # raise an exception
 					__import__(tool)
+			except ImportError as e:
+				e.waf_sys_path = list(sys.path)
+				raise
 			finally:
-				if not with_sys_path: sys.path.remove(waf_dir)
+				if not with_sys_path:
+					sys.path.remove(waf_dir)
 			ret = sys.modules[x % tool]
 			Context.tools[tool] = ret
 			return ret
 	finally:
-		if not with_sys_path: sys.path += origSysPath
+		if not with_sys_path:
+			sys.path += back_path
 
diff --git a/waflib/Errors.py b/waflib/Errors.py
index 104f7d82..bf75c1b6 100644
--- a/waflib/Errors.py
+++ b/waflib/Errors.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2010 (ita)
+# Thomas Nagy, 2010-2018 (ita)
 
 """
 Exceptions used in the Waf code
@@ -17,6 +17,7 @@ def __init__(self, msg='', ex=None):
 		:param ex: exception causing this error (optional)
 		:type ex: exception
 		"""
+		Exception.__init__(self)
 		self.msg = msg
 		assert not isinstance(msg, Exception)
 
@@ -35,9 +36,7 @@ def __str__(self):
 		return str(self.msg)
 
 class BuildError(WafError):
-	"""
-	Errors raised during the build and install phases
-	"""
+	"""Error raised during the build and install phases"""
 	def __init__(self, error_tasks=[]):
 		"""
 		:param error_tasks: tasks that could not complete normally
@@ -47,24 +46,23 @@ def __init__(self, error_tasks=[]):
 		WafError.__init__(self, self.format_error())
 
 	def format_error(self):
-		"""format the error messages from the tasks that failed"""
+		"""Formats the error messages from the tasks that failed"""
 		lst = ['Build failed']
 		for tsk in self.tasks:
 			txt = tsk.format_error()
-			if txt: lst.append(txt)
+			if txt:
+				lst.append(txt)
 		return '\n'.join(lst)
 
 class ConfigurationError(WafError):
-	"""
-	Configuration exception raised in particular by :py:meth:`waflib.Context.Context.fatal`
-	"""
+	"""Configuration exception raised in particular by :py:meth:`waflib.Context.Context.fatal`"""
 	pass
 
 class TaskRescan(WafError):
-	"""task-specific exception type, trigger a signature recomputation"""
+	"""Task-specific exception type signalling required signature recalculations"""
 	pass
 
 class TaskNotReady(WafError):
-	"""task-specific exception type, raised when the task signature cannot be computed"""
+	"""Task-specific exception type signalling that task signatures cannot be computed"""
 	pass
 
diff --git a/waflib/Logs.py b/waflib/Logs.py
index 33fd9042..2a475169 100644
--- a/waflib/Logs.py
+++ b/waflib/Logs.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 logging, colors, terminal width and pretty-print
@@ -23,8 +23,15 @@
 LOG_FORMAT = os.environ.get('WAF_LOG_FORMAT', '%(asctime)s %(c1)s%(zone)s%(c2)s %(message)s')
 HOUR_FORMAT = os.environ.get('WAF_HOUR_FORMAT', '%H:%M:%S')
 
-zones = ''
+zones = []
+"""
+See :py:class:`waflib.Logs.log_filter`
+"""
+
 verbose = 0
+"""
+Global verbosity level, see :py:func:`waflib.Logs.debug` and :py:func:`waflib.Logs.error`
+"""
 
 colors_lst = {
 'USE' : True,
@@ -49,6 +56,15 @@
 	unicode = None
 
 def enable_colors(use):
+	"""
+	If *1* is given, then the system will perform a few verifications
+	before enabling colors, such as checking whether the interpreter
+	is running in a terminal. A value of zero will disable colors,
+	and a value above *1* will force colors.
+
+	:param use: whether to enable colors or not
+	:type use: integer
+	"""
 	if use == 1:
 		if not (sys.stderr.isatty() or sys.stdout.isatty()):
 			use = 0
@@ -74,15 +90,23 @@ def get_term_cols():
 		return 80
 
 get_term_cols.__doc__ = """
-	Get the console width in characters.
+	Returns the console width in characters.
 
 	:return: the number of characters per line
 	:rtype: int
 	"""
 
 def get_color(cl):
-	if not colors_lst['USE']: return ''
-	return colors_lst.get(cl, '')
+	"""
+	Returns the ansi sequence corresponding to the given color name.
+	An empty string is returned when coloring is globally disabled.
+
+	:param cl: color name in capital letters
+	:type cl: string
+	"""
+	if colors_lst['USE']:
+		return colors_lst.get(cl, '')
+	return ''
 
 class color_dict(object):
 	"""attribute-based color access, eg: colors.PINK"""
@@ -96,7 +120,7 @@ def __call__(self, a):
 re_log = re.compile(r'(\w+): (.*)', re.M)
 class log_filter(logging.Filter):
 	"""
-	The waf logs are of the form 'name: message', and can be filtered by 'waf --zones=name'.
+	Waf logs are of the form 'name: message', and can be filtered by 'waf --zones=name'.
 	For example, the following::
 
 		from waflib import Logs
@@ -106,17 +130,14 @@ class log_filter(logging.Filter):
 
 		$ waf --zones=test
 	"""
-	def __init__(self, name=None):
-		pass
+	def __init__(self, name=''):
+		logging.Filter.__init__(self, name)
 
 	def filter(self, rec):
 		"""
-		filter a record, adding the colors automatically
+		Filters log records by zone and by logging level
 
-		* error: red
-		* warning: yellow
-
-		:param rec: message to record
+		:param rec: log entry
 		"""
 		rec.zone = rec.module
 		if rec.levelno >= logging.INFO:
@@ -136,6 +157,9 @@ def filter(self, rec):
 class log_handler(logging.StreamHandler):
 	"""Dispatches messages to stderr/stdout depending on the severity level"""
 	def emit(self, record):
+		"""
+		Delegates the functionality to :py:meth:`waflib.Log.log_handler.emit_override`
+		"""
 		# default implementation
 		try:
 			try:
@@ -153,6 +177,9 @@ def emit(self, record):
 			self.handleError(record)
 
 	def emit_override(self, record, **kw):
+		"""
+		Writes the log record to the desired stream (stderr/stdout)
+		"""
 		self.terminator = getattr(record, 'terminator', '\n')
 		stream = self.stream
 		if unicode:
@@ -169,7 +196,7 @@ def emit_override(self, record, **kw):
 				else:
 					stream.write(fs % msg)
 			except UnicodeError:
-				stream.write((fs % msg).encode("UTF-8"))
+				stream.write((fs % msg).encode('utf-8'))
 		else:
 			logging.StreamHandler.emit(self, record)
 
@@ -179,7 +206,10 @@ def __init__(self):
 		logging.Formatter.__init__(self, LOG_FORMAT, HOUR_FORMAT)
 
 	def format(self, rec):
-		"""Messages in warning, error or info mode are displayed in color by default"""
+		"""
+		Formats records and adds colors as needed. The records do not get
+		a leading hour format if the logging level is above *INFO*.
+		"""
 		try:
 			msg = rec.msg.decode('utf-8')
 		except Exception:
@@ -200,10 +230,14 @@ def format(self, rec):
 			c2 = getattr(rec, 'c2', colors.NORMAL)
 			msg = '%s%s%s' % (c1, msg, c2)
 		else:
-			msg = msg.replace('\r', '\n')
-			msg = re.sub(r'\x1B\[(K|.*?(m|h|l))', '', msg)
+			# remove single \r that make long lines in text files
+			# and other terminal commands
+			msg = re.sub(r'\r(?!\n)|\x1B\[(K|.*?(m|h|l))', '', msg)
 
-		if rec.levelno >= logging.INFO: # ??
+		if rec.levelno >= logging.INFO:
+			# the goal of this is to format without the leading "Logs, hour" prefix
+			if rec.args:
+				return msg % rec.args
 			return msg
 
 		rec.msg = msg
@@ -216,19 +250,17 @@ def format(self, rec):
 
 def debug(*k, **kw):
 	"""
-	Wrap logging.debug, the output is filtered for performance reasons
+	Wraps logging.debug and discards messages if the verbosity level :py:attr:`waflib.Logs.verbose` ≤ 0
 	"""
 	if verbose:
 		k = list(k)
 		k[0] = k[0].replace('\n', ' ')
-		global log
 		log.debug(*k, **kw)
 
 def error(*k, **kw):
 	"""
-	Wrap logging.errors, display the origin of the message when '-vv' is set
+	Wrap logging.errors, adds the stack trace when the verbosity level :py:attr:`waflib.Logs.verbose` ≥ 2
 	"""
-	global log
 	log.error(*k, **kw)
 	if verbose > 2:
 		st = traceback.extract_stack()
@@ -236,28 +268,27 @@ def error(*k, **kw):
 			st = st[:-1]
 			buf = []
 			for filename, lineno, name, line in st:
-				buf.append('  File "%s", line %d, in %s' % (filename, lineno, name))
+				buf.append('  File %r, line %d, in %s' % (filename, lineno, name))
 				if line:
 					buf.append('	%s' % line.strip())
-			if buf: log.error("\n".join(buf))
+			if buf:
+				log.error('\n'.join(buf))
 
 def warn(*k, **kw):
 	"""
-	Wrap logging.warn
+	Wraps logging.warn
 	"""
-	global log
 	log.warn(*k, **kw)
 
 def info(*k, **kw):
 	"""
-	Wrap logging.info
+	Wraps logging.info
 	"""
-	global log
 	log.info(*k, **kw)
 
 def init_log():
 	"""
-	Initialize the loggers globally
+	Initializes the logger :py:attr:`waflib.Logs.log`
 	"""
 	global log
 	log = logging.getLogger('waflib')
@@ -271,7 +302,7 @@ def init_log():
 
 def make_logger(path, name):
 	"""
-	Create a simple logger, which is often used to redirect the context command output::
+	Creates a simple logger, which is often used to redirect the context command output::
 
 		from waflib import Logs
 		bld.logger = Logs.make_logger('test.log', 'build')
@@ -291,7 +322,11 @@ def make_logger(path, name):
 	:type name: string
 	"""
 	logger = logging.getLogger(name)
-	hdlr = logging.FileHandler(path, 'w')
+	if sys.hexversion > 0x3000000:
+		encoding = sys.stdout.encoding
+	else:
+		encoding = None
+	hdlr = logging.FileHandler(path, 'w', encoding=encoding)
 	formatter = logging.Formatter('%(message)s')
 	hdlr.setFormatter(formatter)
 	logger.addHandler(hdlr)
@@ -300,7 +335,7 @@ def make_logger(path, name):
 
 def make_mem_logger(name, to_log, size=8192):
 	"""
-	Create a memory logger to avoid writing concurrently to the main logger
+	Creates a memory logger to avoid writing concurrently to the main logger
 	"""
 	from logging.handlers import MemoryHandler
 	logger = logging.getLogger(name)
@@ -314,7 +349,7 @@ def make_mem_logger(name, to_log, size=8192):
 
 def free_logger(logger):
 	"""
-	Free the resources held by the loggers created through make_logger or make_mem_logger.
+	Frees the resources held by the loggers created through make_logger or make_mem_logger.
 	This is used for file cleanup and for handler removal (logger objects are re-used).
 	"""
 	try:
@@ -326,7 +361,7 @@ def free_logger(logger):
 
 def pprint(col, msg, label='', sep='\n'):
 	"""
-	Print messages in color immediately on stderr::
+	Prints messages in color immediately on stderr::
 
 		from waflib import Logs
 		Logs.pprint('RED', 'Something bad just happened')
@@ -340,5 +375,5 @@ def pprint(col, msg, label='', sep='\n'):
 	:param sep: a string to append at the end (line separator)
 	:type sep: string
 	"""
-	info("%s%s%s %s" % (colors(col), msg, colors.NORMAL, label), extra={'terminator':sep})
+	info('%s%s%s %s', colors(col), msg, colors.NORMAL, label, extra={'terminator':sep})
 
diff --git a/waflib/Node.py b/waflib/Node.py
index fb3cd273..4ac1ea8a 100644
--- a/waflib/Node.py
+++ b/waflib/Node.py
@@ -1,9 +1,9 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
-Node: filesystem structure, contains lists of nodes
+Node: filesystem structure
 
 #. Each file/folder is represented by exactly one node.
 
@@ -11,13 +11,14 @@
    Unused class members can increase the `.wafpickle` file size sensibly.
 
 #. Node objects should never be created directly, use
-   the methods :py:func:`Node.make_node` or :py:func:`Node.find_node`
+   the methods :py:func:`Node.make_node` or :py:func:`Node.find_node` for the low-level operations
 
-#. The methods :py:func:`Node.find_resource`, :py:func:`Node.find_dir` :py:func:`Node.find_or_declare` should be
+#. The methods :py:func:`Node.find_resource`, :py:func:`Node.find_dir` :py:func:`Node.find_or_declare` must be
    used when a build context is present
 
-#. Each instance of :py:class:`waflib.Context.Context` has a unique :py:class:`Node` subclass.
-   (:py:class:`waflib.Node.Nod3`, see the :py:class:`waflib.Context.Context` initializer). A reference to the context owning a node is held as self.ctx
+#. Each instance of :py:class:`waflib.Context.Context` has a unique :py:class:`Node` subclass required for serialization.
+   (:py:class:`waflib.Node.Nod3`, see the :py:class:`waflib.Context.Context` initializer). A reference to the context
+   owning a node is held as *self.ctx*
 """
 
 import os, re, sys, shutil
@@ -29,6 +30,7 @@
 **/.#*
 **/%*%
 **/._*
+**/*.swp
 **/CVS
 **/CVS/**
 **/.cvsignore
@@ -59,99 +61,148 @@
 recursive traversal in :py:meth:`waflib.Node.Node.ant_glob`
 """
 
-# TODO remove in waf 1.9
-split_path = Utils.split_path
-split_path_unix = Utils.split_path_unix
-split_path_cygwin = Utils.split_path_cygwin
-split_path_win32 = Utils.split_path_win32
+def ant_matcher(s, ignorecase):
+	reflags = re.I if ignorecase else 0
+	ret = []
+	for x in Utils.to_list(s):
+		x = x.replace('\\', '/').replace('//', '/')
+		if x.endswith('/'):
+			x += '**'
+		accu = []
+		for k in x.split('/'):
+			if k == '**':
+				accu.append(k)
+			else:
+				k = k.replace('.', '[.]').replace('*','.*').replace('?', '.').replace('+', '\\+')
+				k = '^%s$' % k
+				try:
+					exp = re.compile(k, flags=reflags)
+				except Exception as e:
+					raise Errors.WafError('Invalid pattern: %s' % k, e)
+				else:
+					accu.append(exp)
+		ret.append(accu)
+	return ret
+
+def ant_sub_filter(name, nn):
+	ret = []
+	for lst in nn:
+		if not lst:
+			pass
+		elif lst[0] == '**':
+			ret.append(lst)
+			if len(lst) > 1:
+				if lst[1].match(name):
+					ret.append(lst[2:])
+			else:
+				ret.append([])
+		elif lst[0].match(name):
+			ret.append(lst[1:])
+	return ret
+
+def ant_sub_matcher(name, pats):
+	nacc = ant_sub_filter(name, pats[0])
+	nrej = ant_sub_filter(name, pats[1])
+	if [] in nrej:
+		nacc = []
+	return [nacc, nrej]
 
 class Node(object):
 	"""
-	This class is organized in two parts
+	This class is organized in two parts:
 
 	* The basic methods meant for filesystem access (compute paths, create folders, etc)
 	* The methods bound to a :py:class:`waflib.Build.BuildContext` (require ``bld.srcnode`` and ``bld.bldnode``)
-
-	The Node objects are not thread safe in any way.
 	"""
+
 	dict_class = dict
-	__slots__ = ('name', 'sig', 'children', 'parent', 'cache_abspath', 'cache_isdir', 'cache_sig')
+	"""
+	Subclasses can provide a dict class to enable case insensitivity for example.
+	"""
+
+	__slots__ = ('name', 'parent', 'children', 'cache_abspath', 'cache_isdir')
 	def __init__(self, name, parent):
+		"""
+		.. note:: Use :py:func:`Node.make_node` or :py:func:`Node.find_node` instead of calling this constructor
+		"""
 		self.name = name
 		self.parent = parent
-
 		if parent:
 			if name in parent.children:
 				raise Errors.WafError('node %s exists in the parent files %r already' % (name, parent))
 			parent.children[name] = self
 
 	def __setstate__(self, data):
-		"Deserializes from data"
+		"Deserializes node information, used for persistence"
 		self.name = data[0]
 		self.parent = data[1]
 		if data[2] is not None:
 			# Issue 1480
 			self.children = self.dict_class(data[2])
-		if data[3] is not None:
-			self.sig = data[3]
 
 	def __getstate__(self):
-		"Serialize the node info"
-		return (self.name, self.parent, getattr(self, 'children', None), getattr(self, 'sig', None))
+		"Serializes node information, used for persistence"
+		return (self.name, self.parent, getattr(self, 'children', None))
 
 	def __str__(self):
-		"String representation (name), for debugging purposes"
-		return self.name
+		"""
+		String representation (abspath), for debugging purposes
 
-	def __repr__(self):
-		"String representation (abspath), for debugging purposes"
+		:rtype: string
+		"""
 		return self.abspath()
 
-	def __hash__(self):
-		"Node hash, used for storage in dicts. This hash is not persistent."
-		return id(self)
+	def __repr__(self):
+		"""
+		String representation (abspath), for debugging purposes
 
-	def __eq__(self, node):
-		"Node comparison, based on the IDs"
-		return id(self) == id(node)
+		:rtype: string
+		"""
+		return self.abspath()
 
 	def __copy__(self):
-		"Implemented to prevent nodes from being copied (raises an exception)"
+		"""
+		Provided to prevent nodes from being copied
+
+		:raises: :py:class:`waflib.Errors.WafError`
+		"""
 		raise Errors.WafError('nodes are not supposed to be copied')
 
-	def read(self, flags='r', encoding='ISO8859-1'):
+	def read(self, flags='r', encoding='latin-1'):
 		"""
-		Return the contents of the file represented by this node::
+		Reads and returns the contents of the file represented by this node, see :py:func:`waflib.Utils.readf`::
 
 			def build(bld):
 				bld.path.find_node('wscript').read()
 
-		:type  fname: string
-		:param fname: Path to file
-		:type  m: string
-		:param m: Open mode
-		:rtype: string
+		:param flags: Open mode
+		:type  flags: string
+		:param encoding: encoding value for Python3
+		:type encoding: string
+		:rtype: string or bytes
 		:return: File contents
 		"""
 		return Utils.readf(self.abspath(), flags, encoding)
 
-	def write(self, data, flags='w', encoding='ISO8859-1'):
+	def write(self, data, flags='w', encoding='latin-1'):
 		"""
-		Write some text to the physical file represented by this node::
+		Writes data to the file represented by this node, see :py:func:`waflib.Utils.writef`::
 
 			def build(bld):
 				bld.path.make_node('foo.txt').write('Hello, world!')
 
-		:type  data: string
 		:param data: data to write
-		:type  flags: string
+		:type  data: string
 		:param flags: Write mode
+		:type  flags: string
+		:param encoding: encoding value for Python3
+		:type encoding: string
 		"""
 		Utils.writef(self.abspath(), data, flags, encoding)
 
 	def read_json(self, convert=True, encoding='utf-8'):
 		"""
-		Read and parse the contents of this node as JSON::
+		Reads and parses the contents of this node as JSON (Python ≥ 2.6)::
 
 			def build(bld):
 				bld.path.find_node('abc.json').read_json()
@@ -190,7 +241,7 @@ def object_pairs(pairs):
 
 	def write_json(self, data, pretty=True):
 		"""
-		Writes a python object as JSON to disk. Files are always written as UTF8 as per the JSON standard::
+		Writes a python object as JSON to disk (Python ≥ 2.6) as UTF-8 data (JSON standard)::
 
 			def build(bld):
 				bld.path.find_node('xyz.json').write_json(199)
@@ -212,40 +263,71 @@ def build(bld):
 		output = json.dumps(data, indent=indent, separators=separators, sort_keys=sort_keys) + newline
 		self.write(output, encoding='utf-8')
 
+	def exists(self):
+		"""
+		Returns whether the Node is present on the filesystem
+
+		:rtype: bool
+		"""
+		return os.path.exists(self.abspath())
+
+	def isdir(self):
+		"""
+		Returns whether the Node represents a folder
+
+		:rtype: bool
+		"""
+		return os.path.isdir(self.abspath())
+
 	def chmod(self, val):
 		"""
-		Change file/dir permissions::
+		Changes the file/dir permissions::
 
 			def build(bld):
 				bld.path.chmod(493) # 0755
 		"""
 		os.chmod(self.abspath(), val)
 
-	def delete(self):
-		"""Delete the file/folder, and remove this node from the tree. Do not use this object after calling this method."""
+	def delete(self, evict=True):
+		"""
+		Removes the file/folder from the filesystem (equivalent to `rm -rf`), and remove this object from the Node tree.
+		Do not use this object after calling this method.
+		"""
 		try:
 			try:
-				if hasattr(self, 'children'):
+				if os.path.isdir(self.abspath()):
 					shutil.rmtree(self.abspath())
 				else:
 					os.remove(self.abspath())
-			except OSError as e:
+			except OSError:
 				if os.path.exists(self.abspath()):
-					raise e
+					raise
 		finally:
-			self.evict()
+			if evict:
+				self.evict()
 
 	def evict(self):
-		"""Internal - called when a node is removed"""
+		"""
+		Removes this node from the Node tree
+		"""
 		del self.parent.children[self.name]
 
 	def suffix(self):
-		"""Return the file extension"""
+		"""
+		Returns the file rightmost extension, for example `a.b.c.d → .d`
+
+		:rtype: string
+		"""
 		k = max(0, self.name.rfind('.'))
 		return self.name[k:]
 
 	def height(self):
-		"""Depth in the folder hierarchy from the filesystem root or from all the file drives"""
+		"""
+		Returns the depth in the folder hierarchy from the filesystem root or from all the file drives
+
+		:returns: filesystem depth
+		:rtype: integer
+		"""
 		d = self
 		val = -1
 		while d:
@@ -254,17 +336,23 @@ def height(self):
 		return val
 
 	def listdir(self):
-		"""List the folder contents"""
+		"""
+		Lists the folder contents
+
+		:returns: list of file/folder names ordered alphabetically
+		:rtype: list of string
+		"""
 		lst = Utils.listdir(self.abspath())
 		lst.sort()
 		return lst
 
 	def mkdir(self):
 		"""
-		Create a folder represented by this node, creating intermediate nodes as needed
-		An exception will be raised only when the folder cannot possibly exist there
+		Creates a folder represented by this node. Intermediate folders are created as needed.
+
+		:raises: :py:class:`waflib.Errors.WafError` when the folder is missing
 		"""
-		if getattr(self, 'cache_isdir', None):
+		if self.isdir():
 			return
 
 		try:
@@ -278,26 +366,31 @@ def mkdir(self):
 			except OSError:
 				pass
 
-			if not os.path.isdir(self.abspath()):
-				raise Errors.WafError('Could not create the directory %s' % self.abspath())
+			if not self.isdir():
+				raise Errors.WafError('Could not create the directory %r' % self)
 
 			try:
 				self.children
 			except AttributeError:
 				self.children = self.dict_class()
 
-		self.cache_isdir = True
-
 	def find_node(self, lst):
 		"""
-		Find a node on the file system (files or folders), create intermediate nodes as needed
+		Finds a node on the file system (files or folders), and creates the corresponding Node objects if it exists
 
-		:param lst: path
+		:param lst: relative path
 		:type lst: string or list of string
+		:returns: The corresponding Node object or None if no entry was found on the filesystem
+		:rtype: :py:class:´waflib.Node.Node´
 		"""
 
 		if isinstance(lst, str):
-			lst = [x for x in split_path(lst) if x and x != '.']
+			lst = [x for x in Utils.split_path(lst) if x and x != '.']
+
+		if lst and lst[0].startswith('\\\\') and not self.parent:
+			node = self.ctx.root.make_node(lst[0])
+			node.cache_isdir = True
+			return node.find_node(lst[1:])
 
 		cur = self
 		for x in lst:
@@ -318,38 +411,26 @@ def find_node(self, lst):
 
 			# optimistic: create the node first then look if it was correct to do so
 			cur = self.__class__(x, cur)
-			try:
-				os.stat(cur.abspath())
-			except OSError:
+			if not cur.exists():
 				cur.evict()
 				return None
 
-		ret = cur
-
-		try:
-			os.stat(ret.abspath())
-		except OSError:
-			ret.evict()
+		if not cur.exists():
+			cur.evict()
 			return None
 
-		try:
-			while not getattr(cur.parent, 'cache_isdir', None):
-				cur = cur.parent
-				cur.cache_isdir = True
-		except AttributeError:
-			pass
-
-		return ret
+		return cur
 
 	def make_node(self, lst):
 		"""
-		Find or create a node without looking on the filesystem
+		Returns or creates a Node object corresponding to the input path without considering the filesystem.
 
-		:param lst: path
+		:param lst: relative path
 		:type lst: string or list of string
+		:rtype: :py:class:´waflib.Node.Node´
 		"""
 		if isinstance(lst, str):
-			lst = [x for x in split_path(lst) if x and x != '.']
+			lst = [x for x in Utils.split_path(lst) if x and x != '.']
 
 		cur = self
 		for x in lst:
@@ -357,24 +438,27 @@ def make_node(self, lst):
 				cur = cur.parent or cur
 				continue
 
-			if getattr(cur, 'children', {}):
-				if x in cur.children:
-					cur = cur.children[x]
-					continue
-			else:
+			try:
+				cur = cur.children[x]
+			except AttributeError:
 				cur.children = self.dict_class()
+			except KeyError:
+				pass
+			else:
+				continue
 			cur = self.__class__(x, cur)
 		return cur
 
 	def search_node(self, lst):
 		"""
-		Search for a node without looking on the filesystem
+		Returns a Node previously defined in the data structure. The filesystem is not considered.
 
-		:param lst: path
+		:param lst: relative path
 		:type lst: string or list of string
+		:rtype: :py:class:´waflib.Node.Node´ or None if there is no entry in the Node datastructure
 		"""
 		if isinstance(lst, str):
-			lst = [x for x in split_path(lst) if x and x != '.']
+			lst = [x for x in Utils.split_path(lst) if x and x != '.']
 
 		cur = self
 		for x in lst:
@@ -398,8 +482,9 @@ def build(bld):
 
 		:param node: path to use as a reference
 		:type node: :py:class:`waflib.Node.Node`
+		:returns: a relative path or an absolute one if that is better
+		:rtype: string
 		"""
-
 		c1 = self
 		c2 = node
 
@@ -419,7 +504,7 @@ def build(bld):
 			c2 = c2.parent
 			c2h -= 1
 
-		while id(c1) != id(c2):
+		while not c1 is c2:
 			lst.append(c1.name)
 			up += 1
 
@@ -427,17 +512,17 @@ def build(bld):
 			c2 = c2.parent
 
 		if c1.parent:
-			for i in range(up):
-				lst.append('..')
+			lst.extend(['..'] * up)
+			lst.reverse()
+			return os.sep.join(lst) or '.'
 		else:
-			if lst and not Utils.is_win32:
-				lst.append('')
-		lst.reverse()
-		return os.sep.join(lst) or '.'
+			return self.abspath()
 
 	def abspath(self):
 		"""
-		Absolute path. A cache is kept in the context as ``cache_node_abspath``
+		Returns the absolute path. A cache is kept in the context as ``cache_node_abspath``
+
+		:rtype: string
 		"""
 		try:
 			return self.cache_abspath
@@ -471,7 +556,7 @@ def abspath(self):
 
 	def is_child_of(self, node):
 		"""
-		Does this node belong to the subtree node?::
+		Returns whether the object belongs to a subtree of the input node::
 
 			def build(bld):
 				node = bld.path.find_node('wscript')
@@ -479,17 +564,18 @@ def build(bld):
 
 		:param node: path to use as a reference
 		:type node: :py:class:`waflib.Node.Node`
+		:rtype: bool
 		"""
 		p = self
 		diff = self.height() - node.height()
 		while diff > 0:
 			diff -= 1
 			p = p.parent
-		return id(p) == id(node)
+		return p is node
 
-	def ant_iter(self, accept=None, maxdepth=25, pats=[], dir=False, src=True, remove=True):
+	def ant_iter(self, accept=None, maxdepth=25, pats=[], dir=False, src=True, remove=True, quiet=False):
 		"""
-		Semi-private and recursive method used by ant_glob.
+		Recursive method used by :py:meth:`waflib.Node.ant_glob`.
 
 		:param accept: function used for accepting/rejecting a node, returns the patterns that can be still accepted in recursion
 		:type accept: function
@@ -503,6 +589,10 @@ def ant_iter(self, accept=None, maxdepth=25, pats=[], dir=False, src=True, remov
 		:type src: bool
 		:param remove: remove files/folders that do not exist (True by default)
 		:type remove: bool
+		:param quiet: disable build directory traversal warnings (verbose mode)
+		:type quiet: bool
+		:returns: A generator object to iterate from
+		:rtype: iterator
 		"""
 		dircont = self.listdir()
 		dircont.sort()
@@ -523,25 +613,23 @@ def ant_iter(self, accept=None, maxdepth=25, pats=[], dir=False, src=True, remov
 
 				node = self.make_node([name])
 
-				isdir = os.path.isdir(node.abspath())
+				isdir = node.isdir()
 				if accepted:
 					if isdir:
 						if dir:
 							yield node
-					else:
-						if src:
-							yield node
+					elif src:
+						yield node
 
-				if getattr(node, 'cache_isdir', None) or isdir:
+				if isdir:
 					node.cache_isdir = True
 					if maxdepth:
-						for k in node.ant_iter(accept=accept, maxdepth=maxdepth - 1, pats=npats, dir=dir, src=src, remove=remove):
+						for k in node.ant_iter(accept=accept, maxdepth=maxdepth - 1, pats=npats, dir=dir, src=src, remove=remove, quiet=quiet):
 							yield k
-		raise StopIteration
 
 	def ant_glob(self, *k, **kw):
 		"""
-		This method is used for finding files across folders. It behaves like ant patterns:
+		Finds files across folders and returns Node objects:
 
 		* ``**/*`` find all files recursively
 		* ``**/*.class`` find all files ending by .class
@@ -550,14 +638,51 @@ def ant_glob(self, *k, **kw):
 		For example::
 
 			def configure(cfg):
-				cfg.path.ant_glob('**/*.cpp') # find all .cpp files
-				cfg.root.ant_glob('etc/*.txt') # using the filesystem root can be slow
-				cfg.path.ant_glob('*.cpp', excl=['*.c'], src=True, dir=False)
+				# find all .cpp files
+				cfg.path.ant_glob('**/*.cpp')
+				# find particular files from the root filesystem (can be slow)
+				cfg.root.ant_glob('etc/*.txt')
+				# simple exclusion rule example
+				cfg.path.ant_glob('*.c*', excl=['*.c'], src=True, dir=False)
+
+		For more information about the patterns, consult http://ant.apache.org/manual/dirtasks.html
+		Please remember that the '..' sequence does not represent the parent directory::
+
+			def configure(cfg):
+				cfg.path.ant_glob('../*.h') # incorrect
+				cfg.path.parent.ant_glob('*.h') # correct
+
+		The Node structure is itself a filesystem cache, so certain precautions must
+		be taken while matching files in the build or installation phases.
+		Nodes objects that do have a corresponding file or folder are garbage-collected by default.
+		This garbage collection is usually required to prevent returning files that do not
+		exist anymore. Yet, this may also remove Node objects of files that are yet-to-be built.
+
+		This typically happens when trying to match files in the build directory,
+		but there are also cases when files are created in the source directory.
+		Run ``waf -v`` to display any warnings, and try consider passing ``remove=False``
+		when matching files in the build directory.
 
-		For more information see http://ant.apache.org/manual/dirtasks.html
+		Since ant_glob can traverse both source and build folders, it is a best practice
+		to call this method only from the most specific build node::
+
+			def build(bld):
+				# traverses the build directory, may need ``remove=False``:
+				bld.path.ant_glob('project/dir/**/*.h')
+				# better, no accidental build directory traversal:
+				bld.path.find_node('project/dir').ant_glob('**/*.h') # best
+
+		In addition, files and folders are listed immediately. When matching files in the
+		build folders, consider passing ``generator=True`` so that the generator object
+		returned can defer computation to a later stage. For example::
+
+			def build(bld):
+				bld(rule='tar xvf ${SRC}', source='arch.tar')
+				bld.add_group()
+				gen = bld.bldnode.ant_glob("*.h", generator=True, remove=True)
+				# files will be listed only after the arch.tar is unpacked
+				bld(rule='ls ${SRC}', source=gen, name='XYZ')
 
-		The nodes that correspond to files and folders that do not exist will be removed. To prevent this
-		behaviour, pass 'remove=False'
 
 		:param incl: ant patterns or list of patterns to include
 		:type incl: string or list of strings
@@ -567,126 +692,90 @@ def configure(cfg):
 		:type dir: bool
 		:param src: return files (True by default)
 		:type src: bool
-		:param remove: remove files/folders that do not exist (True by default)
-		:type remove: bool
 		:param maxdepth: maximum depth of recursion
 		:type maxdepth: int
 		:param ignorecase: ignore case while matching (False by default)
 		:type ignorecase: bool
+		:param generator: Whether to evaluate the Nodes lazily
+		:type generator: bool
+		:param remove: remove files/folders that do not exist (True by default)
+		:type remove: bool
+		:param quiet: disable build directory traversal warnings (verbose mode)
+		:type quiet: bool
+		:returns: The corresponding Node objects as a list or as a generator object (generator=True)
+		:rtype: by default, list of :py:class:`waflib.Node.Node` instances
 		"""
-
 		src = kw.get('src', True)
-		dir = kw.get('dir', False)
-
+		dir = kw.get('dir')
 		excl = kw.get('excl', exclude_regs)
 		incl = k and k[0] or kw.get('incl', '**')
-		reflags = kw.get('ignorecase', 0) and re.I
-
-		def to_pat(s):
-			lst = Utils.to_list(s)
-			ret = []
-			for x in lst:
-				x = x.replace('\\', '/').replace('//', '/')
-				if x.endswith('/'):
-					x += '**'
-				lst2 = x.split('/')
-				accu = []
-				for k in lst2:
-					if k == '**':
-						accu.append(k)
-					else:
-						k = k.replace('.', '[.]').replace('*','.*').replace('?', '.').replace('+', '\\+')
-						k = '^%s$' % k
-						try:
-							#print "pattern", k
-							accu.append(re.compile(k, flags=reflags))
-						except Exception as e:
-							raise Errors.WafError("Invalid pattern: %s" % k, e)
-				ret.append(accu)
-			return ret
-
-		def filtre(name, nn):
-			ret = []
-			for lst in nn:
-				if not lst:
-					pass
-				elif lst[0] == '**':
-					ret.append(lst)
-					if len(lst) > 1:
-						if lst[1].match(name):
-							ret.append(lst[2:])
-					else:
-						ret.append([])
-				elif lst[0].match(name):
-					ret.append(lst[1:])
-			return ret
-
-		def accept(name, pats):
-			nacc = filtre(name, pats[0])
-			nrej = filtre(name, pats[1])
-			if [] in nrej:
-				nacc = []
-			return [nacc, nrej]
-
-		ret = [x for x in self.ant_iter(accept=accept, pats=[to_pat(incl), to_pat(excl)], maxdepth=kw.get('maxdepth', 25), dir=dir, src=src, remove=kw.get('remove', True))]
-		if kw.get('flat', False):
-			return ' '.join([x.path_from(self) for x in ret])
+		remove = kw.get('remove', True)
+		maxdepth = kw.get('maxdepth', 25)
+		ignorecase = kw.get('ignorecase', False)
+		quiet = kw.get('quiet', False)
+		pats = (ant_matcher(incl, ignorecase), ant_matcher(excl, ignorecase))
 
-		return ret
+		if kw.get('generator'):
+			return Utils.lazy_generator(self.ant_iter, (ant_sub_matcher, maxdepth, pats, dir, src, remove, quiet))
+
+		it = self.ant_iter(ant_sub_matcher, maxdepth, pats, dir, src, remove, quiet)
+		if kw.get('flat'):
+			# returns relative paths as a space-delimited string
+			# prefer Node objects whenever possible
+			return ' '.join(x.path_from(self) for x in it)
+		return list(it)
 
-	# --------------------------------------------------------------------------------
-	# the following methods require the source/build folders (bld.srcnode/bld.bldnode)
-	# using a subclass is a possibility, but is that really necessary?
-	# --------------------------------------------------------------------------------
+	# ----------------------------------------------------------------------------
+	# the methods below require the source/build folders (bld.srcnode/bld.bldnode)
 
 	def is_src(self):
 		"""
-		True if the node is below the source directory
-		note: !is_src does not imply is_bld()
+		Returns True if the node is below the source directory. Note that ``!is_src() ≠ is_bld()``
 
 		:rtype: bool
 		"""
 		cur = self
-		x = id(self.ctx.srcnode)
-		y = id(self.ctx.bldnode)
+		x = self.ctx.srcnode
+		y = self.ctx.bldnode
 		while cur.parent:
-			if id(cur) == y:
+			if cur is y:
 				return False
-			if id(cur) == x:
+			if cur is x:
 				return True
 			cur = cur.parent
 		return False
 
 	def is_bld(self):
 		"""
-		True if the node is below the build directory
-		note: !is_bld does not imply is_src
+		Returns True if the node is below the build directory. Note that ``!is_bld() ≠ is_src()``
 
 		:rtype: bool
 		"""
 		cur = self
-		y = id(self.ctx.bldnode)
+		y = self.ctx.bldnode
 		while cur.parent:
-			if id(cur) == y:
+			if cur is y:
 				return True
 			cur = cur.parent
 		return False
 
 	def get_src(self):
 		"""
-		Return the equivalent src node (or self if not possible)
+		Returns the corresponding Node object in the source directory (or self if already
+		under the source directory). Use this method only if the purpose is to create
+		a Node object (this is common with folders but not with files, see ticket 1937)
 
 		:rtype: :py:class:`waflib.Node.Node`
 		"""
 		cur = self
-		x = id(self.ctx.srcnode)
-		y = id(self.ctx.bldnode)
+		x = self.ctx.srcnode
+		y = self.ctx.bldnode
 		lst = []
 		while cur.parent:
-			if id(cur) == y:
+			if cur is y:
 				lst.reverse()
-				return self.ctx.srcnode.make_node(lst)
-			if id(cur) == x:
+				return x.make_node(lst)
+			if cur is x:
 				return self
 			lst.append(cur.name)
 			cur = cur.parent
@@ -694,18 +783,20 @@ def get_src(self):
 
 	def get_bld(self):
 		"""
-		Return the equivalent bld node (or self if not possible)
+		Return the corresponding Node object in the build directory (or self if already
+		under the build directory). Use this method only if the purpose is to create
+		a Node object (this is common with folders but not with files, see ticket 1937)
 
 		:rtype: :py:class:`waflib.Node.Node`
 		"""
 		cur = self
-		x = id(self.ctx.srcnode)
-		y = id(self.ctx.bldnode)
+		x = self.ctx.srcnode
+		y = self.ctx.bldnode
 		lst = []
 		while cur.parent:
-			if id(cur) == y:
+			if cur is y:
 				return self
-			if id(cur) == x:
+			if cur is x:
 				lst.reverse()
 				return self.ctx.bldnode.make_node(lst)
 			lst.append(cur.name)
@@ -718,75 +809,66 @@ def get_bld(self):
 
 	def find_resource(self, lst):
 		"""
-		Try to find a declared build node or a source file
+		Use this method in the build phase to find source files corresponding to the relative path given.
 
-		:param lst: path
+		First it looks up the Node data structure to find any declared Node object in the build directory.
+		If None is found, it then considers the filesystem in the source directory.
+
+		:param lst: relative path
 		:type lst: string or list of string
+		:returns: the corresponding Node object or None
+		:rtype: :py:class:`waflib.Node.Node`
 		"""
 		if isinstance(lst, str):
-			lst = [x for x in split_path(lst) if x and x != '.']
+			lst = [x for x in Utils.split_path(lst) if x and x != '.']
 
 		node = self.get_bld().search_node(lst)
 		if not node:
-			self = self.get_src()
-			node = self.find_node(lst)
-		if node:
-			if os.path.isdir(node.abspath()):
-				return None
+			node = self.get_src().find_node(lst)
+		if node and node.isdir():
+			return None
 		return node
 
 	def find_or_declare(self, lst):
 		"""
-		if 'self' is in build directory, try to return an existing node
-		if no node is found, go to the source directory
-		try to find an existing node in the source directory
-		if no node is found, create it in the build directory
+		Use this method in the build phase to declare output files which
+		are meant to be written in the build directory.
 
-		:param lst: path
+		This method creates the Node object and its parent folder
+		as needed.
+
+		:param lst: relative path
 		:type lst: string or list of string
 		"""
-		if isinstance(lst, str):
-			lst = [x for x in split_path(lst) if x and x != '.']
-
-		node = self.get_bld().search_node(lst)
-		if node:
-			if not os.path.isfile(node.abspath()):
-				node.sig = None
-				node.parent.mkdir()
-			return node
-		self = self.get_src()
-		node = self.find_node(lst)
-		if node:
-			if not os.path.isfile(node.abspath()):
-				node.sig = None
-				node.parent.mkdir()
-			return node
-		node = self.get_bld().make_node(lst)
+		if isinstance(lst, str) and os.path.isabs(lst):
+			node = self.ctx.root.make_node(lst)
+		else:
+			node = self.get_bld().make_node(lst)
 		node.parent.mkdir()
 		return node
 
 	def find_dir(self, lst):
 		"""
-		Search for a folder in the filesystem
+		Searches for a folder on the filesystem (see :py:meth:`waflib.Node.Node.find_node`)
 
-		:param lst: path
+		:param lst: relative path
 		:type lst: string or list of string
+		:returns: The corresponding Node object or None if there is no such folder
+		:rtype: :py:class:`waflib.Node.Node`
 		"""
 		if isinstance(lst, str):
-			lst = [x for x in split_path(lst) if x and x != '.']
+			lst = [x for x in Utils.split_path(lst) if x and x != '.']
 
 		node = self.find_node(lst)
-		try:
-			if not os.path.isdir(node.abspath()):
-				return None
-		except (OSError, AttributeError):
-			# the node might be None, and raise an AttributeError
+		if node and not node.isdir():
 			return None
 		return node
 
 	# helpers for building things
 	def change_ext(self, ext, ext_in=None):
 		"""
+		Declares a build node with a distinct extension; this is uses :py:meth:`waflib.Node.Node.find_or_declare`
+
 		:return: A build node of the same path, but with a different extension
 		:rtype: :py:class:`waflib.Node.Node`
 		"""
@@ -803,39 +885,79 @@ def change_ext(self, ext, ext_in=None):
 		return self.parent.find_or_declare([name])
 
 	def bldpath(self):
-		"Path seen from the build directory default/src/foo.cpp"
+		"""
+		Returns the relative path seen from the build directory ``src/foo.cpp``
+
+		:rtype: string
+		"""
 		return self.path_from(self.ctx.bldnode)
 
 	def srcpath(self):
-		"Path seen from the source directory ../src/foo.cpp"
+		"""
+		Returns the relative path seen from the source directory ``../src/foo.cpp``
+
+		:rtype: string
+		"""
 		return self.path_from(self.ctx.srcnode)
 
 	def relpath(self):
-		"If a file in the build directory, bldpath, else srcpath"
+		"""
+		If a file in the build directory, returns :py:meth:`waflib.Node.Node.bldpath`,
+		else returns :py:meth:`waflib.Node.Node.srcpath`
+
+		:rtype: string
+		"""
 		cur = self
-		x = id(self.ctx.bldnode)
+		x = self.ctx.bldnode
 		while cur.parent:
-			if id(cur) == x:
+			if cur is x:
 				return self.bldpath()
 			cur = cur.parent
 		return self.srcpath()
 
 	def bld_dir(self):
-		"Build path without the file name"
+		"""
+		Equivalent to self.parent.bldpath()
+
+		:rtype: string
+		"""
 		return self.parent.bldpath()
 
+	def h_file(self):
+		"""
+		See :py:func:`waflib.Utils.h_file`
+
+		:return: a hash representing the file contents
+		:rtype: string or bytes
+		"""
+		return Utils.h_file(self.abspath())
+
 	def get_bld_sig(self):
 		"""
-		Node signature, assuming the file is in the build directory
+		Returns a signature (see :py:meth:`waflib.Node.Node.h_file`) for the purpose
+		of build dependency calculation. This method uses a per-context cache.
+
+		:return: a hash representing the object contents
+		:rtype: string or bytes
 		"""
+		# previous behaviour can be set by returning self.ctx.node_sigs[self] when a build node
 		try:
-			return self.cache_sig
+			cache = self.ctx.cache_sig
 		except AttributeError:
-			pass
-
-		if not self.is_bld() or self.ctx.bldnode is self.ctx.srcnode:
-			self.sig = Utils.h_file(self.abspath())
-		self.cache_sig = ret = self.sig
+			cache = self.ctx.cache_sig = {}
+		try:
+			ret = cache[self]
+		except KeyError:
+			p = self.abspath()
+			try:
+				ret = cache[self] = self.h_file()
+			except EnvironmentError:
+				if self.isdir():
+					# allow folders as build nodes, do not use the creation time
+					st = os.stat(p)
+					ret = cache[self] = Utils.h_list([p, st.st_ino, st.st_mode])
+					return ret
+				raise
 		return ret
 
 pickle_lock = Utils.threading.Lock()
diff --git a/waflib/Options.py b/waflib/Options.py
index fe2121e3..ad802d4b 100644
--- a/waflib/Options.py
+++ b/waflib/Options.py
@@ -1,66 +1,75 @@
 #!/usr/bin/env python
 # encoding: utf-8
 # Scott Newton, 2005 (scottn)
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 
 """
 Support for waf command-line options
 
-Provides default command-line options,
-as well as custom ones, used by the ``options`` wscript function.
-
+Provides default and command-line options, as well the command
+that reads the ``options`` wscript function.
 """
 
 import os, tempfile, optparse, sys, re
-from waflib import Logs, Utils, Context
+from waflib import Logs, Utils, Context, Errors
 
-cmds = 'distclean configure build install clean uninstall check dist distcheck'.split()
+options = optparse.Values()
 """
-Constant representing the default waf commands displayed in::
-
-	$ waf --help
-
-"""
-
-options = {}
-"""
-A dictionary representing the command-line options::
+A global dictionary representing user-provided command-line options::
 
 	$ waf --foo=bar
-
 """
 
 commands = []
 """
-List of commands to execute extracted from the command-line. This list is consumed during the execution, see :py:func:`waflib.Scripting.run_commands`.
+List of commands to execute extracted from the command-line. This list
+is consumed during the execution by :py:func:`waflib.Scripting.run_commands`.
 """
 
 envvars = []
 """
 List of environment variable declarations placed after the Waf executable name.
-These are detected by searching for "=" in the rest arguments.
+These are detected by searching for "=" in the remaining arguments.
+You probably do not want to use this.
 """
 
 lockfile = os.environ.get('WAFLOCK', '.lock-waf_%s_build' % sys.platform)
-platform = Utils.unversioned_sys_platform()
-
+"""
+Name of the lock file that marks a project as configured
+"""
 
 class opt_parser(optparse.OptionParser):
 	"""
 	Command-line options parser.
 	"""
-	def __init__(self, ctx):
-		optparse.OptionParser.__init__(self, conflict_handler="resolve", version='waf %s (%s)' % (Context.WAFVERSION, Context.WAFREVISION))
-
+	def __init__(self, ctx, allow_unknown=False):
+		optparse.OptionParser.__init__(self, conflict_handler='resolve', add_help_option=False,
+			version='waf %s (%s)' % (Context.WAFVERSION, Context.WAFREVISION))
 		self.formatter.width = Logs.get_term_cols()
 		self.ctx = ctx
+		self.allow_unknown = allow_unknown
+
+	def _process_args(self, largs, rargs, values):
+		"""
+		Custom _process_args to allow unknown options according to the allow_unknown status
+		"""
+		while rargs:
+			try:
+				optparse.OptionParser._process_args(self,largs,rargs,values)
+			except (optparse.BadOptionError, optparse.AmbiguousOptionError) as e:
+				if self.allow_unknown:
+					largs.append(e.opt_str)
+				else:
+					self.error(str(e))
 
 	def print_usage(self, file=None):
 		return self.print_help(file)
 
 	def get_usage(self):
 		"""
-		Return the message to print on ``waf --help``
+		Builds the message to print on ``waf --help``
+
+		:rtype: string
 		"""
 		cmds_str = {}
 		for cls in Context.classes:
@@ -96,10 +105,9 @@ def get_usage(self):
 
 class OptionsContext(Context.Context):
 	"""
-	Collect custom options from wscript files and parses the command line.
-	Set the global :py:const:`waflib.Options.commands` and :py:const:`waflib.Options.options` values.
+	Collects custom options from wscript files and parses the command line.
+	Sets the global :py:const:`waflib.Options.commands` and :py:const:`waflib.Options.options` values.
 	"""
-
 	cmd = 'options'
 	fun = 'options'
 
@@ -114,11 +122,18 @@ def __init__(self, **kw):
 		jobs = self.jobs()
 		p = self.add_option
 		color = os.environ.get('NOCOLOR', '') and 'no' or 'auto'
+		if os.environ.get('CLICOLOR', '') == '0':
+			color = 'no'
+		elif os.environ.get('CLICOLOR_FORCE', '') == '1':
+			color = 'yes'
 		p('-c', '--color',    dest='colors',  default=color, action='store', help='whether to use colors (yes/no/auto) [default: auto]', choices=('yes', 'no', 'auto'))
-		p('-j', '--jobs',     dest='jobs',    default=jobs, type='int', help='amount of parallel jobs (%r)' % jobs)
+		p('-j', '--jobs',     dest='jobs',    default=jobs,  type='int', help='amount of parallel jobs (%r)' % jobs)
 		p('-k', '--keep',     dest='keep',    default=0,     action='count', help='continue despite errors (-kk to try harder)')
 		p('-v', '--verbose',  dest='verbose', default=0,     action='count', help='verbosity level -v -vv or -vvv [default: 0]')
 		p('--zones',          dest='zones',   default='',    action='store', help='debugging zones (task_gen, deps, tasks, etc)')
+		p('--profile',        dest='profile', default=0,     action='store_true', help=optparse.SUPPRESS_HELP)
+		p('--pdb',            dest='pdb',     default=0,     action='store_true', help=optparse.SUPPRESS_HELP)
+		p('-h', '--help',     dest='whelp',   default=0,     action='store_true', help="show this help message and exit")
 
 		gr = self.add_option_group('Configuration options')
 		self.option_groups['configure options'] = gr
@@ -132,7 +147,7 @@ def __init__(self, **kw):
 
 		default_prefix = getattr(Context.g_module, 'default_prefix', os.environ.get('PREFIX'))
 		if not default_prefix:
-			if platform == 'win32':
+			if Utils.unversioned_sys_platform() == 'win32':
 				d = tempfile.gettempdir()
 				default_prefix = d[0].upper() + d[1:]
 				# win32 preserves the case, but gettempdir does not
@@ -161,8 +176,8 @@ def __init__(self, **kw):
 
 	def jobs(self):
 		"""
-		Find the amount of cpu cores to set the default amount of tasks executed in parallel. At
-		runtime the options can be obtained from :py:const:`waflib.Options.options` ::
+		Finds the optimal amount of cpu cores to use for parallel jobs.
+		At runtime the options can be obtained from :py:const:`waflib.Options.options` ::
 
 			from waflib.Options import options
 			njobs = options.jobs
@@ -185,7 +200,7 @@ def jobs(self):
 				if not count and os.name not in ('nt', 'java'):
 					try:
 						tmp = self.cmd_and_log(['sysctl', '-n', 'hw.ncpu'], quiet=0)
-					except Exception:
+					except Errors.WafError:
 						pass
 					else:
 						if re.match('^[0-9]+$', tmp):
@@ -198,21 +213,25 @@ def jobs(self):
 
 	def add_option(self, *k, **kw):
 		"""
-		Wrapper for optparse.add_option::
+		Wraps ``optparse.add_option``::
 
 			def options(ctx):
-				ctx.add_option('-u', '--use', dest='use', default=False, action='store_true',
-					help='a boolean option')
+				ctx.add_option('-u', '--use', dest='use', default=False,
+					action='store_true', help='a boolean option')
+
+		:rtype: optparse option object
 		"""
 		return self.parser.add_option(*k, **kw)
 
 	def add_option_group(self, *k, **kw):
 		"""
-		Wrapper for optparse.add_option_group::
+		Wraps ``optparse.add_option_group``::
 
 			def options(ctx):
 				gr = ctx.add_option_group('some options')
 				gr.add_option('-u', '--use', dest='use', default=False, action='store_true')
+
+		:rtype: optparse option group object
 		"""
 		try:
 			gr = self.option_groups[k[0]]
@@ -223,13 +242,14 @@ def options(ctx):
 
 	def get_option_group(self, opt_str):
 		"""
-		Wrapper for optparse.get_option_group::
+		Wraps ``optparse.get_option_group``::
 
 			def options(ctx):
 				gr = ctx.get_option_group('configure options')
 				gr.add_option('-o', '--out', action='store', default='',
 					help='build dir for the project', dest='out')
 
+		:rtype: optparse option group object
 		"""
 		try:
 			return self.option_groups[opt_str]
@@ -239,35 +259,84 @@ def options(ctx):
 					return group
 			return None
 
-	def parse_args(self, _args=None):
-		"""
-		Parse arguments from a list (not bound to the command-line).
+	def sanitize_path(self, path, cwd=None):
+		if not cwd:
+			cwd = Context.launch_dir
+		p = os.path.expanduser(path)
+		p = os.path.join(cwd, p)
+		p = os.path.normpath(p)
+		p = os.path.abspath(p)
+		return p
 
-		:param _args: arguments
-		:type _args: list of strings
+	def parse_cmd_args(self, _args=None, cwd=None, allow_unknown=False):
+		"""
+		Just parse the arguments
 		"""
-		global options, commands, envvars
+		self.parser.allow_unknown = allow_unknown
 		(options, leftover_args) = self.parser.parse_args(args=_args)
-
+		envvars = []
+		commands = []
 		for arg in leftover_args:
 			if '=' in arg:
 				envvars.append(arg)
-			else:
+			elif arg != 'options':
 				commands.append(arg)
 
-		if options.destdir:
-			options.destdir = Utils.sane_path(options.destdir)
+		for name in 'top out destdir prefix bindir libdir'.split():
+			# those paths are usually expanded from Context.launch_dir
+			if getattr(options, name, None):
+				path = self.sanitize_path(getattr(options, name), cwd)
+				setattr(options, name, path)
+		return options, commands, envvars
+
+	def init_module_vars(self, arg_options, arg_commands, arg_envvars):
+		options.__dict__.clear()
+		del commands[:]
+		del envvars[:]
+
+		options.__dict__.update(arg_options.__dict__)
+		commands.extend(arg_commands)
+		envvars.extend(arg_envvars)
 
+		for var in envvars:
+			(name, value) = var.split('=', 1)
+			os.environ[name.strip()] = value
+
+	def init_logs(self, options, commands, envvars):
+		Logs.verbose = options.verbose
 		if options.verbose >= 1:
 			self.load('errcheck')
 
 		colors = {'yes' : 2, 'auto' : 1, 'no' : 0}[options.colors]
 		Logs.enable_colors(colors)
 
+		if options.zones:
+			Logs.zones = options.zones.split(',')
+			if not Logs.verbose:
+				Logs.verbose = 1
+		elif Logs.verbose > 0:
+			Logs.zones = ['runner']
+		if Logs.verbose > 2:
+			Logs.zones = ['*']
+
+	def parse_args(self, _args=None):
+		"""
+		Parses arguments from a list which is not necessarily the command-line.
+		Initializes the module variables options, commands and envvars
+		If help is requested, prints it and exit the application
+
+		:param _args: arguments
+		:type _args: list of strings
+		"""
+		options, commands, envvars = self.parse_cmd_args()
+		self.init_logs(options, commands, envvars)
+		self.init_module_vars(options, commands, envvars)
+
 	def execute(self):
 		"""
 		See :py:func:`waflib.Context.Context.execute`
 		"""
 		super(OptionsContext, self).execute()
 		self.parse_args()
+		Utils.alloc_process_pool(options.jobs)
 
diff --git a/waflib/Runner.py b/waflib/Runner.py
index b3087292..261084d2 100644
--- a/waflib/Runner.py
+++ b/waflib/Runner.py
@@ -1,98 +1,125 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 Runner.py: Task scheduling and execution
-
 """
 
-import random, atexit
+import heapq, traceback
 try:
-	from queue import Queue
+	from queue import Queue, PriorityQueue
 except ImportError:
 	from Queue import Queue
+	try:
+		from Queue import PriorityQueue
+	except ImportError:
+		class PriorityQueue(Queue):
+			def _init(self, maxsize):
+				self.maxsize = maxsize
+				self.queue = []
+			def _put(self, item):
+				heapq.heappush(self.queue, item)
+			def _get(self):
+				return heapq.heappop(self.queue)
+
 from waflib import Utils, Task, Errors, Logs
 
-GAP = 10
+GAP = 5
 """
-Wait for free tasks if there are at least ``GAP * njobs`` in queue
+Wait for at least ``GAP * njobs`` before trying to enqueue more tasks to run
 """
 
-class TaskConsumer(Utils.threading.Thread):
-	"""
-	Task consumers belong to a pool of workers
+class PriorityTasks(object):
+	def __init__(self):
+		self.lst = []
+	def __len__(self):
+		return len(self.lst)
+	def __iter__(self):
+		return iter(self.lst)
+	def clear(self):
+		self.lst = []
+	def append(self, task):
+		heapq.heappush(self.lst, task)
+	def appendleft(self, task):
+		"Deprecated, do not use"
+		heapq.heappush(self.lst, task)
+	def pop(self):
+		return heapq.heappop(self.lst)
+	def extend(self, lst):
+		if self.lst:
+			for x in lst:
+				self.append(x)
+		else:
+			if isinstance(lst, list):
+				self.lst = lst
+				heapq.heapify(lst)
+			else:
+				self.lst = lst.lst
 
-	They wait for tasks in the queue and then use ``task.process(...)``
+class Consumer(Utils.threading.Thread):
 	"""
-	def __init__(self):
+	Daemon thread object that executes a task. It shares a semaphore with
+	the coordinator :py:class:`waflib.Runner.Spawner`. There is one
+	instance per task to consume.
+	"""
+	def __init__(self, spawner, task):
 		Utils.threading.Thread.__init__(self)
-		self.ready = Queue()
+		self.task = task
+		"""Task to execute"""
+		self.spawner = spawner
+		"""Coordinator object"""
+		self.setDaemon(1)
+		self.start()
+	def run(self):
 		"""
-		Obtain :py:class:`waflib.Task.TaskBase` instances from this queue.
+		Processes a single task
 		"""
+		try:
+			if not self.spawner.master.stop:
+				self.spawner.master.process_task(self.task)
+		finally:
+			self.spawner.sem.release()
+			self.spawner.master.out.put(self.task)
+			self.task = None
+			self.spawner = None
+
+class Spawner(Utils.threading.Thread):
+	"""
+	Daemon thread that consumes tasks from :py:class:`waflib.Runner.Parallel` producer and
+	spawns a consuming thread :py:class:`waflib.Runner.Consumer` for each
+	:py:class:`waflib.Task.Task` instance.
+	"""
+	def __init__(self, master):
+		Utils.threading.Thread.__init__(self)
+		self.master = master
+		""":py:class:`waflib.Runner.Parallel` producer instance"""
+		self.sem = Utils.threading.Semaphore(master.numjobs)
+		"""Bounded semaphore that prevents spawning more than *n* concurrent consumers"""
 		self.setDaemon(1)
 		self.start()
-
 	def run(self):
 		"""
-		Loop over the tasks to execute
+		Spawns new consumers to execute tasks by delegating to :py:meth:`waflib.Runner.Spawner.loop`
 		"""
 		try:
 			self.loop()
 		except Exception:
+			# Python 2 prints unnecessary messages when shutting down
+			# we also want to stop the thread properly
 			pass
-
 	def loop(self):
 		"""
-		Obtain tasks from :py:attr:`waflib.Runner.TaskConsumer.ready` and call
-		:py:meth:`waflib.Task.TaskBase.process`. If the object is a function, execute it.
+		Consumes task objects from the producer; ends when the producer has no more
+		task to provide.
 		"""
+		master = self.master
 		while 1:
-			tsk = self.ready.get()
-			if not isinstance(tsk, Task.TaskBase):
-				tsk(self)
-			else:
-				tsk.process()
-
-pool = Queue()
-"""
-Pool of task consumer objects
-"""
-
-def get_pool():
-	"""
-	Obtain a task consumer from :py:attr:`waflib.Runner.pool`.
-	Do not forget to put it back by using :py:func:`waflib.Runner.put_pool`
-	and reset properly (original waiting queue).
-
-	:rtype: :py:class:`waflib.Runner.TaskConsumer`
-	"""
-	try:
-		return pool.get(False)
-	except Exception:
-		return TaskConsumer()
-
-def put_pool(x):
-	"""
-	Return a task consumer to the thread pool :py:attr:`waflib.Runner.pool`
-
-	:param x: task consumer object
-	:type x: :py:class:`waflib.Runner.TaskConsumer`
-	"""
-	pool.put(x)
-
-def _free_resources():
-	global pool
-	lst = []
-	while pool.qsize():
-		lst.append(pool.get())
-	for x in lst:
-		x.ready.put(None)
-	for x in lst:
-		x.join()
-	pool = None
-atexit.register(_free_resources)
+			task = master.ready.get()
+			self.sem.acquire()
+			if not master.stop:
+				task.log_display(task.generator.bld)
+			Consumer(self, task)
 
 class Parallel(object):
 	"""
@@ -106,7 +133,7 @@ def __init__(self, bld, j=2):
 
 		self.numjobs = j
 		"""
-		Number of consumers in the pool
+		Amount of parallel consumers to use
 		"""
 
 		self.bld = bld
@@ -114,19 +141,25 @@ def __init__(self, bld, j=2):
 		Instance of :py:class:`waflib.Build.BuildContext`
 		"""
 
-		self.outstanding = []
-		"""List of :py:class:`waflib.Task.TaskBase` that may be ready to be executed"""
+		self.outstanding = PriorityTasks()
+		"""Heap of :py:class:`waflib.Task.Task` that may be ready to be executed"""
+
+		self.postponed = PriorityTasks()
+		"""Heap of :py:class:`waflib.Task.Task` which are not ready to run for non-DAG reasons"""
 
-		self.frozen = []
-		"""List of :py:class:`waflib.Task.TaskBase` that cannot be executed immediately"""
+		self.incomplete = set()
+		"""List of :py:class:`waflib.Task.Task` waiting for dependent tasks to complete (DAG)"""
+
+		self.ready = PriorityQueue(0)
+		"""List of :py:class:`waflib.Task.Task` ready to be executed by consumers"""
 
 		self.out = Queue(0)
-		"""List of :py:class:`waflib.Task.TaskBase` returned by the task consumers"""
+		"""List of :py:class:`waflib.Task.Task` returned by the task consumers"""
 
 		self.count = 0
 		"""Amount of tasks that may be processed by :py:class:`waflib.Runner.TaskConsumer`"""
 
-		self.processed = 1
+		self.processed = 0
 		"""Amount of tasks processed"""
 
 		self.stop = False
@@ -139,33 +172,44 @@ def __init__(self, bld, j=2):
 		"""Task iterator which must give groups of parallelizable tasks when calling ``next()``"""
 
 		self.dirty = False
-		"""Flag to indicate that tasks have been executed, and that the build cache must be saved (call :py:meth:`waflib.Build.BuildContext.store`)"""
+		"""
+		Flag that indicates that the build cache must be saved when a task was executed
+		(calls :py:meth:`waflib.Build.BuildContext.store`)"""
+
+		self.revdeps = Utils.defaultdict(set)
+		"""
+		The reverse dependency graph of dependencies obtained from Task.run_after
+		"""
+
+		self.spawner = Spawner(self)
+		"""
+		Coordinating daemon thread that spawns thread consumers
+		"""
 
 	def get_next_task(self):
 		"""
-		Obtain the next task to execute.
+		Obtains the next Task instance to run
 
-		:rtype: :py:class:`waflib.Task.TaskBase`
+		:rtype: :py:class:`waflib.Task.Task`
 		"""
 		if not self.outstanding:
 			return None
-		return self.outstanding.pop(0)
+		return self.outstanding.pop()
 
 	def postpone(self, tsk):
 		"""
-		A task cannot be executed at this point, put it in the list :py:attr:`waflib.Runner.Parallel.frozen`.
+		Adds the task to the list :py:attr:`waflib.Runner.Parallel.postponed`.
+		The order is scrambled so as to consume as many tasks in parallel as possible.
 
-		:param tsk: task
-		:type tsk: :py:class:`waflib.Task.TaskBase`
+		:param tsk: task instance
+		:type tsk: :py:class:`waflib.Task.Task`
 		"""
-		if random.randint(0, 1):
-			self.frozen.insert(0, tsk)
-		else:
-			self.frozen.append(tsk)
+		self.postponed.append(tsk)
 
 	def refill_task_list(self):
 		"""
-		Put the next group of tasks to execute in :py:attr:`waflib.Runner.Parallel.outstanding`.
+		Pulls a next group of tasks to execute in :py:attr:`waflib.Runner.Parallel.outstanding`.
+		Ensures that all tasks in the current build group are complete before processing the next one.
 		"""
 		while self.count > self.numjobs * GAP:
 			self.get_out()
@@ -173,132 +217,224 @@ def refill_task_list(self):
 		while not self.outstanding:
 			if self.count:
 				self.get_out()
-			elif self.frozen:
+				if self.outstanding:
+					break
+			elif self.postponed:
 				try:
 					cond = self.deadlock == self.processed
 				except AttributeError:
 					pass
 				else:
 					if cond:
-						msg = 'check the build order for the tasks'
-						for tsk in self.frozen:
-							if not tsk.run_after:
-								msg = 'check the methods runnable_status'
-								break
+						# The most common reason is conflicting build order declaration
+						# for example: "X run_after Y" and "Y run_after X"
+						# Another can be changing "run_after" dependencies while the build is running
+						# for example: updating "tsk.run_after" in the "runnable_status" method
 						lst = []
-						for tsk in self.frozen:
-							lst.append('%s\t-> %r' % (repr(tsk), [id(x) for x in tsk.run_after]))
-						raise Errors.WafError('Deadlock detected: %s%s' % (msg, ''.join(lst)))
+						for tsk in self.postponed:
+							deps = [id(x) for x in tsk.run_after if not x.hasrun]
+							lst.append('%s\t-> %r' % (repr(tsk), deps))
+							if not deps:
+								lst.append('\n  task %r dependencies are done, check its *runnable_status*?' % id(tsk))
+						raise Errors.WafError('Deadlock detected: check the task build order%s' % ''.join(lst))
 				self.deadlock = self.processed
 
-			if self.frozen:
-				self.outstanding += self.frozen
-				self.frozen = []
+			if self.postponed:
+				self.outstanding.extend(self.postponed)
+				self.postponed.clear()
 			elif not self.count:
-				self.outstanding.extend(next(self.biter))
-				self.total = self.bld.total()
-				break
+				if self.incomplete:
+					for x in self.incomplete:
+						for k in x.run_after:
+							if not k.hasrun:
+								break
+						else:
+							# dependency added after the build started without updating revdeps
+							self.incomplete.remove(x)
+							self.outstanding.append(x)
+							break
+					else:
+						raise Errors.WafError('Broken revdeps detected on %r' % self.incomplete)
+				else:
+					tasks = next(self.biter)
+					ready, waiting = self.prio_and_split(tasks)
+					self.outstanding.extend(ready)
+					self.incomplete.update(waiting)
+					self.total = self.bld.total()
+					break
 
 	def add_more_tasks(self, tsk):
 		"""
-		Tasks may be added dynamically during the build by binding them to the task :py:attr:`waflib.Task.TaskBase.more_tasks`
+		If a task provides :py:attr:`waflib.Task.Task.more_tasks`, then the tasks contained
+		in that list are added to the current build and will be processed before the next build group.
 
-		:param tsk: task
-		:type tsk: :py:attr:`waflib.Task.TaskBase`
+		The priorities for dependent tasks are not re-calculated globally
+
+		:param tsk: task instance
+		:type tsk: :py:attr:`waflib.Task.Task`
 		"""
 		if getattr(tsk, 'more_tasks', None):
-			self.outstanding += tsk.more_tasks
+			more = set(tsk.more_tasks)
+			groups_done = set()
+			def iteri(a, b):
+				for x in a:
+					yield x
+				for x in b:
+					yield x
+
+			# Update the dependency tree
+			# this assumes that task.run_after values were updated
+			for x in iteri(self.outstanding, self.incomplete):
+				for k in x.run_after:
+					if isinstance(k, Task.TaskGroup):
+						if k not in groups_done:
+							groups_done.add(k)
+							for j in k.prev & more:
+								self.revdeps[j].add(k)
+					elif k in more:
+						self.revdeps[k].add(x)
+
+			ready, waiting = self.prio_and_split(tsk.more_tasks)
+			self.outstanding.extend(ready)
+			self.incomplete.update(waiting)
 			self.total += len(tsk.more_tasks)
 
+	def mark_finished(self, tsk):
+		def try_unfreeze(x):
+			# DAG ancestors are likely to be in the incomplete set
+			# This assumes that the run_after contents have not changed
+			# after the build starts, else a deadlock may occur
+			if x in self.incomplete:
+				# TODO remove dependencies to free some memory?
+				# x.run_after.remove(tsk)
+				for k in x.run_after:
+					if not k.hasrun:
+						break
+				else:
+					self.incomplete.remove(x)
+					self.outstanding.append(x)
+
+		if tsk in self.revdeps:
+			for x in self.revdeps[tsk]:
+				if isinstance(x, Task.TaskGroup):
+					x.prev.remove(tsk)
+					if not x.prev:
+						for k in x.next:
+							# TODO necessary optimization?
+							k.run_after.remove(x)
+							try_unfreeze(k)
+						# TODO necessary optimization?
+						x.next = []
+				else:
+					try_unfreeze(x)
+			del self.revdeps[tsk]
+
+		if hasattr(tsk, 'semaphore'):
+			sem = tsk.semaphore
+			sem.release(tsk)
+			while sem.waiting and not sem.is_locked():
+				# take a frozen task, make it ready to run
+				x = sem.waiting.pop()
+				self._add_task(x)
+
 	def get_out(self):
 		"""
-		Obtain one task returned from the task consumers, and update the task count. Add more tasks if necessary through
-		:py:attr:`waflib.Runner.Parallel.add_more_tasks`.
+		Waits for a Task that task consumers add to :py:attr:`waflib.Runner.Parallel.out` after execution.
+		Adds more Tasks if necessary through :py:attr:`waflib.Runner.Parallel.add_more_tasks`.
 
-		:rtype: :py:attr:`waflib.Task.TaskBase`
+		:rtype: :py:attr:`waflib.Task.Task`
 		"""
 		tsk = self.out.get()
 		if not self.stop:
 			self.add_more_tasks(tsk)
+		self.mark_finished(tsk)
+
 		self.count -= 1
 		self.dirty = True
 		return tsk
 
 	def add_task(self, tsk):
 		"""
-		Pass a task to a consumer.
+		Enqueue a Task to :py:attr:`waflib.Runner.Parallel.ready` so that consumers can run them.
 
-		:param tsk: task
-		:type tsk: :py:attr:`waflib.Task.TaskBase`
+		:param tsk: task instance
+		:type tsk: :py:attr:`waflib.Task.Task`
 		"""
-		try:
-			self.pool
-		except AttributeError:
-			self.init_task_pool()
+		# TODO change in waf 2.1
 		self.ready.put(tsk)
 
-	def init_task_pool(self):
-		# lazy creation, and set a common pool for all task consumers
-		pool = self.pool = [get_pool() for i in range(self.numjobs)]
-		self.ready = Queue(0)
-		def setq(consumer):
-			consumer.ready = self.ready
-		for x in pool:
-			x.ready.put(setq)
-		return pool
-
-	def free_task_pool(self):
-		# return the consumers, setting a different queue for each of them
-		def setq(consumer):
-			consumer.ready = Queue(0)
-			self.out.put(self)
-		try:
-			pool = self.pool
-		except AttributeError:
-			pass
+	def _add_task(self, tsk):
+		if hasattr(tsk, 'semaphore'):
+			sem = tsk.semaphore
+			try:
+				sem.acquire(tsk)
+			except IndexError:
+				sem.waiting.add(tsk)
+				return
+
+		self.count += 1
+		self.processed += 1
+		if self.numjobs == 1:
+			tsk.log_display(tsk.generator.bld)
+			try:
+				self.process_task(tsk)
+			finally:
+				self.out.put(tsk)
 		else:
-			for x in pool:
-				self.ready.put(setq)
-			for x in pool:
-				self.get_out()
-			for x in pool:
-				put_pool(x)
-			self.pool = []
+			self.add_task(tsk)
+
+	def process_task(self, tsk):
+		"""
+		Processes a task and attempts to stop the build in case of errors
+		"""
+		tsk.process()
+		if tsk.hasrun != Task.SUCCESS:
+			self.error_handler(tsk)
 
 	def skip(self, tsk):
+		"""
+		Mark a task as skipped/up-to-date
+		"""
 		tsk.hasrun = Task.SKIPPED
+		self.mark_finished(tsk)
+
+	def cancel(self, tsk):
+		"""
+		Mark a task as failed because of unsatisfiable dependencies
+		"""
+		tsk.hasrun = Task.CANCELED
+		self.mark_finished(tsk)
 
 	def error_handler(self, tsk):
 		"""
-		Called when a task cannot be executed. The flag :py:attr:`waflib.Runner.Parallel.stop` is set, unless
-		the build is executed with::
+		Called when a task cannot be executed. The flag :py:attr:`waflib.Runner.Parallel.stop` is set,
+		unless the build is executed with::
 
 			$ waf build -k
 
-		:param tsk: task
-		:type tsk: :py:attr:`waflib.Task.TaskBase`
+		:param tsk: task instance
+		:type tsk: :py:attr:`waflib.Task.Task`
 		"""
-		if hasattr(tsk, 'scan') and hasattr(tsk, 'uid'):
-			# TODO waf 1.9 - this breaks encapsulation
-			key = (tsk.uid(), 'imp')
-			try:
-				del self.bld.task_sigs[key]
-			except KeyError:
-				pass
 		if not self.bld.keep:
 			self.stop = True
 		self.error.append(tsk)
 
 	def task_status(self, tsk):
+		"""
+		Obtains the task status to decide whether to run it immediately or not.
+
+		:return: the exit status, for example :py:attr:`waflib.Task.ASK_LATER`
+		:rtype: integer
+		"""
 		try:
 			return tsk.runnable_status()
 		except Exception:
 			self.processed += 1
-			tsk.err_msg = Utils.ex_stack()
+			tsk.err_msg = traceback.format_exc()
 			if not self.stop and self.bld.keep:
 				self.skip(tsk)
 				if self.bld.keep == 1:
-					# if -k stop at the first exception, if -kk try to go as far as possible
+					# if -k stop on the first exception, if -kk try to go as far as possible
 					if Logs.verbose > 1 or not self.error:
 						self.error.append(tsk)
 					self.stop = True
@@ -306,17 +442,20 @@ def task_status(self, tsk):
 					if Logs.verbose > 1:
 						self.error.append(tsk)
 				return Task.EXCEPTION
-			tsk.hasrun = Task.EXCEPTION
 
+			tsk.hasrun = Task.EXCEPTION
 			self.error_handler(tsk)
+
 			return Task.EXCEPTION
 
 	def start(self):
 		"""
-		Give tasks to :py:class:`waflib.Runner.TaskConsumer` instances until the build finishes or the ``stop`` flag is set.
-		If only one job is used, then execute the tasks one by one, without consumers.
+		Obtains Task instances from the BuildContext instance and adds the ones that need to be executed to
+		:py:class:`waflib.Runner.Parallel.ready` so that the :py:class:`waflib.Runner.Spawner` consumer thread
+		has them executed. Obtains the executed Tasks back from :py:class:`waflib.Runner.Parallel.out`
+		and marks the build as failed by setting the ``stop`` flag.
+		If only one job is used, then executes the tasks one by one, without consumers.
 		"""
-
 		self.total = self.bld.total()
 
 		while not self.stop:
@@ -338,36 +477,135 @@ def start(self):
 				self.processed += 1
 				continue
 
-			if self.stop: # stop immediately after a failure was detected
+			if self.stop: # stop immediately after a failure is detected
 				break
 
-
 			st = self.task_status(tsk)
 			if st == Task.RUN_ME:
-				tsk.position = (self.processed, self.total)
-				self.count += 1
-				tsk.master = self
-				self.processed += 1
-
-				if self.numjobs == 1:
-					tsk.process()
-				else:
-					self.add_task(tsk)
-			if st == Task.ASK_LATER:
+				self._add_task(tsk)
+			elif st == Task.ASK_LATER:
 				self.postpone(tsk)
 			elif st == Task.SKIP_ME:
 				self.processed += 1
 				self.skip(tsk)
 				self.add_more_tasks(tsk)
+			elif st == Task.CANCEL_ME:
+				# A dependency problem has occurred, and the
+				# build is most likely run with `waf -k`
+				if Logs.verbose > 1:
+					self.error.append(tsk)
+				self.processed += 1
+				self.cancel(tsk)
 
 		# self.count represents the tasks that have been made available to the consumer threads
 		# collect all the tasks after an error else the message may be incomplete
 		while self.error and self.count:
 			self.get_out()
 
-		#print loop
-		assert (self.count == 0 or self.stop)
+		self.ready.put(None)
+		if not self.stop:
+			assert not self.count
+			assert not self.postponed
+			assert not self.incomplete
+
+	def prio_and_split(self, tasks):
+		"""
+		Label input tasks with priority values, and return a pair containing
+		the tasks that are ready to run and the tasks that are necessarily
+		waiting for other tasks to complete.
+
+		The priority system is really meant as an optional layer for optimization:
+		dependency cycles are found quickly, and builds should be more efficient.
+		A high priority number means that a task is processed first.
+
+		This method can be overridden to disable the priority system::
+
+			def prio_and_split(self, tasks):
+				return tasks, []
 
-		# free the task pool, if any
-		self.free_task_pool()
+		:return: A pair of task lists
+		:rtype: tuple
+		"""
+		# to disable:
+		#return tasks, []
+		for x in tasks:
+			x.visited = 0
+
+		reverse = self.revdeps
+
+		groups_done = set()
+		for x in tasks:
+			for k in x.run_after:
+				if isinstance(k, Task.TaskGroup):
+					if k not in groups_done:
+						groups_done.add(k)
+						for j in k.prev:
+							reverse[j].add(k)
+				else:
+					reverse[k].add(x)
+
+		# the priority number is not the tree depth
+		def visit(n):
+			if isinstance(n, Task.TaskGroup):
+				return sum(visit(k) for k in n.next)
+
+			if n.visited == 0:
+				n.visited = 1
+
+				if n in reverse:
+					rev = reverse[n]
+					n.prio_order = n.tree_weight + len(rev) + sum(visit(k) for k in rev)
+				else:
+					n.prio_order = n.tree_weight
+
+				n.visited = 2
+			elif n.visited == 1:
+				raise Errors.WafError('Dependency cycle found!')
+			return n.prio_order
+
+		for x in tasks:
+			if x.visited != 0:
+				# must visit all to detect cycles
+				continue
+			try:
+				visit(x)
+			except Errors.WafError:
+				self.debug_cycles(tasks, reverse)
+
+		ready = []
+		waiting = []
+		for x in tasks:
+			for k in x.run_after:
+				if not k.hasrun:
+					waiting.append(x)
+					break
+			else:
+				ready.append(x)
+		return (ready, waiting)
+
+	def debug_cycles(self, tasks, reverse):
+		tmp = {}
+		for x in tasks:
+			tmp[x] = 0
+
+		def visit(n, acc):
+			if isinstance(n, Task.TaskGroup):
+				for k in n.next:
+					visit(k, acc)
+				return
+			if tmp[n] == 0:
+				tmp[n] = 1
+				for k in reverse.get(n, []):
+					visit(k, [n] + acc)
+				tmp[n] = 2
+			elif tmp[n] == 1:
+				lst = []
+				for tsk in acc:
+					lst.append(repr(tsk))
+					if tsk is n:
+						# exclude prior nodes, we want the minimum cycle
+						break
+				raise Errors.WafError('Task dependency cycle in "run_after" constraints: %s' % ''.join(lst))
+		for x in tasks:
+			visit(x, [])
 
diff --git a/waflib/Scripting.py b/waflib/Scripting.py
index a2839a1f..749d4f2e 100644
--- a/waflib/Scripting.py
+++ b/waflib/Scripting.py
@@ -1,9 +1,11 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 "Module called for configuring, compiling and installing targets"
 
+from __future__ import with_statement
+
 import os, shlex, shutil, traceback, errno, sys, stat
 from waflib import Utils, Configure, Logs, Options, ConfigSet, Context, Errors, Build, Node
 
@@ -24,69 +26,66 @@ def waf_entry_point(current_directory, version, wafdir):
 	:param wafdir: absolute path representing the directory of the waf library
 	:type wafdir: string
 	"""
-
 	Logs.init_log()
 
 	if Context.WAFVERSION != version:
-		Logs.error('Waf script %r and library %r do not match (directory %r)' % (version, Context.WAFVERSION, wafdir))
+		Logs.error('Waf script %r and library %r do not match (directory %r)', version, Context.WAFVERSION, wafdir)
 		sys.exit(1)
 
-	if '--version' in sys.argv:
-		Context.run_dir = current_directory
-		ctx = Context.create_context('options')
-		ctx.curdir = current_directory
-		ctx.parse_args()
-		sys.exit(0)
+	# Store current directory before any chdir
+	Context.waf_dir = wafdir
+	Context.run_dir = Context.launch_dir = current_directory
+	start_dir = current_directory
+	no_climb = os.environ.get('NOCLIMB')
 
 	if len(sys.argv) > 1:
-		# os.path.join handles absolute paths in sys.argv[1] accordingly (it discards the previous ones)
+		# os.path.join handles absolute paths
 		# if sys.argv[1] is not an absolute path, then it is relative to the current working directory
 		potential_wscript = os.path.join(current_directory, sys.argv[1])
-		# maybe check if the file is executable
-		# perhaps extract 'wscript' as a constant
-		if os.path.basename(potential_wscript) == 'wscript' and os.path.isfile(potential_wscript):
+		if os.path.basename(potential_wscript) == Context.WSCRIPT_FILE and os.path.isfile(potential_wscript):
 			# need to explicitly normalize the path, as it may contain extra '/.'
-			# TODO abspath?
-			current_directory = os.path.normpath(os.path.dirname(potential_wscript))
+			path = os.path.normpath(os.path.dirname(potential_wscript))
+			start_dir = os.path.abspath(path)
+			no_climb = True
 			sys.argv.pop(1)
 
-	Context.waf_dir = wafdir
-	Context.launch_dir = current_directory
+	ctx = Context.create_context('options')
+	(options, commands, env) = ctx.parse_cmd_args(allow_unknown=True)
+	if options.top:
+		start_dir = Context.run_dir = Context.top_dir = options.top
+		no_climb = True
+	if options.out:
+		Context.out_dir = options.out
 
 	# if 'configure' is in the commands, do not search any further
-	no_climb = os.environ.get('NOCLIMB', None)
 	if not no_climb:
 		for k in no_climb_commands:
-			for y in sys.argv:
+			for y in commands:
 				if y.startswith(k):
 					no_climb = True
 					break
 
-	# if --top is provided assume the build started in the top directory
-	for i, x in enumerate(sys.argv):
-		# WARNING: this modifies sys.argv
-		if x.startswith('--top='):
-			Context.run_dir = Context.top_dir = Utils.sane_path(x[6:])
-			sys.argv[i] = '--top=' + Context.run_dir
-		if x.startswith('--out='):
-			Context.out_dir = Utils.sane_path(x[6:])
-			sys.argv[i] = '--out=' + Context.out_dir
-
 	# try to find a lock file (if the project was configured)
 	# at the same time, store the first wscript file seen
-	cur = current_directory
-	while cur and not Context.top_dir:
-		lst = os.listdir(cur)
+	cur = start_dir
+	while cur:
+		try:
+			lst = os.listdir(cur)
+		except OSError:
+			lst = []
+			Logs.error('Directory %r is unreadable!', cur)
 		if Options.lockfile in lst:
 			env = ConfigSet.ConfigSet()
 			try:
 				env.load(os.path.join(cur, Options.lockfile))
 				ino = os.stat(cur)[stat.ST_INO]
-			except Exception:
+			except EnvironmentError:
 				pass
 			else:
 				# check if the folder was not moved
 				for x in (env.run_dir, env.top_dir, env.out_dir):
+					if not x:
+						continue
 					if Utils.is_win32:
 						if cur == x:
 							load = True
@@ -102,7 +101,7 @@ def waf_entry_point(current_directory, version, wafdir):
 								load = True
 								break
 				else:
-					Logs.warn('invalid lock file in %s' % cur)
+					Logs.warn('invalid lock file in %s', cur)
 					load = False
 
 				if load:
@@ -123,56 +122,62 @@ def waf_entry_point(current_directory, version, wafdir):
 		if no_climb:
 			break
 
-	if not Context.run_dir:
-		if '-h' in sys.argv or '--help' in sys.argv:
-			Logs.warn('No wscript file found: the help message may be incomplete')
-			Context.run_dir = current_directory
-			ctx = Context.create_context('options')
-			ctx.curdir = current_directory
-			ctx.parse_args()
+	wscript = os.path.normpath(os.path.join(Context.run_dir, Context.WSCRIPT_FILE))
+	if not os.path.exists(wscript):
+		if options.whelp:
+			Logs.warn('These are the generic options (no wscript/project found)')
+			ctx.parser.print_help()
 			sys.exit(0)
-		Logs.error('Waf: Run from a directory containing a file named %r' % Context.WSCRIPT_FILE)
+		Logs.error('Waf: Run from a folder containing a %r file (or try -h for the generic options)', Context.WSCRIPT_FILE)
 		sys.exit(1)
 
 	try:
 		os.chdir(Context.run_dir)
 	except OSError:
-		Logs.error('Waf: The folder %r is unreadable' % Context.run_dir)
+		Logs.error('Waf: The folder %r is unreadable', Context.run_dir)
 		sys.exit(1)
 
 	try:
-		set_main_module(os.path.normpath(os.path.join(Context.run_dir, Context.WSCRIPT_FILE)))
+		set_main_module(wscript)
 	except Errors.WafError as e:
 		Logs.pprint('RED', e.verbose_msg)
 		Logs.error(str(e))
 		sys.exit(1)
 	except Exception as e:
-		Logs.error('Waf: The wscript in %r is unreadable' % Context.run_dir, e)
+		Logs.error('Waf: The wscript in %r is unreadable', Context.run_dir)
 		traceback.print_exc(file=sys.stdout)
 		sys.exit(2)
 
-	"""
-	import cProfile, pstats
-	cProfile.runctx("from waflib import Scripting; Scripting.run_commands()", {}, {}, 'profi.txt')
-	p = pstats.Stats('profi.txt')
-	p.sort_stats('time').print_stats(75) # or 'cumulative'
-	"""
-	try:
-		run_commands()
-	except Errors.WafError as e:
-		if Logs.verbose > 1:
-			Logs.pprint('RED', e.verbose_msg)
-		Logs.error(e.msg)
-		sys.exit(1)
-	except SystemExit:
-		raise
-	except Exception as e:
-		traceback.print_exc(file=sys.stdout)
-		sys.exit(2)
-	except KeyboardInterrupt:
-		Logs.pprint('RED', 'Interrupted')
-		sys.exit(68)
-	#"""
+	if options.profile:
+		import cProfile, pstats
+		cProfile.runctx('from waflib import Scripting; Scripting.run_commands()', {}, {}, 'profi.txt')
+		p = pstats.Stats('profi.txt')
+		p.sort_stats('time').print_stats(75) # or 'cumulative'
+	else:
+		try:
+			try:
+				run_commands()
+			except:
+				if options.pdb:
+					import pdb
+					type, value, tb = sys.exc_info()
+					traceback.print_exc()
+					pdb.post_mortem(tb)
+				else:
+					raise
+		except Errors.WafError as e:
+			if Logs.verbose > 1:
+				Logs.pprint('RED', e.verbose_msg)
+			Logs.error(e.msg)
+			sys.exit(1)
+		except SystemExit:
+			raise
+		except Exception as e:
+			traceback.print_exc(file=sys.stdout)
+			sys.exit(2)
+		except KeyboardInterrupt:
+			Logs.pprint('RED', 'Interrupted')
+			sys.exit(68)
 
 def set_main_module(file_path):
 	"""
@@ -193,7 +198,7 @@ def set_def(obj):
 		name = obj.__name__
 		if not name in Context.g_module.__dict__:
 			setattr(Context.g_module, name, obj)
-	for k in (update, dist, distclean, distcheck, update):
+	for k in (dist, distclean, distcheck):
 		set_def(k)
 	# add dummy init and shutdown functions if they're not defined
 	if not 'init' in Context.g_module.__dict__:
@@ -205,36 +210,20 @@ def set_def(obj):
 
 def parse_options():
 	"""
-	Parse the command-line options and initialize the logging system.
+	Parses the command-line options and initialize the logging system.
 	Called by :py:func:`waflib.Scripting.waf_entry_point` during the initialization.
 	"""
-	Context.create_context('options').execute()
-
-	for var in Options.envvars:
-		(name, value) = var.split('=', 1)
-		os.environ[name.strip()] = value
-
+	ctx = Context.create_context('options')
+	ctx.execute()
 	if not Options.commands:
-		Options.commands = [default_cmd]
-	Options.commands = [x for x in Options.commands if x != 'options'] # issue 1076
-
-	# process some internal Waf options
-	Logs.verbose = Options.options.verbose
-	#Logs.init_log()
-
-	if Options.options.zones:
-		Logs.zones = Options.options.zones.split(',')
-		if not Logs.verbose:
-			Logs.verbose = 1
-	elif Logs.verbose > 0:
-		Logs.zones = ['runner']
-
-	if Logs.verbose > 2:
-		Logs.zones = ['*']
+		Options.commands.append(default_cmd)
+	if Options.options.whelp:
+		ctx.parser.print_help()
+		sys.exit(0)
 
 def run_command(cmd_name):
 	"""
-	Execute a single command. Called by :py:func:`waflib.Scripting.run_commands`.
+	Executes a single Waf command. Called by :py:func:`waflib.Scripting.run_commands`.
 
 	:param cmd_name: command to execute, like ``build``
 	:type cmd_name: string
@@ -252,7 +241,7 @@ def run_command(cmd_name):
 
 def run_commands():
 	"""
-	Execute the commands that were given on the command-line, and the other options
+	Execute the Waf commands that were given on the command-line, and the other options
 	Called by :py:func:`waflib.Scripting.waf_entry_point` during the initialization, and executed
 	after :py:func:`waflib.Scripting.parse_options`.
 	"""
@@ -261,18 +250,11 @@ def run_commands():
 	while Options.commands:
 		cmd_name = Options.commands.pop(0)
 		ctx = run_command(cmd_name)
-		Logs.info('%r finished successfully (%s)' % (cmd_name, str(ctx.log_timer)))
+		Logs.info('%r finished successfully (%s)', cmd_name, ctx.log_timer)
 	run_command('shutdown')
 
 ###########################################################################################
 
-def _can_distclean(name):
-	# WARNING: this method may disappear anytime
-	for k in '.o .moc .exe'.split():
-		if name.endswith(k):
-			return True
-	return False
-
 def distclean_dir(dirname):
 	"""
 	Distclean function called in the particular case when::
@@ -284,12 +266,12 @@ def distclean_dir(dirname):
 	"""
 	for (root, dirs, files) in os.walk(dirname):
 		for f in files:
-			if _can_distclean(f):
+			if f.endswith(('.o', '.moc', '.exe')):
 				fname = os.path.join(root, f)
 				try:
 					os.remove(fname)
 				except OSError:
-					Logs.warn('Could not remove %r' % fname)
+					Logs.warn('Could not remove %r', fname)
 
 	for x in (Context.DBFILE, 'config.log'):
 		try:
@@ -303,40 +285,53 @@ def distclean_dir(dirname):
 		pass
 
 def distclean(ctx):
-	'''removes the build directory'''
-	lst = os.listdir('.')
-	for f in lst:
-		if f == Options.lockfile:
-			try:
-				proj = ConfigSet.ConfigSet(f)
-			except IOError:
-				Logs.warn('Could not read %r' % f)
-				continue
+	'''removes build folders and data'''
 
-			if proj['out_dir'] != proj['top_dir']:
-				try:
-					shutil.rmtree(proj['out_dir'])
-				except IOError:
-					pass
-				except OSError as e:
-					if e.errno != errno.ENOENT:
-						Logs.warn('Could not remove %r' % proj['out_dir'])
-			else:
-				distclean_dir(proj['out_dir'])
+	def remove_and_log(k, fun):
+		try:
+			fun(k)
+		except EnvironmentError as e:
+			if e.errno != errno.ENOENT:
+				Logs.warn('Could not remove %r', k)
 
-			for k in (proj['out_dir'], proj['top_dir'], proj['run_dir']):
-				p = os.path.join(k, Options.lockfile)
-				try:
-					os.remove(p)
-				except OSError as e:
-					if e.errno != errno.ENOENT:
-						Logs.warn('Could not remove %r' % p)
+	# remove waf cache folders on the top-level
+	if not Options.commands:
+		for k in os.listdir('.'):
+			for x in '.waf-2 waf-2 .waf3-2 waf3-2'.split():
+				if k.startswith(x):
+					remove_and_log(k, shutil.rmtree)
+
+	# remove a build folder, if any
+	cur = '.'
+	if ctx.options.no_lock_in_top:
+		cur = ctx.options.out
+
+	try:
+		lst = os.listdir(cur)
+	except OSError:
+		Logs.warn('Could not read %r', cur)
+		return
+
+	if Options.lockfile in lst:
+		f = os.path.join(cur, Options.lockfile)
+		try:
+			env = ConfigSet.ConfigSet(f)
+		except EnvironmentError:
+			Logs.warn('Could not read %r', f)
+			return
+
+		if not env.out_dir or not env.top_dir:
+			Logs.warn('Invalid lock file %r', f)
+			return
+
+		if env.out_dir == env.top_dir:
+			distclean_dir(env.out_dir)
+		else:
+			remove_and_log(env.out_dir, shutil.rmtree)
 
-		# remove local waf cache folders
-		if not Options.commands:
-			for x in '.waf-1. waf-1. .waf3-1. waf3-1.'.split():
-				if f.startswith(x):
-					shutil.rmtree(f, ignore_errors=True)
+		for k in (env.out_dir, env.top_dir, env.run_dir):
+			p = os.path.join(k, Options.lockfile)
+			remove_and_log(p, os.remove)
 
 class Dist(Context.Context):
 	'''creates an archive containing the project source code'''
@@ -354,7 +349,7 @@ def execute(self):
 
 	def archive(self):
 		"""
-		Create the archive.
+		Creates the source archive.
 		"""
 		import tarfile
 
@@ -374,14 +369,14 @@ def archive(self):
 		files = self.get_files()
 
 		if self.algo.startswith('tar.'):
-			tar = tarfile.open(arch_name, 'w:' + self.algo.replace('tar.', ''))
+			tar = tarfile.open(node.abspath(), 'w:' + self.algo.replace('tar.', ''))
 
 			for x in files:
 				self.add_tar_file(x, tar)
 			tar.close()
 		elif self.algo == 'zip':
 			import zipfile
-			zip = zipfile.ZipFile(arch_name, 'w', compression=zipfile.ZIP_DEFLATED)
+			zip = zipfile.ZipFile(node.abspath(), 'w', compression=zipfile.ZIP_DEFLATED)
 
 			for x in files:
 				archive_name = self.get_base_name() + '/' + x.path_from(self.base_path)
@@ -391,26 +386,30 @@ def archive(self):
 			self.fatal('Valid algo types are tar.bz2, tar.gz, tar.xz or zip')
 
 		try:
-			from hashlib import sha1 as sha
+			from hashlib import sha256
 		except ImportError:
-			from sha import sha
-		try:
-			digest = " (sha=%r)" % sha(node.read()).hexdigest()
-		except Exception:
 			digest = ''
+		else:
+			digest = ' (sha256=%r)' % sha256(node.read(flags='rb')).hexdigest()
 
-		Logs.info('New archive created: %s%s' % (self.arch_name, digest))
+		Logs.info('New archive created: %s%s', self.arch_name, digest)
 
 	def get_tar_path(self, node):
 		"""
-		return the path to use for a node in the tar archive, the purpose of this
+		Return the path to use for a node in the tar archive, the purpose of this
 		is to let subclases resolve symbolic links or to change file names
+
+		:return: absolute path
+		:rtype: string
 		"""
 		return node.abspath()
 
 	def add_tar_file(self, x, tar):
 		"""
-		Add a file to the tar archive. Transform symlinks into files if the files lie out of the project tree.
+		Adds a file to the tar archive. Symlinks are not verified.
+
+		:param x: file path
+		:param tar: tar file object
 		"""
 		p = self.get_tar_path(x)
 		tinfo = tar.gettarinfo(name=p, arcname=self.get_tar_prefix() + '/' + x.path_from(self.base_path))
@@ -419,15 +418,18 @@ def add_tar_file(self, x, tar):
 		tinfo.uname = 'root'
 		tinfo.gname = 'root'
 
-		fu = None
-		try:
-			fu = open(p, 'rb')
-			tar.addfile(tinfo, fileobj=fu)
-		finally:
-			if fu:
-				fu.close()
+		if os.path.isfile(p):
+			with open(p, 'rb') as f:
+				tar.addfile(tinfo, fileobj=f)
+		else:
+			tar.addfile(tinfo)
 
 	def get_tar_prefix(self):
+		"""
+		Returns the base path for files added into the archive tar file
+
+		:rtype: string
+		"""
 		try:
 			return self.tar_prefix
 		except AttributeError:
@@ -435,7 +437,8 @@ def get_tar_prefix(self):
 
 	def get_arch_name(self):
 		"""
-		Return the name of the archive to create. Change the default value by setting *arch_name*::
+		Returns the archive file name.
+		Set the attribute *arch_name* to change the default value::
 
 			def dist(ctx):
 				ctx.arch_name = 'ctx.tar.bz2'
@@ -450,7 +453,7 @@ def dist(ctx):
 
 	def get_base_name(self):
 		"""
-		Return the default name of the main directory in the archive, which is set to *appname-version*.
+		Returns the default name of the main directory in the archive, which is set to *appname-version*.
 		Set the attribute *base_name* to change the default value::
 
 			def dist(ctx):
@@ -468,8 +471,8 @@ def dist(ctx):
 
 	def get_excl(self):
 		"""
-		Return the patterns to exclude for finding the files in the top-level directory. Set the attribute *excl*
-		to change the default value::
+		Returns the patterns to exclude for finding the files in the top-level directory.
+		Set the attribute *excl* to change the default value::
 
 			def dist(ctx):
 				ctx.excl = 'build **/*.o **/*.class'
@@ -479,7 +482,7 @@ def dist(ctx):
 		try:
 			return self.excl
 		except AttributeError:
-			self.excl = Node.exclude_regs + ' **/waf-1.8.* **/.waf-1.8* **/waf3-1.8.* **/.waf3-1.8* **/*~ **/*.rej **/*.orig **/*.pyc **/*.pyo **/*.bak **/*.swp **/.lock-w*'
+			self.excl = Node.exclude_regs + ' **/waf-2.* **/.waf-2.* **/waf3-2.* **/.waf3-2.* **/*~ **/*.rej **/*.orig **/*.pyc **/*.pyo **/*.bak **/*.swp **/.lock-w*'
 			if Context.out_dir:
 				nd = self.root.find_node(Context.out_dir)
 				if nd:
@@ -488,13 +491,13 @@ def dist(ctx):
 
 	def get_files(self):
 		"""
-		The files to package are searched automatically by :py:func:`waflib.Node.Node.ant_glob`. Set
-		*files* to prevent this behaviour::
+		Files to package are searched automatically by :py:func:`waflib.Node.Node.ant_glob`.
+		Set *files* to prevent this behaviour::
 
 			def dist(ctx):
 				ctx.files = ctx.path.find_node('wscript')
 
-		The files are searched from the directory 'base_path', to change it, set::
+		Files are also searched from the directory 'base_path', to change it, set::
 
 			def dist(ctx):
 				ctx.base_path = path
@@ -507,18 +510,12 @@ def dist(ctx):
 			files = self.base_path.ant_glob('**/*', excl=self.get_excl())
 		return files
 
-
 def dist(ctx):
 	'''makes a tarball for redistributing the sources'''
 	pass
 
 class DistCheck(Dist):
-	"""
-	Create an archive of the project, and try to build the project in a temporary directory::
-
-		$ waf distcheck
-	"""
-
+	"""creates an archive with dist, then tries to build it"""
 	fun = 'distcheck'
 	cmd = 'distcheck'
 
@@ -530,32 +527,30 @@ def execute(self):
 		self.archive()
 		self.check()
 
+	def make_distcheck_cmd(self, tmpdir):
+		cfg = []
+		if Options.options.distcheck_args:
+			cfg = shlex.split(Options.options.distcheck_args)
+		else:
+			cfg = [x for x in sys.argv if x.startswith('-')]
+		cmd = [sys.executable, sys.argv[0], 'configure', 'build', 'install', 'uninstall', '--destdir=' + tmpdir] + cfg
+		return cmd
+
 	def check(self):
 		"""
-		Create the archive, uncompress it and try to build the project
+		Creates the archive, uncompresses it and tries to build the project
 		"""
 		import tempfile, tarfile
 
-		t = None
-		try:
-			t = tarfile.open(self.get_arch_name())
+		with tarfile.open(self.get_arch_name()) as t:
 			for x in t:
 				t.extract(x)
-		finally:
-			if t:
-				t.close()
-
-		cfg = []
-
-		if Options.options.distcheck_args:
-			cfg = shlex.split(Options.options.distcheck_args)
-		else:
-			cfg = [x for x in sys.argv if x.startswith('-')]
 
 		instdir = tempfile.mkdtemp('.inst', self.get_base_name())
-		ret = Utils.subprocess.Popen([sys.executable, sys.argv[0], 'configure', 'install', 'uninstall', '--destdir=' + instdir] + cfg, cwd=self.get_base_name()).wait()
+		cmd = self.make_distcheck_cmd(instdir)
+		ret = Utils.subprocess.Popen(cmd, cwd=self.get_base_name()).wait()
 		if ret:
-			raise Errors.WafError('distcheck failed with code %i' % ret)
+			raise Errors.WafError('distcheck failed with code %r' % ret)
 
 		if os.path.exists(instdir):
 			raise Errors.WafError('distcheck succeeded, but files were left in %s' % instdir)
@@ -567,33 +562,14 @@ def distcheck(ctx):
 	'''checks if the project compiles (tarball from 'dist')'''
 	pass
 
-def update(ctx):
-	lst = Options.options.files
-	if lst:
-		lst = lst.split(',')
-	else:
-		path = os.path.join(Context.waf_dir, 'waflib', 'extras')
-		lst = [x for x in Utils.listdir(path) if x.endswith('.py')]
-	for x in lst:
-		tool = x.replace('.py', '')
-		if not tool:
-			continue
-		try:
-			dl = Configure.download_tool
-		except AttributeError:
-			ctx.fatal('The command "update" is dangerous; include the tool "use_config" in your project!')
-		try:
-			dl(tool, force=True, ctx=ctx)
-		except Errors.WafError:
-			Logs.error('Could not find the tool %r in the remote repository' % x)
-		else:
-			Logs.warn('Updated %r' % tool)
-
 def autoconfigure(execute_method):
 	"""
-	Decorator used to set the commands that can be configured automatically
+	Decorator that enables context commands to run *configure* as needed.
 	"""
 	def execute(self):
+		"""
+		Wraps :py:func:`waflib.Context.Context.execute` on the context class
+		"""
 		if not Configure.autoconfig:
 			return execute_method(self)
 
@@ -601,7 +577,7 @@ def execute(self):
 		do_config = False
 		try:
 			env.load(os.path.join(Context.top_dir, Options.lockfile))
-		except Exception:
+		except EnvironmentError:
 			Logs.warn('Configuring the project')
 			do_config = True
 		else:
@@ -609,18 +585,30 @@ def execute(self):
 				do_config = True
 			else:
 				h = 0
-				for f in env['files']:
-					h = Utils.h_list((h, Utils.readf(f, 'rb')))
-				do_config = h != env.hash
+				for f in env.files:
+					try:
+						h = Utils.h_list((h, Utils.readf(f, 'rb')))
+					except EnvironmentError:
+						do_config = True
+						break
+				else:
+					do_config = h != env.hash
 
 		if do_config:
-			Options.commands.insert(0, self.cmd)
-			Options.commands.insert(0, 'configure')
+			cmd = env.config_cmd or 'configure'
 			if Configure.autoconfig == 'clobber':
-				Options.options.__dict__ = env.options
-			return
-
-		return execute_method(self)
+				tmp = Options.options.__dict__
+				if env.options:
+					Options.options.__dict__ = env.options
+				try:
+					run_command(cmd)
+				finally:
+					Options.options.__dict__ = tmp
+			else:
+				run_command(cmd)
+			run_command(self.cmd)
+		else:
+			return execute_method(self)
 	return execute
 Build.BuildContext.execute = autoconfigure(Build.BuildContext.execute)
 
diff --git a/waflib/Task.py b/waflib/Task.py
index b8475670..0fc449d4 100644
--- a/waflib/Task.py
+++ b/waflib/Task.py
@@ -1,12 +1,12 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 Tasks represent atomic operations such as processes.
 """
 
-import os, re, sys
+import os, re, sys, tempfile, traceback
 from waflib import Utils, Logs, Errors
 
 # task states
@@ -20,7 +20,10 @@
 """The task execution returned a non-zero exit status"""
 
 EXCEPTION = 3
-"""An exception occured in the task execution"""
+"""An exception occurred in the task execution"""
+
+CANCELED = 4
+"""A dependency for the task is missing so it was cancelled"""
 
 SKIPPED = 8
 """The task did not have to be executed"""
@@ -37,18 +40,18 @@
 RUN_ME = -3
 """The task must be executed"""
 
-# To save some memory during the build, consider discarding tsk.last_cmd in the two templates below
+CANCEL_ME = -4
+"""The task cannot be executed because of a dependency problem"""
 
 COMPILE_TEMPLATE_SHELL = '''
 def f(tsk):
 	env = tsk.env
 	gen = tsk.generator
 	bld = gen.bld
-	cwdx = getattr(bld, 'cwdx', bld.bldnode) # TODO single cwd value in waf 1.9
-	wd = getattr(tsk, 'cwd', None)
+	cwdx = tsk.get_cwd()
 	p = env.get_flat
 	tsk.last_cmd = cmd = \'\'\' %s \'\'\' % s
-	return tsk.exec_command(cmd, cwd=wd, env=env.env or None)
+	return tsk.exec_command(cmd, cwd=cwdx, env=env.env or None)
 '''
 
 COMPILE_TEMPLATE_NOSHELL = '''
@@ -56,35 +59,53 @@ def f(tsk):
 	env = tsk.env
 	gen = tsk.generator
 	bld = gen.bld
-	cwdx = getattr(bld, 'cwdx', bld.bldnode) # TODO single cwd value in waf 1.9
-	wd = getattr(tsk, 'cwd', None)
+	cwdx = tsk.get_cwd()
 	def to_list(xx):
 		if isinstance(xx, str): return [xx]
 		return xx
-	tsk.last_cmd = lst = []
+	def merge(lst1, lst2):
+		if lst1 and lst2:
+			return lst1[:-1] + [lst1[-1] + lst2[0]] + lst2[1:]
+		return lst1 + lst2
+	lst = []
 	%s
-	lst = [x for x in lst if x]
-	return tsk.exec_command(lst, cwd=wd, env=env.env or None)
+	if '' in lst:
+		lst = [x for x in lst if x]
+	tsk.last_cmd = lst
+	return tsk.exec_command(lst, cwd=cwdx, env=env.env or None)
+'''
+
+COMPILE_TEMPLATE_SIG_VARS = '''
+def f(tsk):
+	super(tsk.__class__, tsk).sig_vars()
+	env = tsk.env
+	gen = tsk.generator
+	bld = gen.bld
+	cwdx = tsk.get_cwd()
+	p = env.get_flat
+	buf = []
+	%s
+	tsk.m.update(repr(buf).encode())
 '''
 
 classes = {}
-"Class tasks created by user scripts or Waf tools (maps names to class objects). Task classes defined in Waf tools are registered here through the metaclass :py:class:`waflib.Task.store_task_type`."
+"""
+The metaclass :py:class:`waflib.Task.store_task_type` stores all class tasks
+created by user scripts or Waf tools to this dict. It maps class names to class objects.
+"""
 
 class store_task_type(type):
 	"""
-	Metaclass: store the task classes into :py:const:`waflib.Task.classes`, or to the dict pointed
-	by the class attribute 'register'.
-	The attribute 'run_str' will be processed to compute a method 'run' on the task class
-	The decorator :py:func:`waflib.Task.cache_outputs` is also applied to the class
+	Metaclass: store the task classes into the dict pointed by the
+	class attribute 'register' which defaults to :py:const:`waflib.Task.classes`,
+
+	The attribute 'run_str' is compiled into a method 'run' bound to the task class.
 	"""
 	def __init__(cls, name, bases, dict):
 		super(store_task_type, cls).__init__(name, bases, dict)
 		name = cls.__name__
 
-		if name.endswith('_task'):
-			name = name.replace('_task', '')
-		if name != 'evil' and name != 'TaskBase':
-			global classes
+		if name != 'evil' and name != 'Task':
 			if getattr(cls, 'run_str', None):
 				# if a string is provided, convert it to a method
 				(f, dvars) = compile_fun(cls.run_str, cls.shell)
@@ -93,8 +114,13 @@ def __init__(cls, name, bases, dict):
 				# change the name of run_str or it is impossible to subclass with a function
 				cls.run_str = None
 				cls.run = f
+				# process variables
 				cls.vars = list(set(cls.vars + dvars))
 				cls.vars.sort()
+				if cls.vars:
+					fun = compile_sig_vars(cls.vars)
+					if fun:
+						cls.sig_vars = fun
 			elif getattr(cls, 'run', None) and not 'hcode' in cls.__dict__:
 				# getattr(cls, 'hcode') would look in the upper classes
 				cls.hcode = Utils.h_cmd(cls.run)
@@ -105,31 +131,32 @@ def __init__(cls, name, bases, dict):
 evil = store_task_type('evil', (object,), {})
 "Base class provided to avoid writing a metaclass, so the code can run in python 2.6 and 3.x unmodified"
 
-class TaskBase(evil):
+class Task(evil):
 	"""
-	Base class for all Waf tasks, which should be seen as an interface.
-	For illustration purposes, instances of this class will execute the attribute
-	'fun' in :py:meth:`waflib.Task.TaskBase.run`. When in doubt, create
-	subclasses of :py:class:`waflib.Task.Task` instead.
+	Task objects represents actions to perform such as commands to execute by calling the `run` method.
 
-	Subclasses should override these methods:
+	Detecting when to execute a task occurs in the method :py:meth:`waflib.Task.Task.runnable_status`.
 
-	#. __str__: string to display to the user
-	#. runnable_status: ask the task if it should be run, skipped, or if we have to ask later
-	#. run: let threads execute the task
-	#. post_run: let threads update the data regarding the task (cache)
-
-	.. warning:: For backward compatibility reasons, the suffix "_task" is truncated in derived class names. This limitation will be removed in Waf 1.9.
+	Detecting which tasks to execute is performed through a hash value returned by
+	:py:meth:`waflib.Task.Task.signature`. The task signature is persistent from build to build.
 	"""
+	vars = []
+	"""ConfigSet variables that should trigger a rebuild (class attribute used for :py:meth:`waflib.Task.Task.sig_vars`)"""
+
+	always_run = False
+	"""Specify whether task instances must always be executed or not (class attribute)"""
+
+	shell = False
+	"""Execute the command with the shell (class attribute)"""
 
 	color = 'GREEN'
 	"""Color for the console display, see :py:const:`waflib.Logs.colors_lst`"""
 
 	ext_in = []
-	"""File extensions that objects of this task class might use"""
+	"""File extensions that objects of this task class may use"""
 
 	ext_out = []
-	"""File extensions that objects of this task class might create"""
+	"""File extensions that objects of this task class may create"""
 
 	before = []
 	"""List of task class names to execute before instances of this class"""
@@ -137,128 +164,207 @@ class TaskBase(evil):
 	after = []
 	"""List of task class names to execute after instances of this class"""
 
-	hcode = ''
+	hcode = Utils.SIG_NIL
 	"""String representing an additional hash for the class representation"""
 
+	keep_last_cmd = False
+	"""Whether to keep the last command executed on the instance after execution.
+	This may be useful for certain extensions but it can a lot of memory.
+	"""
+
+	weight = 0
+	"""Optional weight to tune the priority for task instances.
+	The higher, the earlier. The weight only applies to single task objects."""
+
+	tree_weight = 0
+	"""Optional weight to tune the priority of task instances and whole subtrees.
+	The higher, the earlier."""
+
+	prio_order = 0
+	"""Priority order set by the scheduler on instances during the build phase.
+	You most likely do not need to set it.
+	"""
+
+	__slots__ = ('hasrun', 'generator', 'env', 'inputs', 'outputs', 'dep_nodes', 'run_after')
+
 	def __init__(self, *k, **kw):
-		"""
-		The base task class requires a task generator, which will be itself if missing
-		"""
 		self.hasrun = NOT_RUN
 		try:
 			self.generator = kw['generator']
 		except KeyError:
 			self.generator = self
 
-	def __repr__(self):
-		"for debugging purposes"
-		return '\n\t{task %r: %s %s}' % (self.__class__.__name__, id(self), str(getattr(self, 'fun', '')))
+		self.env = kw['env']
+		""":py:class:`waflib.ConfigSet.ConfigSet` object (make sure to provide one)"""
 
-	def __str__(self):
-		"string to display to the user"
-		if hasattr(self, 'fun'):
-			return self.fun.__name__
-		return self.__class__.__name__
+		self.inputs  = []
+		"""List of input nodes, which represent the files used by the task instance"""
 
-	def __hash__(self):
-		"Very fast hashing scheme but not persistent (replace/implement in subclasses and see :py:meth:`waflib.Task.Task.uid`)"
-		return id(self)
+		self.outputs = []
+		"""List of output nodes, which represent the files created by the task instance"""
 
-	def keyword(self):
-		if hasattr(self, 'fun'):
-			return 'Function'
-		return 'Processing'
+		self.dep_nodes = []
+		"""List of additional nodes to depend on"""
 
-	def exec_command(self, cmd, **kw):
-		"""
-		Wrapper for :py:meth:`waflib.Context.Context.exec_command` which sets a current working directory to ``build.variant_dir``
+		self.run_after = set()
+		"""Set of tasks that must be executed before this one"""
 
-		:return: the return code
-		:rtype: int
+	def __lt__(self, other):
+		return self.priority() > other.priority()
+	def __le__(self, other):
+		return self.priority() >= other.priority()
+	def __gt__(self, other):
+		return self.priority() < other.priority()
+	def __ge__(self, other):
+		return self.priority() <= other.priority()
+
+	def get_cwd(self):
+		"""
+		:return: current working directory
+		:rtype: :py:class:`waflib.Node.Node`
 		"""
 		bld = self.generator.bld
-		try:
-			if not kw.get('cwd', None):
-				kw['cwd'] = bld.cwd
-		except AttributeError:
-			bld.cwd = kw['cwd'] = bld.variant_dir
-		return bld.exec_command(cmd, **kw)
+		ret = getattr(self, 'cwd', None) or getattr(bld, 'cwd', bld.bldnode)
+		if isinstance(ret, str):
+			if os.path.isabs(ret):
+				ret = bld.root.make_node(ret)
+			else:
+				ret = self.generator.path.make_node(ret)
+		return ret
 
-	def runnable_status(self):
+	def quote_flag(self, x):
+		"""
+		Surround a process argument by quotes so that a list of arguments can be written to a file
+
+		:param x: flag
+		:type x: string
+		:return: quoted flag
+		:rtype: string
+		"""
+		old = x
+		if '\\' in x:
+			x = x.replace('\\', '\\\\')
+		if '"' in x:
+			x = x.replace('"', '\\"')
+		if old != x or ' ' in x or '\t' in x or "'" in x:
+			x = '"%s"' % x
+		return x
+
+	def priority(self):
+		"""
+		Priority of execution; the higher, the earlier
+
+		:return: the priority value
+		:rtype: a tuple of numeric values
+		"""
+		return (self.weight + self.prio_order, - getattr(self.generator, 'tg_idx_count', 0))
+
+	def split_argfile(self, cmd):
+		"""
+		Splits a list of process commands into the executable part and its list of arguments
+
+		:return: a tuple containing the executable first and then the rest of arguments
+		:rtype: tuple
+		"""
+		return ([cmd[0]], [self.quote_flag(x) for x in cmd[1:]])
+
+	def exec_command(self, cmd, **kw):
 		"""
-		State of the task
+		Wrapper for :py:meth:`waflib.Context.Context.exec_command`.
+		This version set the current working directory (``build.variant_dir``),
+		applies PATH settings (if self.env.PATH is provided), and can run long
+		commands through a temporary ``@argfile``.
 
-		:return: a task state in :py:const:`waflib.Task.RUN_ME`, :py:const:`waflib.Task.SKIP_ME` or :py:const:`waflib.Task.ASK_LATER`.
+		:param cmd: process command to execute
+		:type cmd: list of string (best) or string (process will use a shell)
+		:return: the return code
 		:rtype: int
+
+		Optional parameters:
+
+		#. cwd: current working directory (Node or string)
+		#. stdout: set to None to prevent waf from capturing the process standard output
+		#. stderr: set to None to prevent waf from capturing the process standard error
+		#. timeout: timeout value (Python 3)
 		"""
-		return RUN_ME
+		if not 'cwd' in kw:
+			kw['cwd'] = self.get_cwd()
+
+		if hasattr(self, 'timeout'):
+			kw['timeout'] = self.timeout
+
+		if self.env.PATH:
+			env = kw['env'] = dict(kw.get('env') or self.env.env or os.environ)
+			env['PATH'] = self.env.PATH if isinstance(self.env.PATH, str) else os.pathsep.join(self.env.PATH)
+
+		if hasattr(self, 'stdout'):
+			kw['stdout'] = self.stdout
+		if hasattr(self, 'stderr'):
+			kw['stderr'] = self.stderr
+
+		# workaround for command line length limit:
+		# http://support.microsoft.com/kb/830473
+		if not isinstance(cmd, str) and (len(repr(cmd)) >= 8192 if Utils.is_win32 else len(cmd) > 200000):
+			cmd, args = self.split_argfile(cmd)
+			try:
+				(fd, tmp) = tempfile.mkstemp()
+				os.write(fd, '\r\n'.join(args).encode())
+				os.close(fd)
+				if Logs.verbose:
+					Logs.debug('argfile: @%r -> %r', tmp, args)
+				return self.generator.bld.exec_command(cmd + ['@' + tmp], **kw)
+			finally:
+				try:
+					os.remove(tmp)
+				except OSError:
+					# anti-virus and indexers can keep files open -_-
+					pass
+		else:
+			return self.generator.bld.exec_command(cmd, **kw)
 
 	def process(self):
 		"""
-		Assume that the task has had a new attribute ``master`` which is an instance of :py:class:`waflib.Runner.Parallel`.
-		Execute the task and then put it back in the queue :py:attr:`waflib.Runner.Parallel.out` (may be replaced by subclassing).
-		"""
-		m = self.master
-		if m.stop:
-			m.out.put(self)
-			return
+		Runs the task and handles errors
 
+		:return: 0 or None if everything is fine
+		:rtype: integer
+		"""
 		# remove the task signature immediately before it is executed
-		# in case of failure the task will be executed again
+		# so that the task will be executed again in case of failure
 		try:
-			# TODO waf 1.9 - this breaks encapsulation
 			del self.generator.bld.task_sigs[self.uid()]
 		except KeyError:
 			pass
 
 		try:
-			self.generator.bld.returned_tasks.append(self)
-			self.log_display(self.generator.bld)
 			ret = self.run()
 		except Exception:
-			self.err_msg = Utils.ex_stack()
+			self.err_msg = traceback.format_exc()
 			self.hasrun = EXCEPTION
-
-			# TODO cleanup
-			m.error_handler(self)
-			m.out.put(self)
-			return
-
-		if ret:
-			self.err_code = ret
-			self.hasrun = CRASHED
 		else:
+			if ret:
+				self.err_code = ret
+				self.hasrun = CRASHED
+			else:
+				try:
+					self.post_run()
+				except Errors.WafError:
+					pass
+				except Exception:
+					self.err_msg = traceback.format_exc()
+					self.hasrun = EXCEPTION
+				else:
+					self.hasrun = SUCCESS
+
+		if self.hasrun != SUCCESS and self.scan:
+			# rescan dependencies on next run
 			try:
-				self.post_run()
-			except Errors.WafError:
+				del self.generator.bld.imp_sigs[self.uid()]
+			except KeyError:
 				pass
-			except Exception:
-				self.err_msg = Utils.ex_stack()
-				self.hasrun = EXCEPTION
-			else:
-				self.hasrun = SUCCESS
-		if self.hasrun != SUCCESS:
-			m.error_handler(self)
-
-		m.out.put(self)
-
-	def run(self):
-		"""
-		Called by threads to execute the tasks. The default is empty and meant to be overridden in subclasses.
-		It is a bad idea to create nodes in this method (so, no node.ant_glob)
-
-		:rtype: int
-		"""
-		if hasattr(self, 'fun'):
-			return self.fun(self)
-		return 0
-
-	def post_run(self):
-		"Update the cache files (executed by threads). Override in subclasses."
-		pass
 
 	def log_display(self, bld):
-		"Write the execution status on the context logger"
+		"Writes the execution status on the context logger"
 		if self.generator.bld.progress_bar == 3:
 			return
 
@@ -278,20 +384,17 @@ def log_display(self, bld):
 
 	def display(self):
 		"""
-		Return an execution status for the console, the progress bar, or the IDE output.
+		Returns an execution status for the console, the progress bar, or the IDE output.
 
 		:rtype: string
 		"""
 		col1 = Logs.colors(self.color)
 		col2 = Logs.colors.NORMAL
-		master = self.master
+		master = self.generator.bld.producer
 
 		def cur():
 			# the current task position, computed as late as possible
-			tmp = -1
-			if hasattr(master, 'ready'):
-				tmp -= master.ready.qsize()
-			return master.processed + tmp
+			return master.processed - master.ready.qsize()
 
 		if self.generator.bld.progress_bar == 1:
 			return self.generator.bld.progress_line(cur(), master.total, col1, col2)
@@ -320,37 +423,25 @@ def cur():
 			kw += ' '
 		return fs % (cur(), total, kw, col1, s, col2)
 
-	def attr(self, att, default=None):
-		"""
-		Retrieve an attribute from the instance or from the class.
-
-		:param att: variable name
-		:type att: string
-		:param default: default value
-		"""
-		ret = getattr(self, att, self)
-		if ret is self: return getattr(self.__class__, att, default)
-		return ret
-
 	def hash_constraints(self):
 		"""
-		Identify a task type for all the constraints relevant for the scheduler: precedence, file production
+		Identifies a task type for all the constraints relevant for the scheduler: precedence, file production
 
 		:return: a hash value
 		:rtype: string
 		"""
-		cls = self.__class__
-		tup = (str(cls.before), str(cls.after), str(cls.ext_in), str(cls.ext_out), cls.__name__, cls.hcode)
-		h = hash(tup)
-		return h
+		return (tuple(self.before), tuple(self.after), tuple(self.ext_in), tuple(self.ext_out), self.__class__.__name__, self.hcode)
 
 	def format_error(self):
 		"""
-		Error message to display to the user when a build fails
+		Returns an error message to display the build failure reasons
 
 		:rtype: string
 		"""
-		msg = getattr(self, 'last_cmd', '')
+		if Logs.verbose:
+			msg = ': %r\n%r' % (self, getattr(self, 'last_cmd', ''))
+		else:
+			msg = ' (run with -v to display more information)'
 		name = getattr(self.generator, 'name', '')
 		if getattr(self, "err_msg", None):
 			return self.err_msg
@@ -358,27 +449,29 @@ def format_error(self):
 			return 'task in %r was not executed for some reason: %r' % (name, self)
 		elif self.hasrun == CRASHED:
 			try:
-				return ' -> task in %r failed (exit status %r): %r\n%r' % (name, self.err_code, self, msg)
+				return ' -> task in %r failed with exit status %r%s' % (name, self.err_code, msg)
 			except AttributeError:
-				return ' -> task in %r failed: %r\n%r' % (name, self, msg)
+				return ' -> task in %r failed%s' % (name, msg)
 		elif self.hasrun == MISSING:
-			return ' -> missing files in %r: %r\n%r' % (name, self, msg)
+			return ' -> missing files in %r%s' % (name, msg)
+		elif self.hasrun == CANCELED:
+			return ' -> %r canceled because of missing dependencies' % name
 		else:
 			return 'invalid status for task in %r: %r' % (name, self.hasrun)
 
 	def colon(self, var1, var2):
 		"""
-		Support code for scriptlet expressions such as ${FOO_ST:FOO}
+		Enable scriptlet expressions of the form ${FOO_ST:FOO}
 		If the first variable (FOO_ST) is empty, then an empty list is returned
 
 		The results will be slightly different if FOO_ST is a list, for example::
 
-			env.FOO_ST = ['-a', '-b']
+			env.FOO    = ['p1', 'p2']
 			env.FOO_ST = '-I%s'
 			# ${FOO_ST:FOO} returns
 			['-Ip1', '-Ip2']
 
-			env.FOO    = ['p1', 'p2']
+			env.FOO_ST = ['-a', '-b']
 			# ${FOO_ST:FOO} returns
 			['-a', '-b', 'p1', '-a', '-b', 'p2']
 		"""
@@ -399,47 +492,11 @@ def colon(self, var1, var2):
 				lst.append(y)
 			return lst
 
-class Task(TaskBase):
-	"""
-	This class deals with the filesystem (:py:class:`waflib.Node.Node`). The method :py:class:`waflib.Task.Task.runnable_status`
-	uses a hash value (from :py:class:`waflib.Task.Task.signature`) which is persistent from build to build. When the value changes,
-	the task has to be executed. The method :py:class:`waflib.Task.Task.post_run` will assign the task signature to the output
-	nodes (if present).
-
-	.. warning:: For backward compatibility reasons, the suffix "_task" is truncated in derived class names. This limitation will be removed in Waf 1.9.
-	"""
-	vars = []
-	"""Variables to depend on (class attribute used for :py:meth:`waflib.Task.Task.sig_vars`)"""
-
-	shell = False
-	"""Execute the command with the shell (class attribute)"""
-
-	def __init__(self, *k, **kw):
-		TaskBase.__init__(self, *k, **kw)
-
-		self.env = kw['env']
-		"""ConfigSet object (make sure to provide one)"""
-
-		self.inputs  = []
-		"""List of input nodes, which represent the files used by the task instance"""
-
-		self.outputs = []
-		"""List of output nodes, which represent the files created by the task instance"""
-
-		self.dep_nodes = []
-		"""List of additional nodes to depend on"""
-
-		self.run_after = set([])
-		"""Set of tasks that must be executed before this one"""
-
-		# Additionally, you may define the following
-		#self.dep_vars  = 'PREFIX DATADIR'
-
 	def __str__(self):
 		"string to display to the user"
 		name = self.__class__.__name__
 		if self.outputs:
-			if (name.endswith('lib') or name.endswith('program')) or not self.inputs:
+			if name.endswith(('lib', 'program')) or not self.inputs:
 				node = self.outputs[0]
 				return node.path_from(node.ctx.launch_node())
 		if not (self.inputs or self.outputs):
@@ -450,13 +507,16 @@ def __str__(self):
 
 		src_str = ' '.join([a.path_from(a.ctx.launch_node()) for a in self.inputs])
 		tgt_str = ' '.join([a.path_from(a.ctx.launch_node()) for a in self.outputs])
-		if self.outputs: sep = ' -> '
-		else: sep = ''
-		return '%s: %s%s%s' % (self.__class__.__name__.replace('_task', ''), src_str, sep, tgt_str)
+		if self.outputs:
+			sep = ' -> '
+		else:
+			sep = ''
+		return '%s: %s%s%s' % (self.__class__.__name__, src_str, sep, tgt_str)
 
 	def keyword(self):
+		"Display keyword used to prettify the console outputs"
 		name = self.__class__.__name__
-		if name.endswith('lib') or name.endswith('program'):
+		if name.endswith(('lib', 'program')):
 			return 'Linking'
 		if len(self.inputs) == 1 and len(self.outputs) == 1:
 			return 'Compiling'
@@ -479,10 +539,10 @@ def __repr__(self):
 
 	def uid(self):
 		"""
-		Return an identifier used to determine if tasks are up-to-date. Since the
+		Returns an identifier used to determine if tasks are up-to-date. Since the
 		identifier will be stored between executions, it must be:
 
-			- unique: no two tasks return the same value (for a given build context)
+			- unique for a task: no two tasks return the same value (for a given build context)
 			- the same for a given task instance
 
 		By default, the node paths, the class name, and the function are used
@@ -497,44 +557,48 @@ def uid(self):
 		try:
 			return self.uid_
 		except AttributeError:
-			m = Utils.md5()
+			m = Utils.md5(self.__class__.__name__)
 			up = m.update
-			up(self.__class__.__name__)
 			for x in self.inputs + self.outputs:
 				up(x.abspath())
 			self.uid_ = m.digest()
 			return self.uid_
 
-
 	def set_inputs(self, inp):
 		"""
-		Append the nodes to the *inputs*
+		Appends the nodes to the *inputs* list
 
 		:param inp: input nodes
 		:type inp: node or list of nodes
 		"""
-		if isinstance(inp, list): self.inputs += inp
-		else: self.inputs.append(inp)
+		if isinstance(inp, list):
+			self.inputs += inp
+		else:
+			self.inputs.append(inp)
 
 	def set_outputs(self, out):
 		"""
-		Append the nodes to the *outputs*
+		Appends the nodes to the *outputs* list
 
 		:param out: output nodes
 		:type out: node or list of nodes
 		"""
-		if isinstance(out, list): self.outputs += out
-		else: self.outputs.append(out)
+		if isinstance(out, list):
+			self.outputs += out
+		else:
+			self.outputs.append(out)
 
 	def set_run_after(self, task):
 		"""
-		Run this task only after *task*. Affect :py:meth:`waflib.Task.runnable_status`
-		You probably want to use tsk.run_after.add(task) directly
+		Run this task only after the given *task*.
+
+		Calling this method from :py:meth:`waflib.Task.Task.runnable_status` may cause
+		build deadlocks; see :py:meth:`waflib.Tools.fc.fc.runnable_status` for details.
 
 		:param task: task
 		:type task: :py:class:`waflib.Task.Task`
 		"""
-		assert isinstance(task, TaskBase)
+		assert isinstance(task, Task)
 		self.run_after.add(task)
 
 	def signature(self):
@@ -544,7 +608,7 @@ def signature(self):
 
 		* explicit dependencies: files listed in the inputs (list of node objects) :py:meth:`waflib.Task.Task.sig_explicit_deps`
 		* implicit dependencies: list of nodes returned by scanner methods (when present) :py:meth:`waflib.Task.Task.sig_implicit_deps`
-		* hashed data: variables/values read from task.__class__.vars/task.env :py:meth:`waflib.Task.Task.sig_vars`
+		* hashed data: variables/values read from task.vars/task.env :py:meth:`waflib.Task.Task.sig_vars`
 
 		If the signature is expected to give a different result, clear the cache kept in ``self.cache_sig``::
 
@@ -554,12 +618,16 @@ def signature(self):
 					sig = super(Task.Task, self).signature()
 					delattr(self, 'cache_sig')
 					return super(Task.Task, self).signature()
+
+		:return: the signature value
+		:rtype: string or bytes
 		"""
-		try: return self.cache_sig
-		except AttributeError: pass
+		try:
+			return self.cache_sig
+		except AttributeError:
+			pass
 
-		self.m = Utils.md5()
-		self.m.update(self.hcode)
+		self.m = Utils.md5(self.hcode)
 
 		# explicit deps
 		self.sig_explicit_deps()
@@ -579,16 +647,22 @@ def signature(self):
 
 	def runnable_status(self):
 		"""
-		Override :py:meth:`waflib.Task.TaskBase.runnable_status` to determine if the task is ready
-		to be run (:py:attr:`waflib.Task.Task.run_after`)
+		Returns the Task status
+
+		:return: a task state in :py:const:`waflib.Task.RUN_ME`,
+			:py:const:`waflib.Task.SKIP_ME`, :py:const:`waflib.Task.CANCEL_ME` or :py:const:`waflib.Task.ASK_LATER`.
+		:rtype: int
 		"""
-		#return 0 # benchmarking
+		bld = self.generator.bld
+		if bld.is_install < 0:
+			return SKIP_ME
 
 		for t in self.run_after:
 			if not t.hasrun:
 				return ASK_LATER
-
-		bld = self.generator.bld
+			elif t.hasrun < SKIPPED:
+				# a dependency has an error
+				return CANCEL_ME
 
 		# first compute the signature
 		try:
@@ -601,105 +675,110 @@ def runnable_status(self):
 		try:
 			prev_sig = bld.task_sigs[key]
 		except KeyError:
-			Logs.debug("task: task %r must run as it was never run before or the task code changed" % self)
+			Logs.debug('task: task %r must run: it was never run before or the task code changed', self)
+			return RUN_ME
+
+		if new_sig != prev_sig:
+			Logs.debug('task: task %r must run: the task signature changed', self)
 			return RUN_ME
 
 		# compare the signatures of the outputs
 		for node in self.outputs:
-			try:
-				if node.sig != new_sig:
-					return RUN_ME
-			except AttributeError:
-				Logs.debug("task: task %r must run as the output nodes do not exist" % self)
+			sig = bld.node_sigs.get(node)
+			if not sig:
+				Logs.debug('task: task %r must run: an output node has no signature', self)
+				return RUN_ME
+			if sig != key:
+				Logs.debug('task: task %r must run: an output node was produced by another task', self)
+				return RUN_ME
+			if not node.exists():
+				Logs.debug('task: task %r must run: an output node does not exist', self)
 				return RUN_ME
 
-		if new_sig != prev_sig:
-			return RUN_ME
-		return SKIP_ME
+		return (self.always_run and RUN_ME) or SKIP_ME
 
 	def post_run(self):
 		"""
-		Called after successful execution to update the cache data :py:class:`waflib.Node.Node` sigs
-		and :py:attr:`waflib.Build.BuildContext.task_sigs`.
-
-		The node signature is obtained from the task signature, but the output nodes may also get the signature
-		of their contents. See the class decorator :py:func:`waflib.Task.update_outputs` if you need this behaviour.
+		Called after successful execution to record that the task has run by
+		updating the entry in :py:attr:`waflib.Build.BuildContext.task_sigs`.
 		"""
 		bld = self.generator.bld
-		sig = self.signature()
-
 		for node in self.outputs:
-			# check if the node exists ..
-			try:
-				os.stat(node.abspath())
-			except OSError:
+			if not node.exists():
 				self.hasrun = MISSING
 				self.err_msg = '-> missing file: %r' % node.abspath()
 				raise Errors.WafError(self.err_msg)
-
-			# important, store the signature for the next run
-			node.sig = node.cache_sig = sig
-
-		bld.task_sigs[self.uid()] = self.cache_sig
+			bld.node_sigs[node] = self.uid() # make sure this task produced the files in question
+		bld.task_sigs[self.uid()] = self.signature()
+		if not self.keep_last_cmd:
+			try:
+				del self.last_cmd
+			except AttributeError:
+				pass
 
 	def sig_explicit_deps(self):
 		"""
-		Used by :py:meth:`waflib.Task.Task.signature`, hash :py:attr:`waflib.Task.Task.inputs`
+		Used by :py:meth:`waflib.Task.Task.signature`; it hashes :py:attr:`waflib.Task.Task.inputs`
 		and :py:attr:`waflib.Task.Task.dep_nodes` signatures.
-
-		:rtype: hash value
 		"""
 		bld = self.generator.bld
 		upd = self.m.update
 
 		# the inputs
 		for x in self.inputs + self.dep_nodes:
-			try:
-				upd(x.get_bld_sig())
-			except (AttributeError, TypeError):
-				raise Errors.WafError('Missing node signature for %r (required by %r)' % (x, self))
+			upd(x.get_bld_sig())
 
 		# manual dependencies, they can slow down the builds
 		if bld.deps_man:
 			additional_deps = bld.deps_man
 			for x in self.inputs + self.outputs:
 				try:
-					d = additional_deps[id(x)]
+					d = additional_deps[x]
 				except KeyError:
 					continue
 
 				for v in d:
-					if isinstance(v, bld.root.__class__):
-						try:
-							v = v.get_bld_sig()
-						except AttributeError:
-							raise Errors.WafError('Missing node signature for %r (required by %r)' % (v, self))
-					elif hasattr(v, '__call__'):
-						v = v() # dependency is a function, call it
+					try:
+						v = v.get_bld_sig()
+					except AttributeError:
+						if hasattr(v, '__call__'):
+							v = v() # dependency is a function, call it
 					upd(v)
 
-		return self.m.digest()
-
-	def sig_vars(self):
+	def sig_deep_inputs(self):
 		"""
-		Used by :py:meth:`waflib.Task.Task.signature`, hash :py:attr:`waflib.Task.Task.env` variables/values
+		Enable rebuilds on input files task signatures. Not used by default.
 
-		:rtype: hash value
-		"""
-		bld = self.generator.bld
-		env = self.env
-		upd = self.m.update
+		Example: hashes of output programs can be unchanged after being re-linked,
+		despite the libraries being different. This method can thus prevent stale unit test
+		results (waf_unit_test.py).
 
-		# dependencies on the environment vars
-		act_sig = bld.hash_env_vars(env, self.__class__.vars)
-		upd(act_sig)
+		Hashing input file timestamps is another possibility for the implementation.
+		This may cause unnecessary rebuilds when input tasks are frequently executed.
+		Here is an implementation example::
 
-		# additional variable dependencies, if provided
-		dep_vars = getattr(self, 'dep_vars', None)
-		if dep_vars:
-			upd(bld.hash_env_vars(env, dep_vars))
+			lst = []
+			for node in self.inputs + self.dep_nodes:
+				st = os.stat(node.abspath())
+				lst.append(st.st_mtime)
+				lst.append(st.st_size)
+			self.m.update(Utils.h_list(lst))
+
+		The downside of the implementation is that it absolutely requires all build directory
+		files to be declared within the current build.
+		"""
+		bld = self.generator.bld
+		lst = [bld.task_sigs[bld.node_sigs[node]] for node in (self.inputs + self.dep_nodes) if node.is_bld()]
+		self.m.update(Utils.h_list(lst))
 
-		return self.m.digest()
+	def sig_vars(self):
+		"""
+		Used by :py:meth:`waflib.Task.Task.signature`; it hashes :py:attr:`waflib.Task.Task.env` variables/values
+		When overriding this method, and if scriptlet expressions are used, make sure to follow
+		the code in :py:meth:`waflib.Task.Task.compile_sig_vars` to enable dependencies on scriptlet results.
+		"""
+		sig = self.generator.bld.hash_env_vars(self.env, self.vars)
+		self.m.update(sig)
 
 	scan = None
 	"""
@@ -713,29 +792,26 @@ def sig_vars(self):
 		from waflib.Task import Task
 		class mytask(Task):
 			def scan(self, node):
-				return ((), ())
+				return ([], [])
 
-	The first and second lists are stored in :py:attr:`waflib.Build.BuildContext.node_deps` and
+	The first and second lists in the tuple are stored in :py:attr:`waflib.Build.BuildContext.node_deps` and
 	:py:attr:`waflib.Build.BuildContext.raw_deps` respectively.
 	"""
 
 	def sig_implicit_deps(self):
 		"""
-		Used by :py:meth:`waflib.Task.Task.signature` hashes node signatures obtained by scanning for dependencies (:py:meth:`waflib.Task.Task.scan`).
+		Used by :py:meth:`waflib.Task.Task.signature`; it hashes node signatures
+		obtained by scanning for dependencies (:py:meth:`waflib.Task.Task.scan`).
 
 		The exception :py:class:`waflib.Errors.TaskRescan` is thrown
-		when a file has changed. When this occurs, :py:meth:`waflib.Task.Task.signature` is called
-		once again, and this method will be executed once again, this time calling :py:meth:`waflib.Task.Task.scan`
-		for searching the dependencies.
-
-		:rtype: hash value
+		when a file has changed. In this case, the method :py:meth:`waflib.Task.Task.signature` is called
+		once again, and return here to call :py:meth:`waflib.Task.Task.scan` and searching for dependencies.
 		"""
-
 		bld = self.generator.bld
 
 		# get the task signatures from previous runs
 		key = self.uid()
-		prev = bld.task_sigs.get((key, 'imp'), [])
+		prev = bld.imp_sigs.get(key, [])
 
 		# for issue #379
 		if prev:
@@ -745,74 +821,56 @@ def sig_implicit_deps(self):
 			except Errors.TaskNotReady:
 				raise
 			except EnvironmentError:
-				# when a file was renamed (IOError usually), remove the stale nodes (headers in folders without source files)
+				# when a file was renamed, remove the stale nodes (headers in folders without source files)
 				# this will break the order calculation for headers created during the build in the source directory (should be uncommon)
 				# the behaviour will differ when top != out
 				for x in bld.node_deps.get(self.uid(), []):
-					if not x.is_bld():
+					if not x.is_bld() and not x.exists():
 						try:
-							os.stat(x.abspath())
-						except OSError:
-							try:
-								del x.parent.children[x.name]
-							except KeyError:
-								pass
-			del bld.task_sigs[(key, 'imp')]
+							del x.parent.children[x.name]
+						except KeyError:
+							pass
+			del bld.imp_sigs[key]
 			raise Errors.TaskRescan('rescan')
 
 		# no previous run or the signature of the dependencies has changed, rescan the dependencies
-		(nodes, names) = self.scan()
+		(bld.node_deps[key], bld.raw_deps[key]) = self.scan()
 		if Logs.verbose:
-			Logs.debug('deps: scanner for %s returned %s %s' % (str(self), str(nodes), str(names)))
-
-		# store the dependencies in the cache
-		bld.node_deps[key] = nodes
-		bld.raw_deps[key] = names
-
-		# might happen
-		self.are_implicit_nodes_ready()
+			Logs.debug('deps: scanner for %s: %r; unresolved: %r', self, bld.node_deps[key], bld.raw_deps[key])
 
 		# recompute the signature and return it
 		try:
-			bld.task_sigs[(key, 'imp')] = sig = self.compute_sig_implicit_deps()
-		except Exception:
-			if Logs.verbose:
-				for k in bld.node_deps.get(self.uid(), []):
-					try:
-						k.get_bld_sig()
-					except Exception:
-						Logs.warn('Missing signature for node %r (may cause rebuilds)' % k)
-		else:
-			return sig
+			bld.imp_sigs[key] = self.compute_sig_implicit_deps()
+		except EnvironmentError:
+			for k in bld.node_deps.get(self.uid(), []):
+				if not k.exists():
+					Logs.warn('Dependency %r for %r is missing: check the task declaration and the build order!', k, self)
+			raise
 
 	def compute_sig_implicit_deps(self):
 		"""
 		Used by :py:meth:`waflib.Task.Task.sig_implicit_deps` for computing the actual hash of the
 		:py:class:`waflib.Node.Node` returned by the scanner.
 
-		:return: hash value
-		:rtype: string
+		:return: a hash value for the implicit dependencies
+		:rtype: string or bytes
 		"""
-
 		upd = self.m.update
-
-		bld = self.generator.bld
-
 		self.are_implicit_nodes_ready()
 
 		# scanner returns a node that does not have a signature
 		# just *ignore* the error and let them figure out from the compiler output
 		# waf -k behaviour
-		for k in bld.node_deps.get(self.uid(), []):
+		for k in self.generator.bld.node_deps.get(self.uid(), []):
 			upd(k.get_bld_sig())
 		return self.m.digest()
 
 	def are_implicit_nodes_ready(self):
 		"""
-		For each node returned by the scanner, see if there is a task behind it, and force the build order
+		For each node returned by the scanner, see if there is a task that creates it,
+		and infer the build order
 
-		The performance impact on null builds is nearly invisible (1.66s->1.86s), but this is due to
-		agressive caching (1.86s->28s)
+		This has a low performance impact on null builds (1.86s->1.66s) thanks to caching (28s->1.86s)
 		"""
 		bld = self.generator.bld
 		try:
@@ -820,10 +878,11 @@ def are_implicit_nodes_ready(self):
 		except AttributeError:
 			bld.dct_implicit_nodes = cache = {}
 
+		# one cache per build group
 		try:
-			dct = cache[bld.cur]
+			dct = cache[bld.current_group]
 		except KeyError:
-			dct = cache[bld.cur] = {}
+			dct = cache[bld.current_group] = {}
 			for tsk in bld.cur_tasks:
 				for x in tsk.outputs:
 					dct[x] = tsk
@@ -844,11 +903,10 @@ def uid(self):
 		try:
 			return self.uid_
 		except AttributeError:
-			m = Utils.md5()
+			m = Utils.md5(self.__class__.__name__.encode('latin-1', 'xmlcharrefreplace'))
 			up = m.update
-			up(self.__class__.__name__.encode('iso8859-1', 'xmlcharrefreplace'))
 			for x in self.inputs + self.outputs:
-				up(x.abspath().encode('iso8859-1', 'xmlcharrefreplace'))
+				up(x.abspath().encode('latin-1', 'xmlcharrefreplace'))
 			self.uid_ = m.digest()
 			return self.uid_
 	uid.__doc__ = Task.uid.__doc__
@@ -856,7 +914,7 @@ def uid(self):
 
 def is_before(t1, t2):
 	"""
-	Return a non-zero value if task t1 is to be executed before task t2::
+	Returns a non-zero value if task t1 is to be executed before task t2::
 
 		t1.ext_out = '.h'
 		t2.ext_in = '.h'
@@ -864,10 +922,10 @@ def is_before(t1, t2):
 		t1.before = ['t2']
 		waflib.Task.is_before(t1, t2) # True
 
-	:param t1: task
-	:type t1: :py:class:`waflib.Task.TaskBase`
-	:param t2: task
-	:type t2: :py:class:`waflib.Task.TaskBase`
+	:param t1: Task object
+	:type t1: :py:class:`waflib.Task.Task`
+	:param t2: Task object
+	:type t2: :py:class:`waflib.Task.Task`
 	"""
 	to_list = Utils.to_list
 	for k in to_list(t2.ext_in):
@@ -884,30 +942,53 @@ def is_before(t1, t2):
 
 def set_file_constraints(tasks):
 	"""
-	Adds tasks to the task 'run_after' attribute based on the task inputs and outputs
+	Updates the ``run_after`` attribute of all tasks based on the task inputs and outputs
 
 	:param tasks: tasks
-	:type tasks: list of :py:class:`waflib.Task.TaskBase`
+	:type tasks: list of :py:class:`waflib.Task.Task`
 	"""
 	ins = Utils.defaultdict(set)
 	outs = Utils.defaultdict(set)
 	for x in tasks:
-		for a in getattr(x, 'inputs', []) + getattr(x, 'dep_nodes', []):
-			ins[id(a)].add(x)
-		for a in getattr(x, 'outputs', []):
-			outs[id(a)].add(x)
+		for a in x.inputs:
+			ins[a].add(x)
+		for a in x.dep_nodes:
+			ins[a].add(x)
+		for a in x.outputs:
+			outs[a].add(x)
 
 	links = set(ins.keys()).intersection(outs.keys())
 	for k in links:
 		for a in ins[k]:
 			a.run_after.update(outs[k])
 
+
+class TaskGroup(object):
+	"""
+	Wrap nxm task order constraints into a single object
+	to prevent the creation of large list/set objects
+
+	This is an optimization
+	"""
+	def __init__(self, prev, next):
+		self.prev = prev
+		self.next = next
+		self.done = False
+
+	def get_hasrun(self):
+		for k in self.prev:
+			if not k.hasrun:
+				return NOT_RUN
+		return SUCCESS
+
+	hasrun = property(get_hasrun, None)
+
 def set_precedence_constraints(tasks):
 	"""
-	Add tasks to the task 'run_after' attribute based on the after/before/ext_out/ext_in attributes
+	Updates the ``run_after`` attribute of all tasks based on the after/before/ext_out/ext_in attributes
 
 	:param tasks: tasks
-	:type tasks: list of :py:class:`waflib.Task.TaskBase`
+	:type tasks: list of :py:class:`waflib.Task.Task`
 	"""
 	cstr_groups = Utils.defaultdict(list)
 	for x in tasks:
@@ -933,13 +1014,20 @@ def set_precedence_constraints(tasks):
 			else:
 				continue
 
-			aval = set(cstr_groups[keys[a]])
-			for x in cstr_groups[keys[b]]:
-				x.run_after.update(aval)
+			a = cstr_groups[keys[a]]
+			b = cstr_groups[keys[b]]
+
+			if len(a) < 2 or len(b) < 2:
+				for x in b:
+					x.run_after.update(a)
+			else:
+				group = TaskGroup(set(a), set(b))
+				for x in b:
+					x.run_after.add(group)
 
 def funex(c):
 	"""
-	Compile a function by 'exec'
+	Compiles a scriptlet expression into a Python function
 
 	:param c: function to compile
 	:type c: string
@@ -950,118 +1038,183 @@ def funex(c):
 	exec(c, dc)
 	return dc['f']
 
-reg_act = re.compile(r"(?P<backslash>\\)|(?P<dollar>\$\$)|(?P<subst>\$\{(?P<var>\w+)(?P<code>.*?)\})", re.M)
+re_cond = re.compile('(?P<var>\w+)|(?P<or>\|)|(?P<and>&)')
+re_novar = re.compile(r'^(SRC|TGT)\W+.*?$')
+reg_act = re.compile(r'(?P<backslash>\\)|(?P<dollar>\$\$)|(?P<subst>\$\{(?P<var>\w+)(?P<code>.*?)\})', re.M)
 def compile_fun_shell(line):
 	"""
-	Create a compiled function to execute a process with the shell
-	WARNING: this method may disappear anytime, so use compile_fun instead
+	Creates a compiled function to execute a process through a sub-shell
 	"""
-
 	extr = []
 	def repl(match):
 		g = match.group
-		if g('dollar'): return "$"
-		elif g('backslash'): return '\\\\'
-		elif g('subst'): extr.append((g('var'), g('code'))); return "%s"
+		if g('dollar'):
+			return "$"
+		elif g('backslash'):
+			return '\\\\'
+		elif g('subst'):
+			extr.append((g('var'), g('code')))
+			return "%s"
 		return None
-
 	line = reg_act.sub(repl, line) or line
+	dvars = []
+	def add_dvar(x):
+		if x not in dvars:
+			dvars.append(x)
+
+	def replc(m):
+		# performs substitutions and populates dvars
+		if m.group('and'):
+			return ' and '
+		elif m.group('or'):
+			return ' or '
+		else:
+			x = m.group('var')
+			add_dvar(x)
+			return 'env[%r]' % x
 
 	parm = []
-	dvars = []
 	app = parm.append
 	for (var, meth) in extr:
 		if var == 'SRC':
-			if meth: app('tsk.inputs%s' % meth)
-			else: app('" ".join([a.path_from(cwdx) for a in tsk.inputs])')
+			if meth:
+				app('tsk.inputs%s' % meth)
+			else:
+				app('" ".join([a.path_from(cwdx) for a in tsk.inputs])')
 		elif var == 'TGT':
-			if meth: app('tsk.outputs%s' % meth)
-			else: app('" ".join([a.path_from(cwdx) for a in tsk.outputs])')
+			if meth:
+				app('tsk.outputs%s' % meth)
+			else:
+				app('" ".join([a.path_from(cwdx) for a in tsk.outputs])')
 		elif meth:
 			if meth.startswith(':'):
+				add_dvar(var)
 				m = meth[1:]
 				if m == 'SRC':
 					m = '[a.path_from(cwdx) for a in tsk.inputs]'
 				elif m == 'TGT':
 					m = '[a.path_from(cwdx) for a in tsk.outputs]'
-				elif m[:3] not in ('tsk', 'gen', 'bld'):
-					dvars.extend([var, meth[1:]])
-					m = '%r' % m
+				elif re_novar.match(m):
+					m = '[tsk.inputs%s]' % m[3:]
+				elif re_novar.match(m):
+					m = '[tsk.outputs%s]' % m[3:]
+				else:
+					add_dvar(m)
+					if m[:3] not in ('tsk', 'gen', 'bld'):
+						m = '%r' % m
 				app('" ".join(tsk.colon(%r, %s))' % (var, m))
+			elif meth.startswith('?'):
+				# In A?B|C output env.A if one of env.B or env.C is non-empty
+				expr = re_cond.sub(replc, meth[1:])
+				app('p(%r) if (%s) else ""' % (var, expr))
 			else:
-				app('%s%s' % (var, meth))
+				call = '%s%s' % (var, meth)
+				add_dvar(call)
+				app(call)
 		else:
-			if not var in dvars: dvars.append(var)
+			add_dvar(var)
 			app("p('%s')" % var)
-	if parm: parm = "%% (%s) " % (',\n\t\t'.join(parm))
-	else: parm = ''
+	if parm:
+		parm = "%% (%s) " % (',\n\t\t'.join(parm))
+	else:
+		parm = ''
 
 	c = COMPILE_TEMPLATE_SHELL % (line, parm)
-
-	Logs.debug('action: %s' % c.strip().splitlines())
+	Logs.debug('action: %s', c.strip().splitlines())
 	return (funex(c), dvars)
 
+reg_act_noshell = re.compile(r"(?P<space>\s+)|(?P<subst>\$\{(?P<var>\w+)(?P<code>.*?)\})|(?P<text>([^$ \t\n\r\f\v]|\$\$)+)", re.M)
 def compile_fun_noshell(line):
 	"""
-	Create a compiled function to execute a process without the shell
-	WARNING: this method may disappear anytime, so use compile_fun instead
+	Creates a compiled function to execute a process without a sub-shell
 	"""
-	extr = []
-	def repl(match):
-		g = match.group
-		if g('dollar'): return "$"
-		elif g('backslash'): return '\\'
-		elif g('subst'): extr.append((g('var'), g('code'))); return "<<|@|>>"
-		return None
-
-	line2 = reg_act.sub(repl, line)
-	params = line2.split('<<|@|>>')
-	assert(extr)
-
 	buf = []
 	dvars = []
+	merge = False
 	app = buf.append
-	for x in range(len(extr)):
-		params[x] = params[x].strip()
-		if params[x]:
-			app("lst.extend(%r)" % params[x].split())
-		(var, meth) = extr[x]
-		if var == 'SRC':
-			if meth: app('lst.append(tsk.inputs%s)' % meth)
-			else: app("lst.extend([a.path_from(cwdx) for a in tsk.inputs])")
-		elif var == 'TGT':
-			if meth: app('lst.append(tsk.outputs%s)' % meth)
-			else: app("lst.extend([a.path_from(cwdx) for a in tsk.outputs])")
-		elif meth:
-			if meth.startswith(':'):
-				m = meth[1:]
-				if m == 'SRC':
-					m = '[a.path_from(cwdx) for a in tsk.inputs]'
-				elif m == 'TGT':
-					m = '[a.path_from(cwdx) for a in tsk.outputs]'
-				elif m[:3] not in ('tsk', 'gen', 'bld'):
-					dvars.extend([var, m])
-					m = '%r' % m
-				app('lst.extend(tsk.colon(%r, %s))' % (var, m))
-			else:
-				app('lst.extend(gen.to_list(%s%s))' % (var, meth))
-		else:
-			app('lst.extend(to_list(env[%r]))' % var)
-			if not var in dvars: dvars.append(var)
 
-	if extr:
-		if params[-1]:
-			app("lst.extend(%r)" % params[-1].split())
+	def add_dvar(x):
+		if x not in dvars:
+			dvars.append(x)
+
+	def replc(m):
+		# performs substitutions and populates dvars
+		if m.group('and'):
+			return ' and '
+		elif m.group('or'):
+			return ' or '
+		else:
+			x = m.group('var')
+			add_dvar(x)
+			return 'env[%r]' % x
+
+	for m in reg_act_noshell.finditer(line):
+		if m.group('space'):
+			merge = False
+			continue
+		elif m.group('text'):
+			app('[%r]' % m.group('text').replace('$$', '$'))
+		elif m.group('subst'):
+			var = m.group('var')
+			code = m.group('code')
+			if var == 'SRC':
+				if code:
+					app('[tsk.inputs%s]' % code)
+				else:
+					app('[a.path_from(cwdx) for a in tsk.inputs]')
+			elif var == 'TGT':
+				if code:
+					app('[tsk.outputs%s]' % code)
+				else:
+					app('[a.path_from(cwdx) for a in tsk.outputs]')
+			elif code:
+				if code.startswith(':'):
+					# a composed variable ${FOO:OUT}
+					add_dvar(var)
+					m = code[1:]
+					if m == 'SRC':
+						m = '[a.path_from(cwdx) for a in tsk.inputs]'
+					elif m == 'TGT':
+						m = '[a.path_from(cwdx) for a in tsk.outputs]'
+					elif re_novar.match(m):
+						m = '[tsk.inputs%s]' % m[3:]
+					elif re_novar.match(m):
+						m = '[tsk.outputs%s]' % m[3:]
+					else:
+						add_dvar(m)
+						if m[:3] not in ('tsk', 'gen', 'bld'):
+							m = '%r' % m
+					app('tsk.colon(%r, %s)' % (var, m))
+				elif code.startswith('?'):
+					# In A?B|C output env.A if one of env.B or env.C is non-empty
+					expr = re_cond.sub(replc, code[1:])
+					app('to_list(env[%r] if (%s) else [])' % (var, expr))
+				else:
+					# plain code such as ${tsk.inputs[0].abspath()}
+					call = '%s%s' % (var, code)
+					add_dvar(call)
+					app('gen.to_list(%s)' % call)
+			else:
+				# a plain variable such as # a plain variable like ${AR}
+				app('to_list(env[%r])' % var)
+				add_dvar(var)
+		if merge:
+			tmp = 'merge(%s, %s)' % (buf[-2], buf[-1])
+			del buf[-1]
+			buf[-1] = tmp
+		merge = True # next turn
+
+	buf = ['lst.extend(%s)' % x for x in buf]
 	fun = COMPILE_TEMPLATE_NOSHELL % "\n\t".join(buf)
-	Logs.debug('action: %s' % fun.strip().splitlines())
+	Logs.debug('action: %s', fun.strip().splitlines())
 	return (funex(fun), dvars)
 
 def compile_fun(line, shell=False):
 	"""
-	Parse a string expression such as "${CC} ${SRC} -o ${TGT}" and return a pair containing:
+	Parses a string expression such as '${CC} ${SRC} -o ${TGT}' and returns a pair containing:
 
-	* the function created (compiled) for use as :py:meth:`waflib.Task.TaskBase.run`
-	* the list of variables that imply a dependency from self.env
+	* The function created (compiled) for use as :py:meth:`waflib.Task.Task.run`
+	* The list of variables that must cause rebuilds when *env* data is modified
 
 	for example::
 
@@ -1071,8 +1224,8 @@ def compile_fun(line, shell=False):
 		def build(bld):
 			bld(source='wscript', rule='echo "foo\\${SRC[0].name}\\bar"')
 
-	The env variables (CXX, ..) on the task must not hold dicts (order)
-	The reserved keywords *TGT* and *SRC* represent the task input and output nodes
+	The env variables (CXX, ..) on the task must not hold dicts so as to preserve a consistent order.
+	The reserved keywords ``TGT`` and ``SRC`` represent the task input and output nodes
 
 	"""
 	if isinstance(line, str):
@@ -1095,12 +1248,42 @@ def composed_fun(task):
 				if ret:
 					return ret
 			return None
-		return composed_fun, dvars
+		return composed_fun, dvars_lst
 	if shell:
 		return compile_fun_shell(line)
 	else:
 		return compile_fun_noshell(line)
 
+def compile_sig_vars(vars):
+	"""
+	This method produces a sig_vars method suitable for subclasses that provide
+	scriptlet code in their run_str code.
+	If no such method can be created, this method returns None.
+
+	The purpose of the sig_vars method returned is to ensures
+	that rebuilds occur whenever the contents of the expression changes.
+	This is the case B below::
+
+		import time
+		# case A: regular variables
+		tg = bld(rule='echo ${FOO}')
+		tg.env.FOO = '%s' % time.time()
+		# case B
+		bld(rule='echo ${gen.foo}', foo='%s' % time.time())
+
+	:param vars: env variables such as CXXFLAGS or gen.foo
+	:type vars: list of string
+	:return: A sig_vars method relevant for dependencies if adequate, else None
+	:rtype: A function, or None in most cases
+	"""
+	buf = []
+	for x in sorted(vars):
+		if x[:3] in ('tsk', 'gen', 'bld'):
+			buf.append('buf.append(%s)' % x)
+	if buf:
+		return funex(COMPILE_TEMPLATE_SIG_VARS % '\n\t'.join(buf))
+	return None
+
 def task_factory(name, func=None, vars=None, color='GREEN', ext_in=[], ext_out=[], before=[], after=[], shell=False, scan=None):
 	"""
 	Returns a new task subclass with the function ``run`` compiled from the line given.
@@ -1122,10 +1305,6 @@ def task_factory(name, func=None, vars=None, color='GREEN', ext_in=[], ext_out=[
 		'vars': vars or [], # function arguments are static, and this one may be modified by the class
 		'color': color,
 		'name': name,
-		'ext_in': Utils.to_list(ext_in),
-		'ext_out': Utils.to_list(ext_out),
-		'before': Utils.to_list(before),
-		'after': Utils.to_list(after),
 		'shell': shell,
 		'scan': scan,
 	}
@@ -1136,80 +1315,80 @@ def task_factory(name, func=None, vars=None, color='GREEN', ext_in=[], ext_out=[
 		params['run'] = func
 
 	cls = type(Task)(name, (Task,), params)
-	global classes
 	classes[name] = cls
-	return cls
 
+	if ext_in:
+		cls.ext_in = Utils.to_list(ext_in)
+	if ext_out:
+		cls.ext_out = Utils.to_list(ext_out)
+	if before:
+		cls.before = Utils.to_list(before)
+	if after:
+		cls.after = Utils.to_list(after)
 
-def always_run(cls):
-	"""
-	Task class decorator
+	return cls
 
-	Set all task instances of this class to be executed whenever a build is started
-	The task signature is calculated, but the result of the comparation between
-	task signatures is bypassed
+def deep_inputs(cls):
 	"""
-	old = cls.runnable_status
-	def always(self):
-		ret = old(self)
-		if ret == SKIP_ME:
-			ret = RUN_ME
-		return ret
-	cls.runnable_status = always
+	Task class decorator to enable rebuilds on input files task signatures
+	"""
+	def sig_explicit_deps(self):
+		Task.sig_explicit_deps(self)
+		Task.sig_deep_inputs(self)
+	cls.sig_explicit_deps = sig_explicit_deps
 	return cls
 
-def update_outputs(cls):
+TaskBase = Task
+"Provided for compatibility reasons, TaskBase should not be used"
+
+class TaskSemaphore(object):
 	"""
-	Task class decorator
+	Task semaphores provide a simple and efficient way of throttling the amount of
+	a particular task to run concurrently. The throttling value is capped
+	by the amount of maximum jobs, so for example, a `TaskSemaphore(10)`
+	has no effect in a `-j2` build.
 
-	If you want to create files in the source directory. For example, to keep *foo.txt* in the source
-	directory, create it first and declare::
+	Task semaphores are typically specified on the task class level::
 
-		def build(bld):
-			bld(rule='cp ${SRC} ${TGT}', source='wscript', target='foo.txt', update_outputs=True)
+		class compile(waflib.Task.Task):
+			semaphore = waflib.Task.TaskSemaphore(2)
+			run_str = 'touch ${TGT}'
+
+	Task semaphores are meant to be used by the build scheduler in the main
+	thread, so there are no guarantees of thread safety.
 	"""
-	old_post_run = cls.post_run
-	def post_run(self):
-		old_post_run(self)
-		for node in self.outputs:
-			node.sig = node.cache_sig = Utils.h_file(node.abspath())
-			self.generator.bld.task_sigs[node.abspath()] = self.uid() # issue #1017
-	cls.post_run = post_run
+	def __init__(self, num):
+		"""
+		:param num: maximum value of concurrent tasks
+		:type num: int
+		"""
+		self.num = num
+		self.locking = set()
+		self.waiting = set()
 
+	def is_locked(self):
+		"""Returns True if this semaphore cannot be acquired by more tasks"""
+		return len(self.locking) >= self.num
 
-	old_runnable_status = cls.runnable_status
-	def runnable_status(self):
-		status = old_runnable_status(self)
-		if status != RUN_ME:
-			return status
+	def acquire(self, tsk):
+		"""
+		Mark the semaphore as used by the given task (not re-entrant).
 
-		try:
-			# by default, we check that the output nodes have the signature of the task
-			# perform a second check, returning 'SKIP_ME' as we are expecting that
-			# the signatures do not match
-			bld = self.generator.bld
-			prev_sig = bld.task_sigs[self.uid()]
-			if prev_sig == self.signature():
-				for x in self.outputs:
-					if not x.is_child_of(bld.bldnode):
-						# special case of files created in the source directory
-						# hash them here for convenience -_-
-						x.sig = Utils.h_file(x.abspath())
-					if not x.sig or bld.task_sigs[x.abspath()] != self.uid():
-						return RUN_ME
-				return SKIP_ME
-		except OSError:
-			pass
-		except IOError:
-			pass
-		except KeyError:
-			pass
-		except IndexError:
-			pass
-		except AttributeError:
-			pass
-		return RUN_ME
-	cls.runnable_status = runnable_status
+		:param tsk: task object
+		:type tsk: :py:class:`waflib.Task.Task`
+		:raises: :py:class:`IndexError` in case the resource is already acquired
+		"""
+		if self.is_locked():
+			raise IndexError('Cannot lock more %r' % self.locking)
+		self.locking.add(tsk)
 
-	return cls
+	def release(self, tsk):
+		"""
+		Mark the semaphore as unused by the given task.
+
+		:param tsk: task object
+		:type tsk: :py:class:`waflib.Task.Task`
+		:raises: :py:class:`KeyError` in case the resource is not acquired by the task
+		"""
+		self.locking.remove(tsk)
 
diff --git a/waflib/TaskGen.py b/waflib/TaskGen.py
index b897e077..a74e6431 100644
--- a/waflib/TaskGen.py
+++ b/waflib/TaskGen.py
@@ -1,18 +1,16 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 Task generators
 
 The class :py:class:`waflib.TaskGen.task_gen` encapsulates the creation of task objects (low-level code)
 The instances can have various parameters, but the creation of task nodes (Task.py)
-is always postponed. To achieve this, various methods are called from the method "apply"
-
-
+is deferred. To achieve this, various methods are called from the method "apply"
 """
 
-import copy, re, os
+import copy, re, os, functools
 from waflib import Task, Utils, Logs, Errors, ConfigSet, Node
 
 feats = Utils.defaultdict(set)
@@ -22,7 +20,7 @@
 
 class task_gen(object):
 	"""
-	Instances of this class create :py:class:`waflib.Task.TaskBase` when
+	Instances of this class create :py:class:`waflib.Task.Task` when
 	calling the method :py:meth:`waflib.TaskGen.task_gen.post` from the main thread.
 	A few notes:
 
@@ -34,42 +32,28 @@ class task_gen(object):
 	"""
 
 	mappings = Utils.ordered_iter_dict()
-	"""Mappings are global file extension mappings, they are retrieved in the order of definition"""
+	"""Mappings are global file extension mappings that are retrieved in the order of definition"""
 
-	prec = Utils.defaultdict(list)
-	"""Dict holding the precedence rules for task generator methods"""
+	prec = Utils.defaultdict(set)
+	"""Dict that holds the precedence execution rules for task generator methods"""
 
 	def __init__(self, *k, **kw):
 		"""
-		The task generator objects predefine various attributes (source, target) for possible
+		Task generator objects predefine various attributes (source, target) for possible
 		processing by process_rule (make-like rules) or process_source (extensions, misc methods)
 
-		The tasks are stored on the attribute 'tasks'. They are created by calling methods
-		listed in self.meths *or* referenced in the attribute features
-		A topological sort is performed to ease the method re-use.
+		Tasks are stored on the attribute 'tasks'. They are created by calling methods
+		listed in ``self.meths`` or referenced in the attribute ``features``
+		A topological sort is performed to execute the methods in correct order.
 
-		The extra key/value elements passed in kw are set as attributes
+		The extra key/value elements passed in ``kw`` are set as attributes
 		"""
-
-		# so we will have to play with directed acyclic graphs
-		# detect cycles, etc
-		self.source = ''
+		self.source = []
 		self.target = ''
 
 		self.meths = []
 		"""
-		List of method names to execute (it is usually a good idea to avoid touching this)
-		"""
-
-		self.prec = Utils.defaultdict(list)
-		"""
-		Precedence table for sorting the methods in self.meths
-		"""
-
-		self.mappings = {}
-		"""
-		List of mappings {extension -> function} for processing files by extension
-		This is very rarely used, so we do not use an ordered dict here
+		List of method names to execute (internal)
 		"""
 
 		self.features = []
@@ -79,7 +63,7 @@ def __init__(self, *k, **kw):
 
 		self.tasks = []
 		"""
-		List of tasks created.
+		Tasks created are added to this list
 		"""
 
 		if not 'bld' in kw:
@@ -92,31 +76,50 @@ def __init__(self, *k, **kw):
 			self.env = self.bld.env.derive()
 			self.path = self.bld.path # emulate chdir when reading scripts
 
-			# provide a unique id
+			# Provide a unique index per folder
+			# This is part of a measure to prevent output file name collisions
+			path = self.path.abspath()
 			try:
-				self.idx = self.bld.idx[id(self.path)] = self.bld.idx.get(id(self.path), 0) + 1
+				self.idx = self.bld.idx[path] = self.bld.idx.get(path, 0) + 1
 			except AttributeError:
 				self.bld.idx = {}
-				self.idx = self.bld.idx[id(self.path)] = 1
+				self.idx = self.bld.idx[path] = 1
+
+			# Record the global task generator count
+			try:
+				self.tg_idx_count = self.bld.tg_idx_count = self.bld.tg_idx_count + 1
+			except AttributeError:
+				self.tg_idx_count = self.bld.tg_idx_count = 1
 
 		for key, val in kw.items():
 			setattr(self, key, val)
 
 	def __str__(self):
-		"""for debugging purposes"""
+		"""Debugging helper"""
 		return "<task_gen %r declared in %s>" % (self.name, self.path.abspath())
 
 	def __repr__(self):
-		"""for debugging purposes"""
+		"""Debugging helper"""
 		lst = []
-		for x in self.__dict__.keys():
+		for x in self.__dict__:
 			if x not in ('env', 'bld', 'compiled_tasks', 'tasks'):
 				lst.append("%s=%s" % (x, repr(getattr(self, x))))
 		return "bld(%s) in %s" % (", ".join(lst), self.path.abspath())
 
+	def get_cwd(self):
+		"""
+		Current working directory for the task generator, defaults to the build directory.
+		This is still used in a few places but it should disappear at some point as the classes
+		define their own working directory.
+
+		:rtype: :py:class:`waflib.Node.Node`
+		"""
+		return self.bld.bldnode
+
 	def get_name(self):
 		"""
-		If not set, the name is computed from the target name::
+		If the attribute ``name`` is not set on the instance,
+		the name is computed from the target name::
 
 			def build(bld):
 				x = bld(name='foo')
@@ -143,18 +146,20 @@ def set_name(self, name):
 
 	def to_list(self, val):
 		"""
-		Ensure that a parameter is a list
+		Ensures that a parameter is a list, see :py:func:`waflib.Utils.to_list`
 
 		:type val: string or list of string
 		:param val: input to return as a list
 		:rtype: list
 		"""
-		if isinstance(val, str): return val.split()
-		else: return val
+		if isinstance(val, str):
+			return val.split()
+		else:
+			return val
 
 	def post(self):
 		"""
-		Create task objects. The following operations are performed:
+		Creates tasks for this task generators. The following operations are performed:
 
 		#. The body of this method is called only once and sets the attribute ``posted``
 		#. The attribute ``features`` is used to add more methods in ``self.meths``
@@ -162,27 +167,25 @@ def post(self):
 		#. The methods are then executed in order
 		#. The tasks created are added to :py:attr:`waflib.TaskGen.task_gen.tasks`
 		"""
-
-		# we could add a decorator to let the task run once, but then python 2.3 will be difficult to support
 		if getattr(self, 'posted', None):
-			#error("OBJECT ALREADY POSTED" + str( self))
 			return False
 		self.posted = True
 
 		keys = set(self.meths)
+		keys.update(feats['*'])
 
 		# add the methods listed in the features
 		self.features = Utils.to_list(self.features)
-		for x in self.features + ['*']:
+		for x in self.features:
 			st = feats[x]
-			if not st:
-				if not x in Task.classes:
-					Logs.warn('feature %r does not exist - bind at least one method to it' % x)
-			keys.update(list(st)) # ironpython 2.7 wants the cast to list
+			if st:
+				keys.update(st)
+			elif not x in Task.classes:
+				Logs.warn('feature %r does not exist - bind at least one method to it?', x)
 
 		# copy the precedence table
 		prec = {}
-		prec_tbl = self.prec or task_gen.prec
+		prec_tbl = self.prec
 		for x in prec_tbl:
 			if x in keys:
 				prec[x] = prec_tbl[x]
@@ -191,17 +194,19 @@ def post(self):
 		tmp = []
 		for a in keys:
 			for x in prec.values():
-				if a in x: break
+				if a in x:
+					break
 			else:
 				tmp.append(a)
 
-		tmp.sort()
+		tmp.sort(reverse=True)
 
 		# topological sort
 		out = []
 		while tmp:
 			e = tmp.pop()
-			if e in keys: out.append(e)
+			if e in keys:
+				out.append(e)
 			try:
 				nlst = prec[e]
 			except KeyError:
@@ -214,46 +219,52 @@ def post(self):
 							break
 					else:
 						tmp.append(x)
+						tmp.sort(reverse=True)
 
 		if prec:
-			raise Errors.WafError('Cycle detected in the method execution %r' % prec)
-		out.reverse()
+			buf = ['Cycle detected in the method execution:']
+			for k, v in prec.items():
+				buf.append('- %s after %s' % (k, [x for x in v if x in prec]))
+			raise Errors.WafError('\n'.join(buf))
 		self.meths = out
 
 		# then we run the methods in order
-		Logs.debug('task_gen: posting %s %d' % (self, id(self)))
+		Logs.debug('task_gen: posting %s %d', self, id(self))
 		for x in out:
 			try:
 				v = getattr(self, x)
 			except AttributeError:
 				raise Errors.WafError('%r is not a valid task generator method' % x)
-			Logs.debug('task_gen: -> %s (%d)' % (x, id(self)))
+			Logs.debug('task_gen: -> %s (%d)', x, id(self))
 			v()
 
-		Logs.debug('task_gen: posted %s' % self.name)
+		Logs.debug('task_gen: posted %s', self.name)
 		return True
 
 	def get_hook(self, node):
 		"""
+		Returns the ``@extension`` method to call for a Node of a particular extension.
+
 		:param node: Input file to process
 		:type node: :py:class:`waflib.Tools.Node.Node`
 		:return: A method able to process the input node by looking at the extension
 		:rtype: function
 		"""
 		name = node.name
-		if self.mappings:
-			for k in self.mappings:
+		for k in self.mappings:
+			try:
 				if name.endswith(k):
 					return self.mappings[k]
-		for k in task_gen.mappings:
-			if name.endswith(k):
-				return task_gen.mappings[k]
-		raise Errors.WafError("File %r has no mapping in %r (have you forgotten to load a waf tool?)" % (node, task_gen.mappings.keys()))
+			except TypeError:
+				# regexps objects
+				if k.match(name):
+					return self.mappings[k]
+		keys = list(self.mappings.keys())
+		raise Errors.WafError("File %r has no mapping in %r (load a waf tool?)" % (node, keys))
 
 	def create_task(self, name, src=None, tgt=None, **kw):
 		"""
-		Wrapper for creating task instances. The classes are retrieved from the
-		context class if possible, then from the global dict Task.classes.
+		Creates task instances.
 
 		:param name: task class name
 		:type name: string
@@ -262,7 +273,7 @@ def create_task(self, name, src=None, tgt=None, **kw):
 		:param tgt: output nodes
 		:type tgt: list of :py:class:`waflib.Tools.Node.Node`
 		:return: A task object
-		:rtype: :py:class:`waflib.Task.TaskBase`
+		:rtype: :py:class:`waflib.Task.Task`
 		"""
 		task = Task.classes[name](env=self.env.derive(), generator=self)
 		if src:
@@ -275,7 +286,7 @@ def create_task(self, name, src=None, tgt=None, **kw):
 
 	def clone(self, env):
 		"""
-		Make a copy of a task generator. Once the copy is made, it is necessary to ensure that the
+		Makes a copy of a task generator. Once the copy is made, it is necessary to ensure that the
 		it does not create the same output files as the original, or the same files may
 		be compiled several times.
 
@@ -304,7 +315,7 @@ def clone(self, env):
 def declare_chain(name='', rule=None, reentrant=None, color='BLUE',
 	ext_in=[], ext_out=[], before=[], after=[], decider=None, scan=None, install_path=None, shell=False):
 	"""
-	Create a new mapping and a task class for processing files by extension.
+	Creates a new mapping and a task class for processing files by extension.
 	See Tools/flex.py for an example.
 
 	:param name: name for the task class
@@ -323,7 +334,7 @@ def declare_chain(name='', rule=None, reentrant=None, color='BLUE',
 	:type before: list of string
 	:param after: execute instances of this task after classes of the given names
 	:type after: list of string
-	:param decider: if present, use it to create the output nodes for the task
+	:param decider: if present, function that returns a list of output file extensions (overrides ext_out for output files, but not for the build order)
 	:type decider: function
 	:param scan: scanner function for the task
 	:type scan: function
@@ -337,14 +348,13 @@ def declare_chain(name='', rule=None, reentrant=None, color='BLUE',
 	cls = Task.task_factory(name, rule, color=color, ext_in=ext_in, ext_out=ext_out, before=before, after=after, scan=scan, shell=shell)
 
 	def x_file(self, node):
-		ext = decider and decider(self, node) or cls.ext_out
 		if ext_in:
 			_ext_in = ext_in[0]
 
 		tsk = self.create_task(name, node)
 		cnt = 0
 
-		keys = set(self.mappings.keys()) | set(self.__class__.mappings.keys())
+		ext = decider(self, node) if decider else cls.ext_out
 		for x in ext:
 			k = node.change_ext(x, ext_in=_ext_in)
 			tsk.outputs.append(k)
@@ -354,14 +364,14 @@ def x_file(self, node):
 					self.source.append(k)
 			else:
 				# reinject downstream files into the build
-				for y in keys: # ~ nfile * nextensions :-/
+				for y in self.mappings: # ~ nfile * nextensions :-/
 					if k.name.endswith(y):
 						self.source.append(k)
 						break
 			cnt += 1
 
 		if install_path:
-			self.bld.install_files(install_path, tsk.outputs)
+			self.install_task = self.add_install_files(install_to=install_path, install_from=tsk.outputs)
 		return tsk
 
 	for x in cls.ext_in:
@@ -370,7 +380,7 @@ def x_file(self, node):
 
 def taskgen_method(func):
 	"""
-	Decorator: register a method as a task generator method.
+	Decorator that registers method as a task generator method.
 	The function must accept a task generator as first parameter::
 
 		from waflib.TaskGen import taskgen_method
@@ -387,8 +397,8 @@ def mymethod(self):
 
 def feature(*k):
 	"""
-	Decorator: register a task generator method that will be executed when the
-	object attribute 'feature' contains the corresponding key(s)::
+	Decorator that registers a task generator method that will be executed when the
+	object attribute ``feature`` contains the corresponding key(s)::
 
 		from waflib.Task import feature
 		@feature('myfeature')
@@ -409,7 +419,7 @@ def deco(func):
 
 def before_method(*k):
 	"""
-	Decorator: register a task generator method which will be executed
+	Decorator that registera task generator method which will be executed
 	before the functions of given name(s)::
 
 		from waflib.TaskGen import feature, before
@@ -429,16 +439,14 @@ def build(bld):
 	def deco(func):
 		setattr(task_gen, func.__name__, func)
 		for fun_name in k:
-			if not func.__name__ in task_gen.prec[fun_name]:
-				task_gen.prec[fun_name].append(func.__name__)
-				#task_gen.prec[fun_name].sort()
+			task_gen.prec[func.__name__].add(fun_name)
 		return func
 	return deco
 before = before_method
 
 def after_method(*k):
 	"""
-	Decorator: register a task generator method which will be executed
+	Decorator that registers a task generator method which will be executed
 	after the functions of given name(s)::
 
 		from waflib.TaskGen import feature, after
@@ -458,16 +466,14 @@ def build(bld):
 	def deco(func):
 		setattr(task_gen, func.__name__, func)
 		for fun_name in k:
-			if not fun_name in task_gen.prec[func.__name__]:
-				task_gen.prec[func.__name__].append(fun_name)
-				#task_gen.prec[func.__name__].sort()
+			task_gen.prec[fun_name].add(func.__name__)
 		return func
 	return deco
 after = after_method
 
 def extension(*k):
 	"""
-	Decorator: register a task generator method which will be invoked during
+	Decorator that registers a task generator method which will be invoked during
 	the processing of source files for the extension given::
 
 		from waflib import Task
@@ -486,14 +492,11 @@ def deco(func):
 		return func
 	return deco
 
-# ---------------------------------------------------------------
-# The following methods are task generator methods commonly used
-# they are almost examples, the rest of waf core does not depend on them
-
 @taskgen_method
 def to_nodes(self, lst, path=None):
 	"""
-	Convert the input list into a list of nodes.
+	Flatten the input list of string/nodes/lists into a list of nodes.
+
 	It is used by :py:func:`waflib.TaskGen.process_source` and :py:func:`waflib.TaskGen.process_rule`.
 	It is designed for source files, for folders, see :py:func:`waflib.Tools.ccroot.to_incnodes`:
 
@@ -510,21 +513,23 @@ def to_nodes(self, lst, path=None):
 	if isinstance(lst, Node.Node):
 		lst = [lst]
 
-	# either a list or a string, convert to a list of nodes
 	for x in Utils.to_list(lst):
 		if isinstance(x, str):
 			node = find(x)
-		else:
+		elif hasattr(x, 'name'):
 			node = x
+		else:
+			tmp.extend(self.to_nodes(x))
+			continue
 		if not node:
-			raise Errors.WafError("source not found: %r in %r" % (x, self))
+			raise Errors.WafError('source not found: %r in %r' % (x, self))
 		tmp.append(node)
 	return tmp
 
 @feature('*')
 def process_source(self):
 	"""
-	Process each element in the attribute ``source`` by extension.
+	Processes each element in the attribute ``source`` by extension.
 
 	#. The *source* list is converted through :py:meth:`waflib.TaskGen.to_nodes` to a list of :py:class:`waflib.Node.Node` first.
 	#. File extensions are mapped to methods having the signature: ``def meth(self, node)`` by :py:meth:`waflib.TaskGen.extension`
@@ -540,10 +545,29 @@ def process_source(self):
 @before_method('process_source')
 def process_rule(self):
 	"""
-	Process the attribute ``rule``. When present, :py:meth:`waflib.TaskGen.process_source` is disabled::
+	Processes the attribute ``rule``. When present, :py:meth:`waflib.TaskGen.process_source` is disabled::
 
 		def build(bld):
 			bld(rule='cp ${SRC} ${TGT}', source='wscript', target='bar.txt')
+
+	Main attributes processed:
+
+	* rule: command to execute, it can be a tuple of strings for multiple commands
+	* chmod: permissions for the resulting files (integer value such as Utils.O755)
+	* shell: set to False to execute the command directly (default is True to use a shell)
+	* scan: scanner function
+	* vars: list of variables to trigger rebuilds, such as CFLAGS
+	* cls_str: string to display when executing the task
+	* cls_keyword: label to display when executing the task
+	* cache_rule: by default, try to re-use similar classes, set to False to disable
+	* source: list of Node or string objects representing the source files required by this task
+	* target: list of Node or string objects representing the files that this task creates
+	* cwd: current working directory (Node or string)
+	* stdout: standard output, set to None to prevent waf from capturing the text
+	* stderr: standard error, set to None to prevent waf from capturing the text
+	* timeout: timeout for command execution (Python 3)
+	* always: whether to always run the command (False by default)
+	* deep_inputs: whether the task must depend on the input file tasks too (False by default)
 	"""
 	if not getattr(self, 'rule', None):
 		return
@@ -557,28 +581,55 @@ def build(bld):
 	except AttributeError:
 		cache = self.bld.cache_rule_attr = {}
 
+	chmod = getattr(self, 'chmod', None)
+	shell = getattr(self, 'shell', True)
+	color = getattr(self, 'color', 'BLUE')
+	scan = getattr(self, 'scan', None)
+	_vars = getattr(self, 'vars', [])
+	cls_str = getattr(self, 'cls_str', None)
+	cls_keyword = getattr(self, 'cls_keyword', None)
+	use_cache = getattr(self, 'cache_rule', 'True')
+	deep_inputs = getattr(self, 'deep_inputs', False)
+
+	scan_val = has_deps = hasattr(self, 'deps')
+	if scan:
+		scan_val = id(scan)
+
+	key = Utils.h_list((name, self.rule, chmod, shell, color, cls_str, cls_keyword, scan_val, _vars, deep_inputs))
+
 	cls = None
-	if getattr(self, 'cache_rule', 'True'):
+	if use_cache:
 		try:
-			cls = cache[(name, self.rule)]
+			cls = cache[key]
 		except KeyError:
 			pass
 	if not cls:
-
 		rule = self.rule
-		if hasattr(self, 'chmod'):
+		if chmod is not None:
 			def chmod_fun(tsk):
 				for x in tsk.outputs:
-					os.chmod(x.abspath(), self.chmod)
-			rule = (self.rule, chmod_fun)
-
-		cls = Task.task_factory(name, rule,
-			getattr(self, 'vars', []),
-			shell=getattr(self, 'shell', True), color=getattr(self, 'color', 'BLUE'),
-			scan = getattr(self, 'scan', None))
-		if getattr(self, 'scan', None):
+					os.chmod(x.abspath(), tsk.generator.chmod)
+			if isinstance(rule, tuple):
+				rule = list(rule)
+				rule.append(chmod_fun)
+				rule = tuple(rule)
+			else:
+				rule = (rule, chmod_fun)
+
+		cls = Task.task_factory(name, rule, _vars, shell=shell, color=color)
+
+		if cls_str:
+			setattr(cls, '__str__', self.cls_str)
+
+		if cls_keyword:
+			setattr(cls, 'keyword', self.cls_keyword)
+
+		if deep_inputs:
+			Task.deep_inputs(cls)
+
+		if scan:
 			cls.scan = self.scan
-		elif getattr(self, 'deps', None):
+		elif has_deps:
 			def scan(self):
 				nodes = []
 				for x in self.generator.to_list(getattr(self.generator, 'deps', None)):
@@ -589,26 +640,26 @@ def scan(self):
 				return [nodes, []]
 			cls.scan = scan
 
-		if getattr(self, 'update_outputs', None):
-			Task.update_outputs(cls)
+		if use_cache:
+			cache[key] = cls
 
-		if getattr(self, 'always', None):
-			Task.always_run(cls)
+	# now create one instance
+	tsk = self.create_task(name)
 
-		for x in ('after', 'before', 'ext_in', 'ext_out'):
-			setattr(cls, x, getattr(self, x, []))
+	for x in ('after', 'before', 'ext_in', 'ext_out'):
+		setattr(tsk, x, getattr(self, x, []))
 
-		if getattr(self, 'cache_rule', 'True'):
-			cache[(name, self.rule)] = cls
+	if hasattr(self, 'stdout'):
+		tsk.stdout = self.stdout
 
-		if getattr(self, 'cls_str', None):
-			setattr(cls, '__str__', self.cls_str)
+	if hasattr(self, 'stderr'):
+		tsk.stderr = self.stderr
 
-		if getattr(self, 'cls_keyword', None):
-			setattr(cls, 'keyword', self.cls_keyword)
+	if getattr(self, 'timeout', None):
+		tsk.timeout = self.timeout
 
-	# now create one instance
-	tsk = self.create_task(name)
+	if getattr(self, 'always', None):
+		tsk.always_run = True
 
 	if getattr(self, 'target', None):
 		if isinstance(self.target, str):
@@ -622,7 +673,8 @@ def scan(self):
 				x.parent.mkdir() # if a node was given, create the required folders
 				tsk.outputs.append(x)
 		if getattr(self, 'install_path', None):
-			self.bld.install_files(self.install_path, tsk.outputs, chmod=getattr(self, 'chmod', Utils.O644))
+			self.install_task = self.add_install_files(install_to=self.install_path,
+				install_from=tsk.outputs, chmod=getattr(self, 'chmod', Utils.O644))
 
 	if getattr(self, 'source', None):
 		tsk.inputs = self.to_nodes(self.source)
@@ -632,10 +684,16 @@ def scan(self):
 	if getattr(self, 'cwd', None):
 		tsk.cwd = self.cwd
 
+	if isinstance(tsk.run, functools.partial):
+		# Python documentation says: "partial objects defined in classes
+		# behave like static methods and do not transform into bound
+		# methods during instance attribute look-up."
+		tsk.run = functools.partial(tsk.run, tsk)
+
 @feature('seq')
 def sequence_order(self):
 	"""
-	Add a strict sequential constraint between the tasks generated by task generators.
+	Adds a strict sequential constraint between the tasks generated by task generators.
 	It works because task generators are posted in order.
 	It will not post objects which belong to other folders.
 
@@ -673,7 +731,7 @@ def sequence_order(self):
 
 class subst_pc(Task.Task):
 	"""
-	Create *.pc* files from *.pc.in*. The task is executed whenever an input variable used
+	Creates *.pc* files from *.pc.in*. The task is executed whenever an input variable used
 	in the substitution changes.
 	"""
 
@@ -689,6 +747,8 @@ def run(self):
 		if getattr(self.generator, 'is_copy', None):
 			for i, x in enumerate(self.outputs):
 				x.write(self.inputs[i].read('rb'), 'wb')
+				stat = os.stat(self.inputs[i].abspath()) # Preserve mtime of the copy
+				os.utime(self.outputs[i].abspath(), (stat.st_atime, stat.st_mtime))
 			self.force_permissions()
 			return None
 
@@ -698,11 +758,11 @@ def run(self):
 				self.force_permissions()
 			return ret
 
-		code = self.inputs[0].read(encoding=getattr(self.generator, 'encoding', 'ISO8859-1'))
+		code = self.inputs[0].read(encoding=getattr(self.generator, 'encoding', 'latin-1'))
 		if getattr(self.generator, 'subst_fun', None):
 			code = self.generator.subst_fun(self, code)
 			if code is not None:
-				self.outputs[0].write(code, encoding=getattr(self.generator, 'encoding', 'ISO8859-1'))
+				self.outputs[0].write(code, encoding=getattr(self.generator, 'encoding', 'latin-1'))
 			self.force_permissions()
 			return None
 
@@ -717,7 +777,6 @@ def repl(match):
 				lst.append(g(1))
 				return "%%(%s)s" % g(1)
 			return ''
-		global re_m4
 		code = getattr(self.generator, 're_m4', re_m4).sub(repl, code)
 
 		try:
@@ -733,12 +792,14 @@ def repl(match):
 				d[x] = tmp
 
 		code = code % d
-		self.outputs[0].write(code, encoding=getattr(self.generator, 'encoding', 'ISO8859-1'))
-		self.generator.bld.raw_deps[self.uid()] = self.dep_vars = lst
+		self.outputs[0].write(code, encoding=getattr(self.generator, 'encoding', 'latin-1'))
+		self.generator.bld.raw_deps[self.uid()] = lst
 
 		# make sure the signature is updated
-		try: delattr(self, 'cache_sig')
-		except AttributeError: pass
+		try:
+			delattr(self, 'cache_sig')
+		except AttributeError:
+			pass
 
 		self.force_permissions()
 
@@ -770,13 +831,14 @@ def sig_vars(self):
 @extension('.pc.in')
 def add_pcfile(self, node):
 	"""
-	Process *.pc.in* files to *.pc*. Install the results to ``${PREFIX}/lib/pkgconfig/``
+	Processes *.pc.in* files to *.pc*. Installs the results to ``${PREFIX}/lib/pkgconfig/`` by default
 
 		def build(bld):
 			bld(source='foo.pc.in', install_path='${LIBDIR}/pkgconfig/')
 	"""
 	tsk = self.create_task('subst_pc', node, node.change_ext('.pc', '.pc.in'))
-	self.bld.install_files(getattr(self, 'install_path', '${LIBDIR}/pkgconfig/'), tsk.outputs)
+	self.install_task = self.add_install_files(
+		install_to=getattr(self, 'install_path', '${LIBDIR}/pkgconfig/'), install_from=tsk.outputs)
 
 class subst(subst_pc):
 	pass
@@ -785,7 +847,7 @@ class subst(subst_pc):
 @before_method('process_source', 'process_rule')
 def process_subst(self):
 	"""
-	Define a transformation that substitutes the contents of *source* files to *target* files::
+	Defines a transformation that substitutes the contents of *source* files to *target* files::
 
 		def build(bld):
 			bld(
@@ -820,7 +882,6 @@ def build(bld):
 			a = self.path.find_node(x)
 			b = self.path.get_bld().make_node(y)
 			if not os.path.isfile(b.abspath()):
-				b.sig = None
 				b.parent.mkdir()
 		else:
 			if isinstance(x, str):
@@ -835,25 +896,22 @@ def build(bld):
 		if not a:
 			raise Errors.WafError('could not find %r for %r' % (x, self))
 
-		has_constraints = False
 		tsk = self.create_task('subst', a, b)
 		for k in ('after', 'before', 'ext_in', 'ext_out'):
 			val = getattr(self, k, None)
 			if val:
-				has_constraints = True
 				setattr(tsk, k, val)
 
 		# paranoid safety measure for the general case foo.in->foo.h with ambiguous dependencies
-		if not has_constraints:
-			global HEADER_EXTS
-			for xt in HEADER_EXTS:
-				if b.name.endswith(xt):
-					tsk.before = [k for k in ('c', 'cxx') if k in Task.classes]
-					break
+		for xt in HEADER_EXTS:
+			if b.name.endswith(xt):
+				tsk.ext_in = tsk.ext_in + ['.h']
+				break
 
 		inst_to = getattr(self, 'install_path', None)
 		if inst_to:
-			self.bld.install_files(inst_to, b, chmod=getattr(self, 'chmod', Utils.O644))
+			self.install_task = self.add_install_files(install_to=inst_to,
+				install_from=b, chmod=getattr(self, 'chmod', Utils.O644))
 
 	self.source = []
 
diff --git a/waflib/Tools/__init__.py b/waflib/Tools/__init__.py
index c8a3c349..079df358 100644
--- a/waflib/Tools/__init__.py
+++ b/waflib/Tools/__init__.py
@@ -1,3 +1,3 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
diff --git a/waflib/Tools/ar.py b/waflib/Tools/ar.py
index aac39c0c..b39b6459 100644
--- a/waflib/Tools/ar.py
+++ b/waflib/Tools/ar.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 # Ralf Habacker, 2006 (rh)
 
 """
@@ -16,7 +16,7 @@ def find_ar(conf):
 	conf.load('ar')
 
 def configure(conf):
-	"""Find the ar program and set the default flags in ``conf.env.ARFLAGS``"""
+	"""Finds the ar program and sets the default flags in ``conf.env.ARFLAGS``"""
 	conf.find_program('ar', var='AR')
 	conf.add_os_flags('ARFLAGS')
 	if not conf.env.ARFLAGS:
diff --git a/waflib/Tools/asm.py b/waflib/Tools/asm.py
new file mode 100644
index 00000000..b6f26fb3
--- /dev/null
+++ b/waflib/Tools/asm.py
@@ -0,0 +1,73 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2008-2018 (ita)
+
+"""
+Assembly support, used by tools such as gas and nasm
+
+To declare targets using assembly::
+
+	def configure(conf):
+		conf.load('gcc gas')
+
+	def build(bld):
+		bld(
+			features='c cstlib asm',
+			source = 'test.S',
+			target = 'asmtest')
+
+		bld(
+			features='asm asmprogram',
+			source = 'test.S',
+			target = 'asmtest')
+
+Support for pure asm programs and libraries should also work::
+
+	def configure(conf):
+		conf.load('nasm')
+		conf.find_program('ld', 'ASLINK')
+
+	def build(bld):
+		bld(
+			features='asm asmprogram',
+			source = 'test.S',
+			target = 'asmtest')
+"""
+
+from waflib import Task
+from waflib.Tools.ccroot import link_task, stlink_task
+from waflib.TaskGen import extension
+
+class asm(Task.Task):
+	"""
+	Compiles asm files by gas/nasm/yasm/...
+	"""
+	color = 'BLUE'
+	run_str = '${AS} ${ASFLAGS} ${ASMPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} ${AS_SRC_F}${SRC} ${AS_TGT_F}${TGT}'
+
+@extension('.s', '.S', '.asm', '.ASM', '.spp', '.SPP')
+def asm_hook(self, node):
+	"""
+	Binds the asm extension to the asm task
+
+	:param node: input file
+	:type node: :py:class:`waflib.Node.Node`
+	"""
+	return self.create_compiled_task('asm', node)
+
+class asmprogram(link_task):
+	"Links object files into a c program"
+	run_str = '${ASLINK} ${ASLINKFLAGS} ${ASLNK_TGT_F}${TGT} ${ASLNK_SRC_F}${SRC}'
+	ext_out = ['.bin']
+	inst_to = '${BINDIR}'
+
+class asmshlib(asmprogram):
+	"Links object files into a c shared library"
+	inst_to = '${LIBDIR}'
+
+class asmstlib(stlink_task):
+	"Links object files into a c static library"
+	pass # do not remove
+
+def configure(conf):
+	conf.env.ASMPATH_ST = '-I%s'
diff --git a/waflib/Tools/bison.py b/waflib/Tools/bison.py
new file mode 100644
index 00000000..eef56dcd
--- /dev/null
+++ b/waflib/Tools/bison.py
@@ -0,0 +1,49 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# John O'Meara, 2006
+# Thomas Nagy 2009-2018 (ita)
+
+"""
+The **bison** program is a code generator which creates C or C++ files.
+The generated files are compiled into object files.
+"""
+
+from waflib import Task
+from waflib.TaskGen import extension
+
+class bison(Task.Task):
+	"""Compiles bison files"""
+	color   = 'BLUE'
+	run_str = '${BISON} ${BISONFLAGS} ${SRC[0].abspath()} -o ${TGT[0].name}'
+	ext_out = ['.h'] # just to make sure
+
+@extension('.y', '.yc', '.yy')
+def big_bison(self, node):
+	"""
+	Creates a bison task, which must be executed from the directory of the output file.
+	"""
+	has_h = '-d' in self.env.BISONFLAGS
+
+	outs = []
+	if node.name.endswith('.yc'):
+		outs.append(node.change_ext('.tab.cc'))
+		if has_h:
+			outs.append(node.change_ext('.tab.hh'))
+	else:
+		outs.append(node.change_ext('.tab.c'))
+		if has_h:
+			outs.append(node.change_ext('.tab.h'))
+
+	tsk = self.create_task('bison', node, outs)
+	tsk.cwd = node.parent.get_bld()
+
+	# and the c/cxx file must be compiled too
+	self.source.append(outs[0])
+
+def configure(conf):
+	"""
+	Detects the *bison* program
+	"""
+	conf.find_program('bison', var='BISON')
+	conf.env.BISONFLAGS = ['-d']
+
diff --git a/waflib/Tools/c.py b/waflib/Tools/c.py
index 0b06a7fa..effd6b6e 100644
--- a/waflib/Tools/c.py
+++ b/waflib/Tools/c.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 
 "Base for c programs/libraries"
 
@@ -10,30 +10,30 @@
 
 @TaskGen.extension('.c')
 def c_hook(self, node):
-	"Bind the c file extension to the creation of a :py:class:`waflib.Tools.c.c` instance"
+	"Binds the c file extensions create :py:class:`waflib.Tools.c.c` instances"
 	if not self.env.CC and self.env.CXX:
 		return self.create_compiled_task('cxx', node)
 	return self.create_compiled_task('c', node)
 
 class c(Task.Task):
-	"Compile C files into object files"
-	run_str = '${CC} ${ARCH_ST:ARCH} ${CFLAGS} ${CPPFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} ${CC_SRC_F}${SRC} ${CC_TGT_F}${TGT[0].abspath()}'
+	"Compiles C files into object files"
+	run_str = '${CC} ${ARCH_ST:ARCH} ${CFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} ${CC_SRC_F}${SRC} ${CC_TGT_F}${TGT[0].abspath()} ${CPPFLAGS}'
 	vars    = ['CCDEPS'] # unused variable to depend on, just in case
 	ext_in  = ['.h'] # set the build order easily by using ext_out=['.h']
 	scan    = c_preproc.scan
 
 class cprogram(link_task):
-	"Link object files into a c program"
+	"Links object files into c programs"
 	run_str = '${LINK_CC} ${LINKFLAGS} ${CCLNK_SRC_F}${SRC} ${CCLNK_TGT_F}${TGT[0].abspath()} ${RPATH_ST:RPATH} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${FRAMEWORK_ST:FRAMEWORK} ${ARCH_ST:ARCH} ${STLIB_MARKER} ${STLIBPATH_ST:STLIBPATH} ${STLIB_ST:STLIB} ${SHLIB_MARKER} ${LIBPATH_ST:LIBPATH} ${LIB_ST:LIB} ${LDFLAGS}'
 	ext_out = ['.bin']
 	vars    = ['LINKDEPS']
 	inst_to = '${BINDIR}'
 
 class cshlib(cprogram):
-	"Link object files into a c shared library"
+	"Links object files into c shared libraries"
 	inst_to = '${LIBDIR}'
 
 class cstlib(stlink_task):
-	"Link object files into a c static library"
+	"Links object files into a c static libraries"
 	pass # do not remove
 
diff --git a/waflib/Tools/c_aliases.py b/waflib/Tools/c_aliases.py
index 0747abf5..c9d53692 100644
--- a/waflib/Tools/c_aliases.py
+++ b/waflib/Tools/c_aliases.py
@@ -9,6 +9,8 @@
 
 def get_extensions(lst):
 	"""
+	Returns the file extensions for the list of files given as input
+
 	:param lst: files to process
 	:list lst: list of string or :py:class:`waflib.Node.Node`
 	:return: list of file extensions
@@ -16,17 +18,15 @@ def get_extensions(lst):
 	"""
 	ret = []
 	for x in Utils.to_list(lst):
-		try:
-			if not isinstance(x, str):
-				x = x.name
-			ret.append(x[x.rfind('.') + 1:])
-		except Exception:
-			pass
+		if not isinstance(x, str):
+			x = x.name
+		ret.append(x[x.rfind('.') + 1:])
 	return ret
 
 def sniff_features(**kw):
 	"""
-	Look at the source files and return the features for a task generator (mainly cc and cxx)::
+	Computes and returns the features required for a task generator by
+	looking at the file extensions. This aimed for C/C++ mainly::
 
 		snif_features(source=['foo.c', 'foo.cxx'], type='shlib')
 		# returns  ['cxx', 'c', 'cxxshlib', 'cshlib']
@@ -39,7 +39,7 @@ def sniff_features(**kw):
 	:rtype: list of string
 	"""
 	exts = get_extensions(kw['source'])
-	type = kw['_type']
+	typ = kw['typ']
 	feats = []
 
 	# watch the order, cxx will have the precedence
@@ -63,18 +63,27 @@ def sniff_features(**kw):
 		feats.append('java')
 		return 'java'
 
-	if type in ('program', 'shlib', 'stlib'):
+	if typ in ('program', 'shlib', 'stlib'):
 		will_link = False
 		for x in feats:
 			if x in ('cxx', 'd', 'fc', 'c'):
-				feats.append(x + type)
+				feats.append(x + typ)
 				will_link = True
 		if not will_link and not kw.get('features', []):
 			raise Errors.WafError('Cannot link from %r, try passing eg: features="c cprogram"?' % kw)
 	return feats
 
-def set_features(kw, _type):
-	kw['_type'] = _type
+def set_features(kw, typ):
+	"""
+	Inserts data in the input dict *kw* based on existing data and on the type of target
+	required (typ).
+
+	:param kw: task generator parameters
+	:type kw: dict
+	:param typ: type of target
+	:type typ: string
+	"""
+	kw['typ'] = typ
 	kw['features'] = Utils.to_list(kw.get('features', [])) + Utils.to_list(sniff_features(**kw))
 
 @conf
diff --git a/waflib/Tools/c_config.py b/waflib/Tools/c_config.py
index f114dffb..d2b3c0d8 100644
--- a/waflib/Tools/c_config.py
+++ b/waflib/Tools/c_config.py
@@ -1,11 +1,13 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 C/C++/D configuration helpers
 """
 
+from __future__ import with_statement
+
 import os, re, shlex
 from waflib import Build, Utils, Task, Options, Logs, Errors, Runner
 from waflib.TaskGen import after_method, feature
@@ -17,32 +19,6 @@
 DEFKEYS = 'define_key'
 INCKEYS = 'include_key'
 
-cfg_ver = {
-	'atleast-version': '>=',
-	'exact-version': '==',
-	'max-version': '<=',
-}
-
-SNIP_FUNCTION = '''
-int main(int argc, char **argv) {
-	void (*p)();
-	(void)argc; (void)argv;
-	p=(void(*)())(%s);
-	return !p;
-}
-'''
-"""Code template for checking for functions"""
-
-SNIP_TYPE = '''
-int main(int argc, char **argv) {
-	(void)argc; (void)argv;
-	if ((%(type_name)s *) 0) return 0;
-	if (sizeof (%(type_name)s)) return 0;
-	return 1;
-}
-'''
-"""Code template for checking for types"""
-
 SNIP_EMPTY_PROGRAM = '''
 int main(int argc, char **argv) {
 	(void)argc; (void)argv;
@@ -50,15 +26,6 @@
 }
 '''
 
-SNIP_FIELD = '''
-int main(int argc, char **argv) {
-	char *off;
-	(void)argc; (void)argv;
-	off = (char*) &((%(type_name)s*)0)->%(field_name)s;
-	return (size_t) off < sizeof(%(type_name)s);
-}
-'''
-
 MACRO_TO_DESTOS = {
 '__linux__'                                      : 'linux',
 '__GNU__'                                        : 'gnu', # hurd
@@ -75,7 +42,7 @@
 '_WIN64'                                         : 'win32',
 '_WIN32'                                         : 'win32',
 # Note about darwin: this is also tested with 'defined __APPLE__ && defined __MACH__' somewhere below in this file.
-'__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__'  : 'darwin', 
+'__ENVIRONMENT_MAC_OS_X_VERSION_MIN_REQUIRED__'  : 'darwin',
 '__ENVIRONMENT_IPHONE_OS_VERSION_MIN_REQUIRED__' : 'darwin', # iphone
 '__QNX__'                                        : 'qnx',
 '__native_client__'                              : 'nacl' # google native client platform
@@ -100,12 +67,13 @@
 '__s390x__'   : 's390x',
 '__s390__'    : 's390',
 '__sh__'      : 'sh',
+'__xtensa__'  : 'xtensa',
 }
 
 @conf
 def parse_flags(self, line, uselib_store, env=None, force_static=False, posix=None):
 	"""
-	Parse the flags from the input lines, and add them to the relevant use variables::
+	Parses flags from the input lines, and adds them to the relevant use variables::
 
 		def configure(conf):
 			conf.parse_flags('-O3', 'FOO')
@@ -137,9 +105,11 @@ def configure(conf):
 
 	# append_unique is not always possible
 	# for example, apple flags may require both -arch i386 and -arch ppc
-	app = env.append_value
-	appu = env.append_unique
 	uselib = uselib_store
+	def app(var, val):
+		env.append_value('%s_%s' % (var, uselib), val)
+	def appu(var, val):
+		env.append_unique('%s_%s' % (var, uselib), val)
 	static = False
 	while lst:
 		x = lst.pop(0)
@@ -147,69 +117,79 @@ def configure(conf):
 		ot = x[2:]
 
 		if st == '-I' or st == '/I':
-			if not ot: ot = lst.pop(0)
-			appu('INCLUDES_' + uselib, [ot])
+			if not ot:
+				ot = lst.pop(0)
+			appu('INCLUDES', ot)
 		elif st == '-i':
 			tmp = [x, lst.pop(0)]
 			app('CFLAGS', tmp)
 			app('CXXFLAGS', tmp)
 		elif st == '-D' or (env.CXX_NAME == 'msvc' and st == '/D'): # not perfect but..
-			if not ot: ot = lst.pop(0)
-			app('DEFINES_' + uselib, [ot])
+			if not ot:
+				ot = lst.pop(0)
+			app('DEFINES', ot)
 		elif st == '-l':
-			if not ot: ot = lst.pop(0)
-			prefix = (force_static or static) and 'STLIB_' or 'LIB_'
-			appu(prefix + uselib, [ot])
+			if not ot:
+				ot = lst.pop(0)
+			prefix = 'STLIB' if (force_static or static) else 'LIB'
+			app(prefix, ot)
 		elif st == '-L':
-			if not ot: ot = lst.pop(0)
-			prefix = (force_static or static) and 'STLIBPATH_' or 'LIBPATH_'
-			appu(prefix + uselib, [ot])
+			if not ot:
+				ot = lst.pop(0)
+			prefix = 'STLIBPATH' if (force_static or static) else 'LIBPATH'
+			appu(prefix, ot)
 		elif x.startswith('/LIBPATH:'):
-			prefix = (force_static or static) and 'STLIBPATH_' or 'LIBPATH_'
-			appu(prefix + uselib, [x.replace('/LIBPATH:', '')])
-		elif x == '-pthread' or x.startswith('+') or x.startswith('-std'):
-			app('CFLAGS_' + uselib, [x])
-			app('CXXFLAGS_' + uselib, [x])
-			app('LINKFLAGS_' + uselib, [x])
+			prefix = 'STLIBPATH' if (force_static or static) else 'LIBPATH'
+			appu(prefix, x.replace('/LIBPATH:', ''))
+		elif x.startswith('-std='):
+			prefix = 'CXXFLAGS' if '++' in x else 'CFLAGS'
+			app(prefix, x)
+		elif x.startswith('+') or x in ('-pthread', '-fPIC', '-fpic', '-fPIE', '-fpie'):
+			app('CFLAGS', x)
+			app('CXXFLAGS', x)
+			app('LINKFLAGS', x)
 		elif x == '-framework':
-			appu('FRAMEWORK_' + uselib, [lst.pop(0)])
+			appu('FRAMEWORK', lst.pop(0))
 		elif x.startswith('-F'):
-			appu('FRAMEWORKPATH_' + uselib, [x[2:]])
+			appu('FRAMEWORKPATH', x[2:])
 		elif x == '-Wl,-rpath' or x == '-Wl,-R':
-			app('RPATH_' + uselib, lst.pop(0).lstrip('-Wl,'))
+			app('RPATH', lst.pop(0).lstrip('-Wl,'))
 		elif x.startswith('-Wl,-R,'):
-			app('RPATH_' + uselib, x[7:])
+			app('RPATH', x[7:])
 		elif x.startswith('-Wl,-R'):
-			app('RPATH_' + uselib, x[6:])
+			app('RPATH', x[6:])
 		elif x.startswith('-Wl,-rpath,'):
-			app('RPATH_' + uselib, x[11:])
+			app('RPATH', x[11:])
 		elif x == '-Wl,-Bstatic' or x == '-Bstatic':
 			static = True
 		elif x == '-Wl,-Bdynamic' or x == '-Bdynamic':
 			static = False
-		elif x.startswith('-Wl'):
-			app('LINKFLAGS_' + uselib, [x])
-		elif x.startswith('-m') or x.startswith('-f') or x.startswith('-dynamic'):
-			app('CFLAGS_' + uselib, [x])
-			app('CXXFLAGS_' + uselib, [x])
+		elif x.startswith('-Wl') or x in ('-rdynamic', '-pie'):
+			app('LINKFLAGS', x)
+		elif x.startswith(('-m', '-f', '-dynamic', '-O', '-g')):
+			# Adding the -W option breaks python builds on Openindiana
+			app('CFLAGS', x)
+			app('CXXFLAGS', x)
 		elif x.startswith('-bundle'):
-			app('LINKFLAGS_' + uselib, [x])
-		elif x.startswith('-undefined') or x.startswith('-Xlinker'):
+			app('LINKFLAGS', x)
+		elif x.startswith(('-undefined', '-Xlinker')):
 			arg = lst.pop(0)
-			app('LINKFLAGS_' + uselib, [x, arg])
-		elif x.startswith('-arch') or x.startswith('-isysroot'):
+			app('LINKFLAGS', [x, arg])
+		elif x.startswith(('-arch', '-isysroot')):
 			tmp = [x, lst.pop(0)]
-			app('CFLAGS_' + uselib, tmp)
-			app('CXXFLAGS_' + uselib, tmp)
-			app('LINKFLAGS_' + uselib, tmp)
-		elif x.endswith('.a') or x.endswith('.so') or x.endswith('.dylib') or x.endswith('.lib'):
-			appu('LINKFLAGS_' + uselib, [x]) # not cool, #762
+			app('CFLAGS', tmp)
+			app('CXXFLAGS', tmp)
+			app('LINKFLAGS', tmp)
+		elif x.endswith(('.a', '.so', '.dylib', '.lib')):
+			appu('LINKFLAGS', x) # not cool, #762
+		else:
+			self.to_log('Unhandled flag %r' % x)
 
 @conf
 def validate_cfg(self, kw):
 	"""
-	Search for the program *pkg-config* if missing, and validate the parameters to pass to
-	:py:func:`waflib.Tools.c_config.exec_cfg`.
+	Searches for the program *pkg-config* if missing, and validates the
+	parameters to pass to :py:func:`waflib.Tools.c_config.exec_cfg`.
 
 	:param path: the **-config program to use** (default is *pkg-config*)
 	:type path: list of string
@@ -225,47 +205,42 @@ def validate_cfg(self, kw):
 			self.find_program('pkg-config', var='PKGCONFIG')
 		kw['path'] = self.env.PKGCONFIG
 
-	# pkg-config version
-	if 'atleast_pkgconfig_version' in kw:
-		if not 'msg' in kw:
+	# verify that exactly one action is requested
+	s = ('atleast_pkgconfig_version' in kw) + ('modversion' in kw) + ('package' in kw)
+	if s != 1:
+		raise ValueError('exactly one of atleast_pkgconfig_version, modversion and package must be set')
+	if not 'msg' in kw:
+		if 'atleast_pkgconfig_version' in kw:
 			kw['msg'] = 'Checking for pkg-config version >= %r' % kw['atleast_pkgconfig_version']
-		return
+		elif 'modversion' in kw:
+			kw['msg'] = 'Checking for %r version' % kw['modversion']
+		else:
+			kw['msg'] = 'Checking for %r' %(kw['package'])
 
-	if not 'okmsg' in kw:
+	# let the modversion check set the okmsg to the detected version
+	if not 'okmsg' in kw and not 'modversion' in kw:
 		kw['okmsg'] = 'yes'
 	if not 'errmsg' in kw:
 		kw['errmsg'] = 'not found'
 
-	if 'modversion' in kw:
-		if not 'msg' in kw:
-			kw['msg'] = 'Checking for %r version' % kw['modversion']
-		return
-
-	# checking for the version of a module, for the moment, one thing at a time
-	for x in cfg_ver.keys():
-		y = x.replace('-', '_')
-		if y in kw:
-			if not 'package' in kw:
-				raise ValueError('%s requires a package' % x)
-
-			if not 'msg' in kw:
-				kw['msg'] = 'Checking for %r %s %s' % (kw['package'], cfg_ver[x], kw[y])
-			return
-
-	if not 'define_name' in kw:
-		pkgname = kw.get('uselib_store', kw['package'].upper())
-		kw['define_name'] = self.have_define(pkgname)
-
-	if not 'uselib_store' in kw:
-		self.undefine(kw['define_name'])
-
-	if not 'msg' in kw:
-		kw['msg'] = 'Checking for %r' % (kw['package'] or kw['path'])
+	# pkg-config version
+	if 'atleast_pkgconfig_version' in kw:
+		pass
+	elif 'modversion' in kw:
+		if not 'uselib_store' in kw:
+			kw['uselib_store'] = kw['modversion']
+		if not 'define_name' in kw:
+			kw['define_name'] = '%s_VERSION' % Utils.quote_define_name(kw['uselib_store'])
+	else:
+		if not 'uselib_store' in kw:
+			kw['uselib_store'] = Utils.to_list(kw['package'])[0].upper()
+		if not 'define_name' in kw:
+			kw['define_name'] = self.have_define(kw['uselib_store'])
 
 @conf
 def exec_cfg(self, kw):
 	"""
-	Execute the program *pkg-config*:
+	Executes ``pkg-config`` or other ``-config`` applications to collect configuration flags:
 
 	* if atleast_pkgconfig_version is given, check that pkg-config has the version n and return
 	* if modversion is given, then return the module version
@@ -289,42 +264,39 @@ def exec_cfg(self, kw):
 
 	path = Utils.to_list(kw['path'])
 	env = self.env.env or None
+	if kw.get('pkg_config_path'):
+		if not env:
+			env = dict(self.environ)
+		env['PKG_CONFIG_PATH'] = kw['pkg_config_path']
+
 	def define_it():
-		pkgname = kw.get('uselib_store', kw['package'].upper())
-		if kw.get('global_define'):
-			# compatibility, replace by pkgname in WAF 1.9?
-			self.define(self.have_define(kw['package']), 1, False)
+		define_name = kw['define_name']
+		# by default, add HAVE_X to the config.h, else provide DEFINES_X for use=X
+		if kw.get('global_define', 1):
+			self.define(define_name, 1, False)
 		else:
-			self.env.append_unique('DEFINES_%s' % pkgname, "%s=1" % self.have_define(pkgname))
-		self.env[self.have_define(pkgname)] = 1
+			self.env.append_unique('DEFINES_%s' % kw['uselib_store'], "%s=1" % define_name)
+
+		if kw.get('add_have_to_env', 1):
+			self.env[define_name] = 1
 
 	# pkg-config version
 	if 'atleast_pkgconfig_version' in kw:
 		cmd = path + ['--atleast-pkgconfig-version=%s' % kw['atleast_pkgconfig_version']]
 		self.cmd_and_log(cmd, env=env)
-		if not 'okmsg' in kw:
-			kw['okmsg'] = 'yes'
 		return
 
-	# checking for the version of a module
-	for x in cfg_ver:
-		y = x.replace('-', '_')
-		if y in kw:
-			self.cmd_and_log(path + ['--%s=%s' % (x, kw[y]), kw['package']], env=env)
-			if not 'okmsg' in kw:
-				kw['okmsg'] = 'yes'
-			define_it()
-			break
-
-	# retrieving the version of a module
+	# single version for a module
 	if 'modversion' in kw:
 		version = self.cmd_and_log(path + ['--modversion', kw['modversion']], env=env).strip()
-		self.define('%s_VERSION' % Utils.quote_define_name(kw.get('uselib_store', kw['modversion'])), version)
+		if not 'okmsg' in kw:
+			kw['okmsg'] = version
+		self.define(kw['define_name'], version)
 		return version
 
 	lst = [] + path
 
-	defi = kw.get('define_variable', None)
+	defi = kw.get('define_variable')
 	if not defi:
 		defi = self.env.PKG_CONFIG_DEFINES or {}
 	for key, val in defi.items():
@@ -342,39 +314,32 @@ def define_it():
 
 	# retrieving variables of a module
 	if 'variables' in kw:
-		v = kw.get('env', self.env)
-		uselib = kw.get('uselib_store', kw['package'].upper())
+		v_env = kw.get('env', self.env)
 		vars = Utils.to_list(kw['variables'])
 		for v in vars:
 			val = self.cmd_and_log(lst + ['--variable=' + v], env=env).strip()
-			var = '%s_%s' % (uselib, v)
-			v[var] = val
-		if not 'okmsg' in kw:
-			kw['okmsg'] = 'yes'
+			var = '%s_%s' % (kw['uselib_store'], v)
+			v_env[var] = val
 		return
 
 	# so we assume the command-line will output flags to be parsed afterwards
 	ret = self.cmd_and_log(lst, env=env)
-	if not 'okmsg' in kw:
-		kw['okmsg'] = 'yes'
 
 	define_it()
-	self.parse_flags(ret, kw.get('uselib_store', kw['package'].upper()), kw.get('env', self.env), force_static=static, posix=kw.get('posix', None))
+	self.parse_flags(ret, kw['uselib_store'], kw.get('env', self.env), force_static=static, posix=kw.get('posix'))
 	return ret
 
 @conf
 def check_cfg(self, *k, **kw):
 	"""
-	Check for configuration flags using a **-config**-like program (pkg-config, sdl-config, etc).
-	Encapsulate the calls to :py:func:`waflib.Tools.c_config.validate_cfg` and :py:func:`waflib.Tools.c_config.exec_cfg`
+	Checks for configuration flags using a **-config**-like program (pkg-config, sdl-config, etc).
+	This wraps internal calls to :py:func:`waflib.Tools.c_config.validate_cfg` and :py:func:`waflib.Tools.c_config.exec_cfg`
 
 	A few examples::
 
 		def configure(conf):
 			conf.load('compiler_c')
 			conf.check_cfg(package='glib-2.0', args='--libs --cflags')
-			conf.check_cfg(package='glib-2.0', uselib_store='GLIB', atleast_version='2.10.0',
-				args='--cflags --libs')
 			conf.check_cfg(package='pango')
 			conf.check_cfg(package='pango', uselib_store='MYPANGO', args=['--cflags', '--libs'])
 			conf.check_cfg(package='pango',
@@ -387,24 +352,18 @@ def configure(conf):
 			conf.check_cfg(package='gtk+-2.0', variables=['includedir', 'prefix'], uselib_store='FOO')
 			print(conf.env.FOO_includedir)
 	"""
-	if k:
-		lst = k[0].split()
-		kw['package'] = lst[0]
-		kw['args'] = ' '.join(lst[1:])
-
 	self.validate_cfg(kw)
 	if 'msg' in kw:
 		self.start_msg(kw['msg'], **kw)
 	ret = None
 	try:
 		ret = self.exec_cfg(kw)
-	except self.errors.WafError:
+	except self.errors.WafError as e:
 		if 'errmsg' in kw:
 			self.end_msg(kw['errmsg'], 'YELLOW', **kw)
 		if Logs.verbose > 1:
-			raise
-		else:
-			self.fatal('The configuration failed')
+			self.to_log('Command failure: %s' % e)
+		self.fatal('The configuration failed')
 	else:
 		if not ret:
 			ret = True
@@ -415,6 +374,9 @@ def configure(conf):
 	return ret
 
 def build_fun(bld):
+	"""
+	Build function that is used for running configuration tests with ``conf.check()``
+	"""
 	if bld.kw['compile_filename']:
 		node = bld.srcnode.make_node(bld.kw['compile_filename'])
 		node.write(bld.kw['code'])
@@ -424,13 +386,13 @@ def build_fun(bld):
 	for k, v in bld.kw.items():
 		setattr(o, k, v)
 
-	if not bld.kw.get('quiet', None):
+	if not bld.kw.get('quiet'):
 		bld.conf.to_log("==>\n%s\n<==" % bld.kw['code'])
 
 @conf
 def validate_c(self, kw):
 	"""
-	pre-check the parameters that will be given to :py:func:`waflib.Configure.run_build`
+	Pre-checks the parameters that will be given to :py:func:`waflib.Configure.run_build`
 
 	:param compiler: c or cxx (tries to guess what is best)
 	:type compiler: string
@@ -455,6 +417,9 @@ def validate_c(self, kw):
 	:param auto_add_header_name: if header_name was set, add the headers in env.INCKEYS so the next tests will include these headers
 	:type auto_add_header_name: bool
 	"""
+	for x in ('type_name', 'field_name', 'function_name'):
+		if x in kw:
+			Logs.warn('Invalid argument %r in test' % x)
 
 	if not 'build_fun' in kw:
 		kw['build_fun'] = build_fun
@@ -465,17 +430,17 @@ def validate_c(self, kw):
 
 	if not 'compiler' in kw and not 'features' in kw:
 		kw['compiler'] = 'c'
-		if env['CXX_NAME'] and Task.classes.get('cxx', None):
+		if env.CXX_NAME and Task.classes.get('cxx'):
 			kw['compiler'] = 'cxx'
-			if not self.env['CXX']:
+			if not self.env.CXX:
 				self.fatal('a c++ compiler is required')
 		else:
-			if not self.env['CC']:
+			if not self.env.CC:
 				self.fatal('a c compiler is required')
 
 	if not 'compile_mode' in kw:
 		kw['compile_mode'] = 'c'
-		if 'cxx' in Utils.to_list(kw.get('features',[])) or kw.get('compiler', '') == 'cxx':
+		if 'cxx' in Utils.to_list(kw.get('features', [])) or kw.get('compiler') == 'cxx':
 			kw['compile_mode'] = 'cxx'
 
 	if not 'type' in kw:
@@ -492,71 +457,36 @@ def validate_c(self, kw):
 	if not 'compile_filename' in kw:
 		kw['compile_filename'] = 'test.c' + ((kw['compile_mode'] == 'cxx') and 'pp' or '')
 
-
 	def to_header(dct):
 		if 'header_name' in dct:
 			dct = Utils.to_list(dct['header_name'])
 			return ''.join(['#include <%s>\n' % x for x in dct])
 		return ''
 
-	#OSX
 	if 'framework_name' in kw:
+		# OSX, not sure this is used anywhere
 		fwkname = kw['framework_name']
 		if not 'uselib_store' in kw:
 			kw['uselib_store'] = fwkname.upper()
-
-		if not kw.get('no_header', False):
-			if not 'header_name' in kw:
-				kw['header_name'] = []
+		if not kw.get('no_header'):
 			fwk = '%s/%s.h' % (fwkname, fwkname)
-			if kw.get('remove_dot_h', None):
+			if kw.get('remove_dot_h'):
 				fwk = fwk[:-2]
-			kw['header_name'] = Utils.to_list(kw['header_name']) + [fwk]
-
+			val = kw.get('header_name', [])
+			kw['header_name'] = Utils.to_list(val) + [fwk]
 		kw['msg'] = 'Checking for framework %s' % fwkname
 		kw['framework'] = fwkname
-		#kw['frameworkpath'] = set it yourself
-
-	if 'function_name' in kw:
-		fu = kw['function_name']
-		if not 'msg' in kw:
-			kw['msg'] = 'Checking for function %s' % fu
-		kw['code'] = to_header(kw) + SNIP_FUNCTION % fu
-		if not 'uselib_store' in kw:
-			kw['uselib_store'] = fu.upper()
-		if not 'define_name' in kw:
-			kw['define_name'] = self.have_define(fu)
-
-	elif 'type_name' in kw:
-		tu = kw['type_name']
-		if not 'header_name' in kw:
-			kw['header_name'] = 'stdint.h'
-		if 'field_name' in kw:
-			field = kw['field_name']
-			kw['code'] = to_header(kw) + SNIP_FIELD % {'type_name' : tu, 'field_name' : field}
-			if not 'msg' in kw:
-				kw['msg'] = 'Checking for field %s in %s' % (field, tu)
-			if not 'define_name' in kw:
-				kw['define_name'] = self.have_define((tu + '_' + field).upper())
-		else:
-			kw['code'] = to_header(kw) + SNIP_TYPE % {'type_name' : tu}
-			if not 'msg' in kw:
-				kw['msg'] = 'Checking for type %s' % tu
-			if not 'define_name' in kw:
-				kw['define_name'] = self.have_define(tu.upper())
 
 	elif 'header_name' in kw:
 		if not 'msg' in kw:
 			kw['msg'] = 'Checking for header %s' % kw['header_name']
 
 		l = Utils.to_list(kw['header_name'])
-		assert len(l)>0, 'list of headers in header_name is empty'
+		assert len(l), 'list of headers in header_name is empty'
 
 		kw['code'] = to_header(kw) + SNIP_EMPTY_PROGRAM
-
 		if not 'uselib_store' in kw:
 			kw['uselib_store'] = l[0].upper()
-
 		if not 'define_name' in kw:
 			kw['define_name'] = self.have_define(l[0])
 
@@ -592,7 +522,7 @@ def to_header(dct):
 		kw['execute'] = False
 	if kw['execute']:
 		kw['features'].append('test_exec')
-		kw['chmod'] = 493
+		kw['chmod'] = Utils.O755
 
 	if not 'errmsg' in kw:
 		kw['errmsg'] = 'not found'
@@ -608,11 +538,12 @@ def to_header(dct):
 		kw['code'] = '\n'.join(['#include <%s>' % x for x in self.env[INCKEYS]]) + '\n' + kw['code']
 
 	# in case defines lead to very long command-lines
-	if kw.get('merge_config_header', False) or env.merge_config_header:
+	if kw.get('merge_config_header') or env.merge_config_header:
 		kw['code'] = '%s\n\n%s' % (self.get_config_header(), kw['code'])
 		env.DEFINES = [] # modify the copy
 
-	if not kw.get('success'): kw['success'] = None
+	if not kw.get('success'):
+		kw['success'] = None
 
 	if 'define_name' in kw:
 		self.undefine(kw['define_name'])
@@ -621,63 +552,76 @@ def to_header(dct):
 
 @conf
 def post_check(self, *k, **kw):
-	"Set the variables after a test executed in :py:func:`waflib.Tools.c_config.check` was run successfully"
-
+	"""
+	Sets the variables after a test executed in
+	:py:func:`waflib.Tools.c_config.check` was run successfully
+	"""
 	is_success = 0
 	if kw['execute']:
 		if kw['success'] is not None:
-			if kw.get('define_ret', False):
+			if kw.get('define_ret'):
 				is_success = kw['success']
 			else:
 				is_success = (kw['success'] == 0)
 	else:
 		is_success = (kw['success'] == 0)
 
-	if 'define_name' in kw:
-		# TODO simplify!
-		if 'header_name' in kw or 'function_name' in kw or 'type_name' in kw or 'fragment' in kw:
-			if kw['execute'] and kw.get('define_ret', None) and isinstance(is_success, str):
-				self.define(kw['define_name'], is_success, quote=kw.get('quote', 1))
+	if kw.get('define_name'):
+		comment = kw.get('comment', '')
+		define_name = kw['define_name']
+		if kw['execute'] and kw.get('define_ret') and isinstance(is_success, str):
+			if kw.get('global_define', 1):
+				self.define(define_name, is_success, quote=kw.get('quote', 1), comment=comment)
 			else:
-				self.define_cond(kw['define_name'], is_success)
+				if kw.get('quote', 1):
+					succ = '"%s"' % is_success
+				else:
+					succ = int(is_success)
+				val = '%s=%s' % (define_name, succ)
+				var = 'DEFINES_%s' % kw['uselib_store']
+				self.env.append_value(var, val)
 		else:
-			self.define_cond(kw['define_name'], is_success)
-
-		# consistency with check_cfg
-		if kw.get('global_define', None):
-			self.env[kw['define_name']] = is_success
+			if kw.get('global_define', 1):
+				self.define_cond(define_name, is_success, comment=comment)
+			else:
+				var = 'DEFINES_%s' % kw['uselib_store']
+				self.env.append_value(var, '%s=%s' % (define_name, int(is_success)))
+
+		# define conf.env.HAVE_X to 1
+		if kw.get('add_have_to_env', 1):
+			if kw.get('uselib_store'):
+				self.env[self.have_define(kw['uselib_store'])] = 1
+			elif kw['execute'] and kw.get('define_ret'):
+				self.env[define_name] = is_success
+			else:
+				self.env[define_name] = int(is_success)
 
 	if 'header_name' in kw:
-		if kw.get('auto_add_header_name', False):
+		if kw.get('auto_add_header_name'):
 			self.env.append_value(INCKEYS, Utils.to_list(kw['header_name']))
 
 	if is_success and 'uselib_store' in kw:
 		from waflib.Tools import ccroot
-
-		# TODO see get_uselib_vars from ccroot.py
-		_vars = set([])
+		# See get_uselib_vars in ccroot.py
+		_vars = set()
 		for x in kw['features']:
 			if x in ccroot.USELIB_VARS:
 				_vars |= ccroot.USELIB_VARS[x]
 
 		for k in _vars:
-			lk = k.lower()
-			if lk in kw:
-				val = kw[lk]
-				# remove trailing slash
-				if isinstance(val, str):
-					val = val.rstrip(os.path.sep)
-				self.env.append_unique(k + '_' + kw['uselib_store'], Utils.to_list(val))
+			x = k.lower()
+			if x in kw:
+				self.env.append_value(k + '_' + kw['uselib_store'], kw[x])
 	return is_success
 
 @conf
 def check(self, *k, **kw):
 	"""
-	Perform a configuration test by calling :py:func:`waflib.Configure.run_build`.
+	Performs a configuration test by calling :py:func:`waflib.Configure.run_build`.
 	For the complete list of parameters, see :py:func:`waflib.Tools.c_config.validate_c`.
-	To force a specific compiler, pass "compiler='c'" or "compiler='cxx'" in the arguments
+	To force a specific compiler, pass ``compiler='c'`` or ``compiler='cxx'`` to the list of arguments
 
-	Besides build targets, complete builds can be given though a build function. All files will
+	Besides build targets, complete builds can be given through a build function. All files will
 	be written to a temporary directory::
 
 		def build(bld):
@@ -711,7 +655,7 @@ def build(bld):
 
 class test_exec(Task.Task):
 	"""
-	A task for executing a programs after they are built. See :py:func:`waflib.Tools.c_config.test_exec_fun`.
+	A task that runs programs after they are built. See :py:func:`waflib.Tools.c_config.test_exec_fun`.
 	"""
 	color = 'PINK'
 	def run(self):
@@ -746,20 +690,51 @@ def configure(conf):
 
 @conf
 def check_cxx(self, *k, **kw):
-	# DO NOT USE
+	"""
+	Runs a test with a task generator of the form::
+
+		conf.check(features='cxx cxxprogram', ...)
+	"""
 	kw['compiler'] = 'cxx'
 	return self.check(*k, **kw)
 
 @conf
 def check_cc(self, *k, **kw):
-	# DO NOT USE
+	"""
+	Runs a test with a task generator of the form::
+
+		conf.check(features='c cprogram', ...)
+	"""
 	kw['compiler'] = 'c'
 	return self.check(*k, **kw)
 
 @conf
-def define(self, key, val, quote=True):
+def set_define_comment(self, key, comment):
 	"""
-	Store a single define and its state into conf.env.DEFINES. If the value is True, False or None it is cast to 1 or 0.
+	Sets a comment that will appear in the configuration header
+
+	:type key: string
+	:type comment: string
+	"""
+	coms = self.env.DEFINE_COMMENTS
+	if not coms:
+		coms = self.env.DEFINE_COMMENTS = {}
+	coms[key] = comment or ''
+
+@conf
+def get_define_comment(self, key):
+	"""
+	Returns the comment associated to a define
+
+	:type key: string
+	"""
+	coms = self.env.DEFINE_COMMENTS or {}
+	return coms.get(key, '')
+
+@conf
+def define(self, key, val, quote=True, comment=''):
+	"""
+	Stores a single define and its state into ``conf.env.DEFINES``. The value is cast to an integer (0/1).
 
 	:param key: define name
 	:type key: string
@@ -768,8 +743,9 @@ def define(self, key, val, quote=True):
 	:param quote: enclose strings in quotes (yes by default)
 	:type quote: bool
 	"""
-	assert key and isinstance(key, str)
-
+	assert isinstance(key, str)
+	if not key:
+		return
 	if val is True:
 		val = 1
 	elif val in (False, None):
@@ -782,7 +758,7 @@ def define(self, key, val, quote=True):
 	app = s % (key, str(val))
 
 	ban = key + '='
-	lst = self.env['DEFINES']
+	lst = self.env.DEFINES
 	for x in lst:
 		if x.startswith(ban):
 			lst[lst.index(x)] = app
@@ -791,26 +767,29 @@ def define(self, key, val, quote=True):
 		self.env.append_value('DEFINES', app)
 
 	self.env.append_unique(DEFKEYS, key)
+	self.set_define_comment(key, comment)
 
 @conf
-def undefine(self, key):
+def undefine(self, key, comment=''):
 	"""
-	Remove a define from conf.env.DEFINES
+	Removes a global define from ``conf.env.DEFINES``
 
 	:param key: define name
 	:type key: string
 	"""
-	assert key and isinstance(key, str)
-
+	assert isinstance(key, str)
+	if not key:
+		return
 	ban = key + '='
-	lst = [x for x in self.env['DEFINES'] if not x.startswith(ban)]
-	self.env['DEFINES'] = lst
+	lst = [x for x in self.env.DEFINES if not x.startswith(ban)]
+	self.env.DEFINES = lst
 	self.env.append_unique(DEFKEYS, key)
+	self.set_define_comment(key, comment)
 
 @conf
-def define_cond(self, key, val):
+def define_cond(self, key, val, comment=''):
 	"""
-	Conditionally define a name::
+	Conditionally defines a name::
 
 		def configure(conf):
 			conf.define_cond('A', True)
@@ -823,16 +802,19 @@ def configure(conf):
 	:param val: value
 	:type val: int or string
 	"""
-	assert key and isinstance(key, str)
-
+	assert isinstance(key, str)
+	if not key:
+		return
 	if val:
-		self.define(key, 1)
+		self.define(key, 1, comment=comment)
 	else:
-		self.undefine(key)
+		self.undefine(key, comment=comment)
 
 @conf
 def is_defined(self, key):
 	"""
+	Indicates whether a particular define is globally set in ``conf.env.DEFINES``.
+
 	:param key: define name
 	:type key: string
 	:return: True if the define is set
@@ -841,7 +823,7 @@ def is_defined(self, key):
 	assert key and isinstance(key, str)
 
 	ban = key + '='
-	for x in self.env['DEFINES']:
+	for x in self.env.DEFINES:
 		if x.startswith(ban):
 			return True
 	return False
@@ -849,14 +831,16 @@ def is_defined(self, key):
 @conf
 def get_define(self, key):
 	"""
+	Returns the value of an existing define, or None if not found
+
 	:param key: define name
 	:type key: string
-	:return: the value of a previously stored define or None if it is not set
+	:rtype: string
 	"""
 	assert key and isinstance(key, str)
 
 	ban = key + '='
-	for x in self.env['DEFINES']:
+	for x in self.env.DEFINES:
 		if x.startswith(ban):
 			return x[len(ban):]
 	return None
@@ -864,6 +848,9 @@ def get_define(self, key):
 @conf
 def have_define(self, key):
 	"""
+	Returns a variable suitable for command-line or header use by removing invalid characters
+	and prefixing it with ``HAVE_``
+
 	:param key: define name
 	:type key: string
 	:return: the input key prefixed by *HAVE_* and substitute any invalid characters.
@@ -874,7 +861,7 @@ def have_define(self, key):
 @conf
 def write_config_header(self, configfile='', guard='', top=False, defines=True, headers=False, remove=True, define_prefix=''):
 	"""
-	Write a configuration header containing defines and includes::
+	Writes a configuration header containing defines and includes::
 
 		def configure(cnf):
 			cnf.define('A', 1)
@@ -898,7 +885,8 @@ def configure(cnf):
 	:type define_prefix: string
 	:param define_prefix: prefix all the defines in the file with a particular prefix
 	"""
-	if not configfile: configfile = WAF_CONFIG_H
+	if not configfile:
+		configfile = WAF_CONFIG_H
 	waf_guard = guard or 'W_%s_WAF' % Utils.quote_define_name(configfile)
 
 	node = top and self.bldnode or self.path.get_bld()
@@ -923,7 +911,7 @@ def configure(cnf):
 @conf
 def get_config_header(self, defines=True, headers=False, define_prefix=''):
 	"""
-	Create the contents of a ``config.h`` file from the defines and includes
+	Creates the contents of a ``config.h`` file from the defines and includes
 	set in conf.env.define_key / conf.env.include_key. No include guards are added.
 
 	A prelude will be added from the variable env.WAF_CONFIG_H_PRELUDE if provided. This
@@ -953,22 +941,25 @@ def configure(conf):
 
 	if defines:
 		tbl = {}
-		for k in self.env['DEFINES']:
+		for k in self.env.DEFINES:
 			a, _, b = k.partition('=')
 			tbl[a] = b
 
 		for k in self.env[DEFKEYS]:
+			caption = self.get_define_comment(k)
+			if caption:
+				caption = ' /* %s */' % caption
 			try:
-				txt = '#define %s%s %s' % (define_prefix, k, tbl[k])
+				txt = '#define %s%s %s%s' % (define_prefix, k, tbl[k], caption)
 			except KeyError:
-				txt = '/* #undef %s%s */' % (define_prefix, k)
+				txt = '/* #undef %s%s */%s' % (define_prefix, k, caption)
 			lst.append(txt)
 	return "\n".join(lst)
 
 @conf
 def cc_add_flags(conf):
 	"""
-	Add CFLAGS / CPPFLAGS from os.environ to conf.env
+	Adds CFLAGS / CPPFLAGS from os.environ to conf.env
 	"""
 	conf.add_os_flags('CPPFLAGS', dup=False)
 	conf.add_os_flags('CFLAGS', dup=False)
@@ -976,7 +967,7 @@ def cc_add_flags(conf):
 @conf
 def cxx_add_flags(conf):
 	"""
-	Add CXXFLAGS / CPPFLAGS from os.environ to conf.env
+	Adds CXXFLAGS / CPPFLAGS from os.environ to conf.env
 	"""
 	conf.add_os_flags('CPPFLAGS', dup=False)
 	conf.add_os_flags('CXXFLAGS', dup=False)
@@ -984,7 +975,7 @@ def cxx_add_flags(conf):
 @conf
 def link_add_flags(conf):
 	"""
-	Add LINKFLAGS / LDFLAGS from os.environ to conf.env
+	Adds LINKFLAGS / LDFLAGS from os.environ to conf.env
 	"""
 	conf.add_os_flags('LINKFLAGS', dup=False)
 	conf.add_os_flags('LDFLAGS', dup=False)
@@ -992,7 +983,7 @@ def link_add_flags(conf):
 @conf
 def cc_load_tools(conf):
 	"""
-	Load the c tool
+	Loads the Waf c extensions
 	"""
 	if not conf.env.DEST_OS:
 		conf.env.DEST_OS = Utils.unversioned_sys_platform()
@@ -1001,7 +992,7 @@ def cc_load_tools(conf):
 @conf
 def cxx_load_tools(conf):
 	"""
-	Load the cxx tool
+	Loads the Waf c++ extensions
 	"""
 	if not conf.env.DEST_OS:
 		conf.env.DEST_OS = Utils.unversioned_sys_platform()
@@ -1010,15 +1001,17 @@ def cxx_load_tools(conf):
 @conf
 def get_cc_version(conf, cc, gcc=False, icc=False, clang=False):
 	"""
-	Run the preprocessor to determine the compiler version
+	Runs the preprocessor to determine the gcc/icc/clang version
 
 	The variables CC_VERSION, DEST_OS, DEST_BINFMT and DEST_CPU will be set in *conf.env*
+
+	:raise: :py:class:`waflib.Errors.ConfigurationError`
 	"""
 	cmd = cc + ['-dM', '-E', '-']
 	env = conf.env.env or None
 	try:
 		out, err = conf.cmd_and_log(cmd, output=0, input='\n'.encode(), env=env)
-	except Exception:
+	except Errors.WafError:
 		conf.fatal('Could not determine the compiler version %r' % cmd)
 
 	if gcc:
@@ -1066,6 +1059,8 @@ def isD(var):
 			conf.env.DEST_BINFMT = 'elf'
 		elif isD('__WINNT__') or isD('__CYGWIN__') or isD('_WIN32'):
 			conf.env.DEST_BINFMT = 'pe'
+			if not conf.env.IMPLIBDIR:
+				conf.env.IMPLIBDIR = conf.env.LIBDIR # for .lib or .dll.a files
 			conf.env.LIBDIR = conf.env.BINDIR
 		elif isD('__APPLE__'):
 			conf.env.DEST_BINFMT = 'mac-o'
@@ -1082,19 +1077,22 @@ def isD(var):
 		Logs.debug('ccroot: dest platform: ' + ' '.join([conf.env[x] or '?' for x in ('DEST_OS', 'DEST_BINFMT', 'DEST_CPU')]))
 		if icc:
 			ver = k['__INTEL_COMPILER']
-			conf.env['CC_VERSION'] = (ver[:-2], ver[-2], ver[-1])
+			conf.env.CC_VERSION = (ver[:-2], ver[-2], ver[-1])
 		else:
 			if isD('__clang__') and isD('__clang_major__'):
-				conf.env['CC_VERSION'] = (k['__clang_major__'], k['__clang_minor__'], k['__clang_patchlevel__'])
+				conf.env.CC_VERSION = (k['__clang_major__'], k['__clang_minor__'], k['__clang_patchlevel__'])
 			else:
 				# older clang versions and gcc
-				conf.env['CC_VERSION'] = (k['__GNUC__'], k['__GNUC_MINOR__'], k.get('__GNUC_PATCHLEVEL__', '0'))
+				conf.env.CC_VERSION = (k['__GNUC__'], k['__GNUC_MINOR__'], k.get('__GNUC_PATCHLEVEL__', '0'))
 	return k
 
 @conf
 def get_xlc_version(conf, cc):
-	"""Get the compiler version"""
+	"""
+	Returns the Aix compiler version
 
+	:raise: :py:class:`waflib.Errors.ConfigurationError`
+	"""
 	cmd = cc + ['-qversion']
 	try:
 		out, err = conf.cmd_and_log(cmd, output=0)
@@ -1107,15 +1105,18 @@ def get_xlc_version(conf, cc):
 		match = version_re(out or err)
 		if match:
 			k = match.groupdict()
-			conf.env['CC_VERSION'] = (k['major'], k['minor'])
+			conf.env.CC_VERSION = (k['major'], k['minor'])
 			break
 	else:
 		conf.fatal('Could not determine the XLC version.')
 
 @conf
 def get_suncc_version(conf, cc):
-	"""Get the compiler version"""
+	"""
+	Returns the Sun compiler version
 
+	:raise: :py:class:`waflib.Errors.ConfigurationError`
+	"""
 	cmd = cc + ['-V']
 	try:
 		out, err = conf.cmd_and_log(cmd, output=0)
@@ -1129,11 +1130,14 @@ def get_suncc_version(conf, cc):
 	version = (out or err)
 	version = version.splitlines()[0]
 
-	version_re = re.compile(r'cc:\s+sun\s+(c\+\+|c)\s+(?P<major>\d*)\.(?P<minor>\d*)', re.I).search
+	# cc: Sun C 5.10 SunOS_i386 2009/06/03
+	# cc: Studio 12.5 Sun C++ 5.14 SunOS_sparc Beta 2015/11/17
+	# cc: WorkShop Compilers 5.0 98/12/15 C 5.0
+	version_re = re.compile(r'cc: (studio.*?|\s+)?(sun\s+(c\+\+|c)|(WorkShop\s+Compilers))?\s+(?P<major>\d*)\.(?P<minor>\d*)', re.I).search
 	match = version_re(version)
 	if match:
 		k = match.groupdict()
-		conf.env['CC_VERSION'] = (k['major'], k['minor'])
+		conf.env.CC_VERSION = (k['major'], k['minor'])
 	else:
 		conf.fatal('Could not determine the suncc version.')
 
@@ -1142,7 +1146,7 @@ def get_suncc_version(conf, cc):
 @conf
 def add_as_needed(self):
 	"""
-	Add ``--as-needed`` to the *LINKFLAGS*
+	Adds ``--as-needed`` to the *LINKFLAGS*
 	On some platforms, it is a default flag.  In some cases (e.g., in NS-3) it is necessary to explicitly disable this feature with `-Wl,--no-as-needed` flag.
 	"""
 	if self.env.DEST_BINFMT == 'elf' and 'gcc' in (self.env.CXX_NAME, self.env.CC_NAME):
@@ -1150,22 +1154,31 @@ def add_as_needed(self):
 
 # ============ parallel configuration
 
-class cfgtask(Task.TaskBase):
+class cfgtask(Task.Task):
 	"""
-	A task that executes configuration tests
-	make sure that the checks write to conf.env in a thread-safe manner
+	A task that executes build configuration tests (calls conf.check)
 
-	for the moment it only executes conf.check
+	Make sure to use locks if concurrent access to the same conf.env data is necessary.
 	"""
+	def __init__(self, *k, **kw):
+		Task.Task.__init__(self, *k, **kw)
+		self.run_after = set()
+
 	def display(self):
 		return ''
 
 	def runnable_status(self):
+		for x in self.run_after:
+			if not x.hasrun:
+				return Task.ASK_LATER
 		return Task.RUN_ME
 
 	def uid(self):
 		return Utils.SIG_NIL
 
+	def signature(self):
+		return Utils.SIG_NIL
+
 	def run(self):
 		conf = self.conf
 		bld = Build.BuildContext(top_dir=conf.srcnode.abspath(), out_dir=conf.bldnode.abspath())
@@ -1173,22 +1186,73 @@ def run(self):
 		bld.init_dirs()
 		bld.in_msg = 1 # suppress top-level start_msg
 		bld.logger = self.logger
+		bld.multicheck_task = self
+		args = self.args
 		try:
-			bld.check(**self.args)
+			if 'func' in args:
+				bld.test(build_fun=args['func'],
+					msg=args.get('msg', ''),
+					okmsg=args.get('okmsg', ''),
+					errmsg=args.get('errmsg', ''),
+					)
+			else:
+				args['multicheck_mandatory'] = args.get('mandatory', True)
+				args['mandatory'] = True
+				try:
+					bld.check(**args)
+				finally:
+					args['mandatory'] = args['multicheck_mandatory']
 		except Exception:
 			return 1
 
+	def process(self):
+		Task.Task.process(self)
+		if 'msg' in self.args:
+			with self.generator.bld.multicheck_lock:
+				self.conf.start_msg(self.args['msg'])
+				if self.hasrun == Task.NOT_RUN:
+					self.conf.end_msg('test cancelled', 'YELLOW')
+				elif self.hasrun != Task.SUCCESS:
+					self.conf.end_msg(self.args.get('errmsg', 'no'), 'YELLOW')
+				else:
+					self.conf.end_msg(self.args.get('okmsg', 'yes'), 'GREEN')
+
 @conf
 def multicheck(self, *k, **kw):
 	"""
-	Use tuples to perform parallel configuration tests
+	Runs configuration tests in parallel; results are printed sequentially at the end of the build
+	but each test must provide its own msg value to display a line::
+
+		def test_build(ctx):
+			ctx.in_msg = True # suppress console outputs
+			ctx.check_large_file(mandatory=False)
+
+		conf.multicheck(
+			{'header_name':'stdio.h', 'msg':'... stdio', 'uselib_store':'STDIO', 'global_define':False},
+			{'header_name':'xyztabcd.h', 'msg':'... optional xyztabcd.h', 'mandatory': False},
+			{'header_name':'stdlib.h', 'msg':'... stdlib', 'okmsg': 'aye', 'errmsg': 'nope'},
+			{'func': test_build, 'msg':'... testing an arbitrary build function', 'okmsg':'ok'},
+			msg       = 'Checking for headers in parallel',
+			mandatory = True, # mandatory tests raise an error at the end
+			run_all_tests = True, # try running all tests
+		)
+
+	The configuration tests may modify the values in conf.env in any order, and the define
+	values can affect configuration tests being executed. It is hence recommended
+	to provide `uselib_store` values with `global_define=False` to prevent such issues.
 	"""
 	self.start_msg(kw.get('msg', 'Executing %d configuration tests' % len(k)), **kw)
 
+	# Force a copy so that threads append to the same list at least
+	# no order is guaranteed, but the values should not disappear at least
+	for var in ('DEFINES', DEFKEYS):
+		self.env.append_value(var, [])
+	self.env.DEFINE_COMMENTS = self.env.DEFINE_COMMENTS or {}
+
+	# define a task object that will execute our tests
 	class par(object):
 		def __init__(self):
 			self.keep = False
-			self.returned_tasks = []
 			self.task_sigs = {}
 			self.progress_bar = 0
 		def total(self):
@@ -1197,9 +1261,13 @@ def to_log(self, *k, **kw):
 			return
 
 	bld = par()
+	bld.keep = kw.get('run_all_tests', True)
+	bld.imp_sigs = {}
 	tasks = []
+
+	id_to_task = {}
 	for dct in k:
-		x = cfgtask(bld=bld)
+		x = Task.classes['cfgtask'](bld=bld, env=None)
 		tasks.append(x)
 		x.args = dct
 		x.bld = bld
@@ -1209,18 +1277,38 @@ def to_log(self, *k, **kw):
 		# bind a logger that will keep the info in memory
 		x.logger = Logs.make_mem_logger(str(id(x)), self.logger)
 
+		if 'id' in dct:
+			id_to_task[dct['id']] = x
+
+	# second pass to set dependencies with after_test/before_test
+	for x in tasks:
+		for key in Utils.to_list(x.args.get('before_tests', [])):
+			tsk = id_to_task[key]
+			if not tsk:
+				raise ValueError('No test named %r' % key)
+			tsk.run_after.add(x)
+		for key in Utils.to_list(x.args.get('after_tests', [])):
+			tsk = id_to_task[key]
+			if not tsk:
+				raise ValueError('No test named %r' % key)
+			x.run_after.add(tsk)
+
 	def it():
 		yield tasks
 		while 1:
 			yield []
-	p = Runner.Parallel(bld, Options.options.jobs)
+	bld.producer = p = Runner.Parallel(bld, Options.options.jobs)
+	bld.multicheck_lock = Utils.threading.Lock()
 	p.biter = it()
+
+	self.end_msg('started')
 	p.start()
 
 	# flush the logs in order into the config.log
 	for x in tasks:
 		x.logger.memhandler.flush()
 
+	self.start_msg('-> processing test results')
 	if p.error:
 		for x in p.error:
 			if getattr(x, 'err_msg', None):
@@ -1228,10 +1316,36 @@ def it():
 				self.end_msg('fail', color='RED')
 				raise Errors.WafError('There is an error in the library, read config.log for more information')
 
+	failure_count = 0
+	for x in tasks:
+		if x.hasrun not in (Task.SUCCESS, Task.NOT_RUN):
+			failure_count += 1
+
+	if failure_count:
+		self.end_msg(kw.get('errmsg', '%s test failed' % failure_count), color='YELLOW', **kw)
+	else:
+		self.end_msg('all ok', **kw)
+
 	for x in tasks:
 		if x.hasrun != Task.SUCCESS:
-			self.end_msg(kw.get('errmsg', 'no'), color='YELLOW', **kw)
-			self.fatal(kw.get('fatalmsg', None) or 'One of the tests has failed, read config.log for more information')
+			if x.args.get('mandatory', True):
+				self.fatal(kw.get('fatalmsg') or 'One of the tests has failed, read config.log for more information')
 
-	self.end_msg('ok', **kw)
+@conf
+def check_gcc_o_space(self, mode='c'):
+	if int(self.env.CC_VERSION[0]) > 4:
+		# this is for old compilers
+		return
+	self.env.stash()
+	if mode == 'c':
+		self.env.CCLNK_TGT_F = ['-o', '']
+	elif mode == 'cxx':
+		self.env.CXXLNK_TGT_F = ['-o', '']
+	features = '%s %sshlib' % (mode, mode)
+	try:
+		self.check(msg='Checking if the -o link must be split from arguments', fragment=SNIP_EMPTY_PROGRAM, features=features)
+	except self.errors.ConfigurationError:
+		self.env.revert()
+	else:
+		self.env.commit()
 
diff --git a/waflib/Tools/c_osx.py b/waflib/Tools/c_osx.py
index 4337cc68..f70b128b 100644
--- a/waflib/Tools/c_osx.py
+++ b/waflib/Tools/c_osx.py
@@ -1,13 +1,13 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy 2008-2010
+# Thomas Nagy 2008-2018 (ita)
 
 """
 MacOSX related tools
 """
 
 import os, shutil, platform
-from waflib import Task, Utils, Errors
+from waflib import Task, Utils
 from waflib.TaskGen import taskgen_method, feature, after_method, before_method
 
 app_info = '''
@@ -37,8 +37,8 @@ def set_macosx_deployment_target(self):
 	"""
 	see WAF issue 285 and also and also http://trac.macports.org/ticket/17059
 	"""
-	if self.env['MACOSX_DEPLOYMENT_TARGET']:
-		os.environ['MACOSX_DEPLOYMENT_TARGET'] = self.env['MACOSX_DEPLOYMENT_TARGET']
+	if self.env.MACOSX_DEPLOYMENT_TARGET:
+		os.environ['MACOSX_DEPLOYMENT_TARGET'] = self.env.MACOSX_DEPLOYMENT_TARGET
 	elif 'MACOSX_DEPLOYMENT_TARGET' not in os.environ:
 		if Utils.unversioned_sys_platform() == 'darwin':
 			os.environ['MACOSX_DEPLOYMENT_TARGET'] = '.'.join(platform.mac_ver()[0].split('.')[:2])
@@ -46,7 +46,7 @@ def set_macosx_deployment_target(self):
 @taskgen_method
 def create_bundle_dirs(self, name, out):
 	"""
-	Create bundle folders, used by :py:func:`create_task_macplist` and :py:func:`create_task_macapp`
+	Creates bundle folders, used by :py:func:`create_task_macplist` and :py:func:`create_task_macapp`
 	"""
 	dir = out.parent.find_or_declare(name)
 	dir.mkdir()
@@ -78,7 +78,7 @@ def build(bld):
 			bld.env.MACAPP = True
 			bld.shlib(source='a.c', target='foo')
 	"""
-	if self.env['MACAPP'] or getattr(self, 'mac_app', False):
+	if self.env.MACAPP or getattr(self, 'mac_app', False):
 		out = self.link_task.outputs[0]
 
 		name = bundle_name_for_output(out)
@@ -88,7 +88,7 @@ def build(bld):
 
 		self.apptask = self.create_task('macapp', self.link_task.outputs, n1)
 		inst_to = getattr(self, 'install_path', '/Applications') + '/%s/Contents/MacOS/' % name
-		self.bld.install_files(inst_to, n1, chmod=Utils.O755)
+		self.add_install_files(install_to=inst_to, install_from=n1, chmod=Utils.O755)
 
 		if getattr(self, 'mac_files', None):
 			# this only accepts files; they will be installed as seen from mac_files_root
@@ -102,38 +102,19 @@ def build(bld):
 			for node in self.to_nodes(self.mac_files):
 				relpath = node.path_from(mac_files_root or node.parent)
 				self.create_task('macapp', node, res_dir.make_node(relpath))
-				self.bld.install_as(os.path.join(inst_to, relpath), node)
-
-		if getattr(self, 'mac_resources', None):
-			# TODO remove in waf 1.9
-			res_dir = n1.parent.parent.make_node('Resources')
-			inst_to = getattr(self, 'install_path', '/Applications') + '/%s/Resources' % name
-			for x in self.to_list(self.mac_resources):
-				node = self.path.find_node(x)
-				if not node:
-					raise Errors.WafError('Missing mac_resource %r in %r' % (x, self))
-
-				parent = node.parent
-				if os.path.isdir(node.abspath()):
-					nodes = node.ant_glob('**')
-				else:
-					nodes = [node]
-				for node in nodes:
-					rel = node.path_from(parent)
-					self.create_task('macapp', node, res_dir.make_node(rel))
-					self.bld.install_as(inst_to + '/%s' % rel, node)
+				self.add_install_as(install_to=os.path.join(inst_to, relpath), install_from=node)
 
 		if getattr(self.bld, 'is_install', None):
-			# disable the normal binary installation
+			# disable regular binary installation
 			self.install_task.hasrun = Task.SKIP_ME
 
 @feature('cprogram', 'cxxprogram')
 @after_method('apply_link')
 def create_task_macplist(self):
 	"""
-	Create a :py:class:`waflib.Tools.c_osx.macplist` instance.
+	Creates a :py:class:`waflib.Tools.c_osx.macplist` instance.
 	"""
-	if  self.env['MACAPP'] or getattr(self, 'mac_app', False):
+	if  self.env.MACAPP or getattr(self, 'mac_app', False):
 		out = self.link_task.outputs[0]
 
 		name = bundle_name_for_output(out)
@@ -160,7 +141,7 @@ def create_task_macplist(self):
 			plisttask.code = app_info
 
 		inst_to = getattr(self, 'install_path', '/Applications') + '/%s/Contents/' % name
-		self.bld.install_files(inst_to, n1)
+		self.add_install_files(install_to=inst_to, install_from=n1)
 
 @feature('cshlib', 'cxxshlib')
 @before_method('apply_link', 'propagate_uselib_vars')
@@ -177,9 +158,9 @@ def build(bld):
 			bld.env.MACBUNDLE = True
 			bld.shlib(source='a.c', target='foo')
 	"""
-	if self.env['MACBUNDLE'] or getattr(self, 'mac_bundle', False):
-		self.env['LINKFLAGS_cshlib'] = self.env['LINKFLAGS_cxxshlib'] = [] # disable the '-dynamiclib' flag
-		self.env['cshlib_PATTERN'] = self.env['cxxshlib_PATTERN'] = self.env['macbundle_PATTERN']
+	if self.env.MACBUNDLE or getattr(self, 'mac_bundle', False):
+		self.env.LINKFLAGS_cshlib = self.env.LINKFLAGS_cxxshlib = [] # disable the '-dynamiclib' flag
+		self.env.cshlib_PATTERN = self.env.cxxshlib_PATTERN = self.env.macbundle_PATTERN
 		use = self.use = self.to_list(getattr(self, 'use', []))
 		if not 'MACBUNDLE' in use:
 			use.append('MACBUNDLE')
@@ -188,7 +169,7 @@ def build(bld):
 
 class macapp(Task.Task):
 	"""
-	Create mac applications
+	Creates mac applications
 	"""
 	color = 'PINK'
 	def run(self):
@@ -197,7 +178,7 @@ def run(self):
 
 class macplist(Task.Task):
 	"""
-	Create plist files
+	Creates plist files
 	"""
 	color = 'PINK'
 	ext_in = ['.bin']
@@ -209,3 +190,4 @@ def run(self):
 		context = getattr(self, 'context', {})
 		txt = txt.format(**context)
 		self.outputs[0].write(txt)
+
diff --git a/waflib/Tools/c_preproc.py b/waflib/Tools/c_preproc.py
index c244f642..7e04b4a7 100644
--- a/waflib/Tools/c_preproc.py
+++ b/waflib/Tools/c_preproc.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 
 """
 C/C++ preprocessor for finding dependencies
@@ -28,11 +28,13 @@
 
 import re, string, traceback
 from waflib import Logs, Utils, Errors
-from waflib.Logs import debug, error
 
 class PreprocError(Errors.WafError):
 	pass
 
+FILE_CACHE_SIZE = 100000
+LINE_CACHE_SIZE = 100000
+
 POPFILE = '-'
 "Constant representing a special token used in :py:meth:`waflib.Tools.c_preproc.c_parser.start` iteration to switch to a header read previously"
 
@@ -42,15 +44,15 @@ class PreprocError(Errors.WafError):
 go_absolute = False
 "Set to True to track headers on files in /usr/include, else absolute paths are ignored (but it becomes very slow)"
 
-standard_includes = ['/usr/include']
+standard_includes = ['/usr/local/include', '/usr/include']
 if Utils.is_win32:
 	standard_includes = []
 
 use_trigraphs = 0
 """Apply trigraph rules (False by default)"""
 
+# obsolete, do not use
 strict_quotes = 0
-"""Reserve the "#include <>" quotes for system includes (do not search for those includes). False by default."""
 
 g_optrans = {
 'not':'!',
@@ -69,7 +71,7 @@ class PreprocError(Errors.WafError):
 
 # ignore #warning and #error
 re_lines = re.compile(
-	'^[ \t]*(#|%:)[ \t]*(ifdef|ifndef|if|else|elif|endif|include|import|define|undef|pragma)[ \t]*(.*)\r*$',
+	'^[ \t]*(?:#|%:)[ \t]*(ifdef|ifndef|if|else|elif|endif|include|import|define|undef|pragma)[ \t]*(.*)\r*$',
 	re.IGNORECASE | re.MULTILINE)
 """Match #include lines"""
 
@@ -137,54 +139,22 @@ class PreprocError(Errors.WafError):
 
 def repl(m):
 	"""Replace function used with :py:attr:`waflib.Tools.c_preproc.re_cpp`"""
-	s = m.group(0)
-	if s.startswith('/'):
+	s = m.group()
+	if s[0] == '/':
 		return ' '
 	return s
 
-def filter_comments(filename):
-	"""
-	Filter the comments from a c/h file, and return the preprocessor lines.
-	The regexps :py:attr:`waflib.Tools.c_preproc.re_cpp`, :py:attr:`waflib.Tools.c_preproc.re_nl` and :py:attr:`waflib.Tools.c_preproc.re_lines` are used internally.
-
-	:return: the preprocessor directives as a list of (keyword, line)
-	:rtype: a list of string pairs
-	"""
-	# return a list of tuples : keyword, line
-	code = Utils.readf(filename)
-	if use_trigraphs:
-		for (a, b) in trig_def: code = code.split(a).join(b)
-	code = re_nl.sub('', code)
-	code = re_cpp.sub(repl, code)
-	return [(m.group(2), m.group(3)) for m in re.finditer(re_lines, code)]
-
 prec = {}
 """
-Operator precendence rules required for parsing expressions of the form::
+Operator precedence rules required for parsing expressions of the form::
 
 	#if 1 && 2 != 0
 """
 ops = ['* / %', '+ -', '<< >>', '< <= >= >', '== !=', '& | ^', '&& ||', ',']
-for x in range(len(ops)):
-	syms = ops[x]
+for x, syms in enumerate(ops):
 	for u in syms.split():
 		prec[u] = x
 
-def trimquotes(s):
-	"""
-	Remove the single quotes around an expression::
-
-		trimquotes("'test'") == "test"
-
-	:param s: expression to transform
-	:type s: string
-	:rtype: string
-	"""
-	if not s: return ''
-	s = s.rstrip()
-	if s[0] == "'" and s[-1] == "'": return s[1:-1]
-	return s
-
 def reduce_nums(val_1, val_2, val_op):
 	"""
 	Apply arithmetic rules to compute a result
@@ -200,32 +170,56 @@ def reduce_nums(val_1, val_2, val_op):
 	#print val_1, val_2, val_op
 
 	# now perform the operation, make certain a and b are numeric
-	try:    a = 0 + val_1
-	except TypeError: a = int(val_1)
-	try:    b = 0 + val_2
-	except TypeError: b = int(val_2)
+	try:
+		a = 0 + val_1
+	except TypeError:
+		a = int(val_1)
+	try:
+		b = 0 + val_2
+	except TypeError:
+		b = int(val_2)
 
 	d = val_op
-	if d == '%':  c = a%b
-	elif d=='+':  c = a+b
-	elif d=='-':  c = a-b
-	elif d=='*':  c = a*b
-	elif d=='/':  c = a/b
-	elif d=='^':  c = a^b
-	elif d=='==': c = int(a == b)
-	elif d=='|'  or d == 'bitor':  c = a|b
-	elif d=='||' or d == 'or' : c = int(a or b)
-	elif d=='&'  or d == 'bitand':  c = a&b
-	elif d=='&&' or d == 'and': c = int(a and b)
-	elif d=='!=' or d == 'not_eq': c = int(a != b)
-	elif d=='^'  or d == 'xor':  c = int(a^b)
-	elif d=='<=': c = int(a <= b)
-	elif d=='<':  c = int(a < b)
-	elif d=='>':  c = int(a > b)
-	elif d=='>=': c = int(a >= b)
-	elif d=='<<': c = a<<b
-	elif d=='>>': c = a>>b
-	else: c = 0
+	if d == '%':
+		c = a % b
+	elif d=='+':
+		c = a + b
+	elif d=='-':
+		c = a - b
+	elif d=='*':
+		c = a * b
+	elif d=='/':
+		c = a / b
+	elif d=='^':
+		c = a ^ b
+	elif d=='==':
+		c = int(a == b)
+	elif d=='|'  or d == 'bitor':
+		c = a | b
+	elif d=='||' or d == 'or' :
+		c = int(a or b)
+	elif d=='&'  or d == 'bitand':
+		c = a & b
+	elif d=='&&' or d == 'and':
+		c = int(a and b)
+	elif d=='!=' or d == 'not_eq':
+		c = int(a != b)
+	elif d=='^'  or d == 'xor':
+		c = int(a^b)
+	elif d=='<=':
+		c = int(a <= b)
+	elif d=='<':
+		c = int(a < b)
+	elif d=='>':
+		c = int(a > b)
+	elif d=='>=':
+		c = int(a >= b)
+	elif d=='<<':
+		c = a << b
+	elif d=='>>':
+		c = a >> b
+	else:
+		c = 0
 	return c
 
 def get_num(lst):
@@ -237,7 +231,8 @@ def get_num(lst):
 	:return: a pair containing the number and the rest of the list
 	:rtype: tuple(value, list)
 	"""
-	if not lst: raise PreprocError("empty list for get_num")
+	if not lst:
+		raise PreprocError('empty list for get_num')
 	(p, v) = lst[0]
 	if p == OP:
 		if v == '(':
@@ -255,7 +250,7 @@ def get_num(lst):
 						count_par += 1
 				i += 1
 			else:
-				raise PreprocError("rparen expected %r" % lst)
+				raise PreprocError('rparen expected %r' % lst)
 
 			(num, _) = get_term(lst[1:i])
 			return (num, lst[i+1:])
@@ -272,14 +267,14 @@ def get_num(lst):
 			num, lst = get_num(lst[1:])
 			return (~ int(num), lst)
 		else:
-			raise PreprocError("Invalid op token %r for get_num" % lst)
+			raise PreprocError('Invalid op token %r for get_num' % lst)
 	elif p == NUM:
 		return v, lst[1:]
 	elif p == IDENT:
 		# all macros should have been replaced, remaining identifiers eval to 0
 		return 0, lst[1:]
 	else:
-		raise PreprocError("Invalid token %r for get_num" % lst)
+		raise PreprocError('Invalid token %r for get_num' % lst)
 
 def get_term(lst):
 	"""
@@ -293,7 +288,8 @@ def get_term(lst):
 	:rtype: value, list
 	"""
 
-	if not lst: raise PreprocError("empty list for get_term")
+	if not lst:
+		raise PreprocError('empty list for get_term')
 	num, lst = get_num(lst)
 	if not lst:
 		return (num, [])
@@ -318,7 +314,7 @@ def get_term(lst):
 							break
 				i += 1
 			else:
-				raise PreprocError("rparen expected %r" % lst)
+				raise PreprocError('rparen expected %r' % lst)
 
 			if int(num):
 				return get_term(lst[1:i])
@@ -336,7 +332,7 @@ def get_term(lst):
 			# operator precedence
 			p2, v2 = lst[0]
 			if p2 != OP:
-				raise PreprocError("op expected %r" % lst)
+				raise PreprocError('op expected %r' % lst)
 
 			if prec[v2] >= prec[v]:
 				num2 = reduce_nums(num, num2, v)
@@ -347,7 +343,7 @@ def get_term(lst):
 				return get_term([(NUM, num), (p, v), (NUM, num3)] + lst)
 
 
-	raise PreprocError("cannot reduce %r" % lst)
+	raise PreprocError('cannot reduce %r' % lst)
 
 def reduce_eval(lst):
 	"""
@@ -432,7 +428,7 @@ def reduce_tokens(lst, defs, ban=[]):
 					else:
 						lst[i] = (NUM, 0)
 				else:
-					raise PreprocError("Invalid define expression %r" % lst)
+					raise PreprocError('Invalid define expression %r' % lst)
 
 		elif p == IDENT and v in defs:
 
@@ -447,8 +443,8 @@ def reduce_tokens(lst, defs, ban=[]):
 				del lst[i]
 				accu = to_add[:]
 				reduce_tokens(accu, defs, ban+[v])
-				for x in range(len(accu)):
-					lst.insert(i, accu[x])
+				for tmp in accu:
+					lst.insert(i, tmp)
 					i += 1
 			else:
 				# collect the arguments for the funcall
@@ -457,11 +453,11 @@ def reduce_tokens(lst, defs, ban=[]):
 				del lst[i]
 
 				if i >= len(lst):
-					raise PreprocError("expected '(' after %r (got nothing)" % v)
+					raise PreprocError('expected ( after %r (got nothing)' % v)
 
 				(p2, v2) = lst[i]
 				if p2 != OP or v2 != '(':
-					raise PreprocError("expected '(' after %r" % v)
+					raise PreprocError('expected ( after %r' % v)
 
 				del lst[i]
 
@@ -476,18 +472,22 @@ def reduce_tokens(lst, defs, ban=[]):
 							one_param.append((p2, v2))
 							count_paren += 1
 						elif v2 == ')':
-							if one_param: args.append(one_param)
+							if one_param:
+								args.append(one_param)
 							break
 						elif v2 == ',':
-							if not one_param: raise PreprocError("empty param in funcall %s" % v)
+							if not one_param:
+								raise PreprocError('empty param in funcall %r' % v)
 							args.append(one_param)
 							one_param = []
 						else:
 							one_param.append((p2, v2))
 					else:
 						one_param.append((p2, v2))
-						if   v2 == '(': count_paren += 1
-						elif v2 == ')': count_paren -= 1
+						if   v2 == '(':
+							count_paren += 1
+						elif v2 == ')':
+							count_paren -= 1
 				else:
 					raise PreprocError('malformed macro')
 
@@ -524,7 +524,6 @@ def reduce_tokens(lst, defs, ban=[]):
 									accu.append((p2, v2))
 									accu.extend(toks)
 							elif to_add[j+1][0] == IDENT and to_add[j+1][1] == '__VA_ARGS__':
-								# TODO not sure
 								# first collect the tokens
 								va_toks = []
 								st = len(macro_def[0])
@@ -532,7 +531,8 @@ def reduce_tokens(lst, defs, ban=[]):
 								for x in args[pt-st+1:]:
 									va_toks.extend(x)
 									va_toks.append((OP, ','))
-								if va_toks: va_toks.pop() # extra comma
+								if va_toks:
+									va_toks.pop() # extra comma
 								if len(accu)>1:
 									(p3, v3) = accu[-1]
 									(p4, v4) = accu[-2]
@@ -580,8 +580,15 @@ def eval_macro(lst, defs):
 	:rtype: int
 	"""
 	reduce_tokens(lst, defs, [])
-	if not lst: raise PreprocError("missing tokens to evaluate")
-	(p, v) = reduce_eval(lst)
+	if not lst:
+		raise PreprocError('missing tokens to evaluate')
+
+	if lst:
+		p, v = lst[0]
+		if p == IDENT and v not in defs:
+			raise PreprocError('missing macro %r' % lst)
+
+	p, v = reduce_eval(lst)
 	return int(v) != 0
 
 def extract_macro(txt):
@@ -601,7 +608,8 @@ def extract_macro(txt):
 		p, name = t[0]
 
 		p, v = t[1]
-		if p != OP: raise PreprocError("expected open parenthesis")
+		if p != OP:
+			raise PreprocError('expected (')
 
 		i = 1
 		pindex = 0
@@ -620,27 +628,27 @@ def extract_macro(txt):
 				elif p == OP and v == ')':
 					break
 				else:
-					raise PreprocError("unexpected token (3)")
+					raise PreprocError('unexpected token (3)')
 			elif prev == IDENT:
 				if p == OP and v == ',':
 					prev = v
 				elif p == OP and v == ')':
 					break
 				else:
-					raise PreprocError("comma or ... expected")
+					raise PreprocError('comma or ... expected')
 			elif prev == ',':
 				if p == IDENT:
 					params[v] = pindex
 					pindex += 1
 					prev = p
 				elif p == OP and v == '...':
-					raise PreprocError("not implemented (1)")
+					raise PreprocError('not implemented (1)')
 				else:
-					raise PreprocError("comma or ... expected (2)")
+					raise PreprocError('comma or ... expected (2)')
 			elif prev == '...':
-				raise PreprocError("not implemented (2)")
+				raise PreprocError('not implemented (2)')
 			else:
-				raise PreprocError("unexpected else")
+				raise PreprocError('unexpected else')
 
 		#~ print (name, [params, t[i+1:]])
 		return (name, [params, t[i+1:]])
@@ -652,7 +660,7 @@ def extract_macro(txt):
 			# empty define, assign an empty token
 			return (v, [[], [('T','')]])
 
-re_include = re.compile('^\s*(<(?P<a>.*)>|"(?P<b>.*)")')
+re_include = re.compile('^\s*(<(?:.*)>|"(?:.*)")')
 def extract_include(txt, defs):
 	"""
 	Process a line in the form::
@@ -668,15 +676,15 @@ def extract_include(txt, defs):
 	"""
 	m = re_include.search(txt)
 	if m:
-		if m.group('a'): return '<', m.group('a')
-		if m.group('b'): return '"', m.group('b')
+		txt = m.group(1)
+		return txt[0], txt[1:-1]
 
 	# perform preprocessing and look at the result, it must match an include
 	toks = tokenize(txt)
 	reduce_tokens(toks, defs, ['waf_include'])
 
 	if not toks:
-		raise PreprocError("could not parse include %s" % txt)
+		raise PreprocError('could not parse include %r' % txt)
 
 	if len(toks) == 1:
 		if toks[0][0] == STR:
@@ -686,7 +694,7 @@ def extract_include(txt, defs):
 			ret = '<', stringize(toks).lstrip('<').rstrip('>')
 			return ret
 
-	raise PreprocError("could not parse include %s." % txt)
+	raise PreprocError('could not parse include %r' % txt)
 
 def parse_char(txt):
 	"""
@@ -698,21 +706,26 @@ def parse_char(txt):
 	:rtype: string
 	"""
 
-	if not txt: raise PreprocError("attempted to parse a null char")
+	if not txt:
+		raise PreprocError('attempted to parse a null char')
 	if txt[0] != '\\':
 		return ord(txt)
 	c = txt[1]
 	if c == 'x':
-		if len(txt) == 4 and txt[3] in string.hexdigits: return int(txt[2:], 16)
+		if len(txt) == 4 and txt[3] in string.hexdigits:
+			return int(txt[2:], 16)
 		return int(txt[2:], 16)
 	elif c.isdigit():
-		if c == '0' and len(txt)==2: return 0
+		if c == '0' and len(txt)==2:
+			return 0
 		for i in 3, 2, 1:
 			if len(txt) > i and txt[1:1+i].isdigit():
 				return (1+i, int(txt[1:1+i], 8))
 	else:
-		try: return chr_esc[c]
-		except KeyError: raise PreprocError("could not parse char literal '%s'" % txt)
+		try:
+			return chr_esc[c]
+		except KeyError:
+			raise PreprocError('could not parse char literal %r' % txt)
 
 def tokenize(s):
 	"""
@@ -725,7 +738,6 @@ def tokenize(s):
 	"""
 	return tokenize_private(s)[:] # force a copy of the results
 
-@Utils.run_once
 def tokenize_private(s):
 	ret = []
 	for match in re_clexer.finditer(s):
@@ -734,28 +746,32 @@ def tokenize_private(s):
 			v = m(name)
 			if v:
 				if name == IDENT:
-					try:
-						g_optrans[v];
+					if v in g_optrans:
 						name = OP
-					except KeyError:
-						# c++ specific
-						if v.lower() == "true":
-							v = 1
-							name = NUM
-						elif v.lower() == "false":
-							v = 0
-							name = NUM
+					elif v.lower() == "true":
+						v = 1
+						name = NUM
+					elif v.lower() == "false":
+						v = 0
+						name = NUM
 				elif name == NUM:
-					if m('oct'): v = int(v, 8)
-					elif m('hex'): v = int(m('hex'), 16)
-					elif m('n0'): v = m('n0')
+					if m('oct'):
+						v = int(v, 8)
+					elif m('hex'):
+						v = int(m('hex'), 16)
+					elif m('n0'):
+						v = m('n0')
 					else:
 						v = m('char')
-						if v: v = parse_char(v)
-						else: v = m('n2') or m('n4')
+						if v:
+							v = parse_char(v)
+						else:
+							v = m('n2') or m('n4')
 				elif name == OP:
-					if v == '%:': v = '#'
-					elif v == '%:%:': v = '##'
+					if v == '%:':
+						v = '#'
+					elif v == '%:%:':
+						v = '##'
 				elif name == STR:
 					# remove the quotes around the string
 					v = v[1:-1]
@@ -763,15 +779,20 @@ def tokenize_private(s):
 				break
 	return ret
 
-@Utils.run_once
-def define_name(line):
-	"""
-	:param line: define line
-	:type line: string
-	:rtype: string
-	:return: the define name
-	"""
-	return re_mac.match(line).group(0)
+def format_defines(lst):
+	ret = []
+	for y in lst:
+		if y:
+			pos = y.find('=')
+			if pos == -1:
+				# "-DFOO" should give "#define FOO 1"
+				ret.append(y)
+			elif pos > 0:
+				# all others are assumed to be -DX=Y
+				ret.append('%s %s' % (y[:pos], y[pos+1:]))
+			else:
+				raise ValueError('Invalid define expression %r' % y)
+	return ret
 
 class c_parser(object):
 	"""
@@ -803,9 +824,12 @@ def __init__(self, nodepaths=None, defines=None):
 		self.curfile = ''
 		"""Current file"""
 
-		self.ban_includes = set([])
+		self.ban_includes = set()
 		"""Includes that must not be read (#pragma once)"""
 
+		self.listed = set()
+		"""Include nodes/names already listed to avoid duplicates in self.nodes/self.names"""
+
 	def cached_find_resource(self, node, filename):
 		"""
 		Find a file from the input directory
@@ -818,13 +842,13 @@ def cached_find_resource(self, node, filename):
 		:rtype: :py:class:`waflib.Node.Node`
 		"""
 		try:
-			nd = node.ctx.cache_nd
+			cache = node.ctx.preproc_cache_node
 		except AttributeError:
-			nd = node.ctx.cache_nd = {}
+			cache = node.ctx.preproc_cache_node = Utils.lru_cache(FILE_CACHE_SIZE)
 
-		tup = (node, filename)
+		key = (node, filename)
 		try:
-			return nd[tup]
+			return cache[key]
 		except KeyError:
 			ret = node.find_resource(filename)
 			if ret:
@@ -834,10 +858,10 @@ def cached_find_resource(self, node, filename):
 					tmp = node.ctx.srcnode.search_node(ret.path_from(node.ctx.bldnode))
 					if tmp and getattr(tmp, 'children', None):
 						ret = None
-			nd[tup] = ret
+			cache[key] = ret
 			return ret
 
-	def tryfind(self, filename):
+	def tryfind(self, filename, kind='"', env=None):
 		"""
 		Try to obtain a node from the filename based from the include paths. Will add
 		the node found to :py:attr:`waflib.Tools.c_preproc.c_parser.nodes` or the file name to
@@ -851,29 +875,70 @@ def tryfind(self, filename):
 		"""
 		if filename.endswith('.moc'):
 			# we could let the qt4 module use a subclass, but then the function "scan" below must be duplicated
-			# in the qt4 and in the qt5 classes. So we have two lines here and it is sufficient. TODO waf 1.9
+			# in the qt4 and in the qt5 classes. So we have two lines here and it is sufficient.
 			self.names.append(filename)
 			return None
 
 		self.curfile = filename
 
-		# for msvc it should be a for loop over the whole stack
-		found = self.cached_find_resource(self.currentnode_stack[-1], filename)
+		found = None
+		if kind == '"':
+			if env.MSVC_VERSION:
+				for n in reversed(self.currentnode_stack):
+					found = self.cached_find_resource(n, filename)
+					if found:
+						break
+			else:
+				found = self.cached_find_resource(self.currentnode_stack[-1], filename)
 
-		for n in self.nodepaths:
-			if found:
-				break
-			found = self.cached_find_resource(n, filename)
+		if not found:
+			for n in self.nodepaths:
+				found = self.cached_find_resource(n, filename)
+				if found:
+					break
 
+		listed = self.listed
 		if found and not found in self.ban_includes:
-			# TODO duplicates do not increase the no-op build times too much, but they may be worth removing
-			self.nodes.append(found)
+			if found not in listed:
+				listed.add(found)
+				self.nodes.append(found)
 			self.addlines(found)
 		else:
-			if not filename in self.names:
+			if filename not in listed:
+				listed.add(filename)
 				self.names.append(filename)
 		return found
 
+	def filter_comments(self, node):
+		"""
+		Filter the comments from a c/h file, and return the preprocessor lines.
+		The regexps :py:attr:`waflib.Tools.c_preproc.re_cpp`, :py:attr:`waflib.Tools.c_preproc.re_nl` and :py:attr:`waflib.Tools.c_preproc.re_lines` are used internally.
+
+		:return: the preprocessor directives as a list of (keyword, line)
+		:rtype: a list of string pairs
+		"""
+		# return a list of tuples : keyword, line
+		code = node.read()
+		if use_trigraphs:
+			for (a, b) in trig_def:
+				code = code.split(a).join(b)
+		code = re_nl.sub('', code)
+		code = re_cpp.sub(repl, code)
+		return re_lines.findall(code)
+
+	def parse_lines(self, node):
+		try:
+			cache = node.ctx.preproc_cache_lines
+		except AttributeError:
+			cache = node.ctx.preproc_cache_lines = Utils.lru_cache(LINE_CACHE_SIZE)
+		try:
+			return cache[node]
+		except KeyError:
+			cache[node] = lines = self.filter_comments(node)
+			lines.append((POPFILE, ''))
+			lines.reverse()
+			return lines
+
 	def addlines(self, node):
 		"""
 		Add the lines from a header in the list of preprocessor lines to parse
@@ -883,34 +948,23 @@ def addlines(self, node):
 		"""
 
 		self.currentnode_stack.append(node.parent)
-		filepath = node.abspath()
 
 		self.count_files += 1
 		if self.count_files > recursion_limit:
 			# issue #812
-			raise PreprocError("recursion limit exceeded")
-		pc = self.parse_cache
-		debug('preproc: reading file %r', filepath)
-		try:
-			lns = pc[filepath]
-		except KeyError:
-			pass
-		else:
-			self.lines.extend(lns)
-			return
+			raise PreprocError('recursion limit exceeded')
 
+		if Logs.verbose:
+			Logs.debug('preproc: reading file %r', node)
 		try:
-			lines = filter_comments(filepath)
-			lines.append((POPFILE, ''))
-			lines.reverse()
-			pc[filepath] = lines # cache the lines filtered
-			self.lines.extend(lines)
-		except IOError:
-			raise PreprocError("could not read the file %s" % filepath)
+			lines = self.parse_lines(node)
+		except EnvironmentError:
+			raise PreprocError('could not read the file %r' % node)
 		except Exception:
 			if Logs.verbose > 0:
-				error("parsing %s failed" % filepath)
-				traceback.print_exc()
+				Logs.error('parsing %r failed %s', node, traceback.format_exc())
+		else:
+			self.lines.extend(lines)
 
 	def start(self, node, env):
 		"""
@@ -922,27 +976,16 @@ def start(self, node, env):
 		:param env: config set containing additional defines to take into account
 		:type env: :py:class:`waflib.ConfigSet.ConfigSet`
 		"""
-
-		debug('preproc: scanning %s (in %s)', node.name, node.parent.name)
-
-		bld = node.ctx
-		try:
-			self.parse_cache = bld.parse_cache
-		except AttributeError:
-			self.parse_cache = bld.parse_cache = {}
+		Logs.debug('preproc: scanning %s (in %s)', node.name, node.parent.name)
 
 		self.current_file = node
 		self.addlines(node)
 
 		# macros may be defined on the command-line, so they must be parsed as if they were part of the file
-		if env['DEFINES']:
-			try:
-				lst = ['%s %s' % (x[0], trimquotes('='.join(x[1:]))) for x in [y.split('=') for y in env['DEFINES']]]
-				lst.reverse()
-				self.lines.extend([('define', x) for x in lst])
-			except AttributeError:
-				# if the defines are invalid the compiler will tell the user
-				pass
+		if env.DEFINES:
+			lst = format_defines(env.DEFINES)
+			lst.reverse()
+			self.lines.extend([('define', x) for x in lst])
 
 		while self.lines:
 			(token, line) = self.lines.pop()
@@ -952,8 +995,6 @@ def start(self, node, env):
 				continue
 
 			try:
-				ve = Logs.verbose
-				if ve: debug('preproc: line is %s - %s state is %s', token, line, self.state)
 				state = self.state
 
 				# make certain we define the state if we are about to enter in an if block
@@ -969,23 +1010,27 @@ def start(self, node, env):
 
 				if token == 'if':
 					ret = eval_macro(tokenize(line), self.defs)
-					if ret: state[-1] = accepted
-					else: state[-1] = ignored
+					if ret:
+						state[-1] = accepted
+					else:
+						state[-1] = ignored
 				elif token == 'ifdef':
 					m = re_mac.match(line)
-					if m and m.group(0) in self.defs: state[-1] = accepted
-					else: state[-1] = ignored
+					if m and m.group() in self.defs:
+						state[-1] = accepted
+					else:
+						state[-1] = ignored
 				elif token == 'ifndef':
 					m = re_mac.match(line)
-					if m and m.group(0) in self.defs: state[-1] = ignored
-					else: state[-1] = accepted
+					if m and m.group() in self.defs:
+						state[-1] = ignored
+					else:
+						state[-1] = accepted
 				elif token == 'include' or token == 'import':
 					(kind, inc) = extract_include(line, self.defs)
-					if ve: debug('preproc: include found %s    (%s) ', inc, kind)
-					if kind == '"' or not strict_quotes:
-						self.current_file = self.tryfind(inc)
-						if token == 'import':
-							self.ban_includes.add(self.current_file)
+					self.current_file = self.tryfind(inc, kind, env)
+					if token == 'import':
+						self.ban_includes.add(self.current_file)
 				elif token == 'elif':
 					if state[-1] == accepted:
 						state[-1] = skipped
@@ -993,24 +1038,35 @@ def start(self, node, env):
 						if eval_macro(tokenize(line), self.defs):
 							state[-1] = accepted
 				elif token == 'else':
-					if state[-1] == accepted: state[-1] = skipped
-					elif state[-1] == ignored: state[-1] = accepted
+					if state[-1] == accepted:
+						state[-1] = skipped
+					elif state[-1] == ignored:
+						state[-1] = accepted
 				elif token == 'define':
 					try:
-						self.defs[define_name(line)] = line
-					except Exception:
-						raise PreprocError("Invalid define line %s" % line)
+						self.defs[self.define_name(line)] = line
+					except AttributeError:
+						raise PreprocError('Invalid define line %r' % line)
 				elif token == 'undef':
 					m = re_mac.match(line)
-					if m and m.group(0) in self.defs:
-						self.defs.__delitem__(m.group(0))
+					if m and m.group() in self.defs:
+						self.defs.__delitem__(m.group())
 						#print "undef %s" % name
 				elif token == 'pragma':
 					if re_pragma_once.match(line.lower()):
 						self.ban_includes.add(self.current_file)
 			except Exception as e:
 				if Logs.verbose:
-					debug('preproc: line parsing failed (%s): %s %s', e, line, Utils.ex_stack())
+					Logs.debug('preproc: line parsing failed (%s): %s %s', e, line, traceback.format_exc())
+
+	def define_name(self, line):
+		"""
+		:param line: define line
+		:type line: string
+		:rtype: string
+		:return: the define name
+		"""
+		return re_mac.match(line).group()
 
 def scan(task):
 	"""
@@ -1020,9 +1076,6 @@ def scan(task):
 
 	This function is bound as a task method on :py:class:`waflib.Tools.c.c` and :py:class:`waflib.Tools.cxx.cxx` for example
 	"""
-
-	global go_absolute
-
 	try:
 		incn = task.generator.includes_nodes
 	except AttributeError:
@@ -1035,7 +1088,4 @@ def scan(task):
 
 	tmp = c_parser(nodepaths)
 	tmp.start(task.inputs[0], task.env)
-	if Logs.verbose:
-		debug('deps: deps for %r: %r; unresolved %r' % (task.inputs, tmp.nodes, tmp.names))
 	return (tmp.nodes, tmp.names)
-
diff --git a/waflib/Tools/c_tests.py b/waflib/Tools/c_tests.py
index 3b37f54c..f858df57 100644
--- a/waflib/Tools/c_tests.py
+++ b/waflib/Tools/c_tests.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2010 (ita)
+# Thomas Nagy, 2016-2018 (ita)
 
 """
 Various configuration tests.
@@ -58,7 +58,7 @@ def write_test_file(task):
 @conf
 def check_library(self, mode=None, test_exec=True):
 	"""
-	Check if libraries can be linked with the current linker. Uses :py:func:`waflib.Tools.c_tests.link_lib_test_fun`.
+	Checks if libraries can be linked with the current linker. Uses :py:func:`waflib.Tools.c_tests.link_lib_test_fun`.
 
 	:param mode: c or cxx or d
 	:type mode: string
@@ -72,8 +72,7 @@ def check_library(self, mode=None, test_exec=True):
 		features = 'link_lib_test',
 		msg = 'Checking for libraries',
 		mode = mode,
-		test_exec = test_exec,
-		)
+		test_exec = test_exec)
 
 ########################################################################################
 
@@ -89,7 +88,7 @@ def check_library(self, mode=None, test_exec=True):
 @conf
 def check_inline(self, **kw):
 	"""
-	Check for the right value for inline macro.
+	Checks for the right value for inline macro.
 	Define INLINE_MACRO to 1 if the define is found.
 	If the inline macro is not 'inline', add a define to the ``config.h`` (#define inline __inline__)
 
@@ -98,7 +97,6 @@ def check_inline(self, **kw):
 	:param features: by default *c* or *cxx* depending on the compiler present
 	:type features: list of string
 	"""
-
 	self.start_msg('Checking for inline')
 
 	if not 'define_name' in kw:
@@ -135,7 +133,7 @@ def check_inline(self, **kw):
 @conf
 def check_large_file(self, **kw):
 	"""
-	Check for large file support and define the macro HAVE_LARGEFILE
+	Checks for large file support and define the macro HAVE_LARGEFILE
 	The test is skipped on win32 systems (DEST_BINFMT == pe).
 
 	:param define_name: define to set, by default *HAVE_LARGEFILE*
@@ -143,7 +141,6 @@ def check_large_file(self, **kw):
 	:param execute: execute the test (yes by default)
 	:type execute: bool
 	"""
-
 	if not 'define_name' in kw:
 		kw['define_name'] = 'HAVE_LARGEFILE'
 	if not 'execute' in kw:
@@ -197,9 +194,12 @@ def check_large_file(self, **kw):
 '''
 
 class grep_for_endianness(Task.Task):
+	"""
+	Task that reads a binary and tries to determine the endianness
+	"""
 	color = 'PINK'
 	def run(self):
-		txt = self.inputs[0].read(flags='rb').decode('iso8859-1')
+		txt = self.inputs[0].read(flags='rb').decode('latin-1')
 		if txt.find('LiTTleEnDian') > -1:
 			self.generator.tmp.append('little')
 		elif txt.find('BIGenDianSyS') > -1:
@@ -211,18 +211,19 @@ def run(self):
 @after_method('process_source')
 def grep_for_endianness_fun(self):
 	"""
-	Used by the endiannes configuration test
+	Used by the endianness configuration test
 	"""
 	self.create_task('grep_for_endianness', self.compiled_tasks[0].outputs[0])
 
 @conf
 def check_endianness(self):
 	"""
-	Execute a configuration test to determine the endianness
+	Executes a configuration test to determine the endianness
 	"""
 	tmp = []
 	def check_msg(self):
 		return tmp[0]
-	self.check(fragment=ENDIAN_FRAGMENT, features='c grep_for_endianness', msg="Checking for endianness", define='ENDIANNESS', tmp=tmp, okmsg=check_msg)
+	self.check(fragment=ENDIAN_FRAGMENT, features='c grep_for_endianness',
+		msg='Checking for endianness', define='ENDIANNESS', tmp=tmp, okmsg=check_msg)
 	return tmp[0]
 
diff --git a/waflib/Tools/ccroot.py b/waflib/Tools/ccroot.py
index 7fb53bba..cfef8bf5 100644
--- a/waflib/Tools/ccroot.py
+++ b/waflib/Tools/ccroot.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 Classes and methods shared by tools providing support for C-like language such
@@ -8,7 +8,7 @@
 """
 
 import os, re
-from waflib import Task, Utils, Node, Errors
+from waflib import Task, Utils, Node, Errors, Logs
 from waflib.TaskGen import after_method, before_method, feature, taskgen_method, extension
 from waflib.Tools import c_aliases, c_preproc, c_config, c_osx, c_tests
 from waflib.Configure import conf
@@ -77,7 +77,7 @@ def to_incnodes(self, inlst):
 	:return: list of include folders as nodes
 	"""
 	lst = []
-	seen = set([])
+	seen = set()
 	for x in self.to_list(inlst):
 		if x in seen or not x:
 			continue
@@ -118,9 +118,10 @@ def apply_incpaths(self):
 	and the list of include paths in ``tg.env.INCLUDES``.
 	"""
 
-	lst = self.to_incnodes(self.to_list(getattr(self, 'includes', [])) + self.env['INCLUDES'])
+	lst = self.to_incnodes(self.to_list(getattr(self, 'includes', [])) + self.env.INCLUDES)
 	self.includes_nodes = lst
-	self.env['INCPATHS'] = [x.abspath() for x in lst]
+	cwd = self.get_cwd()
+	self.env.INCPATHS = [x.path_from(cwd) for x in lst]
 
 class link_task(Task.Task):
 	"""
@@ -130,6 +131,9 @@ class link_task(Task.Task):
 	"""
 	color   = 'YELLOW'
 
+	weight  = 3
+	"""Try to process link tasks as early as possible"""
+
 	inst_to = None
 	"""Default installation path for the link task outputs, or None to disable"""
 
@@ -142,6 +146,12 @@ def add_target(self, target):
 		The settings are retrieved from ``env.clsname_PATTERN``
 		"""
 		if isinstance(target, str):
+			base = self.generator.path
+			if target.startswith('#'):
+				# for those who like flat structures
+				target = target[1:]
+				base = self.generator.bld.bldnode
+
 			pattern = self.env[self.__class__.__name__ + '_PATTERN']
 			if not pattern:
 				pattern = '%s'
@@ -151,7 +161,7 @@ def add_target(self, target):
 				nums = self.generator.vnum.split('.')
 				if self.env.DEST_BINFMT == 'pe':
 					# include the version in the dll file name,
-					# the import lib file name stays unversionned.
+					# the import lib file name stays unversioned.
 					name = name + '-' + nums[0]
 				elif self.env.DEST_OS == 'openbsd':
 					pattern = '%s.%s' % (pattern, nums[0])
@@ -162,9 +172,51 @@ def add_target(self, target):
 				tmp = folder + os.sep + pattern % name
 			else:
 				tmp = pattern % name
-			target = self.generator.path.find_or_declare(tmp)
+			target = base.find_or_declare(tmp)
 		self.set_outputs(target)
 
+	def exec_command(self, *k, **kw):
+		ret = super(link_task, self).exec_command(*k, **kw)
+		if not ret and self.env.DO_MANIFEST:
+			ret = self.exec_mf()
+		return ret
+
+	def exec_mf(self):
+		"""
+		Create manifest files for VS-like compilers (msvc, ifort, ...)
+		"""
+		if not self.env.MT:
+			return 0
+
+		manifest = None
+		for out_node in self.outputs:
+			if out_node.name.endswith('.manifest'):
+				manifest = out_node.abspath()
+				break
+		else:
+			# Should never get here.  If we do, it means the manifest file was
+			# never added to the outputs list, thus we don't have a manifest file
+			# to embed, so we just return.
+			return 0
+
+		# embedding mode. Different for EXE's and DLL's.
+		# see: http://msdn2.microsoft.com/en-us/library/ms235591(VS.80).aspx
+		mode = ''
+		for x in Utils.to_list(self.generator.features):
+			if x in ('cprogram', 'cxxprogram', 'fcprogram', 'fcprogram_test'):
+				mode = 1
+			elif x in ('cshlib', 'cxxshlib', 'fcshlib'):
+				mode = 2
+
+		Logs.debug('msvc: embedding manifest in mode %r', mode)
+
+		lst = [] + self.env.MT
+		lst.extend(Utils.to_list(self.env.MTFLAGS))
+		lst.extend(['-manifest', manifest])
+		lst.append('-outputresource:%s;%s' % (self.outputs[0].abspath(), mode))
+
+		return super(link_task, self).exec_command(lst)
+
 class stlink_task(link_task):
 	"""
 	Base for static link tasks, which use *ar* most of the time.
@@ -178,8 +230,10 @@ class stlink_task(link_task):
 def rm_tgt(cls):
 	old = cls.run
 	def wrap(self):
-		try: os.remove(self.outputs[0].abspath())
-		except OSError: pass
+		try:
+			os.remove(self.outputs[0].abspath())
+		except OSError:
+			pass
 		return old(self)
 	setattr(cls, 'run', wrap)
 rm_tgt(stlink_task)
@@ -219,10 +273,12 @@ def build(bld):
 	try:
 		inst_to = self.install_path
 	except AttributeError:
-		inst_to = self.link_task.__class__.inst_to
+		inst_to = self.link_task.inst_to
 	if inst_to:
 		# install a copy of the node list we have at this moment (implib not added)
-		self.install_task = self.bld.install_files(inst_to, self.link_task.outputs[:], env=self.env, chmod=self.link_task.chmod, task=self.link_task)
+		self.install_task = self.add_install_files(
+			install_to=inst_to, install_from=self.link_task.outputs[:],
+			chmod=self.link_task.chmod, task=self.link_task)
 
 @taskgen_method
 def use_rec(self, name, **kw):
@@ -282,7 +338,7 @@ def build(bld):
 	See :py:func:`waflib.Tools.ccroot.use_rec`.
 	"""
 
-	use_not = self.tmp_use_not = set([])
+	use_not = self.tmp_use_not = set()
 	self.tmp_use_seen = [] # we would like an ordered set
 	use_prec = self.tmp_use_prec = {}
 	self.uselib = self.to_list(getattr(self, 'uselib', []))
@@ -297,7 +353,7 @@ def build(bld):
 			del use_prec[x]
 
 	# topological sort
-	out = []
+	out = self.tmp_use_sorted = []
 	tmp = []
 	for x in self.tmp_use_seen:
 		for k in use_prec.values():
@@ -333,14 +389,15 @@ def build(bld):
 			if var == 'LIB' or y.tmp_use_stlib or x in names:
 				self.env.append_value(var, [y.target[y.target.rfind(os.sep) + 1:]])
 				self.link_task.dep_nodes.extend(y.link_task.outputs)
-				tmp_path = y.link_task.outputs[0].parent.path_from(self.bld.bldnode)
+				tmp_path = y.link_task.outputs[0].parent.path_from(self.get_cwd())
 				self.env.append_unique(var + 'PATH', [tmp_path])
 		else:
 			if y.tmp_use_objects:
 				self.add_objects_from_tgen(y)
 
 		if getattr(y, 'export_includes', None):
-			self.includes.extend(y.to_incnodes(y.export_includes))
+			# self.includes may come from a global variable #2035
+			self.includes = self.includes + y.to_incnodes(y.export_includes)
 
 		if getattr(y, 'export_defines', None):
 			self.env.append_value('DEFINES', self.to_list(y.export_defines))
@@ -390,7 +447,7 @@ def get_uselib_vars(self):
 	:return: the *uselib* variables associated to the *features* attribute (see :py:attr:`waflib.Tools.ccroot.USELIB_VARS`)
 	:rtype: list of string
 	"""
-	_vars = set([])
+	_vars = set()
 	for x in self.features:
 		if x in USELIB_VARS:
 			_vars |= USELIB_VARS[x]
@@ -405,7 +462,7 @@ def propagate_uselib_vars(self):
 		def build(bld):
 			bld.env.AFLAGS_aaa = ['bar']
 			from waflib.Tools.ccroot import USELIB_VARS
-			USELIB_VARS['aaa'] = set('AFLAGS')
+			USELIB_VARS['aaa'] = ['AFLAGS']
 
 			tg = bld(features='aaa', aflags='test')
 
@@ -447,20 +504,20 @@ def apply_implib(self):
 		name = self.target.name
 	else:
 		name = os.path.split(self.target)[1]
-	implib = self.env['implib_PATTERN'] % name
+	implib = self.env.implib_PATTERN % name
 	implib = dll.parent.find_or_declare(implib)
-	self.env.append_value('LINKFLAGS', self.env['IMPLIB_ST'] % implib.bldpath())
+	self.env.append_value('LINKFLAGS', self.env.IMPLIB_ST % implib.bldpath())
 	self.link_task.outputs.append(implib)
 
 	if getattr(self, 'defs', None) and self.env.DEST_BINFMT == 'pe':
 		node = self.path.find_resource(self.defs)
 		if not node:
 			raise Errors.WafError('invalid def file %r' % self.defs)
-		if 'msvc' in (self.env.CC_NAME, self.env.CXX_NAME):
-			self.env.append_value('LINKFLAGS', '/def:%s' % node.path_from(self.bld.bldnode))
+		if self.env.def_PATTERN:
+			self.env.append_value('LINKFLAGS', self.env.def_PATTERN % node.path_from(self.get_cwd()))
 			self.link_task.dep_nodes.append(node)
 		else:
-			#gcc for windows takes *.def file a an input without any special flag
+			# gcc for windows takes *.def file as input without any special flag
 			self.link_task.inputs.append(node)
 
 	# where to put the import library
@@ -475,10 +532,11 @@ def apply_implib(self):
 			except AttributeError:
 				# else, put the library in BINDIR and the import library in LIBDIR
 				inst_to = '${IMPLIBDIR}'
-				self.install_task.dest = '${BINDIR}'
+				self.install_task.install_to = '${BINDIR}'
 				if not self.env.IMPLIBDIR:
 					self.env.IMPLIBDIR = self.env.LIBDIR
-		self.implib_install_task = self.bld.install_files(inst_to, implib, env=self.env, chmod=self.link_task.chmod, task=self.link_task)
+		self.implib_install_task = self.add_install_files(install_to=inst_to, install_from=implib,
+			chmod=self.link_task.chmod, task=self.link_task)
 
 # ============ the code above must not know anything about vnum processing on unix platforms =========
 
@@ -535,34 +593,34 @@ def build(bld):
 
 	# the following task is just to enable execution from the build dir :-/
 	if self.env.DEST_OS != 'openbsd':
-		outs = [node.parent.find_or_declare(name3)]
+		outs = [node.parent.make_node(name3)]
 		if name2 != name3:
-			outs.append(node.parent.find_or_declare(name2))
+			outs.append(node.parent.make_node(name2))
 		self.create_task('vnum', node, outs)
 
 	if getattr(self, 'install_task', None):
-		self.install_task.hasrun = Task.SKIP_ME
-		bld = self.bld
-		path = self.install_task.dest
+		self.install_task.hasrun = Task.SKIPPED
+		self.install_task.no_errcheck_out = True
+		path = self.install_task.install_to
 		if self.env.DEST_OS == 'openbsd':
 			libname = self.link_task.outputs[0].name
-			t1 = bld.install_as('%s%s%s' % (path, os.sep, libname), node, env=self.env, chmod=self.link_task.chmod)
+			t1 = self.add_install_as(install_to='%s/%s' % (path, libname), install_from=node, chmod=self.link_task.chmod)
 			self.vnum_install_task = (t1,)
 		else:
-			t1 = bld.install_as(path + os.sep + name3, node, env=self.env, chmod=self.link_task.chmod)
-			t3 = bld.symlink_as(path + os.sep + libname, name3)
+			t1 = self.add_install_as(install_to=path + os.sep + name3, install_from=node, chmod=self.link_task.chmod)
+			t3 = self.add_symlink_as(install_to=path + os.sep + libname, install_from=name3)
 			if name2 != name3:
-				t2 = bld.symlink_as(path + os.sep + name2, name3)
+				t2 = self.add_symlink_as(install_to=path + os.sep + name2, install_from=name3)
 				self.vnum_install_task = (t1, t2, t3)
 			else:
 				self.vnum_install_task = (t1, t3)
 
-	if '-dynamiclib' in self.env['LINKFLAGS']:
+	if '-dynamiclib' in self.env.LINKFLAGS:
 		# this requires after(propagate_uselib_vars)
 		try:
 			inst_to = self.install_path
 		except AttributeError:
-			inst_to = self.link_task.__class__.inst_to
+			inst_to = self.link_task.inst_to
 		if inst_to:
 			p = Utils.subst_vars(inst_to, self.env)
 			path = os.path.join(p, name2)
@@ -575,7 +633,6 @@ class vnum(Task.Task):
 	Create the symbolic links for a versioned shared library. Instances are created by :py:func:`waflib.Tools.ccroot.apply_vnum`
 	"""
 	color = 'CYAN'
-	quient = True
 	ext_in = ['.bin']
 	def keyword(self):
 		return 'Symlinking'
@@ -600,9 +657,6 @@ def runnable_status(self):
 		for t in self.run_after:
 			if not t.hasrun:
 				return Task.ASK_LATER
-
-		for x in self.outputs:
-			x.sig = Utils.h_file(x.abspath())
 		return Task.SKIP_ME
 
 class fake_stlib(stlink_task):
@@ -613,9 +667,6 @@ def runnable_status(self):
 		for t in self.run_after:
 			if not t.hasrun:
 				return Task.ASK_LATER
-
-		for x in self.outputs:
-			x.sig = Utils.h_file(x.abspath())
 		return Task.SKIP_ME
 
 @conf
@@ -658,7 +709,10 @@ def process_lib(self):
 		for y in names:
 			node = x.find_node(y)
 			if node:
-				node.sig = Utils.h_file(node.abspath())
+				try:
+					Utils.h_file(node.abspath())
+				except EnvironmentError:
+					raise ValueError('Could not read %r' % y)
 				break
 		else:
 			continue
diff --git a/waflib/Tools/clang.py b/waflib/Tools/clang.py
index 2259c5df..3828e391 100644
--- a/waflib/Tools/clang.py
+++ b/waflib/Tools/clang.py
@@ -12,7 +12,7 @@
 @conf
 def find_clang(conf):
 	"""
-	Find the program clang and execute it to ensure it really is clang
+	Finds the program clang and executes it to ensure it really is clang
 	"""
 	cc = conf.find_program('clang', var='CC')
 	conf.get_cc_version(cc, clang=True)
diff --git a/waflib/Tools/clangxx.py b/waflib/Tools/clangxx.py
index b539b287..152013ce 100644
--- a/waflib/Tools/clangxx.py
+++ b/waflib/Tools/clangxx.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy 2009-2010 (ita)
+# Thomas Nagy 2009-2018 (ita)
 
 """
 Detect the Clang++ C++ compiler
@@ -12,7 +12,7 @@
 @conf
 def find_clangxx(conf):
 	"""
-	Find the program clang++, and execute it to ensure it really is clang++
+	Finds the program clang++, and executes it to ensure it really is clang++
 	"""
 	cxx = conf.find_program('clang++', var='CXX')
 	conf.get_cc_version(cxx, clang=True)
diff --git a/waflib/Tools/compiler_c.py b/waflib/Tools/compiler_c.py
index 7d4e22ca..2dba3f82 100644
--- a/waflib/Tools/compiler_c.py
+++ b/waflib/Tools/compiler_c.py
@@ -47,10 +47,10 @@ def build(bld):
 'osf1V':  ['gcc'],
 'gnu':    ['gcc', 'clang'],
 'java':   ['gcc', 'msvc', 'clang', 'icc'],
-'default':['gcc', 'clang'],
+'default':['clang', 'gcc'],
 }
 """
-Dict mapping the platform names to Waf tools finding specific C compilers::
+Dict mapping platform names to Waf tools finding specific C compilers::
 
 	from waflib.Tools.compiler_c import c_compiler
 	c_compiler['linux'] = ['gcc', 'icc', 'suncc']
@@ -63,10 +63,14 @@ def default_compilers():
 
 def configure(conf):
 	"""
-	Try to find a suitable C compiler or raise a :py:class:`waflib.Errors.ConfigurationError`.
+	Detects a suitable C compiler
+
+	:raises: :py:class:`waflib.Errors.ConfigurationError` when no suitable compiler is found
 	"""
-	try: test_for_compiler = conf.options.check_c_compiler or default_compilers()
-	except AttributeError: conf.fatal("Add options(opt): opt.load('compiler_c')")
+	try:
+		test_for_compiler = conf.options.check_c_compiler or default_compilers()
+	except AttributeError:
+		conf.fatal("Add options(opt): opt.load('compiler_c')")
 
 	for compiler in re.split('[ ,]+', test_for_compiler):
 		conf.env.stash()
@@ -76,19 +80,21 @@ def configure(conf):
 		except conf.errors.ConfigurationError as e:
 			conf.env.revert()
 			conf.end_msg(False)
-			debug('compiler_c: %r' % e)
+			debug('compiler_c: %r', e)
 		else:
-			if conf.env['CC']:
+			if conf.env.CC:
 				conf.end_msg(conf.env.get_flat('CC'))
-				conf.env['COMPILER_CC'] = compiler
+				conf.env.COMPILER_CC = compiler
+				conf.env.commit()
 				break
+			conf.env.revert()
 			conf.end_msg(False)
 	else:
 		conf.fatal('could not configure a C compiler!')
 
 def options(opt):
 	"""
-	Restrict the compiler detection from the command-line::
+	This is how to provide compiler preferences on the command-line::
 
 		$ waf configure --check-c-compiler=gcc
 	"""
diff --git a/waflib/Tools/compiler_cxx.py b/waflib/Tools/compiler_cxx.py
index 6f0ea9ea..1af65a22 100644
--- a/waflib/Tools/compiler_cxx.py
+++ b/waflib/Tools/compiler_cxx.py
@@ -48,7 +48,7 @@ def build(bld):
 'osf1V':  ['g++'],
 'gnu':    ['g++', 'clang++'],
 'java':   ['g++', 'msvc', 'clang++', 'icpc'],
-'default': ['g++', 'clang++']
+'default': ['clang++', 'g++']
 }
 """
 Dict mapping the platform names to Waf tools finding specific C++ compilers::
@@ -64,10 +64,14 @@ def default_compilers():
 
 def configure(conf):
 	"""
-	Try to find a suitable C++ compiler or raise a :py:class:`waflib.Errors.ConfigurationError`.
+	Detects a suitable C++ compiler
+
+	:raises: :py:class:`waflib.Errors.ConfigurationError` when no suitable compiler is found
 	"""
-	try: test_for_compiler = conf.options.check_cxx_compiler or default_compilers()
-	except AttributeError: conf.fatal("Add options(opt): opt.load('compiler_cxx')")
+	try:
+		test_for_compiler = conf.options.check_cxx_compiler or default_compilers()
+	except AttributeError:
+		conf.fatal("Add options(opt): opt.load('compiler_cxx')")
 
 	for compiler in re.split('[ ,]+', test_for_compiler):
 		conf.env.stash()
@@ -77,19 +81,21 @@ def configure(conf):
 		except conf.errors.ConfigurationError as e:
 			conf.env.revert()
 			conf.end_msg(False)
-			debug('compiler_cxx: %r' % e)
+			debug('compiler_cxx: %r', e)
 		else:
-			if conf.env['CXX']:
+			if conf.env.CXX:
 				conf.end_msg(conf.env.get_flat('CXX'))
-				conf.env['COMPILER_CXX'] = compiler
+				conf.env.COMPILER_CXX = compiler
+				conf.env.commit()
 				break
+			conf.env.revert()
 			conf.end_msg(False)
 	else:
 		conf.fatal('could not configure a C++ compiler!')
 
 def options(opt):
 	"""
-	Restrict the compiler detection from the command-line::
+	This is how to provide compiler preferences on the command-line::
 
 		$ waf configure --check-cxx-compiler=gxx
 	"""
diff --git a/waflib/Tools/compiler_d.py b/waflib/Tools/compiler_d.py
new file mode 100644
index 00000000..43bb1f64
--- /dev/null
+++ b/waflib/Tools/compiler_d.py
@@ -0,0 +1,85 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Carlos Rafael Giani, 2007 (dv)
+# Thomas Nagy, 2016-2018 (ita)
+
+"""
+Try to detect a D compiler from the list of supported compilers::
+
+	def options(opt):
+		opt.load('compiler_d')
+	def configure(cnf):
+		cnf.load('compiler_d')
+	def build(bld):
+		bld.program(source='main.d', target='app')
+
+Only three D compilers are really present at the moment:
+
+* gdc
+* dmd, the ldc compiler having a very similar command-line interface
+* ldc2
+"""
+
+import re
+from waflib import Utils, Logs
+
+d_compiler = {
+'default' : ['gdc', 'dmd', 'ldc2']
+}
+"""
+Dict mapping the platform names to lists of names of D compilers to try, in order of preference::
+
+	from waflib.Tools.compiler_d import d_compiler
+	d_compiler['default'] = ['gdc', 'dmd', 'ldc2']
+"""
+
+def default_compilers():
+	build_platform = Utils.unversioned_sys_platform()
+	possible_compiler_list = d_compiler.get(build_platform, d_compiler['default'])
+	return ' '.join(possible_compiler_list)
+
+def configure(conf):
+	"""
+	Detects a suitable D compiler
+
+	:raises: :py:class:`waflib.Errors.ConfigurationError` when no suitable compiler is found
+	"""
+	try:
+		test_for_compiler = conf.options.check_d_compiler or default_compilers()
+	except AttributeError:
+		conf.fatal("Add options(opt): opt.load('compiler_d')")
+
+	for compiler in re.split('[ ,]+', test_for_compiler):
+		conf.env.stash()
+		conf.start_msg('Checking for %r (D compiler)' % compiler)
+		try:
+			conf.load(compiler)
+		except conf.errors.ConfigurationError as e:
+			conf.env.revert()
+			conf.end_msg(False)
+			Logs.debug('compiler_d: %r', e)
+		else:
+			if conf.env.D:
+				conf.end_msg(conf.env.get_flat('D'))
+				conf.env.COMPILER_D = compiler
+				conf.env.commit()
+				break
+			conf.env.revert()
+			conf.end_msg(False)
+	else:
+		conf.fatal('could not configure a D compiler!')
+
+def options(opt):
+	"""
+	This is how to provide compiler preferences on the command-line::
+
+		$ waf configure --check-d-compiler=dmd
+	"""
+	test_for_compiler = default_compilers()
+	d_compiler_opts = opt.add_option_group('Configuration options')
+	d_compiler_opts.add_option('--check-d-compiler', default=None,
+		help='list of D compilers to try [%s]' % test_for_compiler, dest='check_d_compiler')
+
+	for x in test_for_compiler.split():
+		opt.load('%s' % x)
+
diff --git a/waflib/Tools/compiler_fc.py b/waflib/Tools/compiler_fc.py
new file mode 100644
index 00000000..96b58e70
--- /dev/null
+++ b/waflib/Tools/compiler_fc.py
@@ -0,0 +1,73 @@
+#!/usr/bin/env python
+# encoding: utf-8
+
+import re
+from waflib import Utils, Logs
+from waflib.Tools import fc
+
+fc_compiler = {
+	'win32'  : ['gfortran','ifort'],
+	'darwin' : ['gfortran', 'g95', 'ifort'],
+	'linux'  : ['gfortran', 'g95', 'ifort'],
+	'java'   : ['gfortran', 'g95', 'ifort'],
+	'default': ['gfortran'],
+	'aix'    : ['gfortran']
+}
+"""
+Dict mapping the platform names to lists of names of Fortran compilers to try, in order of preference::
+
+	from waflib.Tools.compiler_c import c_compiler
+	c_compiler['linux'] = ['gfortran', 'g95', 'ifort']
+"""
+
+def default_compilers():
+	build_platform = Utils.unversioned_sys_platform()
+	possible_compiler_list = fc_compiler.get(build_platform, fc_compiler['default'])
+	return ' '.join(possible_compiler_list)
+
+def configure(conf):
+	"""
+	Detects a suitable Fortran compiler
+
+	:raises: :py:class:`waflib.Errors.ConfigurationError` when no suitable compiler is found
+	"""
+	try:
+		test_for_compiler = conf.options.check_fortran_compiler or default_compilers()
+	except AttributeError:
+		conf.fatal("Add options(opt): opt.load('compiler_fc')")
+	for compiler in re.split('[ ,]+', test_for_compiler):
+		conf.env.stash()
+		conf.start_msg('Checking for %r (Fortran compiler)' % compiler)
+		try:
+			conf.load(compiler)
+		except conf.errors.ConfigurationError as e:
+			conf.env.revert()
+			conf.end_msg(False)
+			Logs.debug('compiler_fortran: %r', e)
+		else:
+			if conf.env.FC:
+				conf.end_msg(conf.env.get_flat('FC'))
+				conf.env.COMPILER_FORTRAN = compiler
+				conf.env.commit()
+				break
+			conf.env.revert()
+			conf.end_msg(False)
+	else:
+		conf.fatal('could not configure a Fortran compiler!')
+
+def options(opt):
+	"""
+	This is how to provide compiler preferences on the command-line::
+
+		$ waf configure --check-fortran-compiler=ifort
+	"""
+	test_for_compiler = default_compilers()
+	opt.load_special_tools('fc_*.py')
+	fortran_compiler_opts = opt.add_option_group('Configuration options')
+	fortran_compiler_opts.add_option('--check-fortran-compiler', default=None,
+			help='list of Fortran compiler to try [%s]' % test_for_compiler,
+		dest="check_fortran_compiler")
+
+	for x in test_for_compiler.split():
+		opt.load('%s' % x)
+
diff --git a/waflib/Tools/cs.py b/waflib/Tools/cs.py
new file mode 100644
index 00000000..aecca6da
--- /dev/null
+++ b/waflib/Tools/cs.py
@@ -0,0 +1,211 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2018 (ita)
+
+"""
+C# support. A simple example::
+
+	def configure(conf):
+		conf.load('cs')
+	def build(bld):
+		bld(features='cs', source='main.cs', gen='foo')
+
+Note that the configuration may compile C# snippets::
+
+	FRAG = '''
+	namespace Moo {
+		public class Test { public static int Main(string[] args) { return 0; } }
+	}'''
+	def configure(conf):
+		conf.check(features='cs', fragment=FRAG, compile_filename='test.cs', gen='test.exe',
+			bintype='exe', csflags=['-pkg:gtk-sharp-2.0'], msg='Checking for Gtksharp support')
+"""
+
+from waflib import Utils, Task, Options, Errors
+from waflib.TaskGen import before_method, after_method, feature
+from waflib.Tools import ccroot
+from waflib.Configure import conf
+
+ccroot.USELIB_VARS['cs'] = set(['CSFLAGS', 'ASSEMBLIES', 'RESOURCES'])
+ccroot.lib_patterns['csshlib'] = ['%s']
+
+@feature('cs')
+@before_method('process_source')
+def apply_cs(self):
+	"""
+	Create a C# task bound to the attribute *cs_task*. There can be only one C# task by task generator.
+	"""
+	cs_nodes = []
+	no_nodes = []
+	for x in self.to_nodes(self.source):
+		if x.name.endswith('.cs'):
+			cs_nodes.append(x)
+		else:
+			no_nodes.append(x)
+	self.source = no_nodes
+
+	bintype = getattr(self, 'bintype', self.gen.endswith('.dll') and 'library' or 'exe')
+	self.cs_task = tsk = self.create_task('mcs', cs_nodes, self.path.find_or_declare(self.gen))
+	tsk.env.CSTYPE = '/target:%s' % bintype
+	tsk.env.OUT = '/out:%s' % tsk.outputs[0].abspath()
+	self.env.append_value('CSFLAGS', '/platform:%s' % getattr(self, 'platform', 'anycpu'))
+
+	inst_to = getattr(self, 'install_path', bintype=='exe' and '${BINDIR}' or '${LIBDIR}')
+	if inst_to:
+		# note: we are making a copy, so the files added to cs_task.outputs won't be installed automatically
+		mod = getattr(self, 'chmod', bintype=='exe' and Utils.O755 or Utils.O644)
+		self.install_task = self.add_install_files(install_to=inst_to, install_from=self.cs_task.outputs[:], chmod=mod)
+
+@feature('cs')
+@after_method('apply_cs')
+def use_cs(self):
+	"""
+	C# applications honor the **use** keyword::
+
+		def build(bld):
+			bld(features='cs', source='My.cs', bintype='library', gen='my.dll', name='mylib')
+			bld(features='cs', source='Hi.cs', includes='.', bintype='exe', gen='hi.exe', use='mylib', name='hi')
+	"""
+	names = self.to_list(getattr(self, 'use', []))
+	get = self.bld.get_tgen_by_name
+	for x in names:
+		try:
+			y = get(x)
+		except Errors.WafError:
+			self.env.append_value('CSFLAGS', '/reference:%s' % x)
+			continue
+		y.post()
+
+		tsk = getattr(y, 'cs_task', None) or getattr(y, 'link_task', None)
+		if not tsk:
+			self.bld.fatal('cs task has no link task for use %r' % self)
+		self.cs_task.dep_nodes.extend(tsk.outputs) # dependency
+		self.cs_task.set_run_after(tsk) # order (redundant, the order is inferred from the nodes inputs/outputs)
+		self.env.append_value('CSFLAGS', '/reference:%s' % tsk.outputs[0].abspath())
+
+@feature('cs')
+@after_method('apply_cs', 'use_cs')
+def debug_cs(self):
+	"""
+	The C# targets may create .mdb or .pdb files::
+
+		def build(bld):
+			bld(features='cs', source='My.cs', bintype='library', gen='my.dll', csdebug='full')
+			# csdebug is a value in (True, 'full', 'pdbonly')
+	"""
+	csdebug = getattr(self, 'csdebug', self.env.CSDEBUG)
+	if not csdebug:
+		return
+
+	node = self.cs_task.outputs[0]
+	if self.env.CS_NAME == 'mono':
+		out = node.parent.find_or_declare(node.name + '.mdb')
+	else:
+		out = node.change_ext('.pdb')
+	self.cs_task.outputs.append(out)
+
+	if getattr(self, 'install_task', None):
+		self.pdb_install_task = self.add_install_files(
+			install_to=self.install_task.install_to, install_from=out)
+
+	if csdebug == 'pdbonly':
+		val = ['/debug+', '/debug:pdbonly']
+	elif csdebug == 'full':
+		val = ['/debug+', '/debug:full']
+	else:
+		val = ['/debug-']
+	self.env.append_value('CSFLAGS', val)
+
+@feature('cs')
+@after_method('debug_cs')
+def doc_cs(self):
+	"""
+	The C# targets may create .xml documentation files::
+
+		def build(bld):
+			bld(features='cs', source='My.cs', bintype='library', gen='my.dll', csdoc=True)
+			# csdoc is a boolean value
+	"""
+	csdoc = getattr(self, 'csdoc', self.env.CSDOC)
+	if not csdoc:
+		return
+
+	node = self.cs_task.outputs[0]
+	out = node.change_ext('.xml')
+	self.cs_task.outputs.append(out)
+
+	if getattr(self, 'install_task', None):
+		self.doc_install_task = self.add_install_files(
+			install_to=self.install_task.install_to, install_from=out)
+
+	self.env.append_value('CSFLAGS', '/doc:%s' % out.abspath())
+
+class mcs(Task.Task):
+	"""
+	Compile C# files
+	"""
+	color   = 'YELLOW'
+	run_str = '${MCS} ${CSTYPE} ${CSFLAGS} ${ASS_ST:ASSEMBLIES} ${RES_ST:RESOURCES} ${OUT} ${SRC}'
+
+	def split_argfile(self, cmd):
+		inline = [cmd[0]]
+		infile = []
+		for x in cmd[1:]:
+			# csc doesn't want /noconfig in @file
+			if x.lower() == '/noconfig':
+				inline.append(x)
+			else:
+				infile.append(self.quote_flag(x))
+		return (inline, infile)
+
+def configure(conf):
+	"""
+	Find a C# compiler, set the variable MCS for the compiler and CS_NAME (mono or csc)
+	"""
+	csc = getattr(Options.options, 'cscbinary', None)
+	if csc:
+		conf.env.MCS = csc
+	conf.find_program(['csc', 'mcs', 'gmcs'], var='MCS')
+	conf.env.ASS_ST = '/r:%s'
+	conf.env.RES_ST = '/resource:%s'
+
+	conf.env.CS_NAME = 'csc'
+	if str(conf.env.MCS).lower().find('mcs') > -1:
+		conf.env.CS_NAME = 'mono'
+
+def options(opt):
+	"""
+	Add a command-line option for the configuration::
+
+		$ waf configure --with-csc-binary=/foo/bar/mcs
+	"""
+	opt.add_option('--with-csc-binary', type='string', dest='cscbinary')
+
+class fake_csshlib(Task.Task):
+	"""
+	Task used for reading a foreign .net assembly and adding the dependency on it
+	"""
+	color   = 'YELLOW'
+	inst_to = None
+
+	def runnable_status(self):
+		return Task.SKIP_ME
+
+@conf
+def read_csshlib(self, name, paths=[]):
+	"""
+	Read a foreign .net assembly for the *use* system::
+
+		def build(bld):
+			bld.read_csshlib('ManagedLibrary.dll', paths=[bld.env.mylibrarypath])
+			bld(features='cs', source='Hi.cs', bintype='exe', gen='hi.exe', use='ManagedLibrary.dll')
+
+	:param name: Name of the library
+	:type name: string
+	:param paths: Folders in which the library may be found
+	:type paths: list of string
+	:return: A task generator having the feature *fake_lib* which will call :py:func:`waflib.Tools.ccroot.process_lib`
+	:rtype: :py:class:`waflib.TaskGen.task_gen`
+	"""
+	return self(name=name, features='fake_lib', lib_paths=paths, lib_type='csshlib')
+
diff --git a/waflib/Tools/cxx.py b/waflib/Tools/cxx.py
index 311c1e52..194fad74 100644
--- a/waflib/Tools/cxx.py
+++ b/waflib/Tools/cxx.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 "Base for c++ programs and libraries"
 
@@ -10,31 +10,31 @@
 
 @TaskGen.extension('.cpp','.cc','.cxx','.C','.c++')
 def cxx_hook(self, node):
-	"Bind the c++ file extensions to the creation of a :py:class:`waflib.Tools.cxx.cxx` instance"
+	"Binds c++ file extensions to create :py:class:`waflib.Tools.cxx.cxx` instances"
 	return self.create_compiled_task('cxx', node)
 
 if not '.c' in TaskGen.task_gen.mappings:
 	TaskGen.task_gen.mappings['.c'] = TaskGen.task_gen.mappings['.cpp']
 
 class cxx(Task.Task):
-	"Compile C++ files into object files"
-	run_str = '${CXX} ${ARCH_ST:ARCH} ${CXXFLAGS} ${CPPFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} ${CXX_SRC_F}${SRC} ${CXX_TGT_F}${TGT[0].abspath()}'
+	"Compiles C++ files into object files"
+	run_str = '${CXX} ${ARCH_ST:ARCH} ${CXXFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} ${CXX_SRC_F}${SRC} ${CXX_TGT_F}${TGT[0].abspath()} ${CPPFLAGS}'
 	vars    = ['CXXDEPS'] # unused variable to depend on, just in case
 	ext_in  = ['.h'] # set the build order easily by using ext_out=['.h']
 	scan    = c_preproc.scan
 
 class cxxprogram(link_task):
-	"Link object files into a c++ program"
+	"Links object files into c++ programs"
 	run_str = '${LINK_CXX} ${LINKFLAGS} ${CXXLNK_SRC_F}${SRC} ${CXXLNK_TGT_F}${TGT[0].abspath()} ${RPATH_ST:RPATH} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${FRAMEWORK_ST:FRAMEWORK} ${ARCH_ST:ARCH} ${STLIB_MARKER} ${STLIBPATH_ST:STLIBPATH} ${STLIB_ST:STLIB} ${SHLIB_MARKER} ${LIBPATH_ST:LIBPATH} ${LIB_ST:LIB} ${LDFLAGS}'
 	vars    = ['LINKDEPS']
 	ext_out = ['.bin']
 	inst_to = '${BINDIR}'
 
 class cxxshlib(cxxprogram):
-	"Link object files into a c++ shared library"
+	"Links object files into c++ shared libraries"
 	inst_to = '${LIBDIR}'
 
 class cxxstlib(stlink_task):
-	"Link object files into a c++ static library"
+	"Links object files into c++ static libraries"
 	pass # do not remove
 
diff --git a/waflib/Tools/d.py b/waflib/Tools/d.py
new file mode 100644
index 00000000..e4cf73bb
--- /dev/null
+++ b/waflib/Tools/d.py
@@ -0,0 +1,97 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Carlos Rafael Giani, 2007 (dv)
+# Thomas Nagy, 2007-2018 (ita)
+
+from waflib import Utils, Task, Errors
+from waflib.TaskGen import taskgen_method, feature, extension
+from waflib.Tools import d_scan, d_config
+from waflib.Tools.ccroot import link_task, stlink_task
+
+class d(Task.Task):
+	"Compile a d file into an object file"
+	color   = 'GREEN'
+	run_str = '${D} ${DFLAGS} ${DINC_ST:INCPATHS} ${D_SRC_F:SRC} ${D_TGT_F:TGT}'
+	scan    = d_scan.scan
+
+class d_with_header(d):
+	"Compile a d file and generate a header"
+	run_str = '${D} ${DFLAGS} ${DINC_ST:INCPATHS} ${D_HDR_F:tgt.outputs[1].bldpath()} ${D_SRC_F:SRC} ${D_TGT_F:tgt.outputs[0].bldpath()}'
+
+class d_header(Task.Task):
+	"Compile d headers"
+	color   = 'BLUE'
+	run_str = '${D} ${D_HEADER} ${SRC}'
+
+class dprogram(link_task):
+	"Link object files into a d program"
+	run_str = '${D_LINKER} ${LINKFLAGS} ${DLNK_SRC_F}${SRC} ${DLNK_TGT_F:TGT} ${RPATH_ST:RPATH} ${DSTLIB_MARKER} ${DSTLIBPATH_ST:STLIBPATH} ${DSTLIB_ST:STLIB} ${DSHLIB_MARKER} ${DLIBPATH_ST:LIBPATH} ${DSHLIB_ST:LIB}'
+	inst_to = '${BINDIR}'
+
+class dshlib(dprogram):
+	"Link object files into a d shared library"
+	inst_to = '${LIBDIR}'
+
+class dstlib(stlink_task):
+	"Link object files into a d static library"
+	pass # do not remove
+
+@extension('.d', '.di', '.D')
+def d_hook(self, node):
+	"""
+	Compile *D* files. To get .di files as well as .o files, set the following::
+
+		def build(bld):
+			bld.program(source='foo.d', target='app', generate_headers=True)
+
+	"""
+	ext = Utils.destos_to_binfmt(self.env.DEST_OS) == 'pe' and 'obj' or 'o'
+	out = '%s.%d.%s' % (node.name, self.idx, ext)
+	def create_compiled_task(self, name, node):
+		task = self.create_task(name, node, node.parent.find_or_declare(out))
+		try:
+			self.compiled_tasks.append(task)
+		except AttributeError:
+			self.compiled_tasks = [task]
+		return task
+
+	if getattr(self, 'generate_headers', None):
+		tsk = create_compiled_task(self, 'd_with_header', node)
+		tsk.outputs.append(node.change_ext(self.env.DHEADER_ext))
+	else:
+		tsk = create_compiled_task(self, 'd', node)
+	return tsk
+
+@taskgen_method
+def generate_header(self, filename):
+	"""
+	See feature request #104::
+
+		def build(bld):
+			tg = bld.program(source='foo.d', target='app')
+			tg.generate_header('blah.d')
+			# is equivalent to:
+			#tg = bld.program(source='foo.d', target='app', header_lst='blah.d')
+
+	:param filename: header to create
+	:type filename: string
+	"""
+	try:
+		self.header_lst.append([filename, self.install_path])
+	except AttributeError:
+		self.header_lst = [[filename, self.install_path]]
+
+@feature('d')
+def process_header(self):
+	"""
+	Process the attribute 'header_lst' to create the d header compilation tasks::
+
+		def build(bld):
+			bld.program(source='foo.d', target='app', header_lst='blah.d')
+	"""
+	for i in getattr(self, 'header_lst', []):
+		node = self.path.find_resource(i[0])
+		if not node:
+			raise Errors.WafError('file %r not found on d obj' % i[0])
+		self.create_task('d_header', node, node.change_ext('.di'))
+
diff --git a/waflib/Tools/d_config.py b/waflib/Tools/d_config.py
new file mode 100644
index 00000000..66375565
--- /dev/null
+++ b/waflib/Tools/d_config.py
@@ -0,0 +1,64 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2016-2018 (ita)
+
+from waflib import Utils
+from waflib.Configure import conf
+
+@conf
+def d_platform_flags(self):
+	"""
+	Sets the extensions dll/so for d programs and libraries
+	"""
+	v = self.env
+	if not v.DEST_OS:
+		v.DEST_OS = Utils.unversioned_sys_platform()
+	binfmt = Utils.destos_to_binfmt(self.env.DEST_OS)
+	if binfmt == 'pe':
+		v.dprogram_PATTERN = '%s.exe'
+		v.dshlib_PATTERN   = 'lib%s.dll'
+		v.dstlib_PATTERN   = 'lib%s.a'
+	elif binfmt == 'mac-o':
+		v.dprogram_PATTERN = '%s'
+		v.dshlib_PATTERN   = 'lib%s.dylib'
+		v.dstlib_PATTERN   = 'lib%s.a'
+	else:
+		v.dprogram_PATTERN = '%s'
+		v.dshlib_PATTERN   = 'lib%s.so'
+		v.dstlib_PATTERN   = 'lib%s.a'
+
+DLIB = '''
+version(D_Version2) {
+	import std.stdio;
+	int main() {
+		writefln("phobos2");
+		return 0;
+	}
+} else {
+	version(Tango) {
+		import tango.stdc.stdio;
+		int main() {
+			printf("tango");
+			return 0;
+		}
+	} else {
+		import std.stdio;
+		int main() {
+			writefln("phobos1");
+			return 0;
+		}
+	}
+}
+'''
+"""Detection string for the D standard library"""
+
+@conf
+def check_dlibrary(self, execute=True):
+	"""
+	Detects the kind of standard library that comes with the compiler,
+	and sets conf.env.DLIBRARY to tango, phobos1 or phobos2
+	"""
+	ret = self.check_cc(features='d dprogram', fragment=DLIB, compile_filename='test.d', execute=execute, define_ret=True)
+	if execute:
+		self.env.DLIBRARY = ret.strip()
+
diff --git a/waflib/Tools/d_scan.py b/waflib/Tools/d_scan.py
new file mode 100644
index 00000000..14c6c313
--- /dev/null
+++ b/waflib/Tools/d_scan.py
@@ -0,0 +1,211 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2016-2018 (ita)
+
+"""
+Provide a scanner for finding dependencies on d files
+"""
+
+import re
+from waflib import Utils
+
+def filter_comments(filename):
+	"""
+	:param filename: d file name
+	:type filename: string
+	:rtype: list
+	:return: a list of characters
+	"""
+	txt = Utils.readf(filename)
+	i = 0
+	buf = []
+	max = len(txt)
+	begin = 0
+	while i < max:
+		c = txt[i]
+		if c == '"' or c == "'":  # skip a string or character literal
+			buf.append(txt[begin:i])
+			delim = c
+			i += 1
+			while i < max:
+				c = txt[i]
+				if c == delim:
+					break
+				elif c == '\\':  # skip the character following backslash
+					i += 1
+				i += 1
+			i += 1
+			begin = i
+		elif c == '/':  # try to replace a comment with whitespace
+			buf.append(txt[begin:i])
+			i += 1
+			if i == max:
+				break
+			c = txt[i]
+			if c == '+':  # eat nesting /+ +/ comment
+				i += 1
+				nesting = 1
+				c = None
+				while i < max:
+					prev = c
+					c = txt[i]
+					if prev == '/' and c == '+':
+						nesting += 1
+						c = None
+					elif prev == '+' and c == '/':
+						nesting -= 1
+						if nesting == 0:
+							break
+						c = None
+					i += 1
+			elif c == '*':  # eat /* */ comment
+				i += 1
+				c = None
+				while i < max:
+					prev = c
+					c = txt[i]
+					if prev == '*' and c == '/':
+						break
+					i += 1
+			elif c == '/':  # eat // comment
+				i += 1
+				while i < max and txt[i] != '\n':
+					i += 1
+			else:  # no comment
+				begin = i - 1
+				continue
+			i += 1
+			begin = i
+			buf.append(' ')
+		else:
+			i += 1
+	buf.append(txt[begin:])
+	return buf
+
+class d_parser(object):
+	"""
+	Parser for d files
+	"""
+	def __init__(self, env, incpaths):
+		#self.code = ''
+		#self.module = ''
+		#self.imports = []
+
+		self.allnames = []
+
+		self.re_module = re.compile("module\s+([^;]+)")
+		self.re_import = re.compile("import\s+([^;]+)")
+		self.re_import_bindings = re.compile("([^:]+):(.*)")
+		self.re_import_alias = re.compile("[^=]+=(.+)")
+
+		self.env = env
+
+		self.nodes = []
+		self.names = []
+
+		self.incpaths = incpaths
+
+	def tryfind(self, filename):
+		"""
+		Search file a file matching an module/import directive
+
+		:param filename: file to read
+		:type filename: string
+		"""
+		found = 0
+		for n in self.incpaths:
+			found = n.find_resource(filename.replace('.', '/') + '.d')
+			if found:
+				self.nodes.append(found)
+				self.waiting.append(found)
+				break
+		if not found:
+			if not filename in self.names:
+				self.names.append(filename)
+
+	def get_strings(self, code):
+		"""
+		:param code: d code to parse
+		:type code: string
+		:return: the modules that the code uses
+		:rtype: a list of match objects
+		"""
+		#self.imports = []
+		self.module = ''
+		lst = []
+
+		# get the module name (if present)
+
+		mod_name = self.re_module.search(code)
+		if mod_name:
+			self.module = re.sub('\s+', '', mod_name.group(1)) # strip all whitespaces
+
+		# go through the code, have a look at all import occurrences
+
+		# first, lets look at anything beginning with "import" and ending with ";"
+		import_iterator = self.re_import.finditer(code)
+		if import_iterator:
+			for import_match in import_iterator:
+				import_match_str = re.sub('\s+', '', import_match.group(1)) # strip all whitespaces
+
+				# does this end with an import bindings declaration?
+				# (import bindings always terminate the list of imports)
+				bindings_match = self.re_import_bindings.match(import_match_str)
+				if bindings_match:
+					import_match_str = bindings_match.group(1)
+					# if so, extract the part before the ":" (since the module declaration(s) is/are located there)
+
+				# split the matching string into a bunch of strings, separated by a comma
+				matches = import_match_str.split(',')
+
+				for match in matches:
+					alias_match = self.re_import_alias.match(match)
+					if alias_match:
+						# is this an alias declaration? (alias = module name) if so, extract the module name
+						match = alias_match.group(1)
+
+					lst.append(match)
+		return lst
+
+	def start(self, node):
+		"""
+		The parsing starts here
+
+		:param node: input file
+		:type node: :py:class:`waflib.Node.Node`
+		"""
+		self.waiting = [node]
+		# while the stack is not empty, add the dependencies
+		while self.waiting:
+			nd = self.waiting.pop(0)
+			self.iter(nd)
+
+	def iter(self, node):
+		"""
+		Find all the modules that a file depends on, uses :py:meth:`waflib.Tools.d_scan.d_parser.tryfind` to process dependent files
+
+		:param node: input file
+		:type node: :py:class:`waflib.Node.Node`
+		"""
+		path = node.abspath() # obtain the absolute path
+		code = "".join(filter_comments(path)) # read the file and filter the comments
+		names = self.get_strings(code) # obtain the import strings
+		for x in names:
+			# optimization
+			if x in self.allnames:
+				continue
+			self.allnames.append(x)
+
+			# for each name, see if it is like a node or not
+			self.tryfind(x)
+
+def scan(self):
+	"look for .d/.di used by a d file"
+	env = self.env
+	gruik = d_parser(env, self.generator.includes_nodes)
+	node = self.inputs[0]
+	gruik.start(node)
+	nodes = gruik.nodes
+	names = gruik.names
+	return (nodes, names)
+
diff --git a/waflib/Tools/dbus.py b/waflib/Tools/dbus.py
new file mode 100644
index 00000000..d520f1c0
--- /dev/null
+++ b/waflib/Tools/dbus.py
@@ -0,0 +1,70 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Ali Sabil, 2007
+
+"""
+Compiles dbus files with **dbus-binding-tool**
+
+Typical usage::
+
+	def options(opt):
+		opt.load('compiler_c dbus')
+	def configure(conf):
+		conf.load('compiler_c dbus')
+	def build(bld):
+		tg = bld.program(
+			includes = '.',
+			source = bld.path.ant_glob('*.c'),
+			target = 'gnome-hello')
+		tg.add_dbus_file('test.xml', 'test_prefix', 'glib-server')
+"""
+
+from waflib import Task, Errors
+from waflib.TaskGen import taskgen_method, before_method
+
+@taskgen_method
+def add_dbus_file(self, filename, prefix, mode):
+	"""
+	Adds a dbus file to the list of dbus files to process. Store them in the attribute *dbus_lst*.
+
+	:param filename: xml file to compile
+	:type filename: string
+	:param prefix: dbus binding tool prefix (--prefix=prefix)
+	:type prefix: string
+	:param mode: dbus binding tool mode (--mode=mode)
+	:type mode: string
+	"""
+	if not hasattr(self, 'dbus_lst'):
+		self.dbus_lst = []
+	if not 'process_dbus' in self.meths:
+		self.meths.append('process_dbus')
+	self.dbus_lst.append([filename, prefix, mode])
+
+@before_method('process_source')
+def process_dbus(self):
+	"""
+	Processes the dbus files stored in the attribute *dbus_lst* to create :py:class:`waflib.Tools.dbus.dbus_binding_tool` instances.
+	"""
+	for filename, prefix, mode in getattr(self, 'dbus_lst', []):
+		node = self.path.find_resource(filename)
+		if not node:
+			raise Errors.WafError('file not found ' + filename)
+		tsk = self.create_task('dbus_binding_tool', node, node.change_ext('.h'))
+		tsk.env.DBUS_BINDING_TOOL_PREFIX = prefix
+		tsk.env.DBUS_BINDING_TOOL_MODE   = mode
+
+class dbus_binding_tool(Task.Task):
+	"""
+	Compiles a dbus file
+	"""
+	color   = 'BLUE'
+	ext_out = ['.h']
+	run_str = '${DBUS_BINDING_TOOL} --prefix=${DBUS_BINDING_TOOL_PREFIX} --mode=${DBUS_BINDING_TOOL_MODE} --output=${TGT} ${SRC}'
+	shell   = True # temporary workaround for #795
+
+def configure(conf):
+	"""
+	Detects the program dbus-binding-tool and sets ``conf.env.DBUS_BINDING_TOOL``
+	"""
+	conf.find_program('dbus-binding-tool', var='DBUS_BINDING_TOOL')
+
diff --git a/waflib/Tools/dmd.py b/waflib/Tools/dmd.py
new file mode 100644
index 00000000..8917ca1b
--- /dev/null
+++ b/waflib/Tools/dmd.py
@@ -0,0 +1,80 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Carlos Rafael Giani, 2007 (dv)
+# Thomas Nagy, 2008-2018 (ita)
+
+import sys
+from waflib.Tools import ar, d
+from waflib.Configure import conf
+
+@conf
+def find_dmd(conf):
+	"""
+	Finds the program *dmd*, *dmd2*, or *ldc* and set the variable *D*
+	"""
+	conf.find_program(['dmd', 'dmd2', 'ldc'], var='D')
+
+	# make sure that we're dealing with dmd1, dmd2, or ldc(1)
+	out = conf.cmd_and_log(conf.env.D + ['--help'])
+	if out.find("D Compiler v") == -1:
+		out = conf.cmd_and_log(conf.env.D + ['-version'])
+		if out.find("based on DMD v1.") == -1:
+			conf.fatal("detected compiler is not dmd/ldc")
+
+@conf
+def common_flags_ldc(conf):
+	"""
+	Sets the D flags required by *ldc*
+	"""
+	v = conf.env
+	v.DFLAGS        = ['-d-version=Posix']
+	v.LINKFLAGS     = []
+	v.DFLAGS_dshlib = ['-relocation-model=pic']
+
+@conf
+def common_flags_dmd(conf):
+	"""
+	Set the flags required by *dmd* or *dmd2*
+	"""
+	v = conf.env
+
+	v.D_SRC_F           = ['-c']
+	v.D_TGT_F           = '-of%s'
+
+	v.D_LINKER          = v.D
+	v.DLNK_SRC_F        = ''
+	v.DLNK_TGT_F        = '-of%s'
+	v.DINC_ST           = '-I%s'
+
+	v.DSHLIB_MARKER = v.DSTLIB_MARKER = ''
+	v.DSTLIB_ST = v.DSHLIB_ST         = '-L-l%s'
+	v.DSTLIBPATH_ST = v.DLIBPATH_ST   = '-L-L%s'
+
+	v.LINKFLAGS_dprogram= ['-quiet']
+
+	v.DFLAGS_dshlib     = ['-fPIC']
+	v.LINKFLAGS_dshlib  = ['-L-shared']
+
+	v.DHEADER_ext       = '.di'
+	v.DFLAGS_d_with_header = ['-H', '-Hf']
+	v.D_HDR_F           = '%s'
+
+def configure(conf):
+	"""
+	Configuration for *dmd*, *dmd2*, and *ldc*
+	"""
+	conf.find_dmd()
+
+	if sys.platform == 'win32':
+		out = conf.cmd_and_log(conf.env.D + ['--help'])
+		if out.find('D Compiler v2.') > -1:
+			conf.fatal('dmd2 on Windows is not supported, use gdc or ldc2 instead')
+
+	conf.load('ar')
+	conf.load('d')
+	conf.common_flags_dmd()
+	conf.d_platform_flags()
+
+	if str(conf.env.D).find('ldc') > -1:
+		conf.common_flags_ldc()
+
diff --git a/waflib/Tools/errcheck.py b/waflib/Tools/errcheck.py
index 1f548aca..de8d75a4 100644
--- a/waflib/Tools/errcheck.py
+++ b/waflib/Tools/errcheck.py
@@ -3,9 +3,9 @@
 # Thomas Nagy, 2011 (ita)
 
 """
-errcheck: highlight common mistakes
+Common mistakes highlighting.
 
-There is a performance hit, so this tool is only loaded when running "waf -v"
+There is a performance impact, so this tool is only loaded when running ``waf -v``
 """
 
 typos = {
@@ -18,13 +18,14 @@
 'importpath':'includes',
 'installpath':'install_path',
 'iscopy':'is_copy',
+'uses':'use',
 }
 
 meths_typos = ['__call__', 'program', 'shlib', 'stlib', 'objects']
 
 import sys
 from waflib import Logs, Build, Node, Task, TaskGen, ConfigSet, Errors, Utils
-import waflib.Tools.ccroot
+from waflib.Tools import ccroot
 
 def check_same_targets(self):
 	mp = Utils.defaultdict(list)
@@ -33,6 +34,8 @@ def check_same_targets(self):
 	def check_task(tsk):
 		if not isinstance(tsk, Task.Task):
 			return
+		if hasattr(tsk, 'no_errcheck_out'):
+			return
 
 		for node in tsk.outputs:
 			mp[node].append(tsk)
@@ -58,30 +61,34 @@ def check_task(tsk):
 			Logs.error(msg)
 			for x in v:
 				if Logs.verbose > 1:
-					Logs.error('  %d. %r' % (1 + v.index(x), x.generator))
+					Logs.error('  %d. %r', 1 + v.index(x), x.generator)
 				else:
-					Logs.error('  %d. %r in %r' % (1 + v.index(x), x.generator.name, getattr(x.generator, 'path', None)))
+					Logs.error('  %d. %r in %r', 1 + v.index(x), x.generator.name, getattr(x.generator, 'path', None))
+			Logs.error('If you think that this is an error, set no_errcheck_out on the task instance')
 
 	if not dupe:
 		for (k, v) in uids.items():
 			if len(v) > 1:
 				Logs.error('* Several tasks use the same identifier. Please check the information on\n   https://waf.io/apidocs/Task.html?highlight=uid#waflib.Task.Task.uid')
+				tg_details = tsk.generator.name
+				if Logs.verbose > 2:
+					tg_details = tsk.generator
 				for tsk in v:
-					Logs.error('  - object %r (%r) defined in %r' % (tsk.__class__.__name__, tsk, tsk.generator))
+					Logs.error('  - object %r (%r) defined in %r', tsk.__class__.__name__, tsk, tg_details)
 
 def check_invalid_constraints(self):
-	feat = set([])
+	feat = set()
 	for x in list(TaskGen.feats.values()):
 		feat.union(set(x))
 	for (x, y) in TaskGen.task_gen.prec.items():
 		feat.add(x)
 		feat.union(set(y))
-	ext = set([])
+	ext = set()
 	for x in TaskGen.task_gen.mappings.values():
 		ext.add(x.__name__)
 	invalid = ext & feat
 	if invalid:
-		Logs.error('The methods %r have invalid annotations:  @extension <-> @feature/@before_method/@after_method' % list(invalid))
+		Logs.error('The methods %r have invalid annotations:  @extension <-> @feature/@before_method/@after_method', list(invalid))
 
 	# the build scripts have been read, so we can check for invalid after/before attributes on task classes
 	for cls in list(Task.classes.values()):
@@ -90,15 +97,15 @@ def check_invalid_constraints(self):
 
 		for x in ('before', 'after'):
 			for y in Utils.to_list(getattr(cls, x, [])):
-				if not Task.classes.get(y, None):
-					Logs.error('Erroneous order constraint %r=%r on task class %r' % (x, y, cls.__name__))
+				if not Task.classes.get(y):
+					Logs.error('Erroneous order constraint %r=%r on task class %r', x, y, cls.__name__)
 		if getattr(cls, 'rule', None):
-			Logs.error('Erroneous attribute "rule" on task class %r (rename to "run_str")' % cls.__name__)
+			Logs.error('Erroneous attribute "rule" on task class %r (rename to "run_str")', cls.__name__)
 
 def replace(m):
 	"""
-	We could add properties, but they would not work in some cases:
-	bld.program(...) requires 'source' in the attributes
+	Replaces existing BuildContext methods to verify parameter names,
+	for example ``bld(source=)`` has no ending *s*
 	"""
 	oldcall = getattr(Build.BuildContext, m)
 	def call(self, *k, **kw):
@@ -107,13 +114,13 @@ def call(self, *k, **kw):
 			if x in kw:
 				if x == 'iscopy' and 'subst' in getattr(self, 'features', ''):
 					continue
-				Logs.error('Fix the typo %r -> %r on %r' % (x, typos[x], ret))
+				Logs.error('Fix the typo %r -> %r on %r', x, typos[x], ret)
 		return ret
 	setattr(Build.BuildContext, m, call)
 
 def enhance_lib():
 	"""
-	modify existing classes and methods
+	Modifies existing classes and methods to enable error verification
 	"""
 	for m in meths_typos:
 		replace(m)
@@ -121,26 +128,36 @@ def enhance_lib():
 	# catch '..' in ant_glob patterns
 	def ant_glob(self, *k, **kw):
 		if k:
-			lst=Utils.to_list(k[0])
+			lst = Utils.to_list(k[0])
 			for pat in lst:
-				if '..' in pat.split('/'):
-					Logs.error("In ant_glob pattern %r: '..' means 'two dots', not 'parent directory'" % k[0])
-		if kw.get('remove', True):
-			try:
-				if self.is_child_of(self.ctx.bldnode) and not kw.get('quiet', False):
-					Logs.error('Using ant_glob on the build folder (%r) is dangerous (quiet=True to disable this warning)' % self)
-			except AttributeError:
-				pass
+				sp = pat.split('/')
+				if '..' in sp:
+					Logs.error("In ant_glob pattern %r: '..' means 'two dots', not 'parent directory'", k[0])
+				if '.' in sp:
+					Logs.error("In ant_glob pattern %r: '.' means 'one dot', not 'current directory'", k[0])
 		return self.old_ant_glob(*k, **kw)
 	Node.Node.old_ant_glob = Node.Node.ant_glob
 	Node.Node.ant_glob = ant_glob
 
+	# catch ant_glob on build folders
+	def ant_iter(self, accept=None, maxdepth=25, pats=[], dir=False, src=True, remove=True, quiet=False):
+		if remove:
+			try:
+				if self.is_child_of(self.ctx.bldnode) and not quiet:
+					quiet = True
+					Logs.error('Calling ant_glob on build folders (%r) is dangerous: add quiet=True / remove=False', self)
+			except AttributeError:
+				pass
+		return self.old_ant_iter(accept, maxdepth, pats, dir, src, remove, quiet)
+	Node.Node.old_ant_iter = Node.Node.ant_iter
+	Node.Node.ant_iter = ant_iter
+
 	# catch conflicting ext_in/ext_out/before/after declarations
 	old = Task.is_before
 	def is_before(t1, t2):
 		ret = old(t1, t2)
 		if ret and old(t2, t1):
-			Logs.error('Contradictory order constraints in classes %r %r' % (t1, t2))
+			Logs.error('Contradictory order constraints in classes %r %r', t1, t2)
 		return ret
 	Task.is_before = is_before
 
@@ -152,7 +169,7 @@ def check_err_features(self):
 			Logs.error('feature shlib -> cshlib, dshlib or cxxshlib')
 		for x in ('c', 'cxx', 'd', 'fc'):
 			if not x in lst and lst and lst[0] in [x+y for y in ('program', 'shlib', 'stlib')]:
-				Logs.error('%r features is probably missing %r' % (self, x))
+				Logs.error('%r features is probably missing %r', self, x)
 	TaskGen.feature('*')(check_err_features)
 
 	# check for erroneous order constraints
@@ -160,12 +177,12 @@ def check_err_order(self):
 		if not hasattr(self, 'rule') and not 'subst' in Utils.to_list(self.features):
 			for x in ('before', 'after', 'ext_in', 'ext_out'):
 				if hasattr(self, x):
-					Logs.warn('Erroneous order constraint %r on non-rule based task generator %r' % (x, self))
+					Logs.warn('Erroneous order constraint %r on non-rule based task generator %r', x, self)
 		else:
 			for x in ('before', 'after'):
 				for y in self.to_list(getattr(self, x, [])):
-					if not Task.classes.get(y, None):
-						Logs.error('Erroneous order constraint %s=%r on %r (no such class)' % (x, y, self))
+					if not Task.classes.get(y):
+						Logs.error('Erroneous order constraint %s=%r on %r (no such class)', x, y, self)
 	TaskGen.feature('*')(check_err_order)
 
 	# check for @extension used with @feature/@before_method/@after_method
@@ -200,24 +217,21 @@ def use_rec(self, name, **kw):
 	TaskGen.task_gen.use_rec = use_rec
 
 	# check for env.append
-	def getattri(self, name, default=None):
+	def _getattr(self, name, default=None):
 		if name == 'append' or name == 'add':
 			raise Errors.WafError('env.append and env.add do not exist: use env.append_value/env.append_unique')
 		elif name == 'prepend':
 			raise Errors.WafError('env.prepend does not exist: use env.prepend_value')
 		if name in self.__slots__:
-			return object.__getattr__(self, name, default)
+			return super(ConfigSet.ConfigSet, self).__getattr__(name, default)
 		else:
 			return self[name]
-	ConfigSet.ConfigSet.__getattr__ = getattri
+	ConfigSet.ConfigSet.__getattr__ = _getattr
 
 
 def options(opt):
 	"""
-	Add a few methods
+	Error verification can be enabled by default (not just on ``waf -v``) by adding to the user script options
 	"""
 	enhance_lib()
 
-def configure(conf):
-	pass
-
diff --git a/waflib/Tools/fc.py b/waflib/Tools/fc.py
new file mode 100644
index 00000000..d9e8d8c4
--- /dev/null
+++ b/waflib/Tools/fc.py
@@ -0,0 +1,189 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# DC 2008
+# Thomas Nagy 2016-2018 (ita)
+
+"""
+Fortran support
+"""
+
+from waflib import Utils, Task, Errors
+from waflib.Tools import ccroot, fc_config, fc_scan
+from waflib.TaskGen import extension
+from waflib.Configure import conf
+
+ccroot.USELIB_VARS['fc'] = set(['FCFLAGS', 'DEFINES', 'INCLUDES', 'FCPPFLAGS'])
+ccroot.USELIB_VARS['fcprogram_test'] = ccroot.USELIB_VARS['fcprogram'] = set(['LIB', 'STLIB', 'LIBPATH', 'STLIBPATH', 'LINKFLAGS', 'RPATH', 'LINKDEPS'])
+ccroot.USELIB_VARS['fcshlib'] = set(['LIB', 'STLIB', 'LIBPATH', 'STLIBPATH', 'LINKFLAGS', 'RPATH', 'LINKDEPS'])
+ccroot.USELIB_VARS['fcstlib'] = set(['ARFLAGS', 'LINKDEPS'])
+
+@extension('.f','.F','.f90','.F90','.for','.FOR','.f95','.F95','.f03','.F03','.f08','.F08')
+def fc_hook(self, node):
+	"Binds the Fortran file extensions create :py:class:`waflib.Tools.fc.fc` instances"
+	return self.create_compiled_task('fc', node)
+
+@conf
+def modfile(conf, name):
+	"""
+	Turns a module name into the right module file name.
+	Defaults to all lower case.
+	"""
+	return {'lower'     :name.lower() + '.mod',
+		'lower.MOD' :name.lower() + '.MOD',
+		'UPPER.mod' :name.upper() + '.mod',
+		'UPPER'     :name.upper() + '.MOD'}[conf.env.FC_MOD_CAPITALIZATION or 'lower']
+
+def get_fortran_tasks(tsk):
+	"""
+	Obtains all fortran tasks from the same build group. Those tasks must not have
+	the attribute 'nomod' or 'mod_fortran_done'
+
+	:return: a list of :py:class:`waflib.Tools.fc.fc` instances
+	"""
+	bld = tsk.generator.bld
+	tasks = bld.get_tasks_group(bld.get_group_idx(tsk.generator))
+	return [x for x in tasks if isinstance(x, fc) and not getattr(x, 'nomod', None) and not getattr(x, 'mod_fortran_done', None)]
+
+class fc(Task.Task):
+	"""
+	Fortran tasks can only run when all fortran tasks in a current task group are ready to be executed
+	This may cause a deadlock if some fortran task is waiting for something that cannot happen (circular dependency)
+	Should this ever happen, set the 'nomod=True' on those tasks instances to break the loop
+	"""
+	color = 'GREEN'
+	run_str = '${FC} ${FCFLAGS} ${FCINCPATH_ST:INCPATHS} ${FCDEFINES_ST:DEFINES} ${_FCMODOUTFLAGS} ${FC_TGT_F}${TGT[0].abspath()} ${FC_SRC_F}${SRC[0].abspath()} ${FCPPFLAGS}'
+	vars = ["FORTRANMODPATHFLAG"]
+
+	def scan(self):
+		"""Fortran dependency scanner"""
+		tmp = fc_scan.fortran_parser(self.generator.includes_nodes)
+		tmp.task = self
+		tmp.start(self.inputs[0])
+		return (tmp.nodes, tmp.names)
+
+	def runnable_status(self):
+		"""
+		Sets the mod file outputs and the dependencies on the mod files over all Fortran tasks
+		executed by the main thread so there are no concurrency issues
+		"""
+		if getattr(self, 'mod_fortran_done', None):
+			return super(fc, self).runnable_status()
+
+		# now, if we reach this part it is because this fortran task is the first in the list
+		bld = self.generator.bld
+
+		# obtain the fortran tasks
+		lst = get_fortran_tasks(self)
+
+		# disable this method for other tasks
+		for tsk in lst:
+			tsk.mod_fortran_done = True
+
+		# wait for all the .f tasks to be ready for execution
+		# and ensure that the scanners are called at least once
+		for tsk in lst:
+			ret = tsk.runnable_status()
+			if ret == Task.ASK_LATER:
+				# we have to wait for one of the other fortran tasks to be ready
+				# this may deadlock if there are dependencies between fortran tasks
+				# but this should not happen (we are setting them here!)
+				for x in lst:
+					x.mod_fortran_done = None
+
+				return Task.ASK_LATER
+
+		ins = Utils.defaultdict(set)
+		outs = Utils.defaultdict(set)
+
+		# the .mod files to create
+		for tsk in lst:
+			key = tsk.uid()
+			for x in bld.raw_deps[key]:
+				if x.startswith('MOD@'):
+					name = bld.modfile(x.replace('MOD@', ''))
+					node = bld.srcnode.find_or_declare(name)
+					tsk.set_outputs(node)
+					outs[node].add(tsk)
+
+		# the .mod files to use
+		for tsk in lst:
+			key = tsk.uid()
+			for x in bld.raw_deps[key]:
+				if x.startswith('USE@'):
+					name = bld.modfile(x.replace('USE@', ''))
+					node = bld.srcnode.find_resource(name)
+					if node and node not in tsk.outputs:
+						if not node in bld.node_deps[key]:
+							bld.node_deps[key].append(node)
+						ins[node].add(tsk)
+
+		# if the intersection matches, set the order
+		for k in ins.keys():
+			for a in ins[k]:
+				a.run_after.update(outs[k])
+				for x in outs[k]:
+					self.generator.bld.producer.revdeps[x].add(a)
+
+				# the scanner cannot output nodes, so we have to set them
+				# ourselves as task.dep_nodes (additional input nodes)
+				tmp = []
+				for t in outs[k]:
+					tmp.extend(t.outputs)
+				a.dep_nodes.extend(tmp)
+				a.dep_nodes.sort(key=lambda x: x.abspath())
+
+		# the task objects have changed: clear the signature cache
+		for tsk in lst:
+			try:
+				delattr(tsk, 'cache_sig')
+			except AttributeError:
+				pass
+
+		return super(fc, self).runnable_status()
+
+class fcprogram(ccroot.link_task):
+	"""Links Fortran programs"""
+	color = 'YELLOW'
+	run_str = '${FC} ${LINKFLAGS} ${FCLNK_SRC_F}${SRC} ${FCLNK_TGT_F}${TGT[0].abspath()} ${RPATH_ST:RPATH} ${FCSTLIB_MARKER} ${FCSTLIBPATH_ST:STLIBPATH} ${FCSTLIB_ST:STLIB} ${FCSHLIB_MARKER} ${FCLIBPATH_ST:LIBPATH} ${FCLIB_ST:LIB} ${LDFLAGS}'
+	inst_to = '${BINDIR}'
+
+class fcshlib(fcprogram):
+	"""Links Fortran libraries"""
+	inst_to = '${LIBDIR}'
+
+class fcstlib(ccroot.stlink_task):
+	"""Links Fortran static libraries (uses ar by default)"""
+	pass # do not remove the pass statement
+
+class fcprogram_test(fcprogram):
+	"""Custom link task to obtain compiler outputs for Fortran configuration tests"""
+
+	def runnable_status(self):
+		"""This task is always executed"""
+		ret = super(fcprogram_test, self).runnable_status()
+		if ret == Task.SKIP_ME:
+			ret = Task.RUN_ME
+		return ret
+
+	def exec_command(self, cmd, **kw):
+		"""Stores the compiler std our/err onto the build context, to bld.out + bld.err"""
+		bld = self.generator.bld
+
+		kw['shell'] = isinstance(cmd, str)
+		kw['stdout'] = kw['stderr'] = Utils.subprocess.PIPE
+		kw['cwd'] = self.get_cwd()
+		bld.out = bld.err = ''
+
+		bld.to_log('command: %s\n' % cmd)
+
+		kw['output'] = 0
+		try:
+			(bld.out, bld.err) = bld.cmd_and_log(cmd, **kw)
+		except Errors.WafError:
+			return -1
+
+		if bld.out:
+			bld.to_log('out: %s\n' % bld.out)
+		if bld.err:
+			bld.to_log('err: %s\n' % bld.err)
+
diff --git a/waflib/Tools/fc_config.py b/waflib/Tools/fc_config.py
new file mode 100644
index 00000000..222f3a55
--- /dev/null
+++ b/waflib/Tools/fc_config.py
@@ -0,0 +1,488 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# DC 2008
+# Thomas Nagy 2016-2018 (ita)
+
+"""
+Fortran configuration helpers
+"""
+
+import re, os, sys, shlex
+from waflib.Configure import conf
+from waflib.TaskGen import feature, before_method
+
+FC_FRAGMENT = '        program main\n        end     program main\n'
+FC_FRAGMENT2 = '        PROGRAM MAIN\n        END\n' # what's the actual difference between these?
+
+@conf
+def fc_flags(conf):
+	"""
+	Defines common fortran configuration flags and file extensions
+	"""
+	v = conf.env
+
+	v.FC_SRC_F    = []
+	v.FC_TGT_F    = ['-c', '-o']
+	v.FCINCPATH_ST  = '-I%s'
+	v.FCDEFINES_ST  = '-D%s'
+
+	if not v.LINK_FC:
+		v.LINK_FC = v.FC
+
+	v.FCLNK_SRC_F = []
+	v.FCLNK_TGT_F = ['-o']
+
+	v.FCFLAGS_fcshlib   = ['-fpic']
+	v.LINKFLAGS_fcshlib = ['-shared']
+	v.fcshlib_PATTERN   = 'lib%s.so'
+
+	v.fcstlib_PATTERN   = 'lib%s.a'
+
+	v.FCLIB_ST       = '-l%s'
+	v.FCLIBPATH_ST   = '-L%s'
+	v.FCSTLIB_ST     = '-l%s'
+	v.FCSTLIBPATH_ST = '-L%s'
+	v.FCSTLIB_MARKER = '-Wl,-Bstatic'
+	v.FCSHLIB_MARKER = '-Wl,-Bdynamic'
+
+	v.SONAME_ST      = '-Wl,-h,%s'
+
+@conf
+def fc_add_flags(conf):
+	"""
+	Adds FCFLAGS / LDFLAGS / LINKFLAGS from os.environ to conf.env
+	"""
+	conf.add_os_flags('FCPPFLAGS', dup=False)
+	conf.add_os_flags('FCFLAGS', dup=False)
+	conf.add_os_flags('LINKFLAGS', dup=False)
+	conf.add_os_flags('LDFLAGS', dup=False)
+
+@conf
+def check_fortran(self, *k, **kw):
+	"""
+	Compiles a Fortran program to ensure that the settings are correct
+	"""
+	self.check_cc(
+		fragment         = FC_FRAGMENT,
+		compile_filename = 'test.f',
+		features         = 'fc fcprogram',
+		msg              = 'Compiling a simple fortran app')
+
+@conf
+def check_fc(self, *k, **kw):
+	"""
+	Same as :py:func:`waflib.Tools.c_config.check` but defaults to the *Fortran* programming language
+	(this overrides the C defaults in :py:func:`waflib.Tools.c_config.validate_c`)
+	"""
+	kw['compiler'] = 'fc'
+	if not 'compile_mode' in kw:
+		kw['compile_mode'] = 'fc'
+	if not 'type' in kw:
+		kw['type'] = 'fcprogram'
+	if not 'compile_filename' in kw:
+		kw['compile_filename'] = 'test.f90'
+	if not 'code' in kw:
+		kw['code'] = FC_FRAGMENT
+	return self.check(*k, **kw)
+
+# ------------------------------------------------------------------------
+# --- These are the default platform modifiers, refactored here for
+#     convenience.  gfortran and g95 have much overlap.
+# ------------------------------------------------------------------------
+
+@conf
+def fortran_modifier_darwin(conf):
+	"""
+	Defines Fortran flags and extensions for OSX systems
+	"""
+	v = conf.env
+	v.FCFLAGS_fcshlib   = ['-fPIC']
+	v.LINKFLAGS_fcshlib = ['-dynamiclib']
+	v.fcshlib_PATTERN   = 'lib%s.dylib'
+	v.FRAMEWORKPATH_ST  = '-F%s'
+	v.FRAMEWORK_ST      = ['-framework']
+
+	v.LINKFLAGS_fcstlib = []
+
+	v.FCSHLIB_MARKER    = ''
+	v.FCSTLIB_MARKER    = ''
+	v.SONAME_ST         = ''
+
+@conf
+def fortran_modifier_win32(conf):
+	"""
+	Defines Fortran flags for Windows platforms
+	"""
+	v = conf.env
+	v.fcprogram_PATTERN = v.fcprogram_test_PATTERN  = '%s.exe'
+
+	v.fcshlib_PATTERN   = '%s.dll'
+	v.implib_PATTERN    = '%s.dll.a'
+	v.IMPLIB_ST         = '-Wl,--out-implib,%s'
+
+	v.FCFLAGS_fcshlib   = []
+
+	# Auto-import is enabled by default even without this option,
+	# but enabling it explicitly has the nice effect of suppressing the rather boring, debug-level messages
+	# that the linker emits otherwise.
+	v.append_value('LINKFLAGS', ['-Wl,--enable-auto-import'])
+
+@conf
+def fortran_modifier_cygwin(conf):
+	"""
+	Defines Fortran flags for use on cygwin
+	"""
+	fortran_modifier_win32(conf)
+	v = conf.env
+	v.fcshlib_PATTERN = 'cyg%s.dll'
+	v.append_value('LINKFLAGS_fcshlib', ['-Wl,--enable-auto-image-base'])
+	v.FCFLAGS_fcshlib = []
+
+# ------------------------------------------------------------------------
+
+@conf
+def check_fortran_dummy_main(self, *k, **kw):
+	"""
+	Determines if a main function is needed by compiling a code snippet with
+	the C compiler and linking it with the Fortran compiler (useful on unix-like systems)
+	"""
+	if not self.env.CC:
+		self.fatal('A c compiler is required for check_fortran_dummy_main')
+
+	lst = ['MAIN__', '__MAIN', '_MAIN', 'MAIN_', 'MAIN']
+	lst.extend([m.lower() for m in lst])
+	lst.append('')
+
+	self.start_msg('Detecting whether we need a dummy main')
+	for main in lst:
+		kw['fortran_main'] = main
+		try:
+			self.check_cc(
+				fragment = 'int %s() { return 0; }\n' % (main or 'test'),
+				features = 'c fcprogram',
+				mandatory = True
+			)
+			if not main:
+				self.env.FC_MAIN = -1
+				self.end_msg('no')
+			else:
+				self.env.FC_MAIN = main
+				self.end_msg('yes %s' % main)
+			break
+		except self.errors.ConfigurationError:
+			pass
+	else:
+		self.end_msg('not found')
+		self.fatal('could not detect whether fortran requires a dummy main, see the config.log')
+
+# ------------------------------------------------------------------------
+
+GCC_DRIVER_LINE = re.compile('^Driving:')
+POSIX_STATIC_EXT = re.compile('\S+\.a')
+POSIX_LIB_FLAGS = re.compile('-l\S+')
+
+@conf
+def is_link_verbose(self, txt):
+	"""Returns True if 'useful' link options can be found in txt"""
+	assert isinstance(txt, str)
+	for line in txt.splitlines():
+		if not GCC_DRIVER_LINE.search(line):
+			if POSIX_STATIC_EXT.search(line) or POSIX_LIB_FLAGS.search(line):
+				return True
+	return False
+
+@conf
+def check_fortran_verbose_flag(self, *k, **kw):
+	"""
+	Checks what kind of verbose (-v) flag works, then sets it to env.FC_VERBOSE_FLAG
+	"""
+	self.start_msg('fortran link verbose flag')
+	for x in ('-v', '--verbose', '-verbose', '-V'):
+		try:
+			self.check_cc(
+				features = 'fc fcprogram_test',
+				fragment = FC_FRAGMENT2,
+				compile_filename = 'test.f',
+				linkflags = [x],
+				mandatory=True)
+		except self.errors.ConfigurationError:
+			pass
+		else:
+			# output is on stderr or stdout (for xlf)
+			if self.is_link_verbose(self.test_bld.err) or self.is_link_verbose(self.test_bld.out):
+				self.end_msg(x)
+				break
+	else:
+		self.end_msg('failure')
+		self.fatal('Could not obtain the fortran link verbose flag (see config.log)')
+
+	self.env.FC_VERBOSE_FLAG = x
+	return x
+
+# ------------------------------------------------------------------------
+
+# linkflags which match those are ignored
+LINKFLAGS_IGNORED = [r'-lang*', r'-lcrt[a-zA-Z0-9\.]*\.o', r'-lc$', r'-lSystem', r'-libmil', r'-LIST:*', r'-LNO:*']
+if os.name == 'nt':
+	LINKFLAGS_IGNORED.extend([r'-lfrt*', r'-luser32', r'-lkernel32', r'-ladvapi32', r'-lmsvcrt', r'-lshell32', r'-lmingw', r'-lmoldname'])
+else:
+	LINKFLAGS_IGNORED.append(r'-lgcc*')
+RLINKFLAGS_IGNORED = [re.compile(f) for f in LINKFLAGS_IGNORED]
+
+def _match_ignore(line):
+	"""Returns True if the line should be ignored (Fortran verbose flag test)"""
+	for i in RLINKFLAGS_IGNORED:
+		if i.match(line):
+			return True
+	return False
+
+def parse_fortran_link(lines):
+	"""Given the output of verbose link of Fortran compiler, this returns a
+	list of flags necessary for linking using the standard linker."""
+	final_flags = []
+	for line in lines:
+		if not GCC_DRIVER_LINE.match(line):
+			_parse_flink_line(line, final_flags)
+	return final_flags
+
+SPACE_OPTS = re.compile('^-[LRuYz]$')
+NOSPACE_OPTS = re.compile('^-[RL]')
+
+def _parse_flink_token(lexer, token, tmp_flags):
+	# Here we go (convention for wildcard is shell, not regex !)
+	#   1 TODO: we first get some root .a libraries
+	#   2 TODO: take everything starting by -bI:*
+	#   3 Ignore the following flags: -lang* | -lcrt*.o | -lc |
+	#   -lgcc* | -lSystem | -libmil | -LANG:=* | -LIST:* | -LNO:*)
+	#   4 take into account -lkernel32
+	#   5 For options of the kind -[[LRuYz]], as they take one argument
+	#   after, the actual option is the next token
+	#   6 For -YP,*: take and replace by -Larg where arg is the old
+	#   argument
+	#   7 For -[lLR]*: take
+
+	# step 3
+	if _match_ignore(token):
+		pass
+	# step 4
+	elif token.startswith('-lkernel32') and sys.platform == 'cygwin':
+		tmp_flags.append(token)
+	# step 5
+	elif SPACE_OPTS.match(token):
+		t = lexer.get_token()
+		if t.startswith('P,'):
+			t = t[2:]
+		for opt in t.split(os.pathsep):
+			tmp_flags.append('-L%s' % opt)
+	# step 6
+	elif NOSPACE_OPTS.match(token):
+		tmp_flags.append(token)
+	# step 7
+	elif POSIX_LIB_FLAGS.match(token):
+		tmp_flags.append(token)
+	else:
+		# ignore anything not explicitly taken into account
+		pass
+
+	t = lexer.get_token()
+	return t
+
+def _parse_flink_line(line, final_flags):
+	"""private"""
+	lexer = shlex.shlex(line, posix = True)
+	lexer.whitespace_split = True
+
+	t = lexer.get_token()
+	tmp_flags = []
+	while t:
+		t = _parse_flink_token(lexer, t, tmp_flags)
+
+	final_flags.extend(tmp_flags)
+	return final_flags
+
+@conf
+def check_fortran_clib(self, autoadd=True, *k, **kw):
+	"""
+	Obtains the flags for linking with the C library
+	if this check works, add uselib='CLIB' to your task generators
+	"""
+	if not self.env.FC_VERBOSE_FLAG:
+		self.fatal('env.FC_VERBOSE_FLAG is not set: execute check_fortran_verbose_flag?')
+
+	self.start_msg('Getting fortran runtime link flags')
+	try:
+		self.check_cc(
+			fragment = FC_FRAGMENT2,
+			compile_filename = 'test.f',
+			features = 'fc fcprogram_test',
+			linkflags = [self.env.FC_VERBOSE_FLAG]
+		)
+	except Exception:
+		self.end_msg(False)
+		if kw.get('mandatory', True):
+			conf.fatal('Could not find the c library flags')
+	else:
+		out = self.test_bld.err
+		flags = parse_fortran_link(out.splitlines())
+		self.end_msg('ok (%s)' % ' '.join(flags))
+		self.env.LINKFLAGS_CLIB = flags
+		return flags
+	return []
+
+def getoutput(conf, cmd, stdin=False):
+	"""
+	Obtains Fortran command outputs
+	"""
+	from waflib import Errors
+	if conf.env.env:
+		env = conf.env.env
+	else:
+		env = dict(os.environ)
+		env['LANG'] = 'C'
+	input = stdin and '\n'.encode() or None
+	try:
+		out, err = conf.cmd_and_log(cmd, env=env, output=0, input=input)
+	except Errors.WafError as e:
+		# An WafError might indicate an error code during the command
+		# execution, in this case we still obtain the stderr and stdout,
+		# which we can use to find the version string.
+		if not (hasattr(e, 'stderr') and hasattr(e, 'stdout')):
+			raise e
+		else:
+			# Ignore the return code and return the original
+			# stdout and stderr.
+			out = e.stdout
+			err = e.stderr
+	except Exception:
+		conf.fatal('could not determine the compiler version %r' % cmd)
+	return (out, err)
+
+# ------------------------------------------------------------------------
+
+ROUTINES_CODE = """\
+      subroutine foobar()
+      return
+      end
+      subroutine foo_bar()
+      return
+      end
+"""
+
+MAIN_CODE = """
+void %(dummy_func_nounder)s(void);
+void %(dummy_func_under)s(void);
+int %(main_func_name)s() {
+  %(dummy_func_nounder)s();
+  %(dummy_func_under)s();
+  return 0;
+}
+"""
+
+@feature('link_main_routines_func')
+@before_method('process_source')
+def link_main_routines_tg_method(self):
+	"""
+	The configuration test declares a unique task generator,
+	so we create other task generators from there for fortran link tests
+	"""
+	def write_test_file(task):
+		task.outputs[0].write(task.generator.code)
+	bld = self.bld
+	bld(rule=write_test_file, target='main.c', code=MAIN_CODE % self.__dict__)
+	bld(rule=write_test_file, target='test.f', code=ROUTINES_CODE)
+	bld(features='fc fcstlib', source='test.f', target='test')
+	bld(features='c fcprogram', source='main.c', target='app', use='test')
+
+def mangling_schemes():
+	"""
+	Generate triplets for use with mangle_name
+	(used in check_fortran_mangling)
+	the order is tuned for gfortan
+	"""
+	for u in ('_', ''):
+		for du in ('', '_'):
+			for c in ("lower", "upper"):
+				yield (u, du, c)
+
+def mangle_name(u, du, c, name):
+	"""Mangle a name from a triplet (used in check_fortran_mangling)"""
+	return getattr(name, c)() + u + (name.find('_') != -1 and du or '')
+
+@conf
+def check_fortran_mangling(self, *k, **kw):
+	"""
+	Detect the mangling scheme, sets FORTRAN_MANGLING to the triplet found
+
+	This test will compile a fortran static library, then link a c app against it
+	"""
+	if not self.env.CC:
+		self.fatal('A c compiler is required for link_main_routines')
+	if not self.env.FC:
+		self.fatal('A fortran compiler is required for link_main_routines')
+	if not self.env.FC_MAIN:
+		self.fatal('Checking for mangling requires self.env.FC_MAIN (execute "check_fortran_dummy_main" first?)')
+
+	self.start_msg('Getting fortran mangling scheme')
+	for (u, du, c) in mangling_schemes():
+		try:
+			self.check_cc(
+				compile_filename   = [],
+				features           = 'link_main_routines_func',
+				msg                = 'nomsg',
+				errmsg             = 'nomsg',
+				dummy_func_nounder = mangle_name(u, du, c, 'foobar'),
+				dummy_func_under   = mangle_name(u, du, c, 'foo_bar'),
+				main_func_name     = self.env.FC_MAIN
+			)
+		except self.errors.ConfigurationError:
+			pass
+		else:
+			self.end_msg("ok ('%s', '%s', '%s-case')" % (u, du, c))
+			self.env.FORTRAN_MANGLING = (u, du, c)
+			break
+	else:
+		self.end_msg(False)
+		self.fatal('mangler not found')
+	return (u, du, c)
+
+@feature('pyext')
+@before_method('propagate_uselib_vars', 'apply_link')
+def set_lib_pat(self):
+	"""Sets the Fortran flags for linking with Python"""
+	self.env.fcshlib_PATTERN = self.env.pyext_PATTERN
+
+@conf
+def detect_openmp(self):
+	"""
+	Detects openmp flags and sets the OPENMP ``FCFLAGS``/``LINKFLAGS``
+	"""
+	for x in ('-fopenmp','-openmp','-mp','-xopenmp','-omp','-qsmp=omp'):
+		try:
+			self.check_fc(
+				msg          = 'Checking for OpenMP flag %s' % x,
+				fragment     = 'program main\n  call omp_get_num_threads()\nend program main',
+				fcflags      = x,
+				linkflags    = x,
+				uselib_store = 'OPENMP'
+			)
+		except self.errors.ConfigurationError:
+			pass
+		else:
+			break
+	else:
+		self.fatal('Could not find OpenMP')
+
+@conf
+def check_gfortran_o_space(self):
+	if self.env.FC_NAME != 'GFORTRAN' or int(self.env.FC_VERSION[0]) > 4:
+		# This is for old compilers and only for gfortran.
+		# No idea how other implementations handle this. Be safe and bail out.
+		return
+	self.env.stash()
+	self.env.FCLNK_TGT_F = ['-o', '']
+	try:
+		self.check_fc(msg='Checking if the -o link must be split from arguments', fragment=FC_FRAGMENT, features='fc fcshlib')
+	except self.errors.ConfigurationError:
+		self.env.revert()
+	else:
+		self.env.commit()
diff --git a/waflib/Tools/fc_scan.py b/waflib/Tools/fc_scan.py
new file mode 100644
index 00000000..12cb0fc0
--- /dev/null
+++ b/waflib/Tools/fc_scan.py
@@ -0,0 +1,114 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# DC 2008
+# Thomas Nagy 2016-2018 (ita)
+
+import re
+
+INC_REGEX = """(?:^|['">]\s*;)\s*(?:|#\s*)INCLUDE\s+(?:\w+_)?[<"'](.+?)(?=["'>])"""
+USE_REGEX = """(?:^|;)\s*USE(?:\s+|(?:(?:\s*,\s*(?:NON_)?INTRINSIC)?\s*::))\s*(\w+)"""
+MOD_REGEX = """(?:^|;)\s*MODULE(?!\s*PROCEDURE)(?:\s+|(?:(?:\s*,\s*(?:NON_)?INTRINSIC)?\s*::))\s*(\w+)"""
+
+re_inc = re.compile(INC_REGEX, re.I)
+re_use = re.compile(USE_REGEX, re.I)
+re_mod = re.compile(MOD_REGEX, re.I)
+
+class fortran_parser(object):
+	"""
+	This parser returns:
+
+	* the nodes corresponding to the module names to produce
+	* the nodes corresponding to the include files used
+	* the module names used by the fortran files
+	"""
+	def __init__(self, incpaths):
+		self.seen = []
+		"""Files already parsed"""
+
+		self.nodes = []
+		"""List of :py:class:`waflib.Node.Node` representing the dependencies to return"""
+
+		self.names = []
+		"""List of module names to return"""
+
+		self.incpaths = incpaths
+		"""List of :py:class:`waflib.Node.Node` representing the include paths"""
+
+	def find_deps(self, node):
+		"""
+		Parses a Fortran file to obtain the dependencies used/provided
+
+		:param node: fortran file to read
+		:type node: :py:class:`waflib.Node.Node`
+		:return: lists representing the includes, the modules used, and the modules created by a fortran file
+		:rtype: tuple of list of strings
+		"""
+		txt = node.read()
+		incs = []
+		uses = []
+		mods = []
+		for line in txt.splitlines():
+			# line by line regexp search? optimize?
+			m = re_inc.search(line)
+			if m:
+				incs.append(m.group(1))
+			m = re_use.search(line)
+			if m:
+				uses.append(m.group(1))
+			m = re_mod.search(line)
+			if m:
+				mods.append(m.group(1))
+		return (incs, uses, mods)
+
+	def start(self, node):
+		"""
+		Start parsing. Use the stack ``self.waiting`` to hold nodes to iterate on
+
+		:param node: fortran file
+		:type node: :py:class:`waflib.Node.Node`
+		"""
+		self.waiting = [node]
+		while self.waiting:
+			nd = self.waiting.pop(0)
+			self.iter(nd)
+
+	def iter(self, node):
+		"""
+		Processes a single file during dependency parsing. Extracts files used
+		modules used and modules provided.
+		"""
+		incs, uses, mods = self.find_deps(node)
+		for x in incs:
+			if x in self.seen:
+				continue
+			self.seen.append(x)
+			self.tryfind_header(x)
+
+		for x in uses:
+			name = "USE@%s" % x
+			if not name in self.names:
+				self.names.append(name)
+
+		for x in mods:
+			name = "MOD@%s" % x
+			if not name in self.names:
+				self.names.append(name)
+
+	def tryfind_header(self, filename):
+		"""
+		Adds an include file to the list of nodes to process
+
+		:param filename: file name
+		:type filename: string
+		"""
+		found = None
+		for n in self.incpaths:
+			found = n.find_resource(filename)
+			if found:
+				self.nodes.append(found)
+				self.waiting.append(found)
+				break
+		if not found:
+			if not filename in self.names:
+				self.names.append(filename)
+
diff --git a/waflib/Tools/flex.py b/waflib/Tools/flex.py
new file mode 100644
index 00000000..2256657b
--- /dev/null
+++ b/waflib/Tools/flex.py
@@ -0,0 +1,62 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# John O'Meara, 2006
+# Thomas Nagy, 2006-2018 (ita)
+
+"""
+The **flex** program is a code generator which creates C or C++ files.
+The generated files are compiled into object files.
+"""
+
+import os, re
+from waflib import Task, TaskGen
+from waflib.Tools import ccroot
+
+def decide_ext(self, node):
+	if 'cxx' in self.features:
+		return ['.lex.cc']
+	return ['.lex.c']
+
+def flexfun(tsk):
+	env = tsk.env
+	bld = tsk.generator.bld
+	wd = bld.variant_dir
+	def to_list(xx):
+		if isinstance(xx, str):
+			return [xx]
+		return xx
+	tsk.last_cmd = lst = []
+	lst.extend(to_list(env.FLEX))
+	lst.extend(to_list(env.FLEXFLAGS))
+	inputs = [a.path_from(tsk.get_cwd()) for a in tsk.inputs]
+	if env.FLEX_MSYS:
+		inputs = [x.replace(os.sep, '/') for x in inputs]
+	lst.extend(inputs)
+	lst = [x for x in lst if x]
+	txt = bld.cmd_and_log(lst, cwd=wd, env=env.env or None, quiet=0)
+	tsk.outputs[0].write(txt.replace('\r\n', '\n').replace('\r', '\n')) # issue #1207
+
+TaskGen.declare_chain(
+	name = 'flex',
+	rule = flexfun, # issue #854
+	ext_in = '.l',
+	decider = decide_ext,
+)
+
+# To support the following:
+# bld(features='c', flexflags='-P/foo')
+Task.classes['flex'].vars = ['FLEXFLAGS', 'FLEX']
+ccroot.USELIB_VARS['c'].add('FLEXFLAGS')
+ccroot.USELIB_VARS['cxx'].add('FLEXFLAGS')
+
+def configure(conf):
+	"""
+	Detect the *flex* program
+	"""
+	conf.find_program('flex', var='FLEX')
+	conf.env.FLEXFLAGS = ['-t']
+
+	if re.search (r"\\msys\\[0-9.]+\\bin\\flex.exe$", conf.env.FLEX[0]):
+		# this is the flex shipped with MSYS
+		conf.env.FLEX_MSYS = True
+
diff --git a/waflib/Tools/g95.py b/waflib/Tools/g95.py
new file mode 100644
index 00000000..f69ba4f3
--- /dev/null
+++ b/waflib/Tools/g95.py
@@ -0,0 +1,66 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# KWS 2010
+# Thomas Nagy 2016-2018 (ita)
+
+import re
+from waflib import Utils
+from waflib.Tools import fc, fc_config, fc_scan, ar
+from waflib.Configure import conf
+
+@conf
+def find_g95(conf):
+	fc = conf.find_program('g95', var='FC')
+	conf.get_g95_version(fc)
+	conf.env.FC_NAME = 'G95'
+
+@conf
+def g95_flags(conf):
+	v = conf.env
+	v.FCFLAGS_fcshlib   = ['-fPIC']
+	v.FORTRANMODFLAG  = ['-fmod=', ''] # template for module path
+	v.FCFLAGS_DEBUG = ['-Werror'] # why not
+
+@conf
+def g95_modifier_win32(conf):
+	fc_config.fortran_modifier_win32(conf)
+
+@conf
+def g95_modifier_cygwin(conf):
+	fc_config.fortran_modifier_cygwin(conf)
+
+@conf
+def g95_modifier_darwin(conf):
+	fc_config.fortran_modifier_darwin(conf)
+
+@conf
+def g95_modifier_platform(conf):
+	dest_os = conf.env.DEST_OS or Utils.unversioned_sys_platform()
+	g95_modifier_func = getattr(conf, 'g95_modifier_' + dest_os, None)
+	if g95_modifier_func:
+		g95_modifier_func()
+
+@conf
+def get_g95_version(conf, fc):
+	"""get the compiler version"""
+
+	version_re = re.compile(r"g95\s*(?P<major>\d*)\.(?P<minor>\d*)").search
+	cmd = fc + ['--version']
+	out, err = fc_config.getoutput(conf, cmd, stdin=False)
+	if out:
+		match = version_re(out)
+	else:
+		match = version_re(err)
+	if not match:
+		conf.fatal('cannot determine g95 version')
+	k = match.groupdict()
+	conf.env.FC_VERSION = (k['major'], k['minor'])
+
+def configure(conf):
+	conf.find_g95()
+	conf.find_ar()
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.g95_flags()
+	conf.g95_modifier_platform()
+
diff --git a/waflib/Tools/gas.py b/waflib/Tools/gas.py
new file mode 100644
index 00000000..77afed70
--- /dev/null
+++ b/waflib/Tools/gas.py
@@ -0,0 +1,18 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2008-2018 (ita)
+
+"Detect as/gas/gcc for compiling assembly files"
+
+import waflib.Tools.asm # - leave this
+from waflib.Tools import ar
+
+def configure(conf):
+	"""
+	Find the programs gas/as/gcc and set the variable *AS*
+	"""
+	conf.find_program(['gas', 'gcc'], var='AS')
+	conf.env.AS_TGT_F = ['-c', '-o']
+	conf.env.ASLNK_TGT_F = ['-o']
+	conf.find_ar()
+	conf.load('asm')
diff --git a/waflib/Tools/gcc.py b/waflib/Tools/gcc.py
index 0b897c7d..acdd473a 100644
--- a/waflib/Tools/gcc.py
+++ b/waflib/Tools/gcc.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 # Ralf Habacker, 2006 (rh)
 # Yinon Ehrlich, 2009
 
@@ -27,54 +27,51 @@ def gcc_common_flags(conf):
 	"""
 	v = conf.env
 
-	v['CC_SRC_F']            = []
-	v['CC_TGT_F']            = ['-c', '-o']
+	v.CC_SRC_F            = []
+	v.CC_TGT_F            = ['-c', '-o']
 
-	# linker
-	if not v['LINK_CC']: v['LINK_CC'] = v['CC']
-	v['CCLNK_SRC_F']         = []
-	v['CCLNK_TGT_F']         = ['-o']
-	v['CPPPATH_ST']          = '-I%s'
-	v['DEFINES_ST']          = '-D%s'
+	if not v.LINK_CC:
+		v.LINK_CC = v.CC
 
-	v['LIB_ST']              = '-l%s' # template for adding libs
-	v['LIBPATH_ST']          = '-L%s' # template for adding libpaths
-	v['STLIB_ST']            = '-l%s'
-	v['STLIBPATH_ST']        = '-L%s'
-	v['RPATH_ST']            = '-Wl,-rpath,%s'
+	v.CCLNK_SRC_F         = []
+	v.CCLNK_TGT_F         = ['-o']
+	v.CPPPATH_ST          = '-I%s'
+	v.DEFINES_ST          = '-D%s'
 
-	v['SONAME_ST']           = '-Wl,-h,%s'
-	v['SHLIB_MARKER']        = '-Wl,-Bdynamic'
-	v['STLIB_MARKER']        = '-Wl,-Bstatic'
+	v.LIB_ST              = '-l%s' # template for adding libs
+	v.LIBPATH_ST          = '-L%s' # template for adding libpaths
+	v.STLIB_ST            = '-l%s'
+	v.STLIBPATH_ST        = '-L%s'
+	v.RPATH_ST            = '-Wl,-rpath,%s'
 
-	# program
-	v['cprogram_PATTERN']    = '%s'
+	v.SONAME_ST           = '-Wl,-h,%s'
+	v.SHLIB_MARKER        = '-Wl,-Bdynamic'
+	v.STLIB_MARKER        = '-Wl,-Bstatic'
 
-	# shared librar
-	v['CFLAGS_cshlib']       = ['-fPIC']
-	v['LINKFLAGS_cshlib']    = ['-shared']
-	v['cshlib_PATTERN']      = 'lib%s.so'
+	v.cprogram_PATTERN    = '%s'
 
-	# static lib
-	v['LINKFLAGS_cstlib']    = ['-Wl,-Bstatic']
-	v['cstlib_PATTERN']      = 'lib%s.a'
+	v.CFLAGS_cshlib       = ['-fPIC']
+	v.LINKFLAGS_cshlib    = ['-shared']
+	v.cshlib_PATTERN      = 'lib%s.so'
 
-	# osx stuff
-	v['LINKFLAGS_MACBUNDLE'] = ['-bundle', '-undefined', 'dynamic_lookup']
-	v['CFLAGS_MACBUNDLE']    = ['-fPIC']
-	v['macbundle_PATTERN']   = '%s.bundle'
+	v.LINKFLAGS_cstlib    = ['-Wl,-Bstatic']
+	v.cstlib_PATTERN      = 'lib%s.a'
+
+	v.LINKFLAGS_MACBUNDLE = ['-bundle', '-undefined', 'dynamic_lookup']
+	v.CFLAGS_MACBUNDLE    = ['-fPIC']
+	v.macbundle_PATTERN   = '%s.bundle'
 
 @conf
 def gcc_modifier_win32(conf):
 	"""Configuration flags for executing gcc on Windows"""
 	v = conf.env
-	v['cprogram_PATTERN']    = '%s.exe'
+	v.cprogram_PATTERN    = '%s.exe'
 
-	v['cshlib_PATTERN']      = '%s.dll'
-	v['implib_PATTERN']      = 'lib%s.dll.a'
-	v['IMPLIB_ST']           = '-Wl,--out-implib,%s'
+	v.cshlib_PATTERN      = '%s.dll'
+	v.implib_PATTERN      = '%s.dll.a'
+	v.IMPLIB_ST           = '-Wl,--out-implib,%s'
 
-	v['CFLAGS_cshlib']       = []
+	v.CFLAGS_cshlib       = []
 
 	# Auto-import is enabled by default even without this option,
 	# but enabling it explicitly has the nice effect of suppressing the rather boring, debug-level messages
@@ -86,42 +83,42 @@ def gcc_modifier_cygwin(conf):
 	"""Configuration flags for executing gcc on Cygwin"""
 	gcc_modifier_win32(conf)
 	v = conf.env
-	v['cshlib_PATTERN'] = 'cyg%s.dll'
+	v.cshlib_PATTERN = 'cyg%s.dll'
 	v.append_value('LINKFLAGS_cshlib', ['-Wl,--enable-auto-image-base'])
-	v['CFLAGS_cshlib'] = []
+	v.CFLAGS_cshlib = []
 
 @conf
 def gcc_modifier_darwin(conf):
 	"""Configuration flags for executing gcc on MacOS"""
 	v = conf.env
-	v['CFLAGS_cshlib']       = ['-fPIC']
-	v['LINKFLAGS_cshlib']    = ['-dynamiclib']
-	v['cshlib_PATTERN']      = 'lib%s.dylib'
-	v['FRAMEWORKPATH_ST']    = '-F%s'
-	v['FRAMEWORK_ST']        = ['-framework']
-	v['ARCH_ST']             = ['-arch']
+	v.CFLAGS_cshlib       = ['-fPIC']
+	v.LINKFLAGS_cshlib    = ['-dynamiclib']
+	v.cshlib_PATTERN      = 'lib%s.dylib'
+	v.FRAMEWORKPATH_ST    = '-F%s'
+	v.FRAMEWORK_ST        = ['-framework']
+	v.ARCH_ST             = ['-arch']
 
-	v['LINKFLAGS_cstlib']    = []
+	v.LINKFLAGS_cstlib    = []
 
-	v['SHLIB_MARKER']        = []
-	v['STLIB_MARKER']        = []
-	v['SONAME_ST']           = []
+	v.SHLIB_MARKER        = []
+	v.STLIB_MARKER        = []
+	v.SONAME_ST           = []
 
 @conf
 def gcc_modifier_aix(conf):
 	"""Configuration flags for executing gcc on AIX"""
 	v = conf.env
-	v['LINKFLAGS_cprogram']  = ['-Wl,-brtl']
-	v['LINKFLAGS_cshlib']    = ['-shared','-Wl,-brtl,-bexpfull']
-	v['SHLIB_MARKER']        = []
+	v.LINKFLAGS_cprogram  = ['-Wl,-brtl']
+	v.LINKFLAGS_cshlib    = ['-shared','-Wl,-brtl,-bexpfull']
+	v.SHLIB_MARKER        = []
 
 @conf
 def gcc_modifier_hpux(conf):
 	v = conf.env
-	v['SHLIB_MARKER']        = []
-	v['STLIB_MARKER']        = []
-	v['CFLAGS_cshlib']       = ['-fPIC','-DPIC']
-	v['cshlib_PATTERN']      = 'lib%s.sl'
+	v.SHLIB_MARKER        = []
+	v.STLIB_MARKER        = []
+	v.CFLAGS_cshlib       = ['-fPIC','-DPIC']
+	v.cshlib_PATTERN      = 'lib%s.sl'
 
 @conf
 def gcc_modifier_openbsd(conf):
@@ -130,9 +127,9 @@ def gcc_modifier_openbsd(conf):
 @conf
 def gcc_modifier_osf1V(conf):
 	v = conf.env
-	v['SHLIB_MARKER']        = []
-	v['STLIB_MARKER']        = []
-	v['SONAME_ST']           = []
+	v.SHLIB_MARKER        = []
+	v.STLIB_MARKER        = []
+	v.SONAME_ST           = []
 
 @conf
 def gcc_modifier_platform(conf):
@@ -155,5 +152,5 @@ def configure(conf):
 	conf.cc_load_tools()
 	conf.cc_add_flags()
 	conf.link_add_flags()
-
+	conf.check_gcc_o_space()
 
diff --git a/waflib/Tools/gdc.py b/waflib/Tools/gdc.py
new file mode 100644
index 00000000..d89a66d3
--- /dev/null
+++ b/waflib/Tools/gdc.py
@@ -0,0 +1,55 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Carlos Rafael Giani, 2007 (dv)
+
+from waflib.Tools import ar, d
+from waflib.Configure import conf
+
+@conf
+def find_gdc(conf):
+	"""
+	Finds the program gdc and set the variable *D*
+	"""
+	conf.find_program('gdc', var='D')
+
+	out = conf.cmd_and_log(conf.env.D + ['--version'])
+	if out.find("gdc") == -1:
+		conf.fatal("detected compiler is not gdc")
+
+@conf
+def common_flags_gdc(conf):
+	"""
+	Sets the flags required by *gdc*
+	"""
+	v = conf.env
+
+	v.DFLAGS            = []
+
+	v.D_SRC_F           = ['-c']
+	v.D_TGT_F           = '-o%s'
+
+	v.D_LINKER          = v.D
+	v.DLNK_SRC_F        = ''
+	v.DLNK_TGT_F        = '-o%s'
+	v.DINC_ST           = '-I%s'
+
+	v.DSHLIB_MARKER = v.DSTLIB_MARKER = ''
+	v.DSTLIB_ST = v.DSHLIB_ST         = '-l%s'
+	v.DSTLIBPATH_ST = v.DLIBPATH_ST   = '-L%s'
+
+	v.LINKFLAGS_dshlib  = ['-shared']
+
+	v.DHEADER_ext       = '.di'
+	v.DFLAGS_d_with_header = '-fintfc'
+	v.D_HDR_F           = '-fintfc-file=%s'
+
+def configure(conf):
+	"""
+	Configuration for gdc
+	"""
+	conf.find_gdc()
+	conf.load('ar')
+	conf.load('d')
+	conf.common_flags_gdc()
+	conf.d_platform_flags()
+
diff --git a/waflib/Tools/gfortran.py b/waflib/Tools/gfortran.py
new file mode 100644
index 00000000..10506673
--- /dev/null
+++ b/waflib/Tools/gfortran.py
@@ -0,0 +1,93 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# DC 2008
+# Thomas Nagy 2016-2018 (ita)
+
+import re
+from waflib import Utils
+from waflib.Tools import fc, fc_config, fc_scan, ar
+from waflib.Configure import conf
+
+@conf
+def find_gfortran(conf):
+	"""Find the gfortran program (will look in the environment variable 'FC')"""
+	fc = conf.find_program(['gfortran','g77'], var='FC')
+	# (fallback to g77 for systems, where no gfortran is available)
+	conf.get_gfortran_version(fc)
+	conf.env.FC_NAME = 'GFORTRAN'
+
+@conf
+def gfortran_flags(conf):
+	v = conf.env
+	v.FCFLAGS_fcshlib = ['-fPIC']
+	v.FORTRANMODFLAG = ['-J', ''] # template for module path
+	v.FCFLAGS_DEBUG = ['-Werror'] # why not
+
+@conf
+def gfortran_modifier_win32(conf):
+	fc_config.fortran_modifier_win32(conf)
+
+@conf
+def gfortran_modifier_cygwin(conf):
+	fc_config.fortran_modifier_cygwin(conf)
+
+@conf
+def gfortran_modifier_darwin(conf):
+	fc_config.fortran_modifier_darwin(conf)
+
+@conf
+def gfortran_modifier_platform(conf):
+	dest_os = conf.env.DEST_OS or Utils.unversioned_sys_platform()
+	gfortran_modifier_func = getattr(conf, 'gfortran_modifier_' + dest_os, None)
+	if gfortran_modifier_func:
+		gfortran_modifier_func()
+
+@conf
+def get_gfortran_version(conf, fc):
+	"""Get the compiler version"""
+
+	# ensure this is actually gfortran, not an imposter.
+	version_re = re.compile(r"GNU\s*Fortran", re.I).search
+	cmd = fc + ['--version']
+	out, err = fc_config.getoutput(conf, cmd, stdin=False)
+	if out:
+		match = version_re(out)
+	else:
+		match = version_re(err)
+	if not match:
+		conf.fatal('Could not determine the compiler type')
+
+	# --- now get more detailed info -- see c_config.get_cc_version
+	cmd = fc + ['-dM', '-E', '-']
+	out, err = fc_config.getoutput(conf, cmd, stdin=True)
+
+	if out.find('__GNUC__') < 0:
+		conf.fatal('Could not determine the compiler type')
+
+	k = {}
+	out = out.splitlines()
+	import shlex
+
+	for line in out:
+		lst = shlex.split(line)
+		if len(lst)>2:
+			key = lst[1]
+			val = lst[2]
+			k[key] = val
+
+	def isD(var):
+		return var in k
+
+	def isT(var):
+		return var in k and k[var] != '0'
+
+	conf.env.FC_VERSION = (k['__GNUC__'], k['__GNUC_MINOR__'], k['__GNUC_PATCHLEVEL__'])
+
+def configure(conf):
+	conf.find_gfortran()
+	conf.find_ar()
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.gfortran_flags()
+	conf.gfortran_modifier_platform()
+	conf.check_gfortran_o_space()
diff --git a/waflib/Tools/glib2.py b/waflib/Tools/glib2.py
new file mode 100644
index 00000000..949fe37c
--- /dev/null
+++ b/waflib/Tools/glib2.py
@@ -0,0 +1,489 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2018 (ita)
+
+"""
+Support for GLib2 tools:
+
+* marshal
+* enums
+* gsettings
+* gresource
+"""
+
+import os
+import functools
+from waflib import Context, Task, Utils, Options, Errors, Logs
+from waflib.TaskGen import taskgen_method, before_method, feature, extension
+from waflib.Configure import conf
+
+################## marshal files
+
+@taskgen_method
+def add_marshal_file(self, filename, prefix):
+	"""
+	Adds a file to the list of marshal files to process. Store them in the attribute *marshal_list*.
+
+	:param filename: xml file to compile
+	:type filename: string
+	:param prefix: marshal prefix (--prefix=prefix)
+	:type prefix: string
+	"""
+	if not hasattr(self, 'marshal_list'):
+		self.marshal_list = []
+	self.meths.append('process_marshal')
+	self.marshal_list.append((filename, prefix))
+
+@before_method('process_source')
+def process_marshal(self):
+	"""
+	Processes the marshal files stored in the attribute *marshal_list* to create :py:class:`waflib.Tools.glib2.glib_genmarshal` instances.
+	Adds the c file created to the list of source to process.
+	"""
+	for f, prefix in getattr(self, 'marshal_list', []):
+		node = self.path.find_resource(f)
+
+		if not node:
+			raise Errors.WafError('file not found %r' % f)
+
+		h_node = node.change_ext('.h')
+		c_node = node.change_ext('.c')
+
+		task = self.create_task('glib_genmarshal', node, [h_node, c_node])
+		task.env.GLIB_GENMARSHAL_PREFIX = prefix
+	self.source = self.to_nodes(getattr(self, 'source', []))
+	self.source.append(c_node)
+
+class glib_genmarshal(Task.Task):
+	vars    = ['GLIB_GENMARSHAL_PREFIX', 'GLIB_GENMARSHAL']
+	color   = 'BLUE'
+	ext_out = ['.h']
+	def run(self):
+		bld = self.generator.bld
+
+		get = self.env.get_flat
+		cmd1 = "%s %s --prefix=%s --header > %s" % (
+			get('GLIB_GENMARSHAL'),
+			self.inputs[0].srcpath(),
+			get('GLIB_GENMARSHAL_PREFIX'),
+			self.outputs[0].abspath()
+		)
+
+		ret = bld.exec_command(cmd1)
+		if ret:
+			return ret
+
+		#print self.outputs[1].abspath()
+		c = '''#include "%s"\n''' % self.outputs[0].name
+		self.outputs[1].write(c)
+
+		cmd2 = "%s %s --prefix=%s --body >> %s" % (
+			get('GLIB_GENMARSHAL'),
+			self.inputs[0].srcpath(),
+			get('GLIB_GENMARSHAL_PREFIX'),
+			self.outputs[1].abspath()
+		)
+		return bld.exec_command(cmd2)
+
+########################## glib-mkenums
+
+@taskgen_method
+def add_enums_from_template(self, source='', target='', template='', comments=''):
+	"""
+	Adds a file to the list of enum files to process. Stores them in the attribute *enums_list*.
+
+	:param source: enum file to process
+	:type source: string
+	:param target: target file
+	:type target: string
+	:param template: template file
+	:type template: string
+	:param comments: comments
+	:type comments: string
+	"""
+	if not hasattr(self, 'enums_list'):
+		self.enums_list = []
+	self.meths.append('process_enums')
+	self.enums_list.append({'source': source,
+	                        'target': target,
+	                        'template': template,
+	                        'file-head': '',
+	                        'file-prod': '',
+	                        'file-tail': '',
+	                        'enum-prod': '',
+	                        'value-head': '',
+	                        'value-prod': '',
+	                        'value-tail': '',
+	                        'comments': comments})
+
+@taskgen_method
+def add_enums(self, source='', target='',
+              file_head='', file_prod='', file_tail='', enum_prod='',
+              value_head='', value_prod='', value_tail='', comments=''):
+	"""
+	Adds a file to the list of enum files to process. Stores them in the attribute *enums_list*.
+
+	:param source: enum file to process
+	:type source: string
+	:param target: target file
+	:type target: string
+	:param file_head: unused
+	:param file_prod: unused
+	:param file_tail: unused
+	:param enum_prod: unused
+	:param value_head: unused
+	:param value_prod: unused
+	:param value_tail: unused
+	:param comments: comments
+	:type comments: string
+	"""
+	if not hasattr(self, 'enums_list'):
+		self.enums_list = []
+	self.meths.append('process_enums')
+	self.enums_list.append({'source': source,
+	                        'template': '',
+	                        'target': target,
+	                        'file-head': file_head,
+	                        'file-prod': file_prod,
+	                        'file-tail': file_tail,
+	                        'enum-prod': enum_prod,
+	                        'value-head': value_head,
+	                        'value-prod': value_prod,
+	                        'value-tail': value_tail,
+	                        'comments': comments})
+
+@before_method('process_source')
+def process_enums(self):
+	"""
+	Processes the enum files stored in the attribute *enum_list* to create :py:class:`waflib.Tools.glib2.glib_mkenums` instances.
+	"""
+	for enum in getattr(self, 'enums_list', []):
+		task = self.create_task('glib_mkenums')
+		env = task.env
+
+		inputs = []
+
+		# process the source
+		source_list = self.to_list(enum['source'])
+		if not source_list:
+			raise Errors.WafError('missing source ' + str(enum))
+		source_list = [self.path.find_resource(k) for k in source_list]
+		inputs += source_list
+		env.GLIB_MKENUMS_SOURCE = [k.abspath() for k in source_list]
+
+		# find the target
+		if not enum['target']:
+			raise Errors.WafError('missing target ' + str(enum))
+		tgt_node = self.path.find_or_declare(enum['target'])
+		if tgt_node.name.endswith('.c'):
+			self.source.append(tgt_node)
+		env.GLIB_MKENUMS_TARGET = tgt_node.abspath()
+
+
+		options = []
+
+		if enum['template']: # template, if provided
+			template_node = self.path.find_resource(enum['template'])
+			options.append('--template %s' % (template_node.abspath()))
+			inputs.append(template_node)
+		params = {'file-head' : '--fhead',
+		           'file-prod' : '--fprod',
+		           'file-tail' : '--ftail',
+		           'enum-prod' : '--eprod',
+		           'value-head' : '--vhead',
+		           'value-prod' : '--vprod',
+		           'value-tail' : '--vtail',
+		           'comments': '--comments'}
+		for param, option in params.items():
+			if enum[param]:
+				options.append('%s %r' % (option, enum[param]))
+
+		env.GLIB_MKENUMS_OPTIONS = ' '.join(options)
+
+		# update the task instance
+		task.set_inputs(inputs)
+		task.set_outputs(tgt_node)
+
+class glib_mkenums(Task.Task):
+	"""
+	Processes enum files
+	"""
+	run_str = '${GLIB_MKENUMS} ${GLIB_MKENUMS_OPTIONS} ${GLIB_MKENUMS_SOURCE} > ${GLIB_MKENUMS_TARGET}'
+	color   = 'PINK'
+	ext_out = ['.h']
+
+######################################### gsettings
+
+@taskgen_method
+def add_settings_schemas(self, filename_list):
+	"""
+	Adds settings files to process to *settings_schema_files*
+
+	:param filename_list: files
+	:type filename_list: list of string
+	"""
+	if not hasattr(self, 'settings_schema_files'):
+		self.settings_schema_files = []
+
+	if not isinstance(filename_list, list):
+		filename_list = [filename_list]
+
+	self.settings_schema_files.extend(filename_list)
+
+@taskgen_method
+def add_settings_enums(self, namespace, filename_list):
+	"""
+	Called only once by task generator to set the enums namespace.
+
+	:param namespace: namespace
+	:type namespace: string
+	:param filename_list: enum files to process
+	:type filename_list: file list
+	"""
+	if hasattr(self, 'settings_enum_namespace'):
+		raise Errors.WafError("Tried to add gsettings enums to %r more than once" % self.name)
+	self.settings_enum_namespace = namespace
+
+	if not isinstance(filename_list, list):
+		filename_list = [filename_list]
+	self.settings_enum_files = filename_list
+
+@feature('glib2')
+def process_settings(self):
+	"""
+	Processes the schema files in *settings_schema_files* to create :py:class:`waflib.Tools.glib2.glib_mkenums` instances. The
+	same files are validated through :py:class:`waflib.Tools.glib2.glib_validate_schema` tasks.
+
+	"""
+	enums_tgt_node = []
+	install_files = []
+
+	settings_schema_files = getattr(self, 'settings_schema_files', [])
+	if settings_schema_files and not self.env.GLIB_COMPILE_SCHEMAS:
+		raise Errors.WafError ("Unable to process GSettings schemas - glib-compile-schemas was not found during configure")
+
+	# 1. process gsettings_enum_files (generate .enums.xml)
+	#
+	if hasattr(self, 'settings_enum_files'):
+		enums_task = self.create_task('glib_mkenums')
+
+		source_list = self.settings_enum_files
+		source_list = [self.path.find_resource(k) for k in source_list]
+		enums_task.set_inputs(source_list)
+		enums_task.env.GLIB_MKENUMS_SOURCE = [k.abspath() for k in source_list]
+
+		target = self.settings_enum_namespace + '.enums.xml'
+		tgt_node = self.path.find_or_declare(target)
+		enums_task.set_outputs(tgt_node)
+		enums_task.env.GLIB_MKENUMS_TARGET = tgt_node.abspath()
+		enums_tgt_node = [tgt_node]
+
+		install_files.append(tgt_node)
+
+		options = '--comments "<!-- @comment@ -->" --fhead "<schemalist>" --vhead "  <@type@ id=\\"%s.@EnumName@\\">" --vprod "    <value nick=\\"@valuenick@\\" value=\\"@valuenum@\\"/>" --vtail "  </@type@>" --ftail "</schemalist>" ' % (self.settings_enum_namespace)
+		enums_task.env.GLIB_MKENUMS_OPTIONS = options
+
+	# 2. process gsettings_schema_files (validate .gschema.xml files)
+	#
+	for schema in settings_schema_files:
+		schema_task = self.create_task ('glib_validate_schema')
+
+		schema_node = self.path.find_resource(schema)
+		if not schema_node:
+			raise Errors.WafError("Cannot find the schema file %r" % schema)
+		install_files.append(schema_node)
+		source_list = enums_tgt_node + [schema_node]
+
+		schema_task.set_inputs (source_list)
+		schema_task.env.GLIB_COMPILE_SCHEMAS_OPTIONS = [("--schema-file=" + k.abspath()) for k in source_list]
+
+		target_node = schema_node.change_ext('.xml.valid')
+		schema_task.set_outputs (target_node)
+		schema_task.env.GLIB_VALIDATE_SCHEMA_OUTPUT = target_node.abspath()
+
+	# 3. schemas install task
+	def compile_schemas_callback(bld):
+		if not bld.is_install:
+			return
+		compile_schemas = Utils.to_list(bld.env.GLIB_COMPILE_SCHEMAS)
+		destdir = Options.options.destdir
+		paths = bld._compile_schemas_registered
+		if destdir:
+			paths = (os.path.join(destdir, path.lstrip(os.sep)) for path in paths)
+		for path in paths:
+			Logs.pprint('YELLOW', 'Updating GSettings schema cache %r' % path)
+			if self.bld.exec_command(compile_schemas + [path]):
+				Logs.warn('Could not update GSettings schema cache %r' % path)
+
+	if self.bld.is_install:
+		schemadir = self.env.GSETTINGSSCHEMADIR
+		if not schemadir:
+			raise Errors.WafError ('GSETTINGSSCHEMADIR not defined (should have been set up automatically during configure)')
+
+		if install_files:
+			self.add_install_files(install_to=schemadir, install_from=install_files)
+			registered_schemas = getattr(self.bld, '_compile_schemas_registered', None)
+			if not registered_schemas:
+				registered_schemas = self.bld._compile_schemas_registered = set()
+				self.bld.add_post_fun(compile_schemas_callback)
+			registered_schemas.add(schemadir)
+
+class glib_validate_schema(Task.Task):
+	"""
+	Validates schema files
+	"""
+	run_str = 'rm -f ${GLIB_VALIDATE_SCHEMA_OUTPUT} && ${GLIB_COMPILE_SCHEMAS} --dry-run ${GLIB_COMPILE_SCHEMAS_OPTIONS} && touch ${GLIB_VALIDATE_SCHEMA_OUTPUT}'
+	color   = 'PINK'
+
+################## gresource
+
+@extension('.gresource.xml')
+def process_gresource_source(self, node):
+	"""
+	Creates tasks that turn ``.gresource.xml`` files to C code
+	"""
+	if not self.env.GLIB_COMPILE_RESOURCES:
+		raise Errors.WafError ("Unable to process GResource file - glib-compile-resources was not found during configure")
+
+	if 'gresource' in self.features:
+		return
+
+	h_node = node.change_ext('_xml.h')
+	c_node = node.change_ext('_xml.c')
+	self.create_task('glib_gresource_source', node, [h_node, c_node])
+	self.source.append(c_node)
+
+@feature('gresource')
+def process_gresource_bundle(self):
+	"""
+	Creates tasks to turn ``.gresource`` files from ``.gresource.xml`` files::
+
+		def build(bld):
+			bld(
+				features='gresource',
+				source=['resources1.gresource.xml', 'resources2.gresource.xml'],
+				install_path='${LIBDIR}/${PACKAGE}'
+			)
+
+	:param source: XML files to process
+	:type source: list of string
+	:param install_path: installation path
+	:type install_path: string
+	"""
+	for i in self.to_list(self.source):
+		node = self.path.find_resource(i)
+
+		task = self.create_task('glib_gresource_bundle', node, node.change_ext(''))
+		inst_to = getattr(self, 'install_path', None)
+		if inst_to:
+			self.add_install_files(install_to=inst_to, install_from=task.outputs)
+
+class glib_gresource_base(Task.Task):
+	"""
+	Base class for gresource based tasks
+	"""
+	color    = 'BLUE'
+	base_cmd = '${GLIB_COMPILE_RESOURCES} --sourcedir=${SRC[0].parent.srcpath()} --sourcedir=${SRC[0].bld_dir()}'
+
+	def scan(self):
+		"""
+		Scans gresource dependencies through ``glib-compile-resources --generate-dependencies command``
+		"""
+		bld = self.generator.bld
+		kw = {}
+		kw['cwd'] = self.get_cwd()
+		kw['quiet'] = Context.BOTH
+
+		cmd = Utils.subst_vars('${GLIB_COMPILE_RESOURCES} --sourcedir=%s --sourcedir=%s --generate-dependencies %s' % (
+			self.inputs[0].parent.srcpath(),
+			self.inputs[0].bld_dir(),
+			self.inputs[0].bldpath()
+		), self.env)
+
+		output = bld.cmd_and_log(cmd, **kw)
+
+		nodes = []
+		names = []
+		for dep in output.splitlines():
+			if dep:
+				node = bld.bldnode.find_node(dep)
+				if node:
+					nodes.append(node)
+				else:
+					names.append(dep)
+
+		return (nodes, names)
+
+class glib_gresource_source(glib_gresource_base):
+	"""
+	Task to generate C source code (.h and .c files) from a gresource.xml file
+	"""
+	vars    = ['GLIB_COMPILE_RESOURCES']
+	fun_h   = Task.compile_fun_shell(glib_gresource_base.base_cmd + ' --target=${TGT[0].abspath()} --generate-header ${SRC}')
+	fun_c   = Task.compile_fun_shell(glib_gresource_base.base_cmd + ' --target=${TGT[1].abspath()} --generate-source ${SRC}')
+	ext_out = ['.h']
+
+	def run(self):
+		return self.fun_h[0](self) or self.fun_c[0](self)
+
+class glib_gresource_bundle(glib_gresource_base):
+	"""
+	Task to generate a .gresource binary file from a gresource.xml file
+	"""
+	run_str = glib_gresource_base.base_cmd + ' --target=${TGT} ${SRC}'
+	shell   = True # temporary workaround for #795
+
+@conf
+def find_glib_genmarshal(conf):
+	conf.find_program('glib-genmarshal', var='GLIB_GENMARSHAL')
+
+@conf
+def find_glib_mkenums(conf):
+	if not conf.env.PERL:
+		conf.find_program('perl', var='PERL')
+	conf.find_program('glib-mkenums', interpreter='PERL', var='GLIB_MKENUMS')
+
+@conf
+def find_glib_compile_schemas(conf):
+	# when cross-compiling, gsettings.m4 locates the program with the following:
+	#   pkg-config --variable glib_compile_schemas gio-2.0
+	conf.find_program('glib-compile-schemas', var='GLIB_COMPILE_SCHEMAS')
+
+	def getstr(varname):
+		return getattr(Options.options, varname, getattr(conf.env,varname, ''))
+
+	gsettingsschemadir = getstr('GSETTINGSSCHEMADIR')
+	if not gsettingsschemadir:
+		datadir = getstr('DATADIR')
+		if not datadir:
+			prefix = conf.env.PREFIX
+			datadir = os.path.join(prefix, 'share')
+		gsettingsschemadir = os.path.join(datadir, 'glib-2.0', 'schemas')
+
+	conf.env.GSETTINGSSCHEMADIR = gsettingsschemadir
+
+@conf
+def find_glib_compile_resources(conf):
+	conf.find_program('glib-compile-resources', var='GLIB_COMPILE_RESOURCES')
+
+def configure(conf):
+	"""
+	Finds the following programs:
+
+	* *glib-genmarshal* and set *GLIB_GENMARSHAL*
+	* *glib-mkenums* and set *GLIB_MKENUMS*
+	* *glib-compile-schemas* and set *GLIB_COMPILE_SCHEMAS* (not mandatory)
+	* *glib-compile-resources* and set *GLIB_COMPILE_RESOURCES* (not mandatory)
+	"""
+	conf.find_glib_genmarshal()
+	conf.find_glib_mkenums()
+	conf.find_glib_compile_schemas(mandatory=False)
+	conf.find_glib_compile_resources(mandatory=False)
+
+def options(opt):
+	"""
+	Adds the ``--gsettingsschemadir`` command-line option
+	"""
+	gr = opt.add_option_group('Installation directories')
+	gr.add_option('--gsettingsschemadir', help='GSettings schema location [DATADIR/glib-2.0/schemas]', default='', dest='GSETTINGSSCHEMADIR')
+
diff --git a/waflib/Tools/gnu_dirs.py b/waflib/Tools/gnu_dirs.py
new file mode 100644
index 00000000..2847071d
--- /dev/null
+++ b/waflib/Tools/gnu_dirs.py
@@ -0,0 +1,131 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Ali Sabil, 2007
+
+"""
+Sets various standard variables such as INCLUDEDIR. SBINDIR and others. To use this module just call::
+
+	opt.load('gnu_dirs')
+
+and::
+
+	conf.load('gnu_dirs')
+
+Add options for the standard GNU directories, this tool will add the options
+found in autotools, and will update the environment with the following
+installation variables:
+
+============== ========================================= =======================
+Variable       Description                               Default Value
+============== ========================================= =======================
+PREFIX         installation prefix                       /usr/local
+EXEC_PREFIX    installation prefix for binaries          PREFIX
+BINDIR         user commands                             EXEC_PREFIX/bin
+SBINDIR        system binaries                           EXEC_PREFIX/sbin
+LIBEXECDIR     program-specific binaries                 EXEC_PREFIX/libexec
+SYSCONFDIR     host-specific configuration               PREFIX/etc
+SHAREDSTATEDIR architecture-independent variable data    PREFIX/com
+LOCALSTATEDIR  variable data                             PREFIX/var
+LIBDIR         object code libraries                     EXEC_PREFIX/lib
+INCLUDEDIR     header files                              PREFIX/include
+OLDINCLUDEDIR  header files for non-GCC compilers        /usr/include
+DATAROOTDIR    architecture-independent data root        PREFIX/share
+DATADIR        architecture-independent data             DATAROOTDIR
+INFODIR        GNU "info" documentation                  DATAROOTDIR/info
+LOCALEDIR      locale-dependent data                     DATAROOTDIR/locale
+MANDIR         manual pages                              DATAROOTDIR/man
+DOCDIR         documentation root                        DATAROOTDIR/doc/APPNAME
+HTMLDIR        HTML documentation                        DOCDIR
+DVIDIR         DVI documentation                         DOCDIR
+PDFDIR         PDF documentation                         DOCDIR
+PSDIR          PostScript documentation                  DOCDIR
+============== ========================================= =======================
+"""
+
+import os, re
+from waflib import Utils, Options, Context
+
+gnuopts = '''
+bindir, user commands, ${EXEC_PREFIX}/bin
+sbindir, system binaries, ${EXEC_PREFIX}/sbin
+libexecdir, program-specific binaries, ${EXEC_PREFIX}/libexec
+sysconfdir, host-specific configuration, ${PREFIX}/etc
+sharedstatedir, architecture-independent variable data, ${PREFIX}/com
+localstatedir, variable data, ${PREFIX}/var
+libdir, object code libraries, ${EXEC_PREFIX}/lib%s
+includedir, header files, ${PREFIX}/include
+oldincludedir, header files for non-GCC compilers, /usr/include
+datarootdir, architecture-independent data root, ${PREFIX}/share
+datadir, architecture-independent data, ${DATAROOTDIR}
+infodir, GNU "info" documentation, ${DATAROOTDIR}/info
+localedir, locale-dependent data, ${DATAROOTDIR}/locale
+mandir, manual pages, ${DATAROOTDIR}/man
+docdir, documentation root, ${DATAROOTDIR}/doc/${PACKAGE}
+htmldir, HTML documentation, ${DOCDIR}
+dvidir, DVI documentation, ${DOCDIR}
+pdfdir, PDF documentation, ${DOCDIR}
+psdir, PostScript documentation, ${DOCDIR}
+''' % Utils.lib64()
+
+_options = [x.split(', ') for x in gnuopts.splitlines() if x]
+
+def configure(conf):
+	"""
+	Reads the command-line options to set lots of variables in *conf.env*. The variables
+	BINDIR and LIBDIR will be overwritten.
+	"""
+	def get_param(varname, default):
+		return getattr(Options.options, varname, '') or default
+
+	env = conf.env
+	env.LIBDIR = env.BINDIR = []
+	env.EXEC_PREFIX = get_param('EXEC_PREFIX', env.PREFIX)
+	env.PACKAGE = getattr(Context.g_module, 'APPNAME', None) or env.PACKAGE
+
+	complete = False
+	iter = 0
+	while not complete and iter < len(_options) + 1:
+		iter += 1
+		complete = True
+		for name, help, default in _options:
+			name = name.upper()
+			if not env[name]:
+				try:
+					env[name] = Utils.subst_vars(get_param(name, default).replace('/', os.sep), env)
+				except TypeError:
+					complete = False
+
+	if not complete:
+		lst = [x for x, _, _ in _options if not env[x.upper()]]
+		raise conf.errors.WafError('Variable substitution failure %r' % lst)
+
+def options(opt):
+	"""
+	Adds lots of command-line options, for example::
+
+		--exec-prefix: EXEC_PREFIX
+	"""
+	inst_dir = opt.add_option_group('Installation prefix',
+'By default, "waf install" will put the files in\
+ "/usr/local/bin", "/usr/local/lib" etc. An installation prefix other\
+ than "/usr/local" can be given using "--prefix", for example "--prefix=$HOME"')
+
+	for k in ('--prefix', '--destdir'):
+		option = opt.parser.get_option(k)
+		if option:
+			opt.parser.remove_option(k)
+			inst_dir.add_option(option)
+
+	inst_dir.add_option('--exec-prefix',
+		help = 'installation prefix for binaries [PREFIX]',
+		default = '',
+		dest = 'EXEC_PREFIX')
+
+	dirs_options = opt.add_option_group('Installation directories')
+
+	for name, help, default in _options:
+		option_name = '--' + name
+		str_default = default
+		str_help = '%s [%s]' % (help, re.sub(r'\$\{([^}]+)\}', r'\1', str_default))
+		dirs_options.add_option(option_name, help=str_help, default='', dest=name.upper())
+
diff --git a/waflib/Tools/gxx.py b/waflib/Tools/gxx.py
index 9cf52070..22c5d26f 100644
--- a/waflib/Tools/gxx.py
+++ b/waflib/Tools/gxx.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 # Ralf Habacker, 2006 (rh)
 # Yinon Ehrlich, 2009
 
@@ -14,7 +14,7 @@
 @conf
 def find_gxx(conf):
 	"""
-	Find the program g++, and if present, try to detect its version number
+	Finds the program g++, and if present, try to detect its version number
 	"""
 	cxx = conf.find_program(['g++', 'c++'], var='CXX')
 	conf.get_cc_version(cxx, gcc=True)
@@ -27,54 +27,51 @@ def gxx_common_flags(conf):
 	"""
 	v = conf.env
 
-	v['CXX_SRC_F']           = []
-	v['CXX_TGT_F']           = ['-c', '-o']
+	v.CXX_SRC_F           = []
+	v.CXX_TGT_F           = ['-c', '-o']
 
-	# linker
-	if not v['LINK_CXX']: v['LINK_CXX'] = v['CXX']
-	v['CXXLNK_SRC_F']        = []
-	v['CXXLNK_TGT_F']        = ['-o']
-	v['CPPPATH_ST']          = '-I%s'
-	v['DEFINES_ST']          = '-D%s'
+	if not v.LINK_CXX:
+		v.LINK_CXX = v.CXX
 
-	v['LIB_ST']              = '-l%s' # template for adding libs
-	v['LIBPATH_ST']          = '-L%s' # template for adding libpaths
-	v['STLIB_ST']            = '-l%s'
-	v['STLIBPATH_ST']        = '-L%s'
-	v['RPATH_ST']            = '-Wl,-rpath,%s'
+	v.CXXLNK_SRC_F        = []
+	v.CXXLNK_TGT_F        = ['-o']
+	v.CPPPATH_ST          = '-I%s'
+	v.DEFINES_ST          = '-D%s'
 
-	v['SONAME_ST']           = '-Wl,-h,%s'
-	v['SHLIB_MARKER']        = '-Wl,-Bdynamic'
-	v['STLIB_MARKER']        = '-Wl,-Bstatic'
+	v.LIB_ST              = '-l%s' # template for adding libs
+	v.LIBPATH_ST          = '-L%s' # template for adding libpaths
+	v.STLIB_ST            = '-l%s'
+	v.STLIBPATH_ST        = '-L%s'
+	v.RPATH_ST            = '-Wl,-rpath,%s'
 
-	# program
-	v['cxxprogram_PATTERN']  = '%s'
+	v.SONAME_ST           = '-Wl,-h,%s'
+	v.SHLIB_MARKER        = '-Wl,-Bdynamic'
+	v.STLIB_MARKER        = '-Wl,-Bstatic'
 
-	# shared library
-	v['CXXFLAGS_cxxshlib']   = ['-fPIC']
-	v['LINKFLAGS_cxxshlib']  = ['-shared']
-	v['cxxshlib_PATTERN']    = 'lib%s.so'
+	v.cxxprogram_PATTERN  = '%s'
 
-	# static lib
-	v['LINKFLAGS_cxxstlib']  = ['-Wl,-Bstatic']
-	v['cxxstlib_PATTERN']    = 'lib%s.a'
+	v.CXXFLAGS_cxxshlib   = ['-fPIC']
+	v.LINKFLAGS_cxxshlib  = ['-shared']
+	v.cxxshlib_PATTERN    = 'lib%s.so'
 
-	# osx stuff
-	v['LINKFLAGS_MACBUNDLE'] = ['-bundle', '-undefined', 'dynamic_lookup']
-	v['CXXFLAGS_MACBUNDLE']  = ['-fPIC']
-	v['macbundle_PATTERN']   = '%s.bundle'
+	v.LINKFLAGS_cxxstlib  = ['-Wl,-Bstatic']
+	v.cxxstlib_PATTERN    = 'lib%s.a'
+
+	v.LINKFLAGS_MACBUNDLE = ['-bundle', '-undefined', 'dynamic_lookup']
+	v.CXXFLAGS_MACBUNDLE  = ['-fPIC']
+	v.macbundle_PATTERN   = '%s.bundle'
 
 @conf
 def gxx_modifier_win32(conf):
 	"""Configuration flags for executing gcc on Windows"""
 	v = conf.env
-	v['cxxprogram_PATTERN']  = '%s.exe'
+	v.cxxprogram_PATTERN  = '%s.exe'
 
-	v['cxxshlib_PATTERN']    = '%s.dll'
-	v['implib_PATTERN']      = 'lib%s.dll.a'
-	v['IMPLIB_ST']           = '-Wl,--out-implib,%s'
+	v.cxxshlib_PATTERN    = '%s.dll'
+	v.implib_PATTERN      = '%s.dll.a'
+	v.IMPLIB_ST           = '-Wl,--out-implib,%s'
 
-	v['CXXFLAGS_cxxshlib']   = []
+	v.CXXFLAGS_cxxshlib   = []
 
 	# Auto-import is enabled by default even without this option,
 	# but enabling it explicitly has the nice effect of suppressing the rather boring, debug-level messages
@@ -86,43 +83,43 @@ def gxx_modifier_cygwin(conf):
 	"""Configuration flags for executing g++ on Cygwin"""
 	gxx_modifier_win32(conf)
 	v = conf.env
-	v['cxxshlib_PATTERN']    = 'cyg%s.dll'
+	v.cxxshlib_PATTERN    = 'cyg%s.dll'
 	v.append_value('LINKFLAGS_cxxshlib', ['-Wl,--enable-auto-image-base'])
-	v['CXXFLAGS_cxxshlib']   = []
+	v.CXXFLAGS_cxxshlib   = []
 
 @conf
 def gxx_modifier_darwin(conf):
 	"""Configuration flags for executing g++ on MacOS"""
 	v = conf.env
-	v['CXXFLAGS_cxxshlib']   = ['-fPIC']
-	v['LINKFLAGS_cxxshlib']  = ['-dynamiclib']
-	v['cxxshlib_PATTERN']    = 'lib%s.dylib'
-	v['FRAMEWORKPATH_ST']    = '-F%s'
-	v['FRAMEWORK_ST']        = ['-framework']
-	v['ARCH_ST']             = ['-arch']
+	v.CXXFLAGS_cxxshlib   = ['-fPIC']
+	v.LINKFLAGS_cxxshlib  = ['-dynamiclib']
+	v.cxxshlib_PATTERN    = 'lib%s.dylib'
+	v.FRAMEWORKPATH_ST    = '-F%s'
+	v.FRAMEWORK_ST        = ['-framework']
+	v.ARCH_ST             = ['-arch']
 
-	v['LINKFLAGS_cxxstlib']  = []
+	v.LINKFLAGS_cxxstlib  = []
 
-	v['SHLIB_MARKER']        = []
-	v['STLIB_MARKER']        = []
-	v['SONAME_ST']           = []
+	v.SHLIB_MARKER        = []
+	v.STLIB_MARKER        = []
+	v.SONAME_ST           = []
 
 @conf
 def gxx_modifier_aix(conf):
 	"""Configuration flags for executing g++ on AIX"""
 	v = conf.env
-	v['LINKFLAGS_cxxprogram']= ['-Wl,-brtl']
+	v.LINKFLAGS_cxxprogram= ['-Wl,-brtl']
 
-	v['LINKFLAGS_cxxshlib']  = ['-shared', '-Wl,-brtl,-bexpfull']
-	v['SHLIB_MARKER']        = []
+	v.LINKFLAGS_cxxshlib  = ['-shared', '-Wl,-brtl,-bexpfull']
+	v.SHLIB_MARKER        = []
 
 @conf
 def gxx_modifier_hpux(conf):
 	v = conf.env
-	v['SHLIB_MARKER']        = []
-	v['STLIB_MARKER']        = []
-	v['CFLAGS_cxxshlib']     = ['-fPIC','-DPIC']
-	v['cxxshlib_PATTERN']    = 'lib%s.sl'
+	v.SHLIB_MARKER        = []
+	v.STLIB_MARKER        = []
+	v.CFLAGS_cxxshlib     = ['-fPIC','-DPIC']
+	v.cxxshlib_PATTERN    = 'lib%s.sl'
 
 @conf
 def gxx_modifier_openbsd(conf):
@@ -131,9 +128,9 @@ def gxx_modifier_openbsd(conf):
 @conf
 def gcc_modifier_osf1V(conf):
 	v = conf.env
-	v['SHLIB_MARKER']        = []
-	v['STLIB_MARKER']        = []
-	v['SONAME_ST']           = []
+	v.SHLIB_MARKER        = []
+	v.STLIB_MARKER        = []
+	v.SONAME_ST           = []
 
 @conf
 def gxx_modifier_platform(conf):
@@ -156,4 +153,5 @@ def configure(conf):
 	conf.cxx_load_tools()
 	conf.cxx_add_flags()
 	conf.link_add_flags()
+	conf.check_gcc_o_space('cxx')
 
diff --git a/waflib/Tools/icc.py b/waflib/Tools/icc.py
index f3395030..b6492c8e 100644
--- a/waflib/Tools/icc.py
+++ b/waflib/Tools/icc.py
@@ -1,10 +1,10 @@
 #!/usr/bin/env python
 # encoding: utf-8
 # Stian Selnes 2008
-# Thomas Nagy 2009-2010 (ita)
+# Thomas Nagy 2009-2018 (ita)
 
 """
-Detect the Intel C compiler
+Detects the Intel C compiler
 """
 
 import sys
@@ -14,11 +14,8 @@
 @conf
 def find_icc(conf):
 	"""
-	Find the program icc and execute it to ensure it really is icc
+	Finds the program icc and execute it to ensure it really is icc
 	"""
-	if sys.platform == 'cygwin':
-		conf.fatal('The Intel compiler does not work on Cygwin')
-
 	cc = conf.find_program(['icc', 'ICL'], var='CC')
 	conf.get_cc_version(cc, icc=True)
 	conf.env.CC_NAME = 'icc'
diff --git a/waflib/Tools/icpc.py b/waflib/Tools/icpc.py
index 03603d44..8a6cc6c4 100644
--- a/waflib/Tools/icpc.py
+++ b/waflib/Tools/icpc.py
@@ -1,9 +1,9 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy 2009-2010 (ita)
+# Thomas Nagy 2009-2018 (ita)
 
 """
-Detect the Intel C++ compiler
+Detects the Intel C++ compiler
 """
 
 import sys
@@ -13,11 +13,8 @@
 @conf
 def find_icpc(conf):
 	"""
-	Find the program icpc, and execute it to ensure it really is icpc
+	Finds the program icpc, and execute it to ensure it really is icpc
 	"""
-	if sys.platform == 'cygwin':
-		conf.fatal('The Intel compiler does not work on Cygwin')
-
 	cxx = conf.find_program('icpc', var='CXX')
 	conf.get_cc_version(cxx, icc=True)
 	conf.env.CXX_NAME = 'icc'
diff --git a/waflib/Tools/ifort.py b/waflib/Tools/ifort.py
new file mode 100644
index 00000000..74934f3f
--- /dev/null
+++ b/waflib/Tools/ifort.py
@@ -0,0 +1,413 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# DC 2008
+# Thomas Nagy 2016-2018 (ita)
+
+import os, re, traceback
+from waflib import Utils, Logs, Errors
+from waflib.Tools import fc, fc_config, fc_scan, ar, ccroot
+from waflib.Configure import conf
+from waflib.TaskGen import after_method, feature
+
+@conf
+def find_ifort(conf):
+	fc = conf.find_program('ifort', var='FC')
+	conf.get_ifort_version(fc)
+	conf.env.FC_NAME = 'IFORT'
+
+@conf
+def ifort_modifier_win32(self):
+	v = self.env
+	v.IFORT_WIN32 = True
+	v.FCSTLIB_MARKER = ''
+	v.FCSHLIB_MARKER = ''
+
+	v.FCLIB_ST = v.FCSTLIB_ST = '%s.lib'
+	v.FCLIBPATH_ST = v.STLIBPATH_ST = '/LIBPATH:%s'
+	v.FCINCPATH_ST = '/I%s'
+	v.FCDEFINES_ST = '/D%s'
+
+	v.fcprogram_PATTERN = v.fcprogram_test_PATTERN = '%s.exe'
+	v.fcshlib_PATTERN = '%s.dll'
+	v.fcstlib_PATTERN = v.implib_PATTERN = '%s.lib'
+
+	v.FCLNK_TGT_F = '/out:'
+	v.FC_TGT_F = ['/c', '/o', '']
+	v.FCFLAGS_fcshlib = ''
+	v.LINKFLAGS_fcshlib = '/DLL'
+	v.AR_TGT_F = '/out:'
+	v.IMPLIB_ST = '/IMPLIB:%s'
+
+	v.append_value('LINKFLAGS', '/subsystem:console')
+	if v.IFORT_MANIFEST:
+		v.append_value('LINKFLAGS', ['/MANIFEST'])
+
+@conf
+def ifort_modifier_darwin(conf):
+	fc_config.fortran_modifier_darwin(conf)
+
+@conf
+def ifort_modifier_platform(conf):
+	dest_os = conf.env.DEST_OS or Utils.unversioned_sys_platform()
+	ifort_modifier_func = getattr(conf, 'ifort_modifier_' + dest_os, None)
+	if ifort_modifier_func:
+		ifort_modifier_func()
+
+@conf
+def get_ifort_version(conf, fc):
+	"""
+	Detects the compiler version and sets ``conf.env.FC_VERSION``
+	"""
+	version_re = re.compile(r"\bIntel\b.*\bVersion\s*(?P<major>\d*)\.(?P<minor>\d*)",re.I).search
+	if Utils.is_win32:
+		cmd = fc
+	else:
+		cmd = fc + ['-logo']
+
+	out, err = fc_config.getoutput(conf, cmd, stdin=False)
+	match = version_re(out) or version_re(err)
+	if not match:
+		conf.fatal('cannot determine ifort version.')
+	k = match.groupdict()
+	conf.env.FC_VERSION = (k['major'], k['minor'])
+
+def configure(conf):
+	"""
+	Detects the Intel Fortran compilers
+	"""
+	if Utils.is_win32:
+		compiler, version, path, includes, libdirs, arch = conf.detect_ifort()
+		v = conf.env
+		v.DEST_CPU = arch
+		v.PATH = path
+		v.INCLUDES = includes
+		v.LIBPATH = libdirs
+		v.MSVC_COMPILER = compiler
+		try:
+			v.MSVC_VERSION = float(version)
+		except ValueError:
+			v.MSVC_VERSION = float(version[:-3])
+
+		conf.find_ifort_win32()
+		conf.ifort_modifier_win32()
+	else:
+		conf.find_ifort()
+		conf.find_program('xiar', var='AR')
+		conf.find_ar()
+		conf.fc_flags()
+		conf.fc_add_flags()
+		conf.ifort_modifier_platform()
+
+
+all_ifort_platforms = [ ('intel64', 'amd64'), ('em64t', 'amd64'), ('ia32', 'x86'), ('Itanium', 'ia64')]
+"""List of icl platforms"""
+
+@conf
+def gather_ifort_versions(conf, versions):
+	"""
+	List compiler versions by looking up registry keys
+	"""
+	version_pattern = re.compile('^...?.?\....?.?')
+	try:
+		all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Wow6432node\\Intel\\Compilers\\Fortran')
+	except OSError:
+		try:
+			all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Intel\\Compilers\\Fortran')
+		except OSError:
+			return
+	index = 0
+	while 1:
+		try:
+			version = Utils.winreg.EnumKey(all_versions, index)
+		except OSError:
+			break
+		index += 1
+		if not version_pattern.match(version):
+			continue
+		targets = {}
+		for target,arch in all_ifort_platforms:
+			if target=='intel64':
+				targetDir='EM64T_NATIVE'
+			else:
+				targetDir=target
+			try:
+				Utils.winreg.OpenKey(all_versions,version+'\\'+targetDir)
+				icl_version=Utils.winreg.OpenKey(all_versions,version)
+				path,type=Utils.winreg.QueryValueEx(icl_version,'ProductDir')
+			except OSError:
+				pass
+			else:
+				batch_file=os.path.join(path,'bin','ifortvars.bat')
+				if os.path.isfile(batch_file):
+					targets[target] = target_compiler(conf, 'intel', arch, version, target, batch_file)
+
+		for target,arch in all_ifort_platforms:
+			try:
+				icl_version = Utils.winreg.OpenKey(all_versions, version+'\\'+target)
+				path,type = Utils.winreg.QueryValueEx(icl_version,'ProductDir')
+			except OSError:
+				continue
+			else:
+				batch_file=os.path.join(path,'bin','ifortvars.bat')
+				if os.path.isfile(batch_file):
+					targets[target] = target_compiler(conf, 'intel', arch, version, target, batch_file)
+		major = version[0:2]
+		versions['intel ' + major] = targets
+
+@conf
+def setup_ifort(conf, versiondict):
+	"""
+	Checks installed compilers and targets and returns the first combination from the user's
+	options, env, or the global supported lists that checks.
+
+	:param versiondict: dict(platform -> dict(architecture -> configuration))
+	:type versiondict: dict(string -> dict(string -> target_compiler)
+	:return: the compiler, revision, path, include dirs, library paths and target architecture
+	:rtype: tuple of strings
+	"""
+	platforms = Utils.to_list(conf.env.MSVC_TARGETS) or [i for i,j in all_ifort_platforms]
+	desired_versions = conf.env.MSVC_VERSIONS or list(reversed(list(versiondict.keys())))
+	for version in desired_versions:
+		try:
+			targets = versiondict[version]
+		except KeyError:
+			continue
+		for arch in platforms:
+			try:
+				cfg = targets[arch]
+			except KeyError:
+				continue
+			cfg.evaluate()
+			if cfg.is_valid:
+				compiler,revision = version.rsplit(' ', 1)
+				return compiler,revision,cfg.bindirs,cfg.incdirs,cfg.libdirs,cfg.cpu
+	conf.fatal('ifort: Impossible to find a valid architecture for building %r - %r' % (desired_versions, list(versiondict.keys())))
+
+@conf
+def get_ifort_version_win32(conf, compiler, version, target, vcvars):
+	# FIXME hack
+	try:
+		conf.msvc_cnt += 1
+	except AttributeError:
+		conf.msvc_cnt = 1
+	batfile = conf.bldnode.make_node('waf-print-msvc-%d.bat' % conf.msvc_cnt)
+	batfile.write("""@echo off
+set INCLUDE=
+set LIB=
+call "%s" %s
+echo PATH=%%PATH%%
+echo INCLUDE=%%INCLUDE%%
+echo LIB=%%LIB%%;%%LIBPATH%%
+""" % (vcvars,target))
+	sout = conf.cmd_and_log(['cmd.exe', '/E:on', '/V:on', '/C', batfile.abspath()])
+	batfile.delete()
+	lines = sout.splitlines()
+
+	if not lines[0]:
+		lines.pop(0)
+
+	MSVC_PATH = MSVC_INCDIR = MSVC_LIBDIR = None
+	for line in lines:
+		if line.startswith('PATH='):
+			path = line[5:]
+			MSVC_PATH = path.split(';')
+		elif line.startswith('INCLUDE='):
+			MSVC_INCDIR = [i for i in line[8:].split(';') if i]
+		elif line.startswith('LIB='):
+			MSVC_LIBDIR = [i for i in line[4:].split(';') if i]
+	if None in (MSVC_PATH, MSVC_INCDIR, MSVC_LIBDIR):
+		conf.fatal('ifort: Could not find a valid architecture for building (get_ifort_version_win32)')
+
+	# Check if the compiler is usable at all.
+	# The detection may return 64-bit versions even on 32-bit systems, and these would fail to run.
+	env = dict(os.environ)
+	env.update(PATH = path)
+	compiler_name, linker_name, lib_name = _get_prog_names(conf, compiler)
+	fc = conf.find_program(compiler_name, path_list=MSVC_PATH)
+
+	# delete CL if exists. because it could contain parameters which can change cl's behaviour rather catastrophically.
+	if 'CL' in env:
+		del(env['CL'])
+
+	try:
+		conf.cmd_and_log(fc + ['/help'], env=env)
+	except UnicodeError:
+		st = traceback.format_exc()
+		if conf.logger:
+			conf.logger.error(st)
+		conf.fatal('ifort: Unicode error - check the code page?')
+	except Exception as e:
+		Logs.debug('ifort: get_ifort_version: %r %r %r -> failure %s', compiler, version, target, str(e))
+		conf.fatal('ifort: cannot run the compiler in get_ifort_version (run with -v to display errors)')
+	else:
+		Logs.debug('ifort: get_ifort_version: %r %r %r -> OK', compiler, version, target)
+	finally:
+		conf.env[compiler_name] = ''
+
+	return (MSVC_PATH, MSVC_INCDIR, MSVC_LIBDIR)
+
+class target_compiler(object):
+	"""
+	Wraps a compiler configuration; call evaluate() to determine
+	whether the configuration is usable.
+	"""
+	def __init__(self, ctx, compiler, cpu, version, bat_target, bat, callback=None):
+		"""
+		:param ctx: configuration context to use to eventually get the version environment
+		:param compiler: compiler name
+		:param cpu: target cpu
+		:param version: compiler version number
+		:param bat_target: ?
+		:param bat: path to the batch file to run
+		:param callback: optional function to take the realized environment variables tup and map it (e.g. to combine other constant paths)
+		"""
+		self.conf = ctx
+		self.name = None
+		self.is_valid = False
+		self.is_done = False
+
+		self.compiler = compiler
+		self.cpu = cpu
+		self.version = version
+		self.bat_target = bat_target
+		self.bat = bat
+		self.callback = callback
+
+	def evaluate(self):
+		if self.is_done:
+			return
+		self.is_done = True
+		try:
+			vs = self.conf.get_ifort_version_win32(self.compiler, self.version, self.bat_target, self.bat)
+		except Errors.ConfigurationError:
+			self.is_valid = False
+			return
+		if self.callback:
+			vs = self.callback(self, vs)
+		self.is_valid = True
+		(self.bindirs, self.incdirs, self.libdirs) = vs
+
+	def __str__(self):
+		return str((self.bindirs, self.incdirs, self.libdirs))
+
+	def __repr__(self):
+		return repr((self.bindirs, self.incdirs, self.libdirs))
+
+@conf
+def detect_ifort(self):
+	return self.setup_ifort(self.get_ifort_versions(False))
+
+@conf
+def get_ifort_versions(self, eval_and_save=True):
+	"""
+	:return: platforms to compiler configurations
+	:rtype: dict
+	"""
+	dct = {}
+	self.gather_ifort_versions(dct)
+	return dct
+
+def _get_prog_names(self, compiler):
+	if compiler=='intel':
+		compiler_name = 'ifort'
+		linker_name = 'XILINK'
+		lib_name = 'XILIB'
+	else:
+		# assumes CL.exe
+		compiler_name = 'CL'
+		linker_name = 'LINK'
+		lib_name = 'LIB'
+	return compiler_name, linker_name, lib_name
+
+@conf
+def find_ifort_win32(conf):
+	# the autodetection is supposed to be performed before entering in this method
+	v = conf.env
+	path = v.PATH
+	compiler = v.MSVC_COMPILER
+	version = v.MSVC_VERSION
+
+	compiler_name, linker_name, lib_name = _get_prog_names(conf, compiler)
+	v.IFORT_MANIFEST = (compiler == 'intel' and version >= 11)
+
+	# compiler
+	fc = conf.find_program(compiler_name, var='FC', path_list=path)
+
+	# before setting anything, check if the compiler is really intel fortran
+	env = dict(conf.environ)
+	if path:
+		env.update(PATH = ';'.join(path))
+	if not conf.cmd_and_log(fc + ['/nologo', '/help'], env=env):
+		conf.fatal('not intel fortran compiler could not be identified')
+
+	v.FC_NAME = 'IFORT'
+
+	if not v.LINK_FC:
+		conf.find_program(linker_name, var='LINK_FC', path_list=path, mandatory=True)
+
+	if not v.AR:
+		conf.find_program(lib_name, path_list=path, var='AR', mandatory=True)
+		v.ARFLAGS = ['/nologo']
+
+	# manifest tool. Not required for VS 2003 and below. Must have for VS 2005 and later
+	if v.IFORT_MANIFEST:
+		conf.find_program('MT', path_list=path, var='MT')
+		v.MTFLAGS = ['/nologo']
+
+	try:
+		conf.load('winres')
+	except Errors.WafError:
+		Logs.warn('Resource compiler not found. Compiling resource file is disabled')
+
+#######################################################################################################
+##### conf above, build below
+
+@after_method('apply_link')
+@feature('fc')
+def apply_flags_ifort(self):
+	"""
+	Adds additional flags implied by msvc, such as subsystems and pdb files::
+
+		def build(bld):
+			bld.stlib(source='main.c', target='bar', subsystem='gruik')
+	"""
+	if not self.env.IFORT_WIN32 or not getattr(self, 'link_task', None):
+		return
+
+	is_static = isinstance(self.link_task, ccroot.stlink_task)
+
+	subsystem = getattr(self, 'subsystem', '')
+	if subsystem:
+		subsystem = '/subsystem:%s' % subsystem
+		flags = is_static and 'ARFLAGS' or 'LINKFLAGS'
+		self.env.append_value(flags, subsystem)
+
+	if not is_static:
+		for f in self.env.LINKFLAGS:
+			d = f.lower()
+			if d[1:] == 'debug':
+				pdbnode = self.link_task.outputs[0].change_ext('.pdb')
+				self.link_task.outputs.append(pdbnode)
+
+				if getattr(self, 'install_task', None):
+					self.pdb_install_task = self.add_install_files(install_to=self.install_task.install_to, install_from=pdbnode)
+
+				break
+
+@feature('fcprogram', 'fcshlib', 'fcprogram_test')
+@after_method('apply_link')
+def apply_manifest_ifort(self):
+	"""
+	Enables manifest embedding in Fortran DLLs when using ifort on Windows
+	See: http://msdn2.microsoft.com/en-us/library/ms235542(VS.80).aspx
+	"""
+	if self.env.IFORT_WIN32 and getattr(self, 'link_task', None):
+		# it seems ifort.exe cannot be called for linking
+		self.link_task.env.FC = self.env.LINK_FC
+
+	if self.env.IFORT_WIN32 and self.env.IFORT_MANIFEST and getattr(self, 'link_task', None):
+		out_node = self.link_task.outputs[0]
+		man_node = out_node.parent.find_or_declare(out_node.name + '.manifest')
+		self.link_task.outputs.append(man_node)
+		self.env.DO_MANIFEST = True
+
diff --git a/waflib/Tools/intltool.py b/waflib/Tools/intltool.py
new file mode 100644
index 00000000..af95ba80
--- /dev/null
+++ b/waflib/Tools/intltool.py
@@ -0,0 +1,231 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2018 (ita)
+
+"""
+Support for translation tools such as msgfmt and intltool
+
+Usage::
+
+	def configure(conf):
+		conf.load('gnu_dirs intltool')
+
+	def build(bld):
+		# process the .po files into .gmo files, and install them in LOCALEDIR
+		bld(features='intltool_po', appname='myapp', podir='po', install_path="${LOCALEDIR}")
+
+		# process an input file, substituting the translations from the po dir
+		bld(
+			features  = "intltool_in",
+			podir     = "../po",
+			style     = "desktop",
+			flags     = ["-u"],
+			source    = 'kupfer.desktop.in',
+			install_path = "${DATADIR}/applications",
+		)
+
+Usage of the :py:mod:`waflib.Tools.gnu_dirs` is recommended, but not obligatory.
+"""
+
+from __future__ import with_statement
+
+import os, re
+from waflib import Context, Task, Utils, Logs
+import waflib.Tools.ccroot
+from waflib.TaskGen import feature, before_method, taskgen_method
+from waflib.Logs import error
+from waflib.Configure import conf
+
+_style_flags = {
+	'ba': '-b',
+	'desktop': '-d',
+	'keys': '-k',
+	'quoted': '--quoted-style',
+	'quotedxml': '--quotedxml-style',
+	'rfc822deb': '-r',
+	'schemas': '-s',
+	'xml': '-x',
+}
+
+@taskgen_method
+def ensure_localedir(self):
+	"""
+	Expands LOCALEDIR from DATAROOTDIR/locale if possible, or falls back to PREFIX/share/locale
+	"""
+	# use the tool gnu_dirs to provide options to define this
+	if not self.env.LOCALEDIR:
+		if self.env.DATAROOTDIR:
+			self.env.LOCALEDIR = os.path.join(self.env.DATAROOTDIR, 'locale')
+		else:
+			self.env.LOCALEDIR = os.path.join(self.env.PREFIX, 'share', 'locale')
+
+@before_method('process_source')
+@feature('intltool_in')
+def apply_intltool_in_f(self):
+	"""
+	Creates tasks to translate files by intltool-merge::
+
+		def build(bld):
+			bld(
+				features  = "intltool_in",
+				podir     = "../po",
+				style     = "desktop",
+				flags     = ["-u"],
+				source    = 'kupfer.desktop.in',
+				install_path = "${DATADIR}/applications",
+			)
+
+	:param podir: location of the .po files
+	:type podir: string
+	:param source: source files to process
+	:type source: list of string
+	:param style: the intltool-merge mode of operation, can be one of the following values:
+	  ``ba``, ``desktop``, ``keys``, ``quoted``, ``quotedxml``, ``rfc822deb``, ``schemas`` and ``xml``.
+	  See the ``intltool-merge`` man page for more information about supported modes of operation.
+	:type style: string
+	:param flags: compilation flags ("-quc" by default)
+	:type flags: list of string
+	:param install_path: installation path
+	:type install_path: string
+	"""
+	try:
+		self.meths.remove('process_source')
+	except ValueError:
+		pass
+
+	self.ensure_localedir()
+
+	podir = getattr(self, 'podir', '.')
+	podirnode = self.path.find_dir(podir)
+	if not podirnode:
+		error("could not find the podir %r" % podir)
+		return
+
+	cache = getattr(self, 'intlcache', '.intlcache')
+	self.env.INTLCACHE = [os.path.join(str(self.path.get_bld()), podir, cache)]
+	self.env.INTLPODIR = podirnode.bldpath()
+	self.env.append_value('INTLFLAGS', getattr(self, 'flags', self.env.INTLFLAGS_DEFAULT))
+
+	if '-c' in self.env.INTLFLAGS:
+		self.bld.fatal('Redundant -c flag in intltool task %r' % self)
+
+	style = getattr(self, 'style', None)
+	if style:
+		try:
+			style_flag = _style_flags[style]
+		except KeyError:
+			self.bld.fatal('intltool_in style "%s" is not valid' % style)
+
+		self.env.append_unique('INTLFLAGS', [style_flag])
+
+	for i in self.to_list(self.source):
+		node = self.path.find_resource(i)
+
+		task = self.create_task('intltool', node, node.change_ext(''))
+		inst = getattr(self, 'install_path', None)
+		if inst:
+			self.add_install_files(install_to=inst, install_from=task.outputs)
+
+@feature('intltool_po')
+def apply_intltool_po(self):
+	"""
+	Creates tasks to process po files::
+
+		def build(bld):
+			bld(features='intltool_po', appname='myapp', podir='po', install_path="${LOCALEDIR}")
+
+	The relevant task generator arguments are:
+
+	:param podir: directory of the .po files
+	:type podir: string
+	:param appname: name of the application
+	:type appname: string
+	:param install_path: installation directory
+	:type install_path: string
+
+	The file LINGUAS must be present in the directory pointed by *podir* and list the translation files to process.
+	"""
+	try:
+		self.meths.remove('process_source')
+	except ValueError:
+		pass
+
+	self.ensure_localedir()
+
+	appname = getattr(self, 'appname', getattr(Context.g_module, Context.APPNAME, 'set_your_app_name'))
+	podir = getattr(self, 'podir', '.')
+	inst = getattr(self, 'install_path', '${LOCALEDIR}')
+
+	linguas = self.path.find_node(os.path.join(podir, 'LINGUAS'))
+	if linguas:
+		# scan LINGUAS file for locales to process
+		with open(linguas.abspath()) as f:
+			langs = []
+			for line in f.readlines():
+				# ignore lines containing comments
+				if not line.startswith('#'):
+					langs += line.split()
+		re_linguas = re.compile('[-a-zA-Z_@.]+')
+		for lang in langs:
+			# Make sure that we only process lines which contain locales
+			if re_linguas.match(lang):
+				node = self.path.find_resource(os.path.join(podir, re_linguas.match(lang).group() + '.po'))
+				task = self.create_task('po', node, node.change_ext('.mo'))
+
+				if inst:
+					filename = task.outputs[0].name
+					(langname, ext) = os.path.splitext(filename)
+					inst_file = inst + os.sep + langname + os.sep + 'LC_MESSAGES' + os.sep + appname + '.mo'
+					self.add_install_as(install_to=inst_file, install_from=task.outputs[0],
+						chmod=getattr(self, 'chmod', Utils.O644))
+
+	else:
+		Logs.pprint('RED', "Error no LINGUAS file found in po directory")
+
+class po(Task.Task):
+	"""
+	Compiles .po files into .gmo files
+	"""
+	run_str = '${MSGFMT} -o ${TGT} ${SRC}'
+	color   = 'BLUE'
+
+class intltool(Task.Task):
+	"""
+	Calls intltool-merge to update translation files
+	"""
+	run_str = '${INTLTOOL} ${INTLFLAGS} ${INTLCACHE_ST:INTLCACHE} ${INTLPODIR} ${SRC} ${TGT}'
+	color   = 'BLUE'
+
+@conf
+def find_msgfmt(conf):
+	"""
+	Detects msgfmt and sets the ``MSGFMT`` variable
+	"""
+	conf.find_program('msgfmt', var='MSGFMT')
+
+@conf
+def find_intltool_merge(conf):
+	"""
+	Detects intltool-merge
+	"""
+	if not conf.env.PERL:
+		conf.find_program('perl', var='PERL')
+	conf.env.INTLCACHE_ST = '--cache=%s'
+	conf.env.INTLFLAGS_DEFAULT = ['-q', '-u']
+	conf.find_program('intltool-merge', interpreter='PERL', var='INTLTOOL')
+
+def configure(conf):
+	"""
+	Detects the program *msgfmt* and set *conf.env.MSGFMT*.
+	Detects the program *intltool-merge* and set *conf.env.INTLTOOL*.
+	It is possible to set INTLTOOL in the environment, but it must not have spaces in it::
+
+		$ INTLTOOL="/path/to/the program/intltool" waf configure
+
+	If a C/C++ compiler is present, execute a compilation test to find the header *locale.h*.
+	"""
+	conf.find_msgfmt()
+	conf.find_intltool_merge()
+	if conf.env.CC or conf.env.CXX:
+		conf.check(header_name='locale.h')
+
diff --git a/waflib/Tools/irixcc.py b/waflib/Tools/irixcc.py
index 55eb70be..c3ae1ac9 100644
--- a/waflib/Tools/irixcc.py
+++ b/waflib/Tools/irixcc.py
@@ -1,11 +1,12 @@
 #! /usr/bin/env python
+# encoding: utf-8
 # imported from samba
 
 """
-compiler definition for irix/MIPSpro cc compiler
-based on suncc.py from waf
+Compiler definition for irix/MIPSpro cc compiler
 """
 
+from waflib import Errors
 from waflib.Tools import ccroot, ar
 from waflib.Configure import conf
 
@@ -13,41 +14,46 @@
 def find_irixcc(conf):
 	v = conf.env
 	cc = None
-	if v['CC']: cc = v['CC']
-	elif 'CC' in conf.environ: cc = conf.environ['CC']
-	if not cc: cc = conf.find_program('cc', var='CC')
-	if not cc: conf.fatal('irixcc was not found')
+	if v.CC:
+		cc = v.CC
+	elif 'CC' in conf.environ:
+		cc = conf.environ['CC']
+	if not cc:
+		cc = conf.find_program('cc', var='CC')
+	if not cc:
+		conf.fatal('irixcc was not found')
 
 	try:
 		conf.cmd_and_log(cc + ['-version'])
-	except Exception:
+	except Errors.WafError:
 		conf.fatal('%r -version could not be executed' % cc)
 
-	v['CC']  = cc
-	v['CC_NAME'] = 'irix'
+	v.CC = cc
+	v.CC_NAME = 'irix'
 
 @conf
 def irixcc_common_flags(conf):
 	v = conf.env
 
-	v['CC_SRC_F']            = ''
-	v['CC_TGT_F']            = ['-c', '-o']
-	v['CPPPATH_ST']          = '-I%s'
-	v['DEFINES_ST']          = '-D%s'
+	v.CC_SRC_F            = ''
+	v.CC_TGT_F            = ['-c', '-o']
+	v.CPPPATH_ST          = '-I%s'
+	v.DEFINES_ST          = '-D%s'
 
-	# linker
-	if not v['LINK_CC']: v['LINK_CC'] = v['CC']
-	v['CCLNK_SRC_F']         = ''
-	v['CCLNK_TGT_F']         = ['-o']
+	if not v.LINK_CC:
+		v.LINK_CC = v.CC
 
-	v['LIB_ST']              = '-l%s' # template for adding libs
-	v['LIBPATH_ST']          = '-L%s' # template for adding libpaths
-	v['STLIB_ST']            = '-l%s'
-	v['STLIBPATH_ST']        = '-L%s'
+	v.CCLNK_SRC_F         = ''
+	v.CCLNK_TGT_F         = ['-o']
 
-	v['cprogram_PATTERN']     = '%s'
-	v['cshlib_PATTERN']       = 'lib%s.so'
-	v['cstlib_PATTERN']      = 'lib%s.a'
+	v.LIB_ST              = '-l%s' # template for adding libs
+	v.LIBPATH_ST          = '-L%s' # template for adding libpaths
+	v.STLIB_ST            = '-l%s'
+	v.STLIBPATH_ST        = '-L%s'
+
+	v.cprogram_PATTERN    = '%s'
+	v.cshlib_PATTERN      = 'lib%s.so'
+	v.cstlib_PATTERN      = 'lib%s.a'
 
 def configure(conf):
 	conf.find_irixcc()
diff --git a/waflib/Tools/javaw.py b/waflib/Tools/javaw.py
new file mode 100644
index 00000000..f6fd20cc
--- /dev/null
+++ b/waflib/Tools/javaw.py
@@ -0,0 +1,464 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2018 (ita)
+
+"""
+Java support
+
+Javac is one of the few compilers that behaves very badly:
+
+#. it outputs files where it wants to (-d is only for the package root)
+
+#. it recompiles files silently behind your back
+
+#. it outputs an undefined amount of files (inner classes)
+
+Remember that the compilation can be performed using Jython[1] rather than regular Python. Instead of
+running one of the following commands::
+
+   ./waf configure
+   python waf configure
+
+You would have to run::
+
+   java -jar /path/to/jython.jar waf configure
+
+[1] http://www.jython.org/
+"""
+
+import os, shutil
+from waflib import Task, Utils, Errors, Node
+from waflib.Configure import conf
+from waflib.TaskGen import feature, before_method, after_method
+
+from waflib.Tools import ccroot
+ccroot.USELIB_VARS['javac'] = set(['CLASSPATH', 'JAVACFLAGS'])
+
+SOURCE_RE = '**/*.java'
+JAR_RE = '**/*'
+
+class_check_source = '''
+public class Test {
+	public static void main(String[] argv) {
+		Class lib;
+		if (argv.length < 1) {
+			System.err.println("Missing argument");
+			System.exit(77);
+		}
+		try {
+			lib = Class.forName(argv[0]);
+		} catch (ClassNotFoundException e) {
+			System.err.println("ClassNotFoundException");
+			System.exit(1);
+		}
+		lib = null;
+		System.exit(0);
+	}
+}
+'''
+
+@feature('javac')
+@before_method('process_source')
+def apply_java(self):
+	"""
+	Create a javac task for compiling *.java files*. There can be
+	only one javac task by task generator.
+	"""
+	Utils.def_attrs(self, jarname='', classpath='',
+		sourcepath='.', srcdir='.',
+		jar_mf_attributes={}, jar_mf_classpath=[])
+
+	outdir = getattr(self, 'outdir', None)
+	if outdir:
+		if not isinstance(outdir, Node.Node):
+			outdir = self.path.get_bld().make_node(self.outdir)
+	else:
+		outdir = self.path.get_bld()
+	outdir.mkdir()
+	self.outdir = outdir
+	self.env.OUTDIR = outdir.abspath()
+
+	self.javac_task = tsk = self.create_task('javac')
+	tmp = []
+
+	srcdir = getattr(self, 'srcdir', '')
+	if isinstance(srcdir, Node.Node):
+		srcdir = [srcdir]
+	for x in Utils.to_list(srcdir):
+		if isinstance(x, Node.Node):
+			y = x
+		else:
+			y = self.path.find_dir(x)
+			if not y:
+				self.bld.fatal('Could not find the folder %s from %s' % (x, self.path))
+		tmp.append(y)
+
+	tsk.srcdir = tmp
+
+	if getattr(self, 'compat', None):
+		tsk.env.append_value('JAVACFLAGS', ['-source', str(self.compat)])
+
+	if hasattr(self, 'sourcepath'):
+		fold = [isinstance(x, Node.Node) and x or self.path.find_dir(x) for x in self.to_list(self.sourcepath)]
+		names = os.pathsep.join([x.srcpath() for x in fold])
+	else:
+		names = [x.srcpath() for x in tsk.srcdir]
+
+	if names:
+		tsk.env.append_value('JAVACFLAGS', ['-sourcepath', names])
+
+@feature('javac')
+@before_method('propagate_uselib_vars')
+@after_method('apply_java')
+def use_javac_files(self):
+	"""
+	Processes the *use* attribute referring to other java compilations
+	"""
+	lst = []
+	self.uselib = self.to_list(getattr(self, 'uselib', []))
+	names = self.to_list(getattr(self, 'use', []))
+	get = self.bld.get_tgen_by_name
+	for x in names:
+		try:
+			y = get(x)
+		except Errors.WafError:
+			self.uselib.append(x)
+		else:
+			y.post()
+			if hasattr(y, 'jar_task'):
+				lst.append(y.jar_task.outputs[0].abspath())
+				self.javac_task.set_run_after(y.jar_task)
+			else:
+				for tsk in y.tasks:
+					self.javac_task.set_run_after(tsk)
+	self.env.append_value('CLASSPATH', lst)
+
+@feature('javac')
+@after_method('apply_java', 'propagate_uselib_vars', 'use_javac_files')
+def set_classpath(self):
+	"""
+	Sets the CLASSPATH value on the *javac* task previously created.
+	"""
+	if getattr(self, 'classpath', None):
+		self.env.append_unique('CLASSPATH', getattr(self, 'classpath', []))
+	for x in self.tasks:
+		x.env.CLASSPATH = os.pathsep.join(self.env.CLASSPATH) + os.pathsep
+
+@feature('jar')
+@after_method('apply_java', 'use_javac_files')
+@before_method('process_source')
+def jar_files(self):
+	"""
+	Creates a jar task (one maximum per task generator)
+	"""
+	destfile = getattr(self, 'destfile', 'test.jar')
+	jaropts = getattr(self, 'jaropts', [])
+	manifest = getattr(self, 'manifest', None)
+
+	basedir = getattr(self, 'basedir', None)
+	if basedir:
+		if not isinstance(self.basedir, Node.Node):
+			basedir = self.path.get_bld().make_node(basedir)
+	else:
+		basedir = self.path.get_bld()
+	if not basedir:
+		self.bld.fatal('Could not find the basedir %r for %r' % (self.basedir, self))
+
+	self.jar_task = tsk = self.create_task('jar_create')
+	if manifest:
+		jarcreate = getattr(self, 'jarcreate', 'cfm')
+		if not isinstance(manifest,Node.Node):
+			node = self.path.find_resource(manifest)
+		else:
+			node = manifest
+		if not node:
+			self.bld.fatal('invalid manifest file %r for %r' % (manifest, self))
+		tsk.dep_nodes.append(node)
+		jaropts.insert(0, node.abspath())
+	else:
+		jarcreate = getattr(self, 'jarcreate', 'cf')
+	if not isinstance(destfile, Node.Node):
+		destfile = self.path.find_or_declare(destfile)
+	if not destfile:
+		self.bld.fatal('invalid destfile %r for %r' % (destfile, self))
+	tsk.set_outputs(destfile)
+	tsk.basedir = basedir
+
+	jaropts.append('-C')
+	jaropts.append(basedir.bldpath())
+	jaropts.append('.')
+
+	tsk.env.JAROPTS = jaropts
+	tsk.env.JARCREATE = jarcreate
+
+	if getattr(self, 'javac_task', None):
+		tsk.set_run_after(self.javac_task)
+
+@feature('jar')
+@after_method('jar_files')
+def use_jar_files(self):
+	"""
+	Processes the *use* attribute to set the build order on the
+	tasks created by another task generator.
+	"""
+	self.uselib = self.to_list(getattr(self, 'uselib', []))
+	names = self.to_list(getattr(self, 'use', []))
+	get = self.bld.get_tgen_by_name
+	for x in names:
+		try:
+			y = get(x)
+		except Errors.WafError:
+			self.uselib.append(x)
+		else:
+			y.post()
+			self.jar_task.run_after.update(y.tasks)
+
+class JTask(Task.Task):
+	"""
+	Base class for java and jar tasks; provides functionality to run long commands
+	"""
+	def split_argfile(self, cmd):
+		inline = [cmd[0]]
+		infile = []
+		for x in cmd[1:]:
+			# jar and javac do not want -J flags in @file
+			if x.startswith('-J'):
+				inline.append(x)
+			else:
+				infile.append(self.quote_flag(x))
+		return (inline, infile)
+
+class jar_create(JTask):
+	"""
+	Creates a jar file
+	"""
+	color   = 'GREEN'
+	run_str = '${JAR} ${JARCREATE} ${TGT} ${JAROPTS}'
+
+	def runnable_status(self):
+		"""
+		Wait for dependent tasks to be executed, then read the
+		files to update the list of inputs.
+		"""
+		for t in self.run_after:
+			if not t.hasrun:
+				return Task.ASK_LATER
+		if not self.inputs:
+			try:
+				self.inputs = [x for x in self.basedir.ant_glob(JAR_RE, remove=False) if id(x) != id(self.outputs[0])]
+			except Exception:
+				raise Errors.WafError('Could not find the basedir %r for %r' % (self.basedir, self))
+		return super(jar_create, self).runnable_status()
+
+class javac(JTask):
+	"""
+	Compiles java files
+	"""
+	color   = 'BLUE'
+	run_str = '${JAVAC} -classpath ${CLASSPATH} -d ${OUTDIR} ${JAVACFLAGS} ${SRC}'
+	vars = ['CLASSPATH', 'JAVACFLAGS', 'JAVAC', 'OUTDIR']
+	"""
+	The javac task will be executed again if the variables CLASSPATH, JAVACFLAGS, JAVAC or OUTDIR change.
+	"""
+	def uid(self):
+		"""Identify java tasks by input&output folder"""
+		lst = [self.__class__.__name__, self.generator.outdir.abspath()]
+		for x in self.srcdir:
+			lst.append(x.abspath())
+		return Utils.h_list(lst)
+
+	def runnable_status(self):
+		"""
+		Waits for dependent tasks to be complete, then read the file system to find the input nodes.
+		"""
+		for t in self.run_after:
+			if not t.hasrun:
+				return Task.ASK_LATER
+
+		if not self.inputs:
+			self.inputs  = []
+			for x in self.srcdir:
+				if x.exists():
+					self.inputs.extend(x.ant_glob(SOURCE_RE, remove=False))
+		return super(javac, self).runnable_status()
+
+	def post_run(self):
+		"""
+		List class files created
+		"""
+		for node in self.generator.outdir.ant_glob('**/*.class'):
+			self.generator.bld.node_sigs[node] = self.uid()
+		self.generator.bld.task_sigs[self.uid()] = self.cache_sig
+
+@feature('javadoc')
+@after_method('process_rule')
+def create_javadoc(self):
+	"""
+	Creates a javadoc task (feature 'javadoc')
+	"""
+	tsk = self.create_task('javadoc')
+	tsk.classpath = getattr(self, 'classpath', [])
+	self.javadoc_package = Utils.to_list(self.javadoc_package)
+	if not isinstance(self.javadoc_output, Node.Node):
+		self.javadoc_output = self.bld.path.find_or_declare(self.javadoc_output)
+
+class javadoc(Task.Task):
+	"""
+	Builds java documentation
+	"""
+	color = 'BLUE'
+
+	def __str__(self):
+		return '%s: %s -> %s\n' % (self.__class__.__name__, self.generator.srcdir, self.generator.javadoc_output)
+
+	def run(self):
+		env = self.env
+		bld = self.generator.bld
+		wd = bld.bldnode
+
+		#add src node + bld node (for generated java code)
+		srcpath = self.generator.path.abspath() + os.sep + self.generator.srcdir
+		srcpath += os.pathsep
+		srcpath += self.generator.path.get_bld().abspath() + os.sep + self.generator.srcdir
+
+		classpath = env.CLASSPATH
+		classpath += os.pathsep
+		classpath += os.pathsep.join(self.classpath)
+		classpath = "".join(classpath)
+
+		self.last_cmd = lst = []
+		lst.extend(Utils.to_list(env.JAVADOC))
+		lst.extend(['-d', self.generator.javadoc_output.abspath()])
+		lst.extend(['-sourcepath', srcpath])
+		lst.extend(['-classpath', classpath])
+		lst.extend(['-subpackages'])
+		lst.extend(self.generator.javadoc_package)
+		lst = [x for x in lst if x]
+
+		self.generator.bld.cmd_and_log(lst, cwd=wd, env=env.env or None, quiet=0)
+
+	def post_run(self):
+		nodes = self.generator.javadoc_output.ant_glob('**')
+		for node in nodes:
+			self.generator.bld.node_sigs[node] = self.uid()
+		self.generator.bld.task_sigs[self.uid()] = self.cache_sig
+
+def configure(self):
+	"""
+	Detects the javac, java and jar programs
+	"""
+	# If JAVA_PATH is set, we prepend it to the path list
+	java_path = self.environ['PATH'].split(os.pathsep)
+	v = self.env
+
+	if 'JAVA_HOME' in self.environ:
+		java_path = [os.path.join(self.environ['JAVA_HOME'], 'bin')] + java_path
+		self.env.JAVA_HOME = [self.environ['JAVA_HOME']]
+
+	for x in 'javac java jar javadoc'.split():
+		self.find_program(x, var=x.upper(), path_list=java_path)
+
+	if 'CLASSPATH' in self.environ:
+		v.CLASSPATH = self.environ['CLASSPATH']
+
+	if not v.JAR:
+		self.fatal('jar is required for making java packages')
+	if not v.JAVAC:
+		self.fatal('javac is required for compiling java classes')
+
+	v.JARCREATE = 'cf' # can use cvf
+	v.JAVACFLAGS = []
+
+@conf
+def check_java_class(self, classname, with_classpath=None):
+	"""
+	Checks if the specified java class exists
+
+	:param classname: class to check, like java.util.HashMap
+	:type classname: string
+	:param with_classpath: additional classpath to give
+	:type with_classpath: string
+	"""
+	javatestdir = '.waf-javatest'
+
+	classpath = javatestdir
+	if self.env.CLASSPATH:
+		classpath += os.pathsep + self.env.CLASSPATH
+	if isinstance(with_classpath, str):
+		classpath += os.pathsep + with_classpath
+
+	shutil.rmtree(javatestdir, True)
+	os.mkdir(javatestdir)
+
+	Utils.writef(os.path.join(javatestdir, 'Test.java'), class_check_source)
+
+	# Compile the source
+	self.exec_command(self.env.JAVAC + [os.path.join(javatestdir, 'Test.java')], shell=False)
+
+	# Try to run the app
+	cmd = self.env.JAVA + ['-cp', classpath, 'Test', classname]
+	self.to_log("%s\n" % str(cmd))
+	found = self.exec_command(cmd, shell=False)
+
+	self.msg('Checking for java class %s' % classname, not found)
+
+	shutil.rmtree(javatestdir, True)
+
+	return found
+
+@conf
+def check_jni_headers(conf):
+	"""
+	Checks for jni headers and libraries. On success the conf.env variables xxx_JAVA are added for use in C/C++ targets::
+
+		def options(opt):
+			opt.load('compiler_c')
+
+		def configure(conf):
+			conf.load('compiler_c java')
+			conf.check_jni_headers()
+
+		def build(bld):
+			bld.shlib(source='a.c', target='app', use='JAVA')
+	"""
+	if not conf.env.CC_NAME and not conf.env.CXX_NAME:
+		conf.fatal('load a compiler first (gcc, g++, ..)')
+
+	if not conf.env.JAVA_HOME:
+		conf.fatal('set JAVA_HOME in the system environment')
+
+	# jni requires the jvm
+	javaHome = conf.env.JAVA_HOME[0]
+
+	dir = conf.root.find_dir(conf.env.JAVA_HOME[0] + '/include')
+	if dir is None:
+		dir = conf.root.find_dir(conf.env.JAVA_HOME[0] + '/../Headers') # think different?!
+	if dir is None:
+		conf.fatal('JAVA_HOME does not seem to be set properly')
+
+	f = dir.ant_glob('**/(jni|jni_md).h')
+	incDirs = [x.parent.abspath() for x in f]
+
+	dir = conf.root.find_dir(conf.env.JAVA_HOME[0])
+	f = dir.ant_glob('**/*jvm.(so|dll|dylib)')
+	libDirs = [x.parent.abspath() for x in f] or [javaHome]
+
+	# On windows, we need both the .dll and .lib to link.  On my JDK, they are
+	# in different directories...
+	f = dir.ant_glob('**/*jvm.(lib)')
+	if f:
+		libDirs = [[x, y.parent.abspath()] for x in libDirs for y in f]
+
+	if conf.env.DEST_OS == 'freebsd':
+		conf.env.append_unique('LINKFLAGS_JAVA', '-pthread')
+	for d in libDirs:
+		try:
+			conf.check(header_name='jni.h', define_name='HAVE_JNI_H', lib='jvm',
+				libpath=d, includes=incDirs, uselib_store='JAVA', uselib='JAVA')
+		except Exception:
+			pass
+		else:
+			break
+	else:
+		conf.fatal('could not find lib jvm in %r (see config.log)' % libDirs)
+
diff --git a/waflib/Tools/ldc2.py b/waflib/Tools/ldc2.py
new file mode 100644
index 00000000..a51c344b
--- /dev/null
+++ b/waflib/Tools/ldc2.py
@@ -0,0 +1,56 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Alex Rønne Petersen, 2012 (alexrp/Zor)
+
+from waflib.Tools import ar, d
+from waflib.Configure import conf
+
+@conf
+def find_ldc2(conf):
+	"""
+	Finds the program *ldc2* and set the variable *D*
+	"""
+	conf.find_program(['ldc2'], var='D')
+
+	out = conf.cmd_and_log(conf.env.D + ['-version'])
+	if out.find("based on DMD v2.") == -1:
+		conf.fatal("detected compiler is not ldc2")
+
+@conf
+def common_flags_ldc2(conf):
+	"""
+	Sets the D flags required by *ldc2*
+	"""
+	v = conf.env
+
+	v.D_SRC_F           = ['-c']
+	v.D_TGT_F           = '-of%s'
+
+	v.D_LINKER          = v.D
+	v.DLNK_SRC_F        = ''
+	v.DLNK_TGT_F        = '-of%s'
+	v.DINC_ST           = '-I%s'
+
+	v.DSHLIB_MARKER = v.DSTLIB_MARKER = ''
+	v.DSTLIB_ST = v.DSHLIB_ST         = '-L-l%s'
+	v.DSTLIBPATH_ST = v.DLIBPATH_ST   = '-L-L%s'
+
+	v.LINKFLAGS_dshlib  = ['-L-shared']
+
+	v.DHEADER_ext       = '.di'
+	v.DFLAGS_d_with_header = ['-H', '-Hf']
+	v.D_HDR_F           = '%s'
+
+	v.LINKFLAGS     = []
+	v.DFLAGS_dshlib = ['-relocation-model=pic']
+
+def configure(conf):
+	"""
+	Configuration for *ldc2*
+	"""
+	conf.find_ldc2()
+	conf.load('ar')
+	conf.load('d')
+	conf.common_flags_ldc2()
+	conf.d_platform_flags()
+
diff --git a/waflib/Tools/lua.py b/waflib/Tools/lua.py
new file mode 100644
index 00000000..15a333a9
--- /dev/null
+++ b/waflib/Tools/lua.py
@@ -0,0 +1,38 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Sebastian Schlingmann, 2008
+# Thomas Nagy, 2008-2018 (ita)
+
+"""
+Lua support.
+
+Compile *.lua* files into *.luac*::
+
+	def configure(conf):
+		conf.load('lua')
+		conf.env.LUADIR = '/usr/local/share/myapp/scripts/'
+	def build(bld):
+		bld(source='foo.lua')
+"""
+
+from waflib.TaskGen import extension
+from waflib import Task
+
+@extension('.lua')
+def add_lua(self, node):
+	tsk = self.create_task('luac', node, node.change_ext('.luac'))
+	inst_to = getattr(self, 'install_path', self.env.LUADIR and '${LUADIR}' or None)
+	if inst_to:
+		self.add_install_files(install_to=inst_to, install_from=tsk.outputs)
+	return tsk
+
+class luac(Task.Task):
+	run_str = '${LUAC} -s -o ${TGT} ${SRC}'
+	color   = 'PINK'
+
+def configure(conf):
+	"""
+	Detect the luac compiler and set *conf.env.LUAC*
+	"""
+	conf.find_program('luac', var='LUAC')
+
diff --git a/waflib/Tools/md5_tstamp.py b/waflib/Tools/md5_tstamp.py
new file mode 100644
index 00000000..6428e460
--- /dev/null
+++ b/waflib/Tools/md5_tstamp.py
@@ -0,0 +1,39 @@
+#! /usr/bin/env python
+# encoding: utf-8
+
+"""
+Re-calculate md5 hashes of files only when the file times or the file
+size have changed.
+
+The hashes can also reflect either the file contents (STRONGEST=True) or the
+file time and file size.
+
+The performance benefits of this module are usually insignificant.
+"""
+
+import os, stat
+from waflib import Utils, Build, Node
+
+STRONGEST = True
+
+Build.SAVED_ATTRS.append('hashes_md5_tstamp')
+def h_file(self):
+	filename = self.abspath()
+	st = os.stat(filename)
+
+	cache = self.ctx.hashes_md5_tstamp
+	if filename in cache and cache[filename][0] == st.st_mtime:
+		return cache[filename][1]
+
+	if STRONGEST:
+		ret = Utils.h_file(filename)
+	else:
+		if stat.S_ISDIR(st[stat.ST_MODE]):
+			raise IOError('Not a file')
+		ret = Utils.md5(str((st.st_mtime, st.st_size)).encode()).digest()
+
+	cache[filename] = (st.st_mtime, ret)
+	return ret
+h_file.__doc__ = Node.Node.h_file.__doc__
+Node.Node.h_file = h_file
+
diff --git a/waflib/Tools/msvc.py b/waflib/Tools/msvc.py
index d98432e0..17b347d4 100644
--- a/waflib/Tools/msvc.py
+++ b/waflib/Tools/msvc.py
@@ -8,6 +8,12 @@
 """
 Microsoft Visual C++/Intel C++ compiler support
 
+If you get detection problems, first try any of the following::
+
+	chcp 65001
+	set PYTHONIOENCODING=...
+	set PYTHONLEGACYWINDOWSSTDIO=1
+
 Usage::
 
 	$ waf configure --msvc_version="msvc 10.0,msvc 9.0" --msvc_target="x64"
@@ -15,8 +21,8 @@
 or::
 
 	def configure(conf):
-		conf.env['MSVC_VERSIONS'] = ['msvc 10.0', 'msvc 9.0', 'msvc 8.0', 'msvc 7.1', 'msvc 7.0', 'msvc 6.0', 'wsdk 7.0', 'intel 11', 'PocketPC 9.0', 'Smartphone 8.0']
-		conf.env['MSVC_TARGETS'] = ['x64']
+		conf.env.MSVC_VERSIONS = ['msvc 10.0', 'msvc 9.0', 'msvc 8.0', 'msvc 7.1', 'msvc 7.0', 'msvc 6.0', 'wsdk 7.0', 'intel 11', 'PocketPC 9.0', 'Smartphone 8.0']
+		conf.env.MSVC_TARGETS = ['x64']
 		conf.load('msvc')
 
 or::
@@ -31,14 +37,14 @@ def build(bld):
 Platforms and targets will be tested in the order they appear;
 the first good configuration will be used.
 
-To skip testing all the configurations that are not used, use the ``--msvc_lazy_autodetect`` option
-or set ``conf.env['MSVC_LAZY_AUTODETECT']=True``.
+To force testing all the configurations that are not used, use the ``--no-msvc-lazy`` option
+or set ``conf.env.MSVC_LAZY_AUTODETECT=False``.
 
 Supported platforms: ia64, x64, x86, x86_amd64, x86_ia64, x86_arm, amd64_x86, amd64_arm
 
 Compilers supported:
 
-* msvc       => Visual Studio, versions 6.0 (VC 98, VC .NET 2002) to 12.0 (Visual Studio 2013)
+* msvc       => Visual Studio, versions 6.0 (VC 98, VC .NET 2002) to 15 (Visual Studio 2017)
 * wsdk       => Windows SDK, versions 6.0, 6.1, 7.0, 7.1, 8.0
 * icl        => Intel compiler, versions 9, 10, 11, 13
 * winphone   => Visual Studio to target Windows Phone 8 native (version 8.0 for now)
@@ -52,13 +58,12 @@ def build(bld):
 Setting PYTHONUNBUFFERED gives the unbuffered output.
 """
 
-import os, sys, re, tempfile
-from waflib import Utils, Task, Logs, Options, Errors
-from waflib.Logs import debug, warn
+import os, sys, re, traceback
+from waflib import Utils, Logs, Options, Errors
 from waflib.TaskGen import after_method, feature
 
 from waflib.Configure import conf
-from waflib.Tools import ccroot, c, cxx, ar, winres
+from waflib.Tools import ccroot, c, cxx, ar
 
 g_msvc_systemlibs = '''
 aclui activeds ad1 adptif adsiid advapi32 asycfilt authz bhsupp bits bufferoverflowu cabinet
@@ -82,7 +87,9 @@ def build(bld):
 '''.split()
 """importlibs provided by MSVC/Platform SDK. Do NOT search them"""
 
-all_msvc_platforms = [ ('x64', 'amd64'), ('x86', 'x86'), ('ia64', 'ia64'), ('x86_amd64', 'amd64'), ('x86_ia64', 'ia64'), ('x86_arm', 'arm'), ('amd64_x86', 'x86'), ('amd64_arm', 'arm') ]
+all_msvc_platforms = [	('x64', 'amd64'), ('x86', 'x86'), ('ia64', 'ia64'),
+						('x86_amd64', 'amd64'), ('x86_ia64', 'ia64'), ('x86_arm', 'arm'), ('x86_arm64', 'arm64'),
+						('amd64_x86', 'x86'), ('amd64_arm', 'arm'), ('amd64_arm64', 'arm64') ]
 """List of msvc platforms"""
 
 all_wince_platforms = [ ('armv4', 'arm'), ('armv4i', 'arm'), ('mipsii', 'mips'), ('mipsii_fp', 'mips'), ('mipsiv', 'mips'), ('mipsiv_fp', 'mips'), ('sh4', 'sh'), ('x86', 'cex86') ]
@@ -94,45 +101,63 @@ def build(bld):
 def options(opt):
 	opt.add_option('--msvc_version', type='string', help = 'msvc version, eg: "msvc 10.0,msvc 9.0"', default='')
 	opt.add_option('--msvc_targets', type='string', help = 'msvc targets, eg: "x64,arm"', default='')
-	opt.add_option('--msvc_lazy_autodetect', action='store_true', help = 'lazily check msvc target environments')
+	opt.add_option('--no-msvc-lazy', action='store_false', help = 'lazily check msvc target environments', default=True, dest='msvc_lazy')
 
-def setup_msvc(conf, versions, arch = False):
+@conf
+def setup_msvc(conf, versiondict):
 	"""
 	Checks installed compilers and targets and returns the first combination from the user's
 	options, env, or the global supported lists that checks.
 
-	:param versions: A list of tuples of all installed compilers and available targets.
-	:param arch: Whether to return the target architecture.
-	:return: the compiler, revision, path, include dirs, library paths, and (optionally) target architecture
+	:param versiondict: dict(platform -> dict(architecture -> configuration))
+	:type versiondict: dict(string -> dict(string -> target_compiler)
+	:return: the compiler, revision, path, include dirs, library paths and target architecture
 	:rtype: tuple of strings
 	"""
 	platforms = getattr(Options.options, 'msvc_targets', '').split(',')
 	if platforms == ['']:
-		platforms=Utils.to_list(conf.env['MSVC_TARGETS']) or [i for i,j in all_msvc_platforms+all_icl_platforms+all_wince_platforms]
+		platforms=Utils.to_list(conf.env.MSVC_TARGETS) or [i for i,j in all_msvc_platforms+all_icl_platforms+all_wince_platforms]
 	desired_versions = getattr(Options.options, 'msvc_version', '').split(',')
 	if desired_versions == ['']:
-		desired_versions = conf.env['MSVC_VERSIONS'] or [v for v,_ in versions][::-1]
-	versiondict = dict(versions)
+		desired_versions = conf.env.MSVC_VERSIONS or list(reversed(sorted(versiondict.keys())))
+
+	# Override lazy detection by evaluating after the fact.
+	lazy_detect = getattr(Options.options, 'msvc_lazy', True)
+	if conf.env.MSVC_LAZY_AUTODETECT is False:
+		lazy_detect = False
+
+	if not lazy_detect:
+		for val in versiondict.values():
+			for arch in list(val.keys()):
+				cfg = val[arch]
+				cfg.evaluate()
+				if not cfg.is_valid:
+					del val[arch]
+		conf.env.MSVC_INSTALLED_VERSIONS = versiondict
 
 	for version in desired_versions:
+		Logs.debug('msvc: detecting %r - %r', version, desired_versions)
 		try:
-			targets = dict(versiondict[version])
-			for target in platforms:
-				try:
-					try:
-						realtarget,(p1,p2,p3) = targets[target]
-					except conf.errors.ConfigurationError:
-						# lazytup target evaluation errors
-						del(targets[target])
-					else:
-						compiler,revision = version.rsplit(' ', 1)
-						if arch:
-							return compiler,revision,p1,p2,p3,realtarget
-						else:
-							return compiler,revision,p1,p2,p3
-				except KeyError: continue
-		except KeyError: continue
-	conf.fatal('msvc: Impossible to find a valid architecture for building (in setup_msvc)')
+			targets = versiondict[version]
+		except KeyError:
+			continue
+
+		seen = set()
+		for arch in platforms:
+			if arch in seen:
+				continue
+			else:
+				seen.add(arch)
+			try:
+				cfg = targets[arch]
+			except KeyError:
+				continue
+
+			cfg.evaluate()
+			if cfg.is_valid:
+				compiler,revision = version.rsplit(' ', 1)
+				return compiler,revision,cfg.bindirs,cfg.incdirs,cfg.libdirs,cfg.cpu
+	conf.fatal('msvc: Impossible to find a valid architecture for building %r - %r' % (desired_versions, list(versiondict.keys())))
 
 @conf
 def get_msvc_version(conf, compiler, version, target, vcvars):
@@ -147,7 +172,7 @@ def get_msvc_version(conf, compiler, version, target, vcvars):
 	:return: the location of the compiler executable, the location of include dirs, and the library paths
 	:rtype: tuple of strings
 	"""
-	debug('msvc: get_msvc_version: %r %r %r', compiler, version, target)
+	Logs.debug('msvc: get_msvc_version: %r %r %r', compiler, version, target)
 
 	try:
 		conf.msvc_cnt += 1
@@ -187,67 +212,27 @@ def get_msvc_version(conf, compiler, version, target, vcvars):
 	compiler_name, linker_name, lib_name = _get_prog_names(conf, compiler)
 	cxx = conf.find_program(compiler_name, path_list=MSVC_PATH)
 
-	# delete CL if exists. because it could contain parameters wich can change cl's behaviour rather catastrophically.
+	# delete CL if exists. because it could contain parameters which can change cl's behaviour rather catastrophically.
 	if 'CL' in env:
 		del(env['CL'])
 
 	try:
-		try:
-			conf.cmd_and_log(cxx + ['/help'], env=env)
-		except UnicodeError:
-			st = Utils.ex_stack()
-			if conf.logger:
-				conf.logger.error(st)
-			conf.fatal('msvc: Unicode error - check the code page?')
-		except Exception as e:
-			debug('msvc: get_msvc_version: %r %r %r -> failure %s' % (compiler, version, target, str(e)))
-			conf.fatal('msvc: cannot run the compiler in get_msvc_version (run with -v to display errors)')
-		else:
-			debug('msvc: get_msvc_version: %r %r %r -> OK', compiler, version, target)
+		conf.cmd_and_log(cxx + ['/help'], env=env)
+	except UnicodeError:
+		st = traceback.format_exc()
+		if conf.logger:
+			conf.logger.error(st)
+		conf.fatal('msvc: Unicode error - check the code page?')
+	except Exception as e:
+		Logs.debug('msvc: get_msvc_version: %r %r %r -> failure %s', compiler, version, target, str(e))
+		conf.fatal('msvc: cannot run the compiler in get_msvc_version (run with -v to display errors)')
+	else:
+		Logs.debug('msvc: get_msvc_version: %r %r %r -> OK', compiler, version, target)
 	finally:
 		conf.env[compiler_name] = ''
 
 	return (MSVC_PATH, MSVC_INCDIR, MSVC_LIBDIR)
 
-@conf
-def gather_wsdk_versions(conf, versions):
-	"""
-	Use winreg to add the msvc versions to the input list
-
-	:param versions: list to modify
-	:type versions: list
-	"""
-	version_pattern = re.compile('^v..?.?\...?.?')
-	try:
-		all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Wow6432node\\Microsoft\\Microsoft SDKs\\Windows')
-	except WindowsError:
-		try:
-			all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Microsoft\\Microsoft SDKs\\Windows')
-		except WindowsError:
-			return
-	index = 0
-	while 1:
-		try:
-			version = Utils.winreg.EnumKey(all_versions, index)
-		except WindowsError:
-			break
-		index = index + 1
-		if not version_pattern.match(version):
-			continue
-		try:
-			msvc_version = Utils.winreg.OpenKey(all_versions, version)
-			path,type = Utils.winreg.QueryValueEx(msvc_version,'InstallationFolder')
-		except WindowsError:
-			continue
-		if path and os.path.isfile(os.path.join(path, 'bin', 'SetEnv.cmd')):
-			targets = []
-			for target,arch in all_msvc_platforms:
-				try:
-					targets.append((target, (arch, get_compiler_env(conf, 'wsdk', version, '/'+target, os.path.join(path, 'bin', 'SetEnv.cmd')))))
-				except conf.errors.ConfigurationError:
-					pass
-			versions.append(('wsdk ' + version[1:], targets))
-
 def gather_wince_supported_platforms():
 	"""
 	Checks SmartPhones SDKs
@@ -258,31 +243,31 @@ def gather_wince_supported_platforms():
 	supported_wince_platforms = []
 	try:
 		ce_sdk = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Wow6432node\\Microsoft\\Windows CE Tools\\SDKs')
-	except WindowsError:
+	except OSError:
 		try:
 			ce_sdk = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Microsoft\\Windows CE Tools\\SDKs')
-		except WindowsError:
+		except OSError:
 			ce_sdk = ''
 	if not ce_sdk:
 		return supported_wince_platforms
 
-	ce_index = 0
+	index = 0
 	while 1:
 		try:
-			sdk_device = Utils.winreg.EnumKey(ce_sdk, ce_index)
-		except WindowsError:
+			sdk_device = Utils.winreg.EnumKey(ce_sdk, index)
+			sdk = Utils.winreg.OpenKey(ce_sdk, sdk_device)
+		except OSError:
 			break
-		ce_index = ce_index + 1
-		sdk = Utils.winreg.OpenKey(ce_sdk, sdk_device)
+		index += 1
 		try:
 			path,type = Utils.winreg.QueryValueEx(sdk, 'SDKRootDir')
-		except WindowsError:
+		except OSError:
 			try:
 				path,type = Utils.winreg.QueryValueEx(sdk,'SDKInformation')
-				path,xml = os.path.split(path)
-			except WindowsError:
+			except OSError:
 				continue
-		path=str(path)
+			path,xml = os.path.split(path)
+		path = str(path)
 		path,device = os.path.split(path)
 		if not device:
 			path,device = os.path.split(path)
@@ -299,122 +284,140 @@ def gather_msvc_detected_versions():
 	version_pattern = re.compile('^(\d\d?\.\d\d?)(Exp)?$')
 	detected_versions = []
 	for vcver,vcvar in (('VCExpress','Exp'), ('VisualStudio','')):
+		prefix = 'SOFTWARE\\Wow6432node\\Microsoft\\' + vcver
 		try:
-			prefix = 'SOFTWARE\\Wow6432node\\Microsoft\\'+vcver
 			all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, prefix)
-		except WindowsError:
+		except OSError:
+			prefix = 'SOFTWARE\\Microsoft\\' + vcver
 			try:
-				prefix = 'SOFTWARE\\Microsoft\\'+vcver
 				all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, prefix)
-			except WindowsError:
+			except OSError:
 				continue
 
 		index = 0
 		while 1:
 			try:
 				version = Utils.winreg.EnumKey(all_versions, index)
-			except WindowsError:
+			except OSError:
 				break
-			index = index + 1
+			index += 1
 			match = version_pattern.match(version)
-			if not match:
-				continue
-			else:
+			if match:
 				versionnumber = float(match.group(1))
-			detected_versions.append((versionnumber, version+vcvar, prefix+"\\"+version))
+			else:
+				continue
+			detected_versions.append((versionnumber, version+vcvar, prefix+'\\'+version))
 	def fun(tup):
 		return tup[0]
 
 	detected_versions.sort(key = fun)
 	return detected_versions
 
-def get_compiler_env(conf, compiler, version, bat_target, bat, select=None):
-	"""
-	Gets the compiler environment variables as a tuple. Evaluation is eager by default.
-	If set to lazy with ``--msvc_lazy_autodetect`` or ``env.MSVC_LAZY_AUTODETECT``
-	the environment is evaluated when the tuple is destructured or iterated. This means
-	destructuring can throw :py:class:`conf.errors.ConfigurationError`.
-
-	:param conf: configuration context to use to eventually get the version environment
-	:param compiler: compiler name
-	:param version: compiler version number
-	:param bat: path to the batch file to run
-	:param select: optional function to take the realized environment variables tup and map it (e.g. to combine other constant paths)
-	"""
-	lazy = getattr(Options.options, 'msvc_lazy_autodetect', False) or conf.env['MSVC_LAZY_AUTODETECT']
+class target_compiler(object):
+	"""
+	Wrap a compiler configuration; call evaluate() to determine
+	whether the configuration is usable.
+	"""
+	def __init__(self, ctx, compiler, cpu, version, bat_target, bat, callback=None):
+		"""
+		:param ctx: configuration context to use to eventually get the version environment
+		:param compiler: compiler name
+		:param cpu: target cpu
+		:param version: compiler version number
+		:param bat_target: ?
+		:param bat: path to the batch file to run
+		"""
+		self.conf = ctx
+		self.name = None
+		self.is_valid = False
+		self.is_done = False
+
+		self.compiler = compiler
+		self.cpu = cpu
+		self.version = version
+		self.bat_target = bat_target
+		self.bat = bat
+		self.callback = callback
 
-	def msvc_thunk():
-		vs = conf.get_msvc_version(compiler, version, bat_target, bat)
-		if select:
-			return select(vs)
-		else:
-			return vs
-	return lazytup(msvc_thunk, lazy, ([], [], []))
+	def evaluate(self):
+		if self.is_done:
+			return
+		self.is_done = True
+		try:
+			vs = self.conf.get_msvc_version(self.compiler, self.version, self.bat_target, self.bat)
+		except Errors.ConfigurationError:
+			self.is_valid = False
+			return
+		if self.callback:
+			vs = self.callback(self, vs)
+		self.is_valid = True
+		(self.bindirs, self.incdirs, self.libdirs) = vs
+
+	def __str__(self):
+		return str((self.compiler, self.cpu, self.version, self.bat_target, self.bat))
 
-class lazytup(object):
+	def __repr__(self):
+		return repr((self.compiler, self.cpu, self.version, self.bat_target, self.bat))
+
+@conf
+def gather_wsdk_versions(conf, versions):
 	"""
-	A tuple that evaluates its elements from a function when iterated or destructured.
+	Use winreg to add the msvc versions to the input list
 
-	:param fn: thunk to evaluate the tuple on demand
-	:param lazy: whether to delay evaluation or evaluate in the constructor
-	:param default: optional default for :py:func:`repr` if it should not evaluate
+	:param versions: list to modify
+	:type versions: list
 	"""
-	def __init__(self, fn, lazy=True, default=None):
-		self.fn = fn
-		self.default = default
-		if not lazy:
-			self.evaluate()
-	def __len__(self):
-		self.evaluate()
-		return len(self.value)
-	def __iter__(self):
-		self.evaluate()
-		for i, v in enumerate(self.value):
-			yield v
-	def __getitem__(self, i):
-		self.evaluate()
-		return self.value[i]
-	def __repr__(self):
-		if hasattr(self, 'value'):
-			return repr(self.value)
-		elif self.default:
-			return repr(self.default)
-		else:
-			self.evaluate()
-			return repr(self.value)
-	def evaluate(self):
-		if hasattr(self, 'value'):
+	version_pattern = re.compile('^v..?.?\...?.?')
+	try:
+		all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Wow6432node\\Microsoft\\Microsoft SDKs\\Windows')
+	except OSError:
+		try:
+			all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Microsoft\\Microsoft SDKs\\Windows')
+		except OSError:
 			return
-		self.value = self.fn()
+	index = 0
+	while 1:
+		try:
+			version = Utils.winreg.EnumKey(all_versions, index)
+		except OSError:
+			break
+		index += 1
+		if not version_pattern.match(version):
+			continue
+		try:
+			msvc_version = Utils.winreg.OpenKey(all_versions, version)
+			path,type = Utils.winreg.QueryValueEx(msvc_version,'InstallationFolder')
+		except OSError:
+			continue
+		if path and os.path.isfile(os.path.join(path, 'bin', 'SetEnv.cmd')):
+			targets = {}
+			for target,arch in all_msvc_platforms:
+				targets[target] = target_compiler(conf, 'wsdk', arch, version, '/'+target, os.path.join(path, 'bin', 'SetEnv.cmd'))
+			versions['wsdk ' + version[1:]] = targets
 
 @conf
 def gather_msvc_targets(conf, versions, version, vc_path):
 	#Looking for normal MSVC compilers!
-	targets = []
-	if os.path.isfile(os.path.join(vc_path, 'vcvarsall.bat')):
+	targets = {}
+
+	if os.path.isfile(os.path.join(vc_path, 'VC', 'Auxiliary', 'Build', 'vcvarsall.bat')):
 		for target,realtarget in all_msvc_platforms[::-1]:
-			try:
-				targets.append((target, (realtarget, get_compiler_env(conf, 'msvc', version, target, os.path.join(vc_path, 'vcvarsall.bat')))))
-			except conf.errors.ConfigurationError:
-				pass
+			targets[target] = target_compiler(conf, 'msvc', realtarget, version, target, os.path.join(vc_path, 'VC', 'Auxiliary', 'Build', 'vcvarsall.bat'))
+	elif os.path.isfile(os.path.join(vc_path, 'vcvarsall.bat')):
+		for target,realtarget in all_msvc_platforms[::-1]:
+			targets[target] = target_compiler(conf, 'msvc', realtarget, version, target, os.path.join(vc_path, 'vcvarsall.bat'))
 	elif os.path.isfile(os.path.join(vc_path, 'Common7', 'Tools', 'vsvars32.bat')):
-		try:
-			targets.append(('x86', ('x86', get_compiler_env(conf, 'msvc', version, 'x86', os.path.join(vc_path, 'Common7', 'Tools', 'vsvars32.bat')))))
-		except conf.errors.ConfigurationError:
-			pass
+		targets['x86'] = target_compiler(conf, 'msvc', 'x86', version, 'x86', os.path.join(vc_path, 'Common7', 'Tools', 'vsvars32.bat'))
 	elif os.path.isfile(os.path.join(vc_path, 'Bin', 'vcvars32.bat')):
-		try:
-			targets.append(('x86', ('x86', get_compiler_env(conf, 'msvc', version, '', os.path.join(vc_path, 'Bin', 'vcvars32.bat')))))
-		except conf.errors.ConfigurationError:
-			pass
+		targets['x86'] = target_compiler(conf, 'msvc', 'x86', version, '', os.path.join(vc_path, 'Bin', 'vcvars32.bat'))
 	if targets:
-		versions.append(('msvc '+ version, targets))
+		versions['msvc %s' % version] = targets
 
 @conf
 def gather_wince_targets(conf, versions, version, vc_path, vsvars, supported_platforms):
 	#Looking for Win CE compilers!
 	for device,platforms in supported_platforms:
-		cetargets = []
+		targets = {}
 		for platform,compiler,include,lib in platforms:
 			winCEpath = os.path.join(vc_path, 'ce')
 			if not os.path.isdir(winCEpath):
@@ -424,27 +427,52 @@ def gather_wince_targets(conf, versions, version, vc_path, vsvars, supported_pla
 				bindirs = [os.path.join(winCEpath, 'bin', compiler), os.path.join(winCEpath, 'bin', 'x86_'+compiler)]
 				incdirs = [os.path.join(winCEpath, 'include'), os.path.join(winCEpath, 'atlmfc', 'include'), include]
 				libdirs = [os.path.join(winCEpath, 'lib', platform), os.path.join(winCEpath, 'atlmfc', 'lib', platform), lib]
-				def combine_common(compiler_env):
+				def combine_common(obj, compiler_env):
+					# TODO this is likely broken, remove in waf 2.1
 					(common_bindirs,_1,_2) = compiler_env
 					return (bindirs + common_bindirs, incdirs, libdirs)
-				try:
-					cetargets.append((platform, (platform, get_compiler_env(conf, 'msvc', version, 'x86', vsvars, combine_common))))
-				except conf.errors.ConfigurationError:
-					continue
-		if cetargets:
-			versions.append((device + ' ' + version, cetargets))
+				targets[platform] = target_compiler(conf, 'msvc', platform, version, 'x86', vsvars, combine_common)
+		if targets:
+			versions[device + ' ' + version] = targets
 
 @conf
 def gather_winphone_targets(conf, versions, version, vc_path, vsvars):
 	#Looking for WinPhone compilers
-	targets = []
+	targets = {}
 	for target,realtarget in all_msvc_platforms[::-1]:
-		try:
-			targets.append((target, (realtarget, get_compiler_env(conf, 'winphone', version, target, vsvars))))
-		except conf.errors.ConfigurationError:
-			pass
+		targets[target] = target_compiler(conf, 'winphone', realtarget, version, target, vsvars)
 	if targets:
-		versions.append(('winphone '+ version, targets))
+		versions['winphone ' + version] = targets
+
+@conf
+def gather_vswhere_versions(conf, versions):
+	try:
+		import json
+	except ImportError:
+		Logs.error('Visual Studio 2017 detection requires Python 2.6')
+		return
+
+	prg_path = os.environ.get('ProgramFiles(x86)', os.environ.get('ProgramFiles', 'C:\\Program Files (x86)'))
+
+	vswhere = os.path.join(prg_path, 'Microsoft Visual Studio', 'Installer', 'vswhere.exe')
+	args = [vswhere, '-products', '*', '-legacy', '-format', 'json']
+	try:
+		txt = conf.cmd_and_log(args)
+	except Errors.WafError as e:
+		Logs.debug('msvc: vswhere.exe failed %s', e)
+		return
+
+	if sys.version_info[0] < 3:
+		txt = txt.decode(Utils.console_encoding())
+
+	arr = json.loads(txt)
+	arr.sort(key=lambda x: x['installationVersion'])
+	for entry in arr:
+		ver = entry['installationVersion']
+		ver = str('.'.join(ver.split('.')[:2]))
+		path = str(os.path.abspath(entry['installationPath']))
+		if os.path.exists(path) and ('msvc %s' % ver) not in versions:
+			conf.gather_msvc_targets(versions, ver, path)
 
 @conf
 def gather_msvc_versions(conf, versions):
@@ -453,12 +481,20 @@ def gather_msvc_versions(conf, versions):
 		try:
 			try:
 				msvc_version = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, reg + "\\Setup\\VC")
-			except WindowsError:
+			except OSError:
 				msvc_version = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, reg + "\\Setup\\Microsoft Visual C++")
 			path,type = Utils.winreg.QueryValueEx(msvc_version, 'ProductDir')
-			vc_paths.append((version, os.path.abspath(str(path))))
-		except WindowsError:
+		except OSError:
+			try:
+				msvc_version = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, "SOFTWARE\\Wow6432node\\Microsoft\\VisualStudio\\SxS\\VS7")
+				path,type = Utils.winreg.QueryValueEx(msvc_version, version)
+			except OSError:
+				continue
+			else:
+				vc_paths.append((version, os.path.abspath(str(path))))
 			continue
+		else:
+			vc_paths.append((version, os.path.abspath(str(path))))
 
 	wince_supported_platforms = gather_wince_supported_platforms()
 
@@ -492,50 +528,48 @@ def gather_icl_versions(conf, versions):
 	version_pattern = re.compile('^...?.?\....?.?')
 	try:
 		all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Wow6432node\\Intel\\Compilers\\C++')
-	except WindowsError:
+	except OSError:
 		try:
 			all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Intel\\Compilers\\C++')
-		except WindowsError:
+		except OSError:
 			return
 	index = 0
 	while 1:
 		try:
 			version = Utils.winreg.EnumKey(all_versions, index)
-		except WindowsError:
+		except OSError:
 			break
-		index = index + 1
+		index += 1
 		if not version_pattern.match(version):
 			continue
-		targets = []
+		targets = {}
 		for target,arch in all_icl_platforms:
+			if target=='intel64':
+				targetDir='EM64T_NATIVE'
+			else:
+				targetDir=target
 			try:
-				if target=='intel64': targetDir='EM64T_NATIVE'
-				else: targetDir=target
 				Utils.winreg.OpenKey(all_versions,version+'\\'+targetDir)
 				icl_version=Utils.winreg.OpenKey(all_versions,version)
 				path,type=Utils.winreg.QueryValueEx(icl_version,'ProductDir')
+			except OSError:
+				pass
+			else:
 				batch_file=os.path.join(path,'bin','iclvars.bat')
 				if os.path.isfile(batch_file):
-					try:
-						targets.append((target,(arch,get_compiler_env(conf,'intel',version,target,batch_file))))
-					except conf.errors.ConfigurationError:
-						pass
-			except WindowsError:
-				pass
+					targets[target] = target_compiler(conf, 'intel', arch, version, target, batch_file)
 		for target,arch in all_icl_platforms:
 			try:
 				icl_version = Utils.winreg.OpenKey(all_versions, version+'\\'+target)
 				path,type = Utils.winreg.QueryValueEx(icl_version,'ProductDir')
+			except OSError:
+				continue
+			else:
 				batch_file=os.path.join(path,'bin','iclvars.bat')
 				if os.path.isfile(batch_file):
-					try:
-						targets.append((target, (arch, get_compiler_env(conf, 'intel', version, target, batch_file))))
-					except conf.errors.ConfigurationError:
-						pass
-			except WindowsError:
-				continue
+					targets[target] = target_compiler(conf, 'intel', arch, version, target, batch_file)
 		major = version[0:2]
-		versions.append(('intel ' + major, targets))
+		versions['intel ' + major] = targets
 
 @conf
 def gather_intel_composer_versions(conf, versions):
@@ -548,42 +582,44 @@ def gather_intel_composer_versions(conf, versions):
 	version_pattern = re.compile('^...?.?\...?.?.?')
 	try:
 		all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Wow6432node\\Intel\\Suites')
-	except WindowsError:
+	except OSError:
 		try:
 			all_versions = Utils.winreg.OpenKey(Utils.winreg.HKEY_LOCAL_MACHINE, 'SOFTWARE\\Intel\\Suites')
-		except WindowsError:
+		except OSError:
 			return
 	index = 0
 	while 1:
 		try:
 			version = Utils.winreg.EnumKey(all_versions, index)
-		except WindowsError:
+		except OSError:
 			break
-		index = index + 1
+		index += 1
 		if not version_pattern.match(version):
 			continue
-		targets = []
+		targets = {}
 		for target,arch in all_icl_platforms:
+			if target=='intel64':
+				targetDir='EM64T_NATIVE'
+			else:
+				targetDir=target
 			try:
-				if target=='intel64': targetDir='EM64T_NATIVE'
-				else: targetDir=target
 				try:
 					defaults = Utils.winreg.OpenKey(all_versions,version+'\\Defaults\\C++\\'+targetDir)
-				except WindowsError:
-					if targetDir=='EM64T_NATIVE':
+				except OSError:
+					if targetDir == 'EM64T_NATIVE':
 						defaults = Utils.winreg.OpenKey(all_versions,version+'\\Defaults\\C++\\EM64T')
 					else:
-						raise WindowsError
+						raise
 				uid,type = Utils.winreg.QueryValueEx(defaults, 'SubKey')
 				Utils.winreg.OpenKey(all_versions,version+'\\'+uid+'\\C++\\'+targetDir)
 				icl_version=Utils.winreg.OpenKey(all_versions,version+'\\'+uid+'\\C++')
 				path,type=Utils.winreg.QueryValueEx(icl_version,'ProductDir')
+			except OSError:
+				pass
+			else:
 				batch_file=os.path.join(path,'bin','iclvars.bat')
 				if os.path.isfile(batch_file):
-					try:
-						targets.append((target,(arch,get_compiler_env(conf,'intel',version,target,batch_file))))
-					except conf.errors.ConfigurationError:
-						pass
+					targets[target] = target_compiler(conf, 'intel', arch, version, target, batch_file)
 				# The intel compilervar_arch.bat is broken when used with Visual Studio Express 2012
 				# http://software.intel.com/en-us/forums/topic/328487
 				compilervars_warning_attr = '_compilervars_warning_key'
@@ -601,72 +637,41 @@ def gather_intel_composer_versions(conf, versions):
 								'(VSWinExpress.exe) but it does not seem to be installed at %r. '
 								'The intel command line set up will fail to configure unless the file %r'
 								'is patched. See: %s') % (vs_express_path, compilervars_arch, patch_url))
-			except WindowsError:
-				pass
 		major = version[0:2]
-		versions.append(('intel ' + major, targets))
+		versions['intel ' + major] = targets
 
 @conf
-def get_msvc_versions(conf, eval_and_save=True):
-	"""
-	:return: list of compilers installed
-	:rtype: list of string
-	"""
-	if conf.env['MSVC_INSTALLED_VERSIONS']:
-		return conf.env['MSVC_INSTALLED_VERSIONS']
-
-	# Gather all the compiler versions and targets. This phase can be lazy
-	# per lazy detection settings.
-	lst = []
-	conf.gather_icl_versions(lst)
-	conf.gather_intel_composer_versions(lst)
-	conf.gather_wsdk_versions(lst)
-	conf.gather_msvc_versions(lst)
-
-	# Override lazy detection by evaluating after the fact.
-	if eval_and_save:
-		def checked_target(t):
-			target,(arch,paths) = t
-			try:
-				paths.evaluate()
-			except conf.errors.ConfigurationError:
-				return None
-			else:
-				return t
-		lst = [(version, list(filter(checked_target, targets))) for version, targets in lst]
-		conf.env['MSVC_INSTALLED_VERSIONS'] = lst
-
-	return lst
+def detect_msvc(self):
+	return self.setup_msvc(self.get_msvc_versions())
 
 @conf
-def print_all_msvc_detected(conf):
+def get_msvc_versions(self):
 	"""
-	Print the contents of *conf.env.MSVC_INSTALLED_VERSIONS*
+	:return: platform to compiler configurations
+	:rtype: dict
 	"""
-	for version,targets in conf.env['MSVC_INSTALLED_VERSIONS']:
-		Logs.info(version)
-		for target,l in targets:
-			Logs.info("\t"+target)
-
-@conf
-def detect_msvc(conf, arch = False):
-	# Save installed versions only if lazy detection is disabled.
-	lazy_detect = getattr(Options.options, 'msvc_lazy_autodetect', False) or conf.env['MSVC_LAZY_AUTODETECT']
-	versions = get_msvc_versions(conf, not lazy_detect)
-	return setup_msvc(conf, versions, arch)
+	dct = Utils.ordered_iter_dict()
+	self.gather_icl_versions(dct)
+	self.gather_intel_composer_versions(dct)
+	self.gather_wsdk_versions(dct)
+	self.gather_msvc_versions(dct)
+	self.gather_vswhere_versions(dct)
+	Logs.debug('msvc: detected versions %r', list(dct.keys()))
+	return dct
 
 @conf
 def find_lt_names_msvc(self, libname, is_static=False):
 	"""
 	Win32/MSVC specific code to glean out information from libtool la files.
-	this function is not attached to the task_gen class
+	this function is not attached to the task_gen class. Returns a triplet:
+	(library absolute path, library name without extension, whether the library is static)
 	"""
 	lt_names=[
 		'lib%s.la' % libname,
 		'%s.la' % libname,
 	]
 
-	for path in self.env['LIBPATH']:
+	for path in self.env.LIBPATH:
 		for la in lt_names:
 			laf=os.path.join(path,la)
 			dll=None
@@ -708,14 +713,14 @@ def libname_msvc(self, libname, is_static=False):
 	(lt_path, lt_libname, lt_static) = self.find_lt_names_msvc(lib, is_static)
 
 	if lt_path != None and lt_libname != None:
-		if lt_static == True:
-			# file existance check has been made by find_lt_names
+		if lt_static:
+			# file existence check has been made by find_lt_names
 			return os.path.join(lt_path,lt_libname)
 
 	if lt_path != None:
-		_libpaths=[lt_path] + self.env['LIBPATH']
+		_libpaths = [lt_path] + self.env.LIBPATH
 	else:
-		_libpaths=self.env['LIBPATH']
+		_libpaths = self.env.LIBPATH
 
 	static_libs=[
 		'lib%ss.lib' % lib,
@@ -741,11 +746,11 @@ def libname_msvc(self, libname, is_static=False):
 	for path in _libpaths:
 		for libn in libnames:
 			if os.path.exists(os.path.join(path, libn)):
-				debug('msvc: lib found: %s' % os.path.join(path,libn))
+				Logs.debug('msvc: lib found: %s', os.path.join(path,libn))
 				return re.sub('\.lib$', '',libn)
 
 	#if no lib can be found, just return the libname as msvc expects it
-	self.fatal("The library %r could not be found" % libname)
+	self.fatal('The library %r could not be found' % libname)
 	return re.sub('\.lib$', '', libname)
 
 @conf
@@ -753,7 +758,7 @@ def check_lib_msvc(self, libname, is_static=False, uselib_store=None):
 	"""
 	Ideally we should be able to place the lib in the right env var, either STLIB or LIB,
 	but we don't distinguish static libs from shared libs.
-	This is ok since msvc doesn't have any special linker flag to select static libs (no env['STLIB_MARKER'])
+	This is ok since msvc doesn't have any special linker flag to select static libs (no env.STLIB_MARKER)
 	"""
 	libn = self.libname_msvc(libname, is_static)
 
@@ -790,27 +795,26 @@ def no_autodetect(conf):
 	configure(conf)
 
 @conf
-def autodetect(conf, arch = False):
+def autodetect(conf, arch=False):
 	v = conf.env
 	if v.NO_MSVC_DETECT:
 		return
+
+	compiler, version, path, includes, libdirs, cpu = conf.detect_msvc()
 	if arch:
-		compiler, version, path, includes, libdirs, arch = conf.detect_msvc(True)
-		v['DEST_CPU'] = arch
-	else:
-		compiler, version, path, includes, libdirs = conf.detect_msvc()
+		v.DEST_CPU = cpu
 
-	v['PATH'] = path
-	v['INCLUDES'] = includes
-	v['LIBPATH'] = libdirs
-	v['MSVC_COMPILER'] = compiler
+	v.PATH = path
+	v.INCLUDES = includes
+	v.LIBPATH = libdirs
+	v.MSVC_COMPILER = compiler
 	try:
-		v['MSVC_VERSION'] = float(version)
-	except Exception:
-		v['MSVC_VERSION'] = float(version[:-3])
+		v.MSVC_VERSION = float(version)
+	except ValueError:
+		v.MSVC_VERSION = float(version[:-3])
 
 def _get_prog_names(conf, compiler):
-	if compiler=='intel':
+	if compiler == 'intel':
 		compiler_name = 'ICL'
 		linker_name = 'XILINK'
 		lib_name = 'XILIB'
@@ -829,9 +833,9 @@ def find_msvc(conf):
 
 	# the autodetection is supposed to be performed before entering in this method
 	v = conf.env
-	path = v['PATH']
-	compiler = v['MSVC_COMPILER']
-	version = v['MSVC_VERSION']
+	path = v.PATH
+	compiler = v.MSVC_COMPILER
+	version = v.MSVC_VERSION
 
 	compiler_name, linker_name, lib_name = _get_prog_names(conf, compiler)
 	v.MSVC_MANIFEST = (compiler == 'msvc' and version >= 8) or (compiler == 'wsdk' and version >= 6) or (compiler == 'intel' and version >= 11)
@@ -841,48 +845,47 @@ def find_msvc(conf):
 
 	# before setting anything, check if the compiler is really msvc
 	env = dict(conf.environ)
-	if path: env.update(PATH = ';'.join(path))
+	if path:
+		env.update(PATH = ';'.join(path))
 	if not conf.cmd_and_log(cxx + ['/nologo', '/help'], env=env):
 		conf.fatal('the msvc compiler could not be identified')
 
 	# c/c++ compiler
-	v['CC'] = v['CXX'] = cxx
-	v['CC_NAME'] = v['CXX_NAME'] = 'msvc'
+	v.CC = v.CXX = cxx
+	v.CC_NAME = v.CXX_NAME = 'msvc'
 
 	# linker
-	if not v['LINK_CXX']:
-		link = conf.find_program(linker_name, path_list=path)
-		if link: v['LINK_CXX'] = link
-		else: conf.fatal('%s was not found (linker)' % linker_name)
-		v['LINK'] = link
+	if not v.LINK_CXX:
+		conf.find_program(linker_name, path_list=path, errmsg='%s was not found (linker)' % linker_name, var='LINK_CXX')
 
-	if not v['LINK_CC']:
-		v['LINK_CC'] = v['LINK_CXX']
+	if not v.LINK_CC:
+		v.LINK_CC = v.LINK_CXX
 
 	# staticlib linker
-	if not v['AR']:
+	if not v.AR:
 		stliblink = conf.find_program(lib_name, path_list=path, var='AR')
-		if not stliblink: return
-		v['ARFLAGS'] = ['/NOLOGO']
+		if not stliblink:
+			return
+		v.ARFLAGS = ['/nologo']
 
 	# manifest tool. Not required for VS 2003 and below. Must have for VS 2005 and later
 	if v.MSVC_MANIFEST:
 		conf.find_program('MT', path_list=path, var='MT')
-		v['MTFLAGS'] = ['/NOLOGO']
+		v.MTFLAGS = ['/nologo']
 
 	try:
 		conf.load('winres')
-	except Errors.WafError:
-		warn('Resource compiler not found. Compiling resource file is disabled')
+	except Errors.ConfigurationError:
+		Logs.warn('Resource compiler not found. Compiling resource file is disabled')
 
 @conf
 def visual_studio_add_flags(self):
 	"""visual studio flags found in the system environment"""
 	v = self.env
-	try: v.prepend_value('INCLUDES', [x for x in self.environ['INCLUDE'].split(';') if x]) # notice the 'S'
-	except Exception: pass
-	try: v.prepend_value('LIBPATH', [x for x in self.environ['LIB'].split(';') if x])
-	except Exception: pass
+	if self.environ.get('INCLUDE'):
+		v.prepend_value('INCLUDES', [x for x in self.environ['INCLUDE'].split(';') if x]) # notice the 'S'
+	if self.environ.get('LIB'):
+		v.prepend_value('LIBPATH', [x for x in self.environ['LIB'].split(';') if x])
 
 @conf
 def msvc_common_flags(conf):
@@ -891,62 +894,53 @@ def msvc_common_flags(conf):
 	"""
 	v = conf.env
 
-	v['DEST_BINFMT'] = 'pe'
+	v.DEST_BINFMT = 'pe'
 	v.append_value('CFLAGS', ['/nologo'])
 	v.append_value('CXXFLAGS', ['/nologo'])
-	v['DEFINES_ST']     = '/D%s'
+	v.append_value('LINKFLAGS', ['/nologo'])
+	v.DEFINES_ST   = '/D%s'
 
-	v['CC_SRC_F']     = ''
-	v['CC_TGT_F']     = ['/c', '/Fo']
-	v['CXX_SRC_F']    = ''
-	v['CXX_TGT_F']    = ['/c', '/Fo']
+	v.CC_SRC_F     = ''
+	v.CC_TGT_F     = ['/c', '/Fo']
+	v.CXX_SRC_F    = ''
+	v.CXX_TGT_F    = ['/c', '/Fo']
 
 	if (v.MSVC_COMPILER == 'msvc' and v.MSVC_VERSION >= 8) or (v.MSVC_COMPILER == 'wsdk' and v.MSVC_VERSION >= 6):
-		v['CC_TGT_F']= ['/FC'] + v['CC_TGT_F']
-		v['CXX_TGT_F']= ['/FC'] + v['CXX_TGT_F']
-
-	v['CPPPATH_ST']   = '/I%s' # template for adding include paths
+		v.CC_TGT_F = ['/FC'] + v.CC_TGT_F
+		v.CXX_TGT_F = ['/FC'] + v.CXX_TGT_F
 
-	v['AR_TGT_F'] = v['CCLNK_TGT_F'] = v['CXXLNK_TGT_F'] = '/OUT:'
+	v.CPPPATH_ST = '/I%s' # template for adding include paths
 
-	# Subsystem specific flags
-	v['CFLAGS_CONSOLE']   = v['CXXFLAGS_CONSOLE']   = ['/SUBSYSTEM:CONSOLE']
-	v['CFLAGS_NATIVE']    = v['CXXFLAGS_NATIVE']    = ['/SUBSYSTEM:NATIVE']
-	v['CFLAGS_POSIX']     = v['CXXFLAGS_POSIX']     = ['/SUBSYSTEM:POSIX']
-	v['CFLAGS_WINDOWS']   = v['CXXFLAGS_WINDOWS']   = ['/SUBSYSTEM:WINDOWS']
-	v['CFLAGS_WINDOWSCE'] = v['CXXFLAGS_WINDOWSCE'] = ['/SUBSYSTEM:WINDOWSCE']
+	v.AR_TGT_F = v.CCLNK_TGT_F = v.CXXLNK_TGT_F = '/OUT:'
 
 	# CRT specific flags
-	v['CFLAGS_CRT_MULTITHREADED']     = v['CXXFLAGS_CRT_MULTITHREADED']     = ['/MT']
-	v['CFLAGS_CRT_MULTITHREADED_DLL'] = v['CXXFLAGS_CRT_MULTITHREADED_DLL'] = ['/MD']
+	v.CFLAGS_CRT_MULTITHREADED     = v.CXXFLAGS_CRT_MULTITHREADED     = ['/MT']
+	v.CFLAGS_CRT_MULTITHREADED_DLL = v.CXXFLAGS_CRT_MULTITHREADED_DLL = ['/MD']
 
-	v['CFLAGS_CRT_MULTITHREADED_DBG']     = v['CXXFLAGS_CRT_MULTITHREADED_DBG']     = ['/MTd']
-	v['CFLAGS_CRT_MULTITHREADED_DLL_DBG'] = v['CXXFLAGS_CRT_MULTITHREADED_DLL_DBG'] = ['/MDd']
+	v.CFLAGS_CRT_MULTITHREADED_DBG     = v.CXXFLAGS_CRT_MULTITHREADED_DBG     = ['/MTd']
+	v.CFLAGS_CRT_MULTITHREADED_DLL_DBG = v.CXXFLAGS_CRT_MULTITHREADED_DLL_DBG = ['/MDd']
 
-	# linker
-	v['LIB_ST']            = '%s.lib' # template for adding shared libs
-	v['LIBPATH_ST']        = '/LIBPATH:%s' # template for adding libpaths
-	v['STLIB_ST']          = '%s.lib'
-	v['STLIBPATH_ST']      = '/LIBPATH:%s'
+	v.LIB_ST            = '%s.lib'
+	v.LIBPATH_ST        = '/LIBPATH:%s'
+	v.STLIB_ST          = '%s.lib'
+	v.STLIBPATH_ST      = '/LIBPATH:%s'
 
-	v.append_value('LINKFLAGS', ['/NOLOGO'])
-	if v['MSVC_MANIFEST']:
+	if v.MSVC_MANIFEST:
 		v.append_value('LINKFLAGS', ['/MANIFEST'])
 
-	# shared library
-	v['CFLAGS_cshlib']     = []
-	v['CXXFLAGS_cxxshlib'] = []
-	v['LINKFLAGS_cshlib']  = v['LINKFLAGS_cxxshlib'] = ['/DLL']
-	v['cshlib_PATTERN']    = v['cxxshlib_PATTERN'] = '%s.dll'
-	v['implib_PATTERN']    = '%s.lib'
-	v['IMPLIB_ST']         = '/IMPLIB:%s'
+	v.CFLAGS_cshlib     = []
+	v.CXXFLAGS_cxxshlib = []
+	v.LINKFLAGS_cshlib  = v.LINKFLAGS_cxxshlib = ['/DLL']
+	v.cshlib_PATTERN    = v.cxxshlib_PATTERN = '%s.dll'
+	v.implib_PATTERN    = '%s.lib'
+	v.IMPLIB_ST         = '/IMPLIB:%s'
+
+	v.LINKFLAGS_cstlib  = []
+	v.cstlib_PATTERN    = v.cxxstlib_PATTERN = '%s.lib'
 
-	# static library
-	v['LINKFLAGS_cstlib']  = []
-	v['cstlib_PATTERN']    = v['cxxstlib_PATTERN'] = '%s.lib'
+	v.cprogram_PATTERN  = v.cxxprogram_PATTERN = '%s.exe'
 
-	# program
-	v['cprogram_PATTERN']  = v['cxxprogram_PATTERN']    = '%s.exe'
+	v.def_PATTERN       = '/def:%s'
 
 
 #######################################################################################################
@@ -980,12 +974,10 @@ def build(bld):
 				self.link_task.outputs.append(pdbnode)
 
 				if getattr(self, 'install_task', None):
-					self.pdb_install_task = self.bld.install_files(self.install_task.dest, pdbnode, env=self.env)
-
+					self.pdb_install_task = self.add_install_files(
+						install_to=self.install_task.install_to, install_from=pdbnode)
 				break
 
-# split the manifest file processing from the link task, like for the rc processing
-
 @feature('cprogram', 'cshlib', 'cxxprogram', 'cxxshlib')
 @after_method('apply_link')
 def apply_manifest(self):
@@ -995,161 +987,16 @@ def apply_manifest(self):
 	the manifest file, the binaries are unusable.
 	See: http://msdn2.microsoft.com/en-us/library/ms235542(VS.80).aspx
 	"""
-
 	if self.env.CC_NAME == 'msvc' and self.env.MSVC_MANIFEST and getattr(self, 'link_task', None):
 		out_node = self.link_task.outputs[0]
 		man_node = out_node.parent.find_or_declare(out_node.name + '.manifest')
 		self.link_task.outputs.append(man_node)
-		self.link_task.do_manifest = True
-
-def exec_mf(self):
-	"""
-	Create the manifest file
-	"""
-	env = self.env
-	mtool = env['MT']
-	if not mtool:
-		return 0
-
-	self.do_manifest = False
-
-	outfile = self.outputs[0].abspath()
-
-	manifest = None
-	for out_node in self.outputs:
-		if out_node.name.endswith('.manifest'):
-			manifest = out_node.abspath()
-			break
-	if manifest is None:
-		# Should never get here.  If we do, it means the manifest file was
-		# never added to the outputs list, thus we don't have a manifest file
-		# to embed, so we just return.
-		return 0
-
-	# embedding mode. Different for EXE's and DLL's.
-	# see: http://msdn2.microsoft.com/en-us/library/ms235591(VS.80).aspx
-	mode = ''
-	if 'cprogram' in self.generator.features or 'cxxprogram' in self.generator.features:
-		mode = '1'
-	elif 'cshlib' in self.generator.features or 'cxxshlib' in self.generator.features:
-		mode = '2'
-
-	debug('msvc: embedding manifest in mode %r' % mode)
-
-	lst = [] + mtool
-	lst.extend(Utils.to_list(env['MTFLAGS']))
-	lst.extend(['-manifest', manifest])
-	lst.append('-outputresource:%s;%s' % (outfile, mode))
-
-	return self.exec_command(lst)
-
-def quote_response_command(self, flag):
-	if flag.find(' ') > -1:
-		for x in ('/LIBPATH:', '/IMPLIB:', '/OUT:', '/I'):
-			if flag.startswith(x):
-				flag = '%s"%s"' % (x, flag[len(x):])
-				break
-		else:
-			flag = '"%s"' % flag
-	return flag
-
-def exec_response_command(self, cmd, **kw):
-	# not public yet
-	try:
-		tmp = None
-		if sys.platform.startswith('win') and isinstance(cmd, list) and len(' '.join(cmd)) >= 8192:
-			program = cmd[0] #unquoted program name, otherwise exec_command will fail
-			cmd = [self.quote_response_command(x) for x in cmd]
-			(fd, tmp) = tempfile.mkstemp()
-			os.write(fd, '\r\n'.join(i.replace('\\', '\\\\') for i in cmd[1:]).encode())
-			os.close(fd)
-			cmd = [program, '@' + tmp]
-		# no return here, that's on purpose
-		ret = self.generator.bld.exec_command(cmd, **kw)
-	finally:
-		if tmp:
-			try:
-				os.remove(tmp)
-			except OSError:
-				pass # anti-virus and indexers can keep the files open -_-
-	return ret
-
-########## stupid evil command modification: concatenate the tokens /Fx, /doc, and /x: with the next token
-
-def exec_command_msvc(self, *k, **kw):
-	"""
-	Change the command-line execution for msvc programs.
-	Instead of quoting all the paths and keep using the shell, we can just join the options msvc is interested in
-	"""
-	if isinstance(k[0], list):
-		lst = []
-		carry = ''
-		for a in k[0]:
-			if a == '/Fo' or a == '/doc' or a[-1] == ':':
-				carry = a
-			else:
-				lst.append(carry + a)
-				carry = ''
-		k = [lst]
-
-	if self.env['PATH']:
-		env = dict(self.env.env or os.environ)
-		env.update(PATH = ';'.join(self.env['PATH']))
-		kw['env'] = env
-
-	bld = self.generator.bld
-	try:
-		if not kw.get('cwd', None):
-			kw['cwd'] = bld.cwd
-	except AttributeError:
-		bld.cwd = kw['cwd'] = bld.variant_dir
-
-	ret = self.exec_response_command(k[0], **kw)
-	if not ret and getattr(self, 'do_manifest', None):
-		ret = self.exec_mf()
-	return ret
-
-def wrap_class(class_name):
-	"""
-	Manifest file processing and @response file workaround for command-line length limits on Windows systems
-	The indicated task class is replaced by a subclass to prevent conflicts in case the class is wrapped more than once
-	"""
-	cls = Task.classes.get(class_name, None)
-
-	if not cls:
-		return None
-
-	derived_class = type(class_name, (cls,), {})
-
-	def exec_command(self, *k, **kw):
-		if self.env['CC_NAME'] == 'msvc':
-			return self.exec_command_msvc(*k, **kw)
-		else:
-			return super(derived_class, self).exec_command(*k, **kw)
-
-	# Chain-up monkeypatch needed since exec_command() is in base class API
-	derived_class.exec_command = exec_command
-
-	# No chain-up behavior needed since the following methods aren't in
-	# base class API
-	derived_class.exec_response_command = exec_response_command
-	derived_class.quote_response_command = quote_response_command
-	derived_class.exec_command_msvc = exec_command_msvc
-	derived_class.exec_mf = exec_mf
-
-	if hasattr(cls, 'hcode'):
-		derived_class.hcode = cls.hcode
-
-	return derived_class
-
-for k in 'c cxx cprogram cxxprogram cshlib cxxshlib cstlib cxxstlib'.split():
-	wrap_class(k)
+		self.env.DO_MANIFEST = True
 
 def make_winapp(self, family):
 	append = self.env.append_unique
 	append('DEFINES', 'WINAPI_FAMILY=%s' % family)
-	append('CXXFLAGS', '/ZW')
-	append('CXXFLAGS', '/TP')
+	append('CXXFLAGS', ['/ZW', '/TP'])
 	for lib_path in self.env.LIBPATH:
 		append('CXXFLAGS','/AI%s'%lib_path)
 
@@ -1161,9 +1008,7 @@ def make_winphone_app(self):
 	Insert configuration flags for windows phone applications (adds /ZW, /TP...)
 	"""
 	make_winapp(self, 'WINAPI_FAMILY_PHONE_APP')
-	conf.env.append_unique('LINKFLAGS', '/NODEFAULTLIB:ole32.lib')
-	conf.env.append_unique('LINKFLAGS', 'PhoneAppModelHost.lib')
-
+	self.env.append_unique('LINKFLAGS', ['/NODEFAULTLIB:ole32.lib', 'PhoneAppModelHost.lib'])
 
 @feature('winapp')
 @after_method('process_use')
diff --git a/waflib/Tools/nasm.py b/waflib/Tools/nasm.py
new file mode 100644
index 00000000..411d5826
--- /dev/null
+++ b/waflib/Tools/nasm.py
@@ -0,0 +1,26 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2008-2018 (ita)
+
+"""
+Nasm tool (asm processing)
+"""
+
+import os
+import waflib.Tools.asm # leave this
+from waflib.TaskGen import feature
+
+@feature('asm')
+def apply_nasm_vars(self):
+	"""provided for compatibility"""
+	self.env.append_value('ASFLAGS', self.to_list(getattr(self, 'nasm_flags', [])))
+
+def configure(conf):
+	"""
+	Detect nasm/yasm and set the variable *AS*
+	"""
+	conf.find_program(['nasm', 'yasm'], var='AS')
+	conf.env.AS_TGT_F = ['-o']
+	conf.env.ASLNK_TGT_F = ['-o']
+	conf.load('asm')
+	conf.env.ASMPATH_ST = '-I%s' + os.sep
diff --git a/waflib/Tools/nobuild.py b/waflib/Tools/nobuild.py
new file mode 100644
index 00000000..2e4b055e
--- /dev/null
+++ b/waflib/Tools/nobuild.py
@@ -0,0 +1,24 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2015 (ita)
+
+"""
+Override the build commands to write empty files.
+This is useful for profiling and evaluating the Python overhead.
+
+To use::
+
+    def build(bld):
+        ...
+        bld.load('nobuild')
+
+"""
+
+from waflib import Task
+def build(bld):
+	def run(self):
+		for x in self.outputs:
+			x.write('')
+	for (name, cls) in Task.classes.items():
+		cls.run = run
+
diff --git a/waflib/Tools/perl.py b/waflib/Tools/perl.py
new file mode 100644
index 00000000..32b03fba
--- /dev/null
+++ b/waflib/Tools/perl.py
@@ -0,0 +1,156 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# andersg at 0x63.nu 2007
+# Thomas Nagy 2016-2018 (ita)
+
+"""
+Support for Perl extensions. A C/C++ compiler is required::
+
+	def options(opt):
+		opt.load('compiler_c perl')
+	def configure(conf):
+		conf.load('compiler_c perl')
+		conf.check_perl_version((5,6,0))
+		conf.check_perl_ext_devel()
+		conf.check_perl_module('Cairo')
+		conf.check_perl_module('Devel::PPPort 4.89')
+	def build(bld):
+		bld(
+			features     = 'c cshlib perlext',
+			source       = 'Mytest.xs',
+			target       = 'Mytest',
+			install_path = '${ARCHDIR_PERL}/auto')
+		bld.install_files('${ARCHDIR_PERL}', 'Mytest.pm')
+"""
+
+import os
+from waflib import Task, Options, Utils, Errors
+from waflib.Configure import conf
+from waflib.TaskGen import extension, feature, before_method
+
+@before_method('apply_incpaths', 'apply_link', 'propagate_uselib_vars')
+@feature('perlext')
+def init_perlext(self):
+	"""
+	Change the values of *cshlib_PATTERN* and *cxxshlib_PATTERN* to remove the
+	*lib* prefix from library names.
+	"""
+	self.uselib = self.to_list(getattr(self, 'uselib', []))
+	if not 'PERLEXT' in self.uselib:
+		self.uselib.append('PERLEXT')
+	self.env.cshlib_PATTERN = self.env.cxxshlib_PATTERN = self.env.perlext_PATTERN
+
+@extension('.xs')
+def xsubpp_file(self, node):
+	"""
+	Create :py:class:`waflib.Tools.perl.xsubpp` tasks to process *.xs* files
+	"""
+	outnode = node.change_ext('.c')
+	self.create_task('xsubpp', node, outnode)
+	self.source.append(outnode)
+
+class xsubpp(Task.Task):
+	"""
+	Process *.xs* files
+	"""
+	run_str = '${PERL} ${XSUBPP} -noprototypes -typemap ${EXTUTILS_TYPEMAP} ${SRC} > ${TGT}'
+	color   = 'BLUE'
+	ext_out = ['.h']
+
+@conf
+def check_perl_version(self, minver=None):
+	"""
+	Check if Perl is installed, and set the variable PERL.
+	minver is supposed to be a tuple
+	"""
+	res = True
+	if minver:
+		cver = '.'.join(map(str,minver))
+	else:
+		cver = ''
+
+	self.start_msg('Checking for minimum perl version %s' % cver)
+
+	perl = self.find_program('perl', var='PERL', value=getattr(Options.options, 'perlbinary', None))
+	version = self.cmd_and_log(perl + ["-e", 'printf \"%vd\", $^V'])
+	if not version:
+		res = False
+		version = "Unknown"
+	elif not minver is None:
+		ver = tuple(map(int, version.split(".")))
+		if ver < minver:
+			res = False
+
+	self.end_msg(version, color=res and 'GREEN' or 'YELLOW')
+	return res
+
+@conf
+def check_perl_module(self, module):
+	"""
+	Check if specified perlmodule is installed.
+
+	The minimum version can be specified by specifying it after modulename
+	like this::
+
+		def configure(conf):
+			conf.check_perl_module("Some::Module 2.92")
+	"""
+	cmd = self.env.PERL + ['-e', 'use %s' % module]
+	self.start_msg('perl module %s' % module)
+	try:
+		r = self.cmd_and_log(cmd)
+	except Errors.WafError:
+		self.end_msg(False)
+		return None
+	self.end_msg(r or True)
+	return r
+
+@conf
+def check_perl_ext_devel(self):
+	"""
+	Check for configuration needed to build perl extensions.
+
+	Sets different xxx_PERLEXT variables in the environment.
+
+	Also sets the ARCHDIR_PERL variable useful as installation path,
+	which can be overridden by ``--with-perl-archdir`` option.
+	"""
+
+	env = self.env
+	perl = env.PERL
+	if not perl:
+		self.fatal('find perl first')
+
+	def cmd_perl_config(s):
+		return perl + ['-MConfig', '-e', 'print \"%s\"' % s]
+	def cfg_str(cfg):
+		return self.cmd_and_log(cmd_perl_config(cfg))
+	def cfg_lst(cfg):
+		return Utils.to_list(cfg_str(cfg))
+	def find_xsubpp():
+		for var in ('privlib', 'vendorlib'):
+			xsubpp = cfg_lst('$Config{%s}/ExtUtils/xsubpp$Config{exe_ext}' % var)
+			if xsubpp and os.path.isfile(xsubpp[0]):
+				return xsubpp
+		return self.find_program('xsubpp')
+
+	env.LINKFLAGS_PERLEXT = cfg_lst('$Config{lddlflags}')
+	env.INCLUDES_PERLEXT = cfg_lst('$Config{archlib}/CORE')
+	env.CFLAGS_PERLEXT = cfg_lst('$Config{ccflags} $Config{cccdlflags}')
+	env.EXTUTILS_TYPEMAP = cfg_lst('$Config{privlib}/ExtUtils/typemap')
+	env.XSUBPP = find_xsubpp()
+
+	if not getattr(Options.options, 'perlarchdir', None):
+		env.ARCHDIR_PERL = cfg_str('$Config{sitearch}')
+	else:
+		env.ARCHDIR_PERL = getattr(Options.options, 'perlarchdir')
+
+	env.perlext_PATTERN = '%s.' + cfg_str('$Config{dlext}')
+
+def options(opt):
+	"""
+	Add the ``--with-perl-archdir`` and ``--with-perl-binary`` command-line options.
+	"""
+	opt.add_option('--with-perl-binary', type='string', dest='perlbinary', help = 'Specify alternate perl binary', default=None)
+	opt.add_option('--with-perl-archdir', type='string', dest='perlarchdir', help = 'Specify directory where to install arch specific files', default=None)
+
diff --git a/waflib/Tools/python.py b/waflib/Tools/python.py
new file mode 100644
index 00000000..25841d03
--- /dev/null
+++ b/waflib/Tools/python.py
@@ -0,0 +1,627 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2007-2015 (ita)
+# Gustavo Carneiro (gjc), 2007
+
+"""
+Support for Python, detect the headers and libraries and provide
+*use* variables to link C/C++ programs against them::
+
+	def options(opt):
+		opt.load('compiler_c python')
+	def configure(conf):
+		conf.load('compiler_c python')
+		conf.check_python_version((2,4,2))
+		conf.check_python_headers()
+	def build(bld):
+		bld.program(features='pyembed', source='a.c', target='myprog')
+		bld.shlib(features='pyext', source='b.c', target='mylib')
+"""
+
+import os, sys
+from waflib import Errors, Logs, Node, Options, Task, Utils
+from waflib.TaskGen import extension, before_method, after_method, feature
+from waflib.Configure import conf
+
+FRAG = '''
+#include <Python.h>
+#ifdef __cplusplus
+extern "C" {
+#endif
+	void Py_Initialize(void);
+	void Py_Finalize(void);
+#ifdef __cplusplus
+}
+#endif
+int main(int argc, char **argv)
+{
+   (void)argc; (void)argv;
+   Py_Initialize();
+   Py_Finalize();
+   return 0;
+}
+'''
+"""
+Piece of C/C++ code used in :py:func:`waflib.Tools.python.check_python_headers`
+"""
+
+INST = '''
+import sys, py_compile
+py_compile.compile(sys.argv[1], sys.argv[2], sys.argv[3], True)
+'''
+"""
+Piece of Python code used in :py:class:`waflib.Tools.python.pyo` and :py:class:`waflib.Tools.python.pyc` for byte-compiling python files
+"""
+
+DISTUTILS_IMP = ['from distutils.sysconfig import get_config_var, get_python_lib']
+
+@before_method('process_source')
+@feature('py')
+def feature_py(self):
+	"""
+	Create tasks to byte-compile .py files and install them, if requested
+	"""
+	self.install_path = getattr(self, 'install_path', '${PYTHONDIR}')
+	install_from = getattr(self, 'install_from', None)
+	if install_from and not isinstance(install_from, Node.Node):
+		install_from = self.path.find_dir(install_from)
+	self.install_from = install_from
+
+	ver = self.env.PYTHON_VERSION
+	if not ver:
+		self.bld.fatal('Installing python files requires PYTHON_VERSION, try conf.check_python_version')
+
+	if int(ver.replace('.', '')) > 31:
+		self.install_32 = True
+
+@extension('.py')
+def process_py(self, node):
+	"""
+	Add signature of .py file, so it will be byte-compiled when necessary
+	"""
+	assert(hasattr(self, 'install_path')), 'add features="py"'
+
+	# where to install the python file
+	if self.install_path:
+		if self.install_from:
+			self.add_install_files(install_to=self.install_path, install_from=node, cwd=self.install_from, relative_trick=True)
+		else:
+			self.add_install_files(install_to=self.install_path, install_from=node, relative_trick=True)
+
+	lst = []
+	if self.env.PYC:
+		lst.append('pyc')
+	if self.env.PYO:
+		lst.append('pyo')
+
+	if self.install_path:
+		if self.install_from:
+			pyd = Utils.subst_vars("%s/%s" % (self.install_path, node.path_from(self.install_from)), self.env)
+		else:
+			pyd = Utils.subst_vars("%s/%s" % (self.install_path, node.path_from(self.path)), self.env)
+	else:
+		pyd = node.abspath()
+
+	for ext in lst:
+		if self.env.PYTAG and not self.env.NOPYCACHE:
+			# __pycache__ installation for python 3.2 - PEP 3147
+			name = node.name[:-3]
+			pyobj = node.parent.get_bld().make_node('__pycache__').make_node("%s.%s.%s" % (name, self.env.PYTAG, ext))
+			pyobj.parent.mkdir()
+		else:
+			pyobj = node.change_ext(".%s" % ext)
+
+		tsk = self.create_task(ext, node, pyobj)
+		tsk.pyd = pyd
+
+		if self.install_path:
+			self.add_install_files(install_to=os.path.dirname(pyd), install_from=pyobj, cwd=node.parent.get_bld(), relative_trick=True)
+
+class pyc(Task.Task):
+	"""
+	Byte-compiling python files
+	"""
+	color = 'PINK'
+	def __str__(self):
+		node = self.outputs[0]
+		return node.path_from(node.ctx.launch_node())
+	def run(self):
+		cmd = [Utils.subst_vars('${PYTHON}', self.env), '-c', INST, self.inputs[0].abspath(), self.outputs[0].abspath(), self.pyd]
+		ret = self.generator.bld.exec_command(cmd)
+		return ret
+
+class pyo(Task.Task):
+	"""
+	Byte-compiling python files
+	"""
+	color = 'PINK'
+	def __str__(self):
+		node = self.outputs[0]
+		return node.path_from(node.ctx.launch_node())
+	def run(self):
+		cmd = [Utils.subst_vars('${PYTHON}', self.env), Utils.subst_vars('${PYFLAGS_OPT}', self.env), '-c', INST, self.inputs[0].abspath(), self.outputs[0].abspath(), self.pyd]
+		ret = self.generator.bld.exec_command(cmd)
+		return ret
+
+@feature('pyext')
+@before_method('propagate_uselib_vars', 'apply_link')
+@after_method('apply_bundle')
+def init_pyext(self):
+	"""
+	Change the values of *cshlib_PATTERN* and *cxxshlib_PATTERN* to remove the
+	*lib* prefix from library names.
+	"""
+	self.uselib = self.to_list(getattr(self, 'uselib', []))
+	if not 'PYEXT' in self.uselib:
+		self.uselib.append('PYEXT')
+	# override shlib_PATTERN set by the osx module
+	self.env.cshlib_PATTERN = self.env.cxxshlib_PATTERN = self.env.macbundle_PATTERN = self.env.pyext_PATTERN
+	self.env.fcshlib_PATTERN = self.env.dshlib_PATTERN = self.env.pyext_PATTERN
+
+	try:
+		if not self.install_path:
+			return
+	except AttributeError:
+		self.install_path = '${PYTHONARCHDIR}'
+
+@feature('pyext')
+@before_method('apply_link', 'apply_bundle')
+def set_bundle(self):
+	"""Mac-specific pyext extension that enables bundles from c_osx.py"""
+	if Utils.unversioned_sys_platform() == 'darwin':
+		self.mac_bundle = True
+
+@before_method('propagate_uselib_vars')
+@feature('pyembed')
+def init_pyembed(self):
+	"""
+	Add the PYEMBED variable.
+	"""
+	self.uselib = self.to_list(getattr(self, 'uselib', []))
+	if not 'PYEMBED' in self.uselib:
+		self.uselib.append('PYEMBED')
+
+@conf
+def get_python_variables(self, variables, imports=None):
+	"""
+	Spawn a new python process to dump configuration variables
+
+	:param variables: variables to print
+	:type variables: list of string
+	:param imports: one import by element
+	:type imports: list of string
+	:return: the variable values
+	:rtype: list of string
+	"""
+	if not imports:
+		try:
+			imports = self.python_imports
+		except AttributeError:
+			imports = DISTUTILS_IMP
+
+	program = list(imports) # copy
+	program.append('')
+	for v in variables:
+		program.append("print(repr(%s))" % v)
+	os_env = dict(os.environ)
+	try:
+		del os_env['MACOSX_DEPLOYMENT_TARGET'] # see comments in the OSX tool
+	except KeyError:
+		pass
+
+	try:
+		out = self.cmd_and_log(self.env.PYTHON + ['-c', '\n'.join(program)], env=os_env)
+	except Errors.WafError:
+		self.fatal('The distutils module is unusable: install "python-devel"?')
+	self.to_log(out)
+	return_values = []
+	for s in out.splitlines():
+		s = s.strip()
+		if not s:
+			continue
+		if s == 'None':
+			return_values.append(None)
+		elif (s[0] == "'" and s[-1] == "'") or (s[0] == '"' and s[-1] == '"'):
+			return_values.append(eval(s))
+		elif s[0].isdigit():
+			return_values.append(int(s))
+		else: break
+	return return_values
+
+@conf
+def test_pyembed(self, mode, msg='Testing pyembed configuration'):
+	self.check(header_name='Python.h', define_name='HAVE_PYEMBED', msg=msg,
+		fragment=FRAG, errmsg='Could not build a python embedded interpreter',
+		features='%s %sprogram pyembed' % (mode, mode))
+
+@conf
+def test_pyext(self, mode, msg='Testing pyext configuration'):
+	self.check(header_name='Python.h', define_name='HAVE_PYEXT', msg=msg,
+		fragment=FRAG, errmsg='Could not build python extensions',
+		features='%s %sshlib pyext' % (mode, mode))
+
+@conf
+def python_cross_compile(self, features='pyembed pyext'):
+	"""
+	For cross-compilation purposes, it is possible to bypass the normal detection and set the flags that you want:
+	PYTHON_VERSION='3.4' PYTAG='cpython34' pyext_PATTERN="%s.so" PYTHON_LDFLAGS='-lpthread -ldl' waf configure
+
+	The following variables are used:
+	PYTHON_VERSION    required
+	PYTAG             required
+	PYTHON_LDFLAGS    required
+	pyext_PATTERN     required
+	PYTHON_PYEXT_LDFLAGS
+	PYTHON_PYEMBED_LDFLAGS
+	"""
+	features = Utils.to_list(features)
+	if not ('PYTHON_LDFLAGS' in self.environ or 'PYTHON_PYEXT_LDFLAGS' in self.environ or 'PYTHON_PYEMBED_LDFLAGS' in self.environ):
+		return False
+
+	for x in 'PYTHON_VERSION PYTAG pyext_PATTERN'.split():
+		if not x in self.environ:
+			self.fatal('Please set %s in the os environment' % x)
+		else:
+			self.env[x] = self.environ[x]
+
+	xx = self.env.CXX_NAME and 'cxx' or 'c'
+	if 'pyext' in features:
+		flags = self.environ.get('PYTHON_PYEXT_LDFLAGS', self.environ.get('PYTHON_LDFLAGS'))
+		if flags is None:
+			self.fatal('No flags provided through PYTHON_PYEXT_LDFLAGS as required')
+		else:
+			self.parse_flags(flags, 'PYEXT')
+		self.test_pyext(xx)
+	if 'pyembed' in features:
+		flags = self.environ.get('PYTHON_PYEMBED_LDFLAGS', self.environ.get('PYTHON_LDFLAGS'))
+		if flags is None:
+			self.fatal('No flags provided through PYTHON_PYEMBED_LDFLAGS as required')
+		else:
+			self.parse_flags(flags, 'PYEMBED')
+		self.test_pyembed(xx)
+	return True
+
+@conf
+def check_python_headers(conf, features='pyembed pyext'):
+	"""
+	Check for headers and libraries necessary to extend or embed python by using the module *distutils*.
+	On success the environment variables xxx_PYEXT and xxx_PYEMBED are added:
+
+	* PYEXT: for compiling python extensions
+	* PYEMBED: for embedding a python interpreter
+	"""
+	features = Utils.to_list(features)
+	assert ('pyembed' in features) or ('pyext' in features), "check_python_headers features must include 'pyembed' and/or 'pyext'"
+	env = conf.env
+	if not env.CC_NAME and not env.CXX_NAME:
+		conf.fatal('load a compiler first (gcc, g++, ..)')
+
+	# bypass all the code below for cross-compilation
+	if conf.python_cross_compile(features):
+		return
+
+	if not env.PYTHON_VERSION:
+		conf.check_python_version()
+
+	pybin = env.PYTHON
+	if not pybin:
+		conf.fatal('Could not find the python executable')
+
+	# so we actually do all this for compatibility reasons and for obtaining pyext_PATTERN below
+	v = 'prefix SO LDFLAGS LIBDIR LIBPL INCLUDEPY Py_ENABLE_SHARED MACOSX_DEPLOYMENT_TARGET LDSHARED CFLAGS LDVERSION'.split()
+	try:
+		lst = conf.get_python_variables(["get_config_var('%s') or ''" % x for x in v])
+	except RuntimeError:
+		conf.fatal("Python development headers not found (-v for details).")
+
+	vals = ['%s = %r' % (x, y) for (x, y) in zip(v, lst)]
+	conf.to_log("Configuration returned from %r:\n%s\n" % (pybin, '\n'.join(vals)))
+
+	dct = dict(zip(v, lst))
+	x = 'MACOSX_DEPLOYMENT_TARGET'
+	if dct[x]:
+		env[x] = conf.environ[x] = dct[x]
+	env.pyext_PATTERN = '%s' + dct['SO'] # not a mistake
+
+
+	# Try to get pythonX.Y-config
+	num = '.'.join(env.PYTHON_VERSION.split('.')[:2])
+	conf.find_program([''.join(pybin) + '-config', 'python%s-config' % num, 'python-config-%s' % num, 'python%sm-config' % num], var='PYTHON_CONFIG', msg="python-config", mandatory=False)
+
+	if env.PYTHON_CONFIG:
+		# python2.6-config requires 3 runs
+		all_flags = [['--cflags', '--libs', '--ldflags']]
+		if sys.hexversion < 0x2070000:
+			all_flags = [[k] for k in all_flags[0]]
+
+		xx = env.CXX_NAME and 'cxx' or 'c'
+
+		if 'pyembed' in features:
+			for flags in all_flags:
+				conf.check_cfg(msg='Asking python-config for pyembed %r flags' % ' '.join(flags), path=env.PYTHON_CONFIG, package='', uselib_store='PYEMBED', args=flags)
+
+			try:
+				conf.test_pyembed(xx)
+			except conf.errors.ConfigurationError:
+				# python bug 7352
+				if dct['Py_ENABLE_SHARED'] and dct['LIBDIR']:
+					env.append_unique('LIBPATH_PYEMBED', [dct['LIBDIR']])
+					conf.test_pyembed(xx)
+				else:
+					raise
+
+		if 'pyext' in features:
+			for flags in all_flags:
+				conf.check_cfg(msg='Asking python-config for pyext %r flags' % ' '.join(flags), path=env.PYTHON_CONFIG, package='', uselib_store='PYEXT', args=flags)
+
+			try:
+				conf.test_pyext(xx)
+			except conf.errors.ConfigurationError:
+				# python bug 7352
+				if dct['Py_ENABLE_SHARED'] and dct['LIBDIR']:
+					env.append_unique('LIBPATH_PYEXT', [dct['LIBDIR']])
+					conf.test_pyext(xx)
+				else:
+					raise
+
+		conf.define('HAVE_PYTHON_H', 1)
+		return
+
+	# No python-config, do something else on windows systems
+	all_flags = dct['LDFLAGS'] + ' ' + dct['CFLAGS']
+	conf.parse_flags(all_flags, 'PYEMBED')
+
+	all_flags = dct['LDFLAGS'] + ' ' + dct['LDSHARED'] + ' ' + dct['CFLAGS']
+	conf.parse_flags(all_flags, 'PYEXT')
+
+	result = None
+	if not dct["LDVERSION"]:
+		dct["LDVERSION"] = env.PYTHON_VERSION
+
+	# further simplification will be complicated
+	for name in ('python' + dct['LDVERSION'], 'python' + env.PYTHON_VERSION + 'm', 'python' + env.PYTHON_VERSION.replace('.', '')):
+
+		# LIBPATH_PYEMBED is already set; see if it works.
+		if not result and env.LIBPATH_PYEMBED:
+			path = env.LIBPATH_PYEMBED
+			conf.to_log("\n\n# Trying default LIBPATH_PYEMBED: %r\n" % path)
+			result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBPATH_PYEMBED' % name)
+
+		if not result and dct['LIBDIR']:
+			path = [dct['LIBDIR']]
+			conf.to_log("\n\n# try again with -L$python_LIBDIR: %r\n" % path)
+			result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in LIBDIR' % name)
+
+		if not result and dct['LIBPL']:
+			path = [dct['LIBPL']]
+			conf.to_log("\n\n# try again with -L$python_LIBPL (some systems don't install the python library in $prefix/lib)\n")
+			result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in python_LIBPL' % name)
+
+		if not result:
+			path = [os.path.join(dct['prefix'], "libs")]
+			conf.to_log("\n\n# try again with -L$prefix/libs, and pythonXY name rather than pythonX.Y (win32)\n")
+			result = conf.check(lib=name, uselib='PYEMBED', libpath=path, mandatory=False, msg='Checking for library %s in $prefix/libs' % name)
+
+		if result:
+			break # do not forget to set LIBPATH_PYEMBED
+
+	if result:
+		env.LIBPATH_PYEMBED = path
+		env.append_value('LIB_PYEMBED', [name])
+	else:
+		conf.to_log("\n\n### LIB NOT FOUND\n")
+
+	# under certain conditions, python extensions must link to
+	# python libraries, not just python embedding programs.
+	if Utils.is_win32 or dct['Py_ENABLE_SHARED']:
+		env.LIBPATH_PYEXT = env.LIBPATH_PYEMBED
+		env.LIB_PYEXT = env.LIB_PYEMBED
+
+	conf.to_log("Include path for Python extensions (found via distutils module): %r\n" % (dct['INCLUDEPY'],))
+	env.INCLUDES_PYEXT = [dct['INCLUDEPY']]
+	env.INCLUDES_PYEMBED = [dct['INCLUDEPY']]
+
+	# Code using the Python API needs to be compiled with -fno-strict-aliasing
+	if env.CC_NAME == 'gcc':
+		env.append_value('CFLAGS_PYEMBED', ['-fno-strict-aliasing'])
+		env.append_value('CFLAGS_PYEXT', ['-fno-strict-aliasing'])
+	if env.CXX_NAME == 'gcc':
+		env.append_value('CXXFLAGS_PYEMBED', ['-fno-strict-aliasing'])
+		env.append_value('CXXFLAGS_PYEXT', ['-fno-strict-aliasing'])
+
+	if env.CC_NAME == "msvc":
+		from distutils.msvccompiler import MSVCCompiler
+		dist_compiler = MSVCCompiler()
+		dist_compiler.initialize()
+		env.append_value('CFLAGS_PYEXT', dist_compiler.compile_options)
+		env.append_value('CXXFLAGS_PYEXT', dist_compiler.compile_options)
+		env.append_value('LINKFLAGS_PYEXT', dist_compiler.ldflags_shared)
+
+	# See if it compiles
+	conf.check(header_name='Python.h', define_name='HAVE_PYTHON_H', uselib='PYEMBED', fragment=FRAG, errmsg='Distutils not installed? Broken python installation? Get python-config now!')
+
+@conf
+def check_python_version(conf, minver=None):
+	"""
+	Check if the python interpreter is found matching a given minimum version.
+	minver should be a tuple, eg. to check for python >= 2.4.2 pass (2,4,2) as minver.
+
+	If successful, PYTHON_VERSION is defined as 'MAJOR.MINOR' (eg. '2.4')
+	of the actual python version found, and PYTHONDIR and PYTHONARCHDIR
+	are defined, pointing to the site-packages directories appropriate for
+	this python version, where modules/packages/extensions should be
+	installed.
+
+	:param minver: minimum version
+	:type minver: tuple of int
+	"""
+	assert minver is None or isinstance(minver, tuple)
+	pybin = conf.env.PYTHON
+	if not pybin:
+		conf.fatal('could not find the python executable')
+
+	# Get python version string
+	cmd = pybin + ['-c', 'import sys\nfor x in sys.version_info: print(str(x))']
+	Logs.debug('python: Running python command %r', cmd)
+	lines = conf.cmd_and_log(cmd).split()
+	assert len(lines) == 5, "found %r lines, expected 5: %r" % (len(lines), lines)
+	pyver_tuple = (int(lines[0]), int(lines[1]), int(lines[2]), lines[3], int(lines[4]))
+
+	# Compare python version with the minimum required
+	result = (minver is None) or (pyver_tuple >= minver)
+
+	if result:
+		# define useful environment variables
+		pyver = '.'.join([str(x) for x in pyver_tuple[:2]])
+		conf.env.PYTHON_VERSION = pyver
+
+		if 'PYTHONDIR' in conf.env:
+			# Check if --pythondir was specified
+			pydir = conf.env.PYTHONDIR
+		elif 'PYTHONDIR' in conf.environ:
+			# Check environment for PYTHONDIR
+			pydir = conf.environ['PYTHONDIR']
+		else:
+			# Finally, try to guess
+			if Utils.is_win32:
+				(python_LIBDEST, pydir) = conf.get_python_variables(
+					  ["get_config_var('LIBDEST') or ''",
+					   "get_python_lib(standard_lib=0) or ''"])
+			else:
+				python_LIBDEST = None
+				(pydir,) = conf.get_python_variables( ["get_python_lib(standard_lib=0, prefix=%r) or ''" % conf.env.PREFIX])
+			if python_LIBDEST is None:
+				if conf.env.LIBDIR:
+					python_LIBDEST = os.path.join(conf.env.LIBDIR, 'python' + pyver)
+				else:
+					python_LIBDEST = os.path.join(conf.env.PREFIX, 'lib', 'python' + pyver)
+
+		if 'PYTHONARCHDIR' in conf.env:
+			# Check if --pythonarchdir was specified
+			pyarchdir = conf.env.PYTHONARCHDIR
+		elif 'PYTHONARCHDIR' in conf.environ:
+			# Check environment for PYTHONDIR
+			pyarchdir = conf.environ['PYTHONARCHDIR']
+		else:
+			# Finally, try to guess
+			(pyarchdir, ) = conf.get_python_variables( ["get_python_lib(plat_specific=1, standard_lib=0, prefix=%r) or ''" % conf.env.PREFIX])
+			if not pyarchdir:
+				pyarchdir = pydir
+
+		if hasattr(conf, 'define'): # conf.define is added by the C tool, so may not exist
+			conf.define('PYTHONDIR', pydir)
+			conf.define('PYTHONARCHDIR', pyarchdir)
+
+		conf.env.PYTHONDIR = pydir
+		conf.env.PYTHONARCHDIR = pyarchdir
+
+	# Feedback
+	pyver_full = '.'.join(map(str, pyver_tuple[:3]))
+	if minver is None:
+		conf.msg('Checking for python version', pyver_full)
+	else:
+		minver_str = '.'.join(map(str, minver))
+		conf.msg('Checking for python version >= %s' % (minver_str,), pyver_full, color=result and 'GREEN' or 'YELLOW')
+
+	if not result:
+		conf.fatal('The python version is too old, expecting %r' % (minver,))
+
+PYTHON_MODULE_TEMPLATE = '''
+import %s as current_module
+version = getattr(current_module, '__version__', None)
+if version is not None:
+	print(str(version))
+else:
+	print('unknown version')
+'''
+
+@conf
+def check_python_module(conf, module_name, condition=''):
+	"""
+	Check if the selected python interpreter can import the given python module::
+
+		def configure(conf):
+			conf.check_python_module('pygccxml')
+			conf.check_python_module('re', condition="ver > num(2, 0, 4) and ver <= num(3, 0, 0)")
+
+	:param module_name: module
+	:type module_name: string
+	"""
+	msg = "Checking for python module %r" % module_name
+	if condition:
+		msg = '%s (%s)' % (msg, condition)
+	conf.start_msg(msg)
+	try:
+		ret = conf.cmd_and_log(conf.env.PYTHON + ['-c', PYTHON_MODULE_TEMPLATE % module_name])
+	except Errors.WafError:
+		conf.end_msg(False)
+		conf.fatal('Could not find the python module %r' % module_name)
+
+	ret = ret.strip()
+	if condition:
+		conf.end_msg(ret)
+		if ret == 'unknown version':
+			conf.fatal('Could not check the %s version' % module_name)
+
+		from distutils.version import LooseVersion
+		def num(*k):
+			if isinstance(k[0], int):
+				return LooseVersion('.'.join([str(x) for x in k]))
+			else:
+				return LooseVersion(k[0])
+		d = {'num': num, 'ver': LooseVersion(ret)}
+		ev = eval(condition, {}, d)
+		if not ev:
+			conf.fatal('The %s version does not satisfy the requirements' % module_name)
+	else:
+		if ret == 'unknown version':
+			conf.end_msg(True)
+		else:
+			conf.end_msg(ret)
+
+def configure(conf):
+	"""
+	Detect the python interpreter
+	"""
+	v = conf.env
+	if getattr(Options.options, 'pythondir', None):
+		v.PYTHONDIR = Options.options.pythondir
+	if getattr(Options.options, 'pythonarchdir', None):
+		v.PYTHONARCHDIR = Options.options.pythonarchdir
+	if getattr(Options.options, 'nopycache', None):
+		v.NOPYCACHE=Options.options.nopycache
+
+	if not v.PYTHON:
+		v.PYTHON = [getattr(Options.options, 'python', None) or sys.executable]
+	v.PYTHON = Utils.to_list(v.PYTHON)
+	conf.find_program('python', var='PYTHON')
+
+	v.PYFLAGS = ''
+	v.PYFLAGS_OPT = '-O'
+
+	v.PYC = getattr(Options.options, 'pyc', 1)
+	v.PYO = getattr(Options.options, 'pyo', 1)
+
+	try:
+		v.PYTAG = conf.cmd_and_log(conf.env.PYTHON + ['-c', "import imp;print(imp.get_tag())"]).strip()
+	except Errors.WafError:
+		pass
+
+def options(opt):
+	"""
+	Add python-specific options
+	"""
+	pyopt=opt.add_option_group("Python Options")
+	pyopt.add_option('--nopyc', dest = 'pyc', action='store_false', default=1,
+					 help = 'Do not install bytecode compiled .pyc files (configuration) [Default:install]')
+	pyopt.add_option('--nopyo', dest='pyo', action='store_false', default=1,
+					 help='Do not install optimised compiled .pyo files (configuration) [Default:install]')
+	pyopt.add_option('--nopycache',dest='nopycache', action='store_true',
+					 help='Do not use __pycache__ directory to install objects [Default:auto]')
+	pyopt.add_option('--python', dest="python",
+					 help='python binary to be used [Default: %s]' % sys.executable)
+	pyopt.add_option('--pythondir', dest='pythondir',
+					 help='Installation path for python modules (py, platform-independent .py and .pyc files)')
+	pyopt.add_option('--pythonarchdir', dest='pythonarchdir',
+					 help='Installation path for python extension (pyext, platform-dependent .so or .dylib files)')
+
diff --git a/waflib/Tools/qt5.py b/waflib/Tools/qt5.py
new file mode 100644
index 00000000..4f9c6908
--- /dev/null
+++ b/waflib/Tools/qt5.py
@@ -0,0 +1,796 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2018 (ita)
+
+"""
+This tool helps with finding Qt5 tools and libraries,
+and also provides syntactic sugar for using Qt5 tools.
+
+The following snippet illustrates the tool usage::
+
+	def options(opt):
+		opt.load('compiler_cxx qt5')
+
+	def configure(conf):
+		conf.load('compiler_cxx qt5')
+
+	def build(bld):
+		bld(
+			features = 'qt5 cxx cxxprogram',
+			uselib   = 'QT5CORE QT5GUI QT5OPENGL QT5SVG',
+			source   = 'main.cpp textures.qrc aboutDialog.ui',
+			target   = 'window',
+		)
+
+Here, the UI description and resource files will be processed
+to generate code.
+
+Usage
+=====
+
+Load the "qt5" tool.
+
+You also need to edit your sources accordingly:
+
+- the normal way of doing things is to have your C++ files
+  include the .moc file.
+  This is regarded as the best practice (and provides much faster
+  compilations).
+  It also implies that the include paths have beenset properly.
+
+- to have the include paths added automatically, use the following::
+
+     from waflib.TaskGen import feature, before_method, after_method
+     @feature('cxx')
+     @after_method('process_source')
+     @before_method('apply_incpaths')
+     def add_includes_paths(self):
+        incs = set(self.to_list(getattr(self, 'includes', '')))
+        for x in self.compiled_tasks:
+            incs.add(x.inputs[0].parent.path_from(self.path))
+        self.includes = sorted(incs)
+
+Note: another tool provides Qt processing that does not require
+.moc includes, see 'playground/slow_qt/'.
+
+A few options (--qt{dir,bin,...}) and environment variables
+(QT5_{ROOT,DIR,MOC,UIC,XCOMPILE}) allow finer tuning of the tool,
+tool path selection, etc; please read the source for more info.
+
+The detection uses pkg-config on Linux by default. To force static library detection use:
+QT5_XCOMPILE=1 QT5_FORCE_STATIC=1 waf configure
+"""
+
+from __future__ import with_statement
+
+try:
+	from xml.sax import make_parser
+	from xml.sax.handler import ContentHandler
+except ImportError:
+	has_xml = False
+	ContentHandler = object
+else:
+	has_xml = True
+
+import os, sys, re
+from waflib.Tools import cxx
+from waflib import Task, Utils, Options, Errors, Context
+from waflib.TaskGen import feature, after_method, extension, before_method
+from waflib.Configure import conf
+from waflib import Logs
+
+MOC_H = ['.h', '.hpp', '.hxx', '.hh']
+"""
+File extensions associated to .moc files
+"""
+
+EXT_RCC = ['.qrc']
+"""
+File extension for the resource (.qrc) files
+"""
+
+EXT_UI  = ['.ui']
+"""
+File extension for the user interface (.ui) files
+"""
+
+EXT_QT5 = ['.cpp', '.cc', '.cxx', '.C']
+"""
+File extensions of C++ files that may require a .moc processing
+"""
+
+class qxx(Task.classes['cxx']):
+	"""
+	Each C++ file can have zero or several .moc files to create.
+	They are known only when the files are scanned (preprocessor)
+	To avoid scanning the c++ files each time (parsing C/C++), the results
+	are retrieved from the task cache (bld.node_deps/bld.raw_deps).
+	The moc tasks are also created *dynamically* during the build.
+	"""
+
+	def __init__(self, *k, **kw):
+		Task.Task.__init__(self, *k, **kw)
+		self.moc_done = 0
+
+	def runnable_status(self):
+		"""
+		Compute the task signature to make sure the scanner was executed. Create the
+		moc tasks by using :py:meth:`waflib.Tools.qt5.qxx.add_moc_tasks` (if necessary),
+		then postpone the task execution (there is no need to recompute the task signature).
+		"""
+		if self.moc_done:
+			return Task.Task.runnable_status(self)
+		else:
+			for t in self.run_after:
+				if not t.hasrun:
+					return Task.ASK_LATER
+			self.add_moc_tasks()
+			return Task.Task.runnable_status(self)
+
+	def create_moc_task(self, h_node, m_node):
+		"""
+		If several libraries use the same classes, it is possible that moc will run several times (Issue 1318)
+		It is not possible to change the file names, but we can assume that the moc transformation will be identical,
+		and the moc tasks can be shared in a global cache.
+		"""
+		try:
+			moc_cache = self.generator.bld.moc_cache
+		except AttributeError:
+			moc_cache = self.generator.bld.moc_cache = {}
+
+		try:
+			return moc_cache[h_node]
+		except KeyError:
+			tsk = moc_cache[h_node] = Task.classes['moc'](env=self.env, generator=self.generator)
+			tsk.set_inputs(h_node)
+			tsk.set_outputs(m_node)
+			tsk.env.append_unique('MOC_FLAGS', '-i')
+
+			if self.generator:
+				self.generator.tasks.append(tsk)
+
+			# direct injection in the build phase (safe because called from the main thread)
+			gen = self.generator.bld.producer
+			gen.outstanding.append(tsk)
+			gen.total += 1
+
+			return tsk
+
+		else:
+			# remove the signature, it must be recomputed with the moc task
+			delattr(self, 'cache_sig')
+
+	def add_moc_tasks(self):
+		"""
+		Creates moc tasks by looking in the list of file dependencies ``bld.raw_deps[self.uid()]``
+		"""
+		node = self.inputs[0]
+		bld = self.generator.bld
+
+		try:
+			# compute the signature once to know if there is a moc file to create
+			self.signature()
+		except KeyError:
+			# the moc file may be referenced somewhere else
+			pass
+		else:
+			# remove the signature, it must be recomputed with the moc task
+			delattr(self, 'cache_sig')
+
+		include_nodes = [node.parent] + self.generator.includes_nodes
+
+		moctasks = []
+		mocfiles = set()
+		for d in bld.raw_deps.get(self.uid(), []):
+			if not d.endswith('.moc'):
+				continue
+
+			# process that base.moc only once
+			if d in mocfiles:
+				continue
+			mocfiles.add(d)
+
+			# find the source associated with the moc file
+			h_node = None
+			base2 = d[:-4]
+
+			# foo.moc from foo.cpp
+			prefix = node.name[:node.name.rfind('.')]
+			if base2 == prefix:
+				h_node = node
+			else:
+				# this deviates from the standard
+				# if bar.cpp includes foo.moc, then assume it is from foo.h
+				for x in include_nodes:
+					for e in MOC_H:
+						h_node = x.find_node(base2 + e)
+						if h_node:
+							break
+					else:
+						continue
+					break
+			if h_node:
+				m_node = h_node.change_ext('.moc')
+			else:
+				raise Errors.WafError('No source found for %r which is a moc file' % d)
+
+			# create the moc task
+			task = self.create_moc_task(h_node, m_node)
+			moctasks.append(task)
+
+		# simple scheduler dependency: run the moc task before others
+		self.run_after.update(set(moctasks))
+		self.moc_done = 1
+
+class trans_update(Task.Task):
+	"""Updates a .ts files from a list of C++ files"""
+	run_str = '${QT_LUPDATE} ${SRC} -ts ${TGT}'
+	color   = 'BLUE'
+
+class XMLHandler(ContentHandler):
+	"""
+	Parses ``.qrc`` files
+	"""
+	def __init__(self):
+		ContentHandler.__init__(self)
+		self.buf = []
+		self.files = []
+	def startElement(self, name, attrs):
+		if name == 'file':
+			self.buf = []
+	def endElement(self, name):
+		if name == 'file':
+			self.files.append(str(''.join(self.buf)))
+	def characters(self, cars):
+		self.buf.append(cars)
+
+@extension(*EXT_RCC)
+def create_rcc_task(self, node):
+	"Creates rcc and cxx tasks for ``.qrc`` files"
+	rcnode = node.change_ext('_rc.%d.cpp' % self.idx)
+	self.create_task('rcc', node, rcnode)
+	cpptask = self.create_task('cxx', rcnode, rcnode.change_ext('.o'))
+	try:
+		self.compiled_tasks.append(cpptask)
+	except AttributeError:
+		self.compiled_tasks = [cpptask]
+	return cpptask
+
+@extension(*EXT_UI)
+def create_uic_task(self, node):
+	"Create uic tasks for user interface ``.ui`` definition files"
+
+	"""
+	If UIC file is used in more than one bld, we would have a conflict in parallel execution
+	It is not possible to change the file names (like .self.idx. as for objects) as they have 
+	to be referenced by the source file, but we can assume that the transformation will be identical 
+	and the tasks can be shared in a global cache.
+	"""
+	try:
+		uic_cache = self.bld.uic_cache
+	except AttributeError:
+		uic_cache = self.bld.uic_cache = {}
+
+	if node not in uic_cache:
+		uictask = uic_cache[node] = self.create_task('ui5', node)
+		uictask.outputs = [node.parent.find_or_declare(self.env.ui_PATTERN % node.name[:-3])]
+
+@extension('.ts')
+def add_lang(self, node):
+	"""Adds all the .ts file into ``self.lang``"""
+	self.lang = self.to_list(getattr(self, 'lang', [])) + [node]
+
+@feature('qt5')
+@before_method('process_source')
+def process_mocs(self):
+	"""
+	Processes MOC files included in headers::
+
+		def build(bld):
+			bld.program(features='qt5', source='main.cpp', target='app', use='QT5CORE', moc='foo.h')
+
+	The build will run moc on foo.h to create moc_foo.n.cpp. The number in the file name
+	is provided to avoid name clashes when the same headers are used by several targets.
+	"""
+	lst = self.to_nodes(getattr(self, 'moc', []))
+	self.source = self.to_list(getattr(self, 'source', []))
+	for x in lst:
+		prefix = x.name[:x.name.rfind('.')] # foo.h -> foo
+		moc_target = 'moc_%s.%d.cpp' % (prefix, self.idx)
+		moc_node = x.parent.find_or_declare(moc_target)
+		self.source.append(moc_node)
+
+		self.create_task('moc', x, moc_node)
+
+@feature('qt5')
+@after_method('apply_link')
+def apply_qt5(self):
+	"""
+	Adds MOC_FLAGS which may be necessary for moc::
+
+		def build(bld):
+			bld.program(features='qt5', source='main.cpp', target='app', use='QT5CORE')
+
+	The additional parameters are:
+
+	:param lang: list of translation files (\*.ts) to process
+	:type lang: list of :py:class:`waflib.Node.Node` or string without the .ts extension
+	:param update: whether to process the C++ files to update the \*.ts files (use **waf --translate**)
+	:type update: bool
+	:param langname: if given, transform the \*.ts files into a .qrc files to include in the binary file
+	:type langname: :py:class:`waflib.Node.Node` or string without the .qrc extension
+	"""
+	if getattr(self, 'lang', None):
+		qmtasks = []
+		for x in self.to_list(self.lang):
+			if isinstance(x, str):
+				x = self.path.find_resource(x + '.ts')
+			qmtasks.append(self.create_task('ts2qm', x, x.change_ext('.%d.qm' % self.idx)))
+
+		if getattr(self, 'update', None) and Options.options.trans_qt5:
+			cxxnodes = [a.inputs[0] for a in self.compiled_tasks] + [
+				a.inputs[0] for a in self.tasks if a.inputs and a.inputs[0].name.endswith('.ui')]
+			for x in qmtasks:
+				self.create_task('trans_update', cxxnodes, x.inputs)
+
+		if getattr(self, 'langname', None):
+			qmnodes = [x.outputs[0] for x in qmtasks]
+			rcnode = self.langname
+			if isinstance(rcnode, str):
+				rcnode = self.path.find_or_declare(rcnode + ('.%d.qrc' % self.idx))
+			t = self.create_task('qm2rcc', qmnodes, rcnode)
+			k = create_rcc_task(self, t.outputs[0])
+			self.link_task.inputs.append(k.outputs[0])
+
+	lst = []
+	for flag in self.to_list(self.env.CXXFLAGS):
+		if len(flag) < 2:
+			continue
+		f = flag[0:2]
+		if f in ('-D', '-I', '/D', '/I'):
+			if (f[0] == '/'):
+				lst.append('-' + flag[1:])
+			else:
+				lst.append(flag)
+	self.env.append_value('MOC_FLAGS', lst)
+
+@extension(*EXT_QT5)
+def cxx_hook(self, node):
+	"""
+	Re-maps C++ file extensions to the :py:class:`waflib.Tools.qt5.qxx` task.
+	"""
+	return self.create_compiled_task('qxx', node)
+
+class rcc(Task.Task):
+	"""
+	Processes ``.qrc`` files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_RCC} -name ${tsk.rcname()} ${SRC[0].abspath()} ${RCC_ST} -o ${TGT}'
+	ext_out = ['.h']
+
+	def rcname(self):
+		return os.path.splitext(self.inputs[0].name)[0]
+
+	def scan(self):
+		"""Parse the *.qrc* files"""
+		if not has_xml:
+			Logs.error('No xml.sax support was found, rcc dependencies will be incomplete!')
+			return ([], [])
+
+		parser = make_parser()
+		curHandler = XMLHandler()
+		parser.setContentHandler(curHandler)
+		with open(self.inputs[0].abspath(), 'r') as f:
+			parser.parse(f)
+
+		nodes = []
+		names = []
+		root = self.inputs[0].parent
+		for x in curHandler.files:
+			nd = root.find_resource(x)
+			if nd:
+				nodes.append(nd)
+			else:
+				names.append(x)
+		return (nodes, names)
+
+	def quote_flag(self, x):
+		"""
+		Override Task.quote_flag. QT parses the argument files
+		differently than cl.exe and link.exe
+
+		:param x: flag
+		:type x: string
+		:return: quoted flag
+		:rtype: string
+		"""
+		return x
+
+
+class moc(Task.Task):
+	"""
+	Creates ``.moc`` files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_MOC} ${MOC_FLAGS} ${MOCCPPPATH_ST:INCPATHS} ${MOCDEFINES_ST:DEFINES} ${SRC} ${MOC_ST} ${TGT}'
+
+	def quote_flag(self, x):
+		"""
+		Override Task.quote_flag. QT parses the argument files
+		differently than cl.exe and link.exe
+
+		:param x: flag
+		:type x: string
+		:return: quoted flag
+		:rtype: string
+		"""
+		return x
+
+
+class ui5(Task.Task):
+	"""
+	Processes ``.ui`` files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_UIC} ${SRC} -o ${TGT}'
+	ext_out = ['.h']
+
+class ts2qm(Task.Task):
+	"""
+	Generates ``.qm`` files from ``.ts`` files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_LRELEASE} ${QT_LRELEASE_FLAGS} ${SRC} -qm ${TGT}'
+
+class qm2rcc(Task.Task):
+	"""
+	Generates ``.qrc`` files from ``.qm`` files
+	"""
+	color = 'BLUE'
+	after = 'ts2qm'
+	def run(self):
+		"""Create a qrc file including the inputs"""
+		txt = '\n'.join(['<file>%s</file>' % k.path_from(self.outputs[0].parent) for k in self.inputs])
+		code = '<!DOCTYPE RCC><RCC version="1.0">\n<qresource>\n%s\n</qresource>\n</RCC>' % txt
+		self.outputs[0].write(code)
+
+def configure(self):
+	"""
+	Besides the configuration options, the environment variable QT5_ROOT may be used
+	to give the location of the qt5 libraries (absolute path).
+
+	The detection uses the program ``pkg-config`` through :py:func:`waflib.Tools.config_c.check_cfg`
+	"""
+	self.find_qt5_binaries()
+	self.set_qt5_libs_dir()
+	self.set_qt5_libs_to_check()
+	self.set_qt5_defines()
+	self.find_qt5_libraries()
+	self.add_qt5_rpath()
+	self.simplify_qt5_libs()
+
+	# warn about this during the configuration too
+	if not has_xml:
+		Logs.error('No xml.sax support was found, rcc dependencies will be incomplete!')
+
+	if 'COMPILER_CXX' not in self.env:
+		self.fatal('No CXX compiler defined: did you forget to configure compiler_cxx first?')
+
+	# Qt5 may be compiled with '-reduce-relocations' which requires dependent programs to have -fPIE or -fPIC?
+	frag = '#include <QApplication>\nint main(int argc, char **argv) {return 0;}\n'
+	uses = 'QT5CORE QT5WIDGETS QT5GUI'
+	for flag in [[], '-fPIE', '-fPIC', '-std=c++11' , ['-std=c++11', '-fPIE'], ['-std=c++11', '-fPIC']]:
+		msg = 'See if Qt files compile '
+		if flag:
+			msg += 'with %s' % flag
+		try:
+			self.check(features='qt5 cxx', use=uses, uselib_store='qt5', cxxflags=flag, fragment=frag, msg=msg)
+		except self.errors.ConfigurationError:
+			pass
+		else:
+			break
+	else:
+		self.fatal('Could not build a simple Qt application')
+
+	# FreeBSD does not add /usr/local/lib and the pkg-config files do not provide it either :-/
+	if Utils.unversioned_sys_platform() == 'freebsd':
+		frag = '#include <QApplication>\nint main(int argc, char **argv) { QApplication app(argc, argv); return NULL != (void*) (&app);}\n'
+		try:
+			self.check(features='qt5 cxx cxxprogram', use=uses, fragment=frag, msg='Can we link Qt programs on FreeBSD directly?')
+		except self.errors.ConfigurationError:
+			self.check(features='qt5 cxx cxxprogram', use=uses, uselib_store='qt5', libpath='/usr/local/lib', fragment=frag, msg='Is /usr/local/lib required?')
+
+@conf
+def find_qt5_binaries(self):
+	"""
+	Detects Qt programs such as qmake, moc, uic, lrelease
+	"""
+	env = self.env
+	opt = Options.options
+
+	qtdir = getattr(opt, 'qtdir', '')
+	qtbin = getattr(opt, 'qtbin', '')
+
+	paths = []
+
+	if qtdir:
+		qtbin = os.path.join(qtdir, 'bin')
+
+	# the qt directory has been given from QT5_ROOT - deduce the qt binary path
+	if not qtdir:
+		qtdir = self.environ.get('QT5_ROOT', '')
+		qtbin = self.environ.get('QT5_BIN') or os.path.join(qtdir, 'bin')
+
+	if qtbin:
+		paths = [qtbin]
+
+	# no qtdir, look in the path and in /usr/local/Trolltech
+	if not qtdir:
+		paths = self.environ.get('PATH', '').split(os.pathsep)
+		paths.extend(['/usr/share/qt5/bin', '/usr/local/lib/qt5/bin'])
+		try:
+			lst = Utils.listdir('/usr/local/Trolltech/')
+		except OSError:
+			pass
+		else:
+			if lst:
+				lst.sort()
+				lst.reverse()
+
+				# keep the highest version
+				qtdir = '/usr/local/Trolltech/%s/' % lst[0]
+				qtbin = os.path.join(qtdir, 'bin')
+				paths.append(qtbin)
+
+	# at the end, try to find qmake in the paths given
+	# keep the one with the highest version
+	cand = None
+	prev_ver = ['5', '0', '0']
+	for qmk in ('qmake-qt5', 'qmake5', 'qmake'):
+		try:
+			qmake = self.find_program(qmk, path_list=paths)
+		except self.errors.ConfigurationError:
+			pass
+		else:
+			try:
+				version = self.cmd_and_log(qmake + ['-query', 'QT_VERSION']).strip()
+			except self.errors.WafError:
+				pass
+			else:
+				if version:
+					new_ver = version.split('.')
+					if new_ver > prev_ver:
+						cand = qmake
+						prev_ver = new_ver
+
+	# qmake could not be found easily, rely on qtchooser
+	if not cand:
+		try:
+			self.find_program('qtchooser')
+		except self.errors.ConfigurationError:
+			pass
+		else:
+			cmd = self.env.QTCHOOSER + ['-qt=5', '-run-tool=qmake']
+			try:
+				version = self.cmd_and_log(cmd + ['-query', 'QT_VERSION'])
+			except self.errors.WafError:
+				pass
+			else:
+				cand = cmd
+
+	if cand:
+		self.env.QMAKE = cand
+	else:
+		self.fatal('Could not find qmake for qt5')
+
+	self.env.QT_HOST_BINS = qtbin = self.cmd_and_log(self.env.QMAKE + ['-query', 'QT_HOST_BINS']).strip()
+	paths.insert(0, qtbin)
+
+	def find_bin(lst, var):
+		if var in env:
+			return
+		for f in lst:
+			try:
+				ret = self.find_program(f, path_list=paths)
+			except self.errors.ConfigurationError:
+				pass
+			else:
+				env[var]=ret
+				break
+
+	find_bin(['uic-qt5', 'uic'], 'QT_UIC')
+	if not env.QT_UIC:
+		self.fatal('cannot find the uic compiler for qt5')
+
+	self.start_msg('Checking for uic version')
+	uicver = self.cmd_and_log(env.QT_UIC + ['-version'], output=Context.BOTH)
+	uicver = ''.join(uicver).strip()
+	uicver = uicver.replace('Qt User Interface Compiler ','').replace('User Interface Compiler for Qt', '')
+	self.end_msg(uicver)
+	if uicver.find(' 3.') != -1 or uicver.find(' 4.') != -1:
+		self.fatal('this uic compiler is for qt3 or qt4, add uic for qt5 to your path')
+
+	find_bin(['moc-qt5', 'moc'], 'QT_MOC')
+	find_bin(['rcc-qt5', 'rcc'], 'QT_RCC')
+	find_bin(['lrelease-qt5', 'lrelease'], 'QT_LRELEASE')
+	find_bin(['lupdate-qt5', 'lupdate'], 'QT_LUPDATE')
+
+	env.UIC_ST = '%s -o %s'
+	env.MOC_ST = '-o'
+	env.ui_PATTERN = 'ui_%s.h'
+	env.QT_LRELEASE_FLAGS = ['-silent']
+	env.MOCCPPPATH_ST = '-I%s'
+	env.MOCDEFINES_ST = '-D%s'
+
+@conf
+def set_qt5_libs_dir(self):
+	env = self.env
+	qtlibs = getattr(Options.options, 'qtlibs', None) or self.environ.get('QT5_LIBDIR')
+	if not qtlibs:
+		try:
+			qtlibs = self.cmd_and_log(env.QMAKE + ['-query', 'QT_INSTALL_LIBS']).strip()
+		except Errors.WafError:
+			qtdir = self.cmd_and_log(env.QMAKE + ['-query', 'QT_INSTALL_PREFIX']).strip()
+			qtlibs = os.path.join(qtdir, 'lib')
+	self.msg('Found the Qt5 libraries in', qtlibs)
+	env.QTLIBS = qtlibs
+
+@conf
+def find_single_qt5_lib(self, name, uselib, qtlibs, qtincludes, force_static):
+	env = self.env
+	if force_static:
+		exts = ('.a', '.lib')
+		prefix = 'STLIB'
+	else:
+		exts = ('.so', '.lib')
+		prefix = 'LIB'
+
+	def lib_names():
+		for x in exts:
+			for k in ('', '5') if Utils.is_win32 else ['']:
+				for p in ('lib', ''):
+					yield (p, name, k, x)
+
+	for tup in lib_names():
+		k = ''.join(tup)
+		path = os.path.join(qtlibs, k)
+		if os.path.exists(path):
+			if env.DEST_OS == 'win32':
+				libval = ''.join(tup[:-1])
+			else:
+				libval = name
+			env.append_unique(prefix + '_' + uselib, libval)
+			env.append_unique('%sPATH_%s' % (prefix, uselib), qtlibs)
+			env.append_unique('INCLUDES_' + uselib, qtincludes)
+			env.append_unique('INCLUDES_' + uselib, os.path.join(qtincludes, name.replace('Qt5', 'Qt')))
+			return k
+	return False
+
+@conf
+def find_qt5_libraries(self):
+	env = self.env
+
+	qtincludes =  self.environ.get('QT5_INCLUDES') or self.cmd_and_log(env.QMAKE + ['-query', 'QT_INSTALL_HEADERS']).strip()
+	force_static = self.environ.get('QT5_FORCE_STATIC')
+	try:
+		if self.environ.get('QT5_XCOMPILE'):
+			self.fatal('QT5_XCOMPILE Disables pkg-config detection')
+		self.check_cfg(atleast_pkgconfig_version='0.1')
+	except self.errors.ConfigurationError:
+		for i in self.qt5_vars:
+			uselib = i.upper()
+			if Utils.unversioned_sys_platform() == 'darwin':
+				# Since at least qt 4.7.3 each library locates in separate directory
+				fwk = i.replace('Qt5', 'Qt')
+				frameworkName = fwk + '.framework'
+
+				qtDynamicLib = os.path.join(env.QTLIBS, frameworkName, fwk)
+				if os.path.exists(qtDynamicLib):
+					env.append_unique('FRAMEWORK_' + uselib, fwk)
+					env.append_unique('FRAMEWORKPATH_' + uselib, env.QTLIBS)
+					self.msg('Checking for %s' % i, qtDynamicLib, 'GREEN')
+				else:
+					self.msg('Checking for %s' % i, False, 'YELLOW')
+				env.append_unique('INCLUDES_' + uselib, os.path.join(env.QTLIBS, frameworkName, 'Headers'))
+			else:
+				ret = self.find_single_qt5_lib(i, uselib, env.QTLIBS, qtincludes, force_static)
+				if not force_static and not ret:
+					ret = self.find_single_qt5_lib(i, uselib, env.QTLIBS, qtincludes, True)
+				self.msg('Checking for %s' % i, ret, 'GREEN' if ret else 'YELLOW')
+	else:
+		path = '%s:%s:%s/pkgconfig:/usr/lib/qt5/lib/pkgconfig:/opt/qt5/lib/pkgconfig:/usr/lib/qt5/lib:/opt/qt5/lib' % (
+			self.environ.get('PKG_CONFIG_PATH', ''), env.QTLIBS, env.QTLIBS)
+		for i in self.qt5_vars:
+			self.check_cfg(package=i, args='--cflags --libs', mandatory=False, force_static=force_static, pkg_config_path=path)
+
+@conf
+def simplify_qt5_libs(self):
+	"""
+	Since library paths make really long command-lines,
+	and since everything depends on qtcore, remove the qtcore ones from qtgui, etc
+	"""
+	env = self.env
+	def process_lib(vars_, coreval):
+		for d in vars_:
+			var = d.upper()
+			if var == 'QTCORE':
+				continue
+
+			value = env['LIBPATH_'+var]
+			if value:
+				core = env[coreval]
+				accu = []
+				for lib in value:
+					if lib in core:
+						continue
+					accu.append(lib)
+				env['LIBPATH_'+var] = accu
+	process_lib(self.qt5_vars,       'LIBPATH_QTCORE')
+
+@conf
+def add_qt5_rpath(self):
+	"""
+	Defines rpath entries for Qt libraries
+	"""
+	env = self.env
+	if getattr(Options.options, 'want_rpath', False):
+		def process_rpath(vars_, coreval):
+			for d in vars_:
+				var = d.upper()
+				value = env['LIBPATH_' + var]
+				if value:
+					core = env[coreval]
+					accu = []
+					for lib in value:
+						if var != 'QTCORE':
+							if lib in core:
+								continue
+						accu.append('-Wl,--rpath='+lib)
+					env['RPATH_' + var] = accu
+		process_rpath(self.qt5_vars,       'LIBPATH_QTCORE')
+
+@conf
+def set_qt5_libs_to_check(self):
+	self.qt5_vars = Utils.to_list(getattr(self, 'qt5_vars', []))
+	if not self.qt5_vars:
+		dirlst = Utils.listdir(self.env.QTLIBS)
+
+		pat = self.env.cxxshlib_PATTERN
+		if Utils.is_win32:
+			pat = pat.replace('.dll', '.lib')
+		if self.environ.get('QT5_FORCE_STATIC'):
+			pat = self.env.cxxstlib_PATTERN
+		if Utils.unversioned_sys_platform() == 'darwin':
+			pat = "%s\.framework"
+		re_qt = re.compile(pat%'Qt5?(?P<name>.*)'+'$')
+		for x in dirlst:
+			m = re_qt.match(x)
+			if m:
+				self.qt5_vars.append("Qt5%s" % m.group('name'))
+		if not self.qt5_vars:
+			self.fatal('cannot find any Qt5 library (%r)' % self.env.QTLIBS)
+
+	qtextralibs = getattr(Options.options, 'qtextralibs', None)
+	if qtextralibs:
+		self.qt5_vars.extend(qtextralibs.split(','))
+
+@conf
+def set_qt5_defines(self):
+	if sys.platform != 'win32':
+		return
+	for x in self.qt5_vars:
+		y=x.replace('Qt5', 'Qt')[2:].upper()
+		self.env.append_unique('DEFINES_%s' % x.upper(), 'QT_%s_LIB' % y)
+
+def options(opt):
+	"""
+	Command-line options
+	"""
+	opt.add_option('--want-rpath', action='store_true', default=False, dest='want_rpath', help='enable the rpath for qt libraries')
+	for i in 'qtdir qtbin qtlibs'.split():
+		opt.add_option('--'+i, type='string', default='', dest=i)
+
+	opt.add_option('--translate', action='store_true', help='collect translation strings', dest='trans_qt5', default=False)
+	opt.add_option('--qtextralibs', type='string', default='', dest='qtextralibs', help='additional qt libraries on the system to add to default ones, comma separated')
+
diff --git a/waflib/Tools/ruby.py b/waflib/Tools/ruby.py
new file mode 100644
index 00000000..8d92a79a
--- /dev/null
+++ b/waflib/Tools/ruby.py
@@ -0,0 +1,186 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# daniel.svensson at purplescout.se 2008
+# Thomas Nagy 2016-2018 (ita)
+
+"""
+Support for Ruby extensions. A C/C++ compiler is required::
+
+	def options(opt):
+		opt.load('compiler_c ruby')
+	def configure(conf):
+		conf.load('compiler_c ruby')
+		conf.check_ruby_version((1,8,0))
+		conf.check_ruby_ext_devel()
+		conf.check_ruby_module('libxml')
+	def build(bld):
+		bld(
+			features = 'c cshlib rubyext',
+			source = 'rb_mytest.c',
+			target = 'mytest_ext',
+			install_path = '${ARCHDIR_RUBY}')
+		bld.install_files('${LIBDIR_RUBY}', 'Mytest.rb')
+"""
+
+import os
+from waflib import Errors, Options, Task, Utils
+from waflib.TaskGen import before_method, feature, extension
+from waflib.Configure import conf
+
+@feature('rubyext')
+@before_method('apply_incpaths', 'process_source', 'apply_bundle', 'apply_link')
+def init_rubyext(self):
+	"""
+	Add required variables for ruby extensions
+	"""
+	self.install_path = '${ARCHDIR_RUBY}'
+	self.uselib = self.to_list(getattr(self, 'uselib', ''))
+	if not 'RUBY' in self.uselib:
+		self.uselib.append('RUBY')
+	if not 'RUBYEXT' in self.uselib:
+		self.uselib.append('RUBYEXT')
+
+@feature('rubyext')
+@before_method('apply_link', 'propagate_uselib_vars')
+def apply_ruby_so_name(self):
+	"""
+	Strip the *lib* prefix from ruby extensions
+	"""
+	self.env.cshlib_PATTERN = self.env.cxxshlib_PATTERN = self.env.rubyext_PATTERN
+
+@conf
+def check_ruby_version(self, minver=()):
+	"""
+	Checks if ruby is installed.
+	If installed the variable RUBY will be set in environment.
+	The ruby binary can be overridden by ``--with-ruby-binary`` command-line option.
+	"""
+
+	ruby = self.find_program('ruby', var='RUBY', value=Options.options.rubybinary)
+
+	try:
+		version = self.cmd_and_log(ruby + ['-e', 'puts defined?(VERSION) ? VERSION : RUBY_VERSION']).strip()
+	except Errors.WafError:
+		self.fatal('could not determine ruby version')
+	self.env.RUBY_VERSION = version
+
+	try:
+		ver = tuple(map(int, version.split('.')))
+	except Errors.WafError:
+		self.fatal('unsupported ruby version %r' % version)
+
+	cver = ''
+	if minver:
+		cver = '> ' + '.'.join(str(x) for x in minver)
+		if ver < minver:
+			self.fatal('ruby is too old %r' % ver)
+
+	self.msg('Checking for ruby version %s' % cver, version)
+
+@conf
+def check_ruby_ext_devel(self):
+	"""
+	Check if a ruby extension can be created
+	"""
+	if not self.env.RUBY:
+		self.fatal('ruby detection is required first')
+
+	if not self.env.CC_NAME and not self.env.CXX_NAME:
+		self.fatal('load a c/c++ compiler first')
+
+	version = tuple(map(int, self.env.RUBY_VERSION.split(".")))
+
+	def read_out(cmd):
+		return Utils.to_list(self.cmd_and_log(self.env.RUBY + ['-rrbconfig', '-e', cmd]))
+
+	def read_config(key):
+		return read_out('puts RbConfig::CONFIG[%r]' % key)
+
+	cpppath = archdir = read_config('archdir')
+
+	if version >= (1, 9, 0):
+		ruby_hdrdir = read_config('rubyhdrdir')
+		cpppath += ruby_hdrdir
+		if version >= (2, 0, 0):
+			cpppath += read_config('rubyarchhdrdir')
+		cpppath += [os.path.join(ruby_hdrdir[0], read_config('arch')[0])]
+
+	self.check(header_name='ruby.h', includes=cpppath, errmsg='could not find ruby header file', link_header_test=False)
+
+	self.env.LIBPATH_RUBYEXT = read_config('libdir')
+	self.env.LIBPATH_RUBYEXT += archdir
+	self.env.INCLUDES_RUBYEXT = cpppath
+	self.env.CFLAGS_RUBYEXT = read_config('CCDLFLAGS')
+	self.env.rubyext_PATTERN = '%s.' + read_config('DLEXT')[0]
+
+	# ok this is really stupid, but the command and flags are combined.
+	# so we try to find the first argument...
+	flags = read_config('LDSHARED')
+	while flags and flags[0][0] != '-':
+		flags = flags[1:]
+
+	# we also want to strip out the deprecated ppc flags
+	if len(flags) > 1 and flags[1] == "ppc":
+		flags = flags[2:]
+
+	self.env.LINKFLAGS_RUBYEXT = flags
+	self.env.LINKFLAGS_RUBYEXT += read_config('LIBS')
+	self.env.LINKFLAGS_RUBYEXT += read_config('LIBRUBYARG_SHARED')
+
+	if Options.options.rubyarchdir:
+		self.env.ARCHDIR_RUBY = Options.options.rubyarchdir
+	else:
+		self.env.ARCHDIR_RUBY = read_config('sitearchdir')[0]
+
+	if Options.options.rubylibdir:
+		self.env.LIBDIR_RUBY = Options.options.rubylibdir
+	else:
+		self.env.LIBDIR_RUBY = read_config('sitelibdir')[0]
+
+@conf
+def check_ruby_module(self, module_name):
+	"""
+	Check if the selected ruby interpreter can require the given ruby module::
+
+		def configure(conf):
+			conf.check_ruby_module('libxml')
+
+	:param module_name: module
+	:type  module_name: string
+	"""
+	self.start_msg('Ruby module %s' % module_name)
+	try:
+		self.cmd_and_log(self.env.RUBY + ['-e', 'require \'%s\';puts 1' % module_name])
+	except Errors.WafError:
+		self.end_msg(False)
+		self.fatal('Could not find the ruby module %r' % module_name)
+	self.end_msg(True)
+
+@extension('.rb')
+def process(self, node):
+	return self.create_task('run_ruby', node)
+
+class run_ruby(Task.Task):
+	"""
+	Task to run ruby files detected by file extension .rb::
+
+		def options(opt):
+			opt.load('ruby')
+
+		def configure(ctx):
+			ctx.check_ruby_version()
+
+		def build(bld):
+			bld.env.RBFLAGS = '-e puts "hello world"'
+			bld(source='a_ruby_file.rb')
+	"""
+	run_str = '${RUBY} ${RBFLAGS} -I ${SRC[0].parent.abspath()} ${SRC}'
+
+def options(opt):
+	"""
+	Add the ``--with-ruby-archdir``, ``--with-ruby-libdir`` and ``--with-ruby-binary`` options
+	"""
+	opt.add_option('--with-ruby-archdir', type='string', dest='rubyarchdir', help='Specify directory where to install arch specific files')
+	opt.add_option('--with-ruby-libdir', type='string', dest='rubylibdir', help='Specify alternate ruby library path')
+	opt.add_option('--with-ruby-binary', type='string', dest='rubybinary', help='Specify alternate ruby binary')
+
diff --git a/waflib/Tools/suncc.py b/waflib/Tools/suncc.py
index 4909854e..33d34fc9 100644
--- a/waflib/Tools/suncc.py
+++ b/waflib/Tools/suncc.py
@@ -1,27 +1,26 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 # Ralf Habacker, 2006 (rh)
 
+from waflib import Errors
 from waflib.Tools import ccroot, ar
 from waflib.Configure import conf
 
 @conf
 def find_scc(conf):
 	"""
-	Detect the Sun C compiler
+	Detects the Sun C compiler
 	"""
 	v = conf.env
 	cc = conf.find_program('cc', var='CC')
-
 	try:
 		conf.cmd_and_log(cc + ['-flags'])
-	except Exception:
+	except Errors.WafError:
 		conf.fatal('%r is not a Sun compiler' % cc)
 	v.CC_NAME = 'sun'
 	conf.get_suncc_version(cc)
 
-
 @conf
 def scc_common_flags(conf):
 	"""
@@ -29,36 +28,34 @@ def scc_common_flags(conf):
 	"""
 	v = conf.env
 
-	v['CC_SRC_F']            = []
-	v['CC_TGT_F']            = ['-c', '-o']
+	v.CC_SRC_F            = []
+	v.CC_TGT_F            = ['-c', '-o', '']
 
-	# linker
-	if not v['LINK_CC']: v['LINK_CC'] = v['CC']
-	v['CCLNK_SRC_F']         = ''
-	v['CCLNK_TGT_F']         = ['-o']
-	v['CPPPATH_ST']          = '-I%s'
-	v['DEFINES_ST']          = '-D%s'
+	if not v.LINK_CC:
+		v.LINK_CC = v.CC
 
-	v['LIB_ST']              = '-l%s' # template for adding libs
-	v['LIBPATH_ST']          = '-L%s' # template for adding libpaths
-	v['STLIB_ST']            = '-l%s'
-	v['STLIBPATH_ST']        = '-L%s'
+	v.CCLNK_SRC_F         = ''
+	v.CCLNK_TGT_F         = ['-o', '']
+	v.CPPPATH_ST          = '-I%s'
+	v.DEFINES_ST          = '-D%s'
 
-	v['SONAME_ST']           = '-Wl,-h,%s'
-	v['SHLIB_MARKER']        = '-Bdynamic'
-	v['STLIB_MARKER']        = '-Bstatic'
+	v.LIB_ST              = '-l%s' # template for adding libs
+	v.LIBPATH_ST          = '-L%s' # template for adding libpaths
+	v.STLIB_ST            = '-l%s'
+	v.STLIBPATH_ST        = '-L%s'
 
-	# program
-	v['cprogram_PATTERN']    = '%s'
+	v.SONAME_ST           = '-Wl,-h,%s'
+	v.SHLIB_MARKER        = '-Bdynamic'
+	v.STLIB_MARKER        = '-Bstatic'
 
-	# shared library
-	v['CFLAGS_cshlib']       = ['-xcode=pic32', '-DPIC']
-	v['LINKFLAGS_cshlib']    = ['-G']
-	v['cshlib_PATTERN']      = 'lib%s.so'
+	v.cprogram_PATTERN    = '%s'
 
-	# static lib
-	v['LINKFLAGS_cstlib']    = ['-Bstatic']
-	v['cstlib_PATTERN']      = 'lib%s.a'
+	v.CFLAGS_cshlib       = ['-xcode=pic32', '-DPIC']
+	v.LINKFLAGS_cshlib    = ['-G']
+	v.cshlib_PATTERN      = 'lib%s.so'
+
+	v.LINKFLAGS_cstlib    = ['-Bstatic']
+	v.cstlib_PATTERN      = 'lib%s.a'
 
 def configure(conf):
 	conf.find_scc()
@@ -67,3 +64,4 @@ def configure(conf):
 	conf.cc_load_tools()
 	conf.cc_add_flags()
 	conf.link_add_flags()
+
diff --git a/waflib/Tools/suncxx.py b/waflib/Tools/suncxx.py
index 5a04b483..3b384f6f 100644
--- a/waflib/Tools/suncxx.py
+++ b/waflib/Tools/suncxx.py
@@ -1,21 +1,22 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 # Ralf Habacker, 2006 (rh)
 
+from waflib import Errors
 from waflib.Tools import ccroot, ar
 from waflib.Configure import conf
 
 @conf
 def find_sxx(conf):
 	"""
-	Detect the sun C++ compiler
+	Detects the sun C++ compiler
 	"""
 	v = conf.env
 	cc = conf.find_program(['CC', 'c++'], var='CXX')
 	try:
 		conf.cmd_and_log(cc + ['-flags'])
-	except Exception:
+	except Errors.WafError:
 		conf.fatal('%r is not a Sun compiler' % cc)
 	v.CXX_NAME = 'sun'
 	conf.get_suncc_version(cc)
@@ -27,36 +28,34 @@ def sxx_common_flags(conf):
 	"""
 	v = conf.env
 
-	v['CXX_SRC_F']           = []
-	v['CXX_TGT_F']           = ['-c', '-o']
+	v.CXX_SRC_F           = []
+	v.CXX_TGT_F           = ['-c', '-o', '']
 
-	# linker
-	if not v['LINK_CXX']: v['LINK_CXX'] = v['CXX']
-	v['CXXLNK_SRC_F']        = []
-	v['CXXLNK_TGT_F']        = ['-o']
-	v['CPPPATH_ST']          = '-I%s'
-	v['DEFINES_ST']          = '-D%s'
+	if not v.LINK_CXX:
+		v.LINK_CXX = v.CXX
 
-	v['LIB_ST']              = '-l%s' # template for adding libs
-	v['LIBPATH_ST']          = '-L%s' # template for adding libpaths
-	v['STLIB_ST']            = '-l%s'
-	v['STLIBPATH_ST']        = '-L%s'
+	v.CXXLNK_SRC_F        = []
+	v.CXXLNK_TGT_F        = ['-o', '']
+	v.CPPPATH_ST          = '-I%s'
+	v.DEFINES_ST          = '-D%s'
 
-	v['SONAME_ST']           = '-Wl,-h,%s'
-	v['SHLIB_MARKER']        = '-Bdynamic'
-	v['STLIB_MARKER']        = '-Bstatic'
+	v.LIB_ST              = '-l%s' # template for adding libs
+	v.LIBPATH_ST          = '-L%s' # template for adding libpaths
+	v.STLIB_ST            = '-l%s'
+	v.STLIBPATH_ST        = '-L%s'
 
-	# program
-	v['cxxprogram_PATTERN']  = '%s'
+	v.SONAME_ST           = '-Wl,-h,%s'
+	v.SHLIB_MARKER        = '-Bdynamic'
+	v.STLIB_MARKER        = '-Bstatic'
 
-	# shared library
-	v['CXXFLAGS_cxxshlib']   = ['-xcode=pic32', '-DPIC']
-	v['LINKFLAGS_cxxshlib']  = ['-G']
-	v['cxxshlib_PATTERN']    = 'lib%s.so'
+	v.cxxprogram_PATTERN  = '%s'
 
-	# static lib
-	v['LINKFLAGS_cxxstlib']  = ['-Bstatic']
-	v['cxxstlib_PATTERN']    = 'lib%s.a'
+	v.CXXFLAGS_cxxshlib   = ['-xcode=pic32', '-DPIC']
+	v.LINKFLAGS_cxxshlib  = ['-G']
+	v.cxxshlib_PATTERN    = 'lib%s.so'
+
+	v.LINKFLAGS_cxxstlib  = ['-Bstatic']
+	v.cxxstlib_PATTERN    = 'lib%s.a'
 
 def configure(conf):
 	conf.find_sxx()
diff --git a/waflib/Tools/tex.py b/waflib/Tools/tex.py
new file mode 100644
index 00000000..eaf9fdb5
--- /dev/null
+++ b/waflib/Tools/tex.py
@@ -0,0 +1,543 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2018 (ita)
+
+"""
+TeX/LaTeX/PDFLaTeX/XeLaTeX support
+
+Example::
+
+	def configure(conf):
+		conf.load('tex')
+		if not conf.env.LATEX:
+			conf.fatal('The program LaTex is required')
+
+	def build(bld):
+		bld(
+			features = 'tex',
+			type     = 'latex', # pdflatex or xelatex
+			source   = 'document.ltx', # mandatory, the source
+			outs     = 'ps', # 'pdf' or 'ps pdf'
+			deps     = 'crossreferencing.lst', # to give dependencies directly
+			prompt   = 1, # 0 for the batch mode
+		)
+
+Notes:
+
+- To configure with a special program, use::
+
+     $ PDFLATEX=luatex waf configure
+
+- This tool does not use the target attribute of the task generator
+  (``bld(target=...)``); the target file name is built from the source
+  base name and the output type(s)
+"""
+
+import os, re
+from waflib import Utils, Task, Errors, Logs, Node
+from waflib.TaskGen import feature, before_method
+
+re_bibunit = re.compile(r'\\(?P<type>putbib)\[(?P<file>[^\[\]]*)\]',re.M)
+def bibunitscan(self):
+	"""
+	Parses TeX inputs and try to find the *bibunit* file dependencies
+
+	:return: list of bibunit files
+	:rtype: list of :py:class:`waflib.Node.Node`
+	"""
+	node = self.inputs[0]
+
+	nodes = []
+	if not node:
+		return nodes
+
+	code = node.read()
+	for match in re_bibunit.finditer(code):
+		path = match.group('file')
+		if path:
+			found = None
+			for k in ('', '.bib'):
+				# add another loop for the tex include paths?
+				Logs.debug('tex: trying %s%s', path, k)
+				fi = node.parent.find_resource(path + k)
+				if fi:
+					found = True
+					nodes.append(fi)
+					# no break
+			if not found:
+				Logs.debug('tex: could not find %s', path)
+
+	Logs.debug('tex: found the following bibunit files: %s', nodes)
+	return nodes
+
+exts_deps_tex = ['', '.ltx', '.tex', '.bib', '.pdf', '.png', '.eps', '.ps', '.sty']
+"""List of typical file extensions included in latex files"""
+
+exts_tex = ['.ltx', '.tex']
+"""List of typical file extensions that contain latex"""
+
+re_tex = re.compile(r'\\(?P<type>usepackage|RequirePackage|include|bibliography([^\[\]{}]*)|putbib|includegraphics|input|import|bringin|lstinputlisting)(\[[^\[\]]*\])?{(?P<file>[^{}]*)}',re.M)
+"""Regexp for expressions that may include latex files"""
+
+g_bibtex_re = re.compile('bibdata', re.M)
+"""Regexp for bibtex files"""
+
+g_glossaries_re = re.compile('\\@newglossary', re.M)
+"""Regexp for expressions that create glossaries"""
+
+class tex(Task.Task):
+	"""
+	Compiles a tex/latex file.
+
+	.. inheritance-diagram:: waflib.Tools.tex.latex waflib.Tools.tex.xelatex waflib.Tools.tex.pdflatex
+	"""
+
+	bibtex_fun, _ = Task.compile_fun('${BIBTEX} ${BIBTEXFLAGS} ${SRCFILE}', shell=False)
+	bibtex_fun.__doc__ = """
+	Execute the program **bibtex**
+	"""
+
+	makeindex_fun, _ = Task.compile_fun('${MAKEINDEX} ${MAKEINDEXFLAGS} ${SRCFILE}', shell=False)
+	makeindex_fun.__doc__ = """
+	Execute the program **makeindex**
+	"""
+
+	makeglossaries_fun, _ = Task.compile_fun('${MAKEGLOSSARIES} ${SRCFILE}', shell=False)
+	makeglossaries_fun.__doc__ = """
+	Execute the program **makeglossaries**
+	"""
+
+	def exec_command(self, cmd, **kw):
+		"""
+		Executes TeX commands without buffering (latex may prompt for inputs)
+
+		:return: the return code
+		:rtype: int
+		"""
+		if self.env.PROMPT_LATEX:
+			# capture the outputs in configuration tests
+			kw['stdout'] = kw['stderr'] = None
+		return super(tex, self).exec_command(cmd, **kw)
+
+	def scan_aux(self, node):
+		"""
+		Recursive regex-based scanner that finds included auxiliary files.
+		"""
+		nodes = [node]
+		re_aux = re.compile(r'\\@input{(?P<file>[^{}]*)}', re.M)
+
+		def parse_node(node):
+			code = node.read()
+			for match in re_aux.finditer(code):
+				path = match.group('file')
+				found = node.parent.find_or_declare(path)
+				if found and found not in nodes:
+					Logs.debug('tex: found aux node %r', found)
+					nodes.append(found)
+					parse_node(found)
+		parse_node(node)
+		return nodes
+
+	def scan(self):
+		"""
+		Recursive regex-based scanner that finds latex dependencies. It uses :py:attr:`waflib.Tools.tex.re_tex`
+
+		Depending on your needs you might want:
+
+		* to change re_tex::
+
+			from waflib.Tools import tex
+			tex.re_tex = myregex
+
+		* or to change the method scan from the latex tasks::
+
+			from waflib.Task import classes
+			classes['latex'].scan = myscanfunction
+		"""
+		node = self.inputs[0]
+
+		nodes = []
+		names = []
+		seen = []
+		if not node:
+			return (nodes, names)
+
+		def parse_node(node):
+			if node in seen:
+				return
+			seen.append(node)
+			code = node.read()
+			for match in re_tex.finditer(code):
+
+				multibib = match.group('type')
+				if multibib and multibib.startswith('bibliography'):
+					multibib = multibib[len('bibliography'):]
+					if multibib.startswith('style'):
+						continue
+				else:
+					multibib = None
+
+				for path in match.group('file').split(','):
+					if path:
+						add_name = True
+						found = None
+						for k in exts_deps_tex:
+
+							# issue 1067, scan in all texinputs folders
+							for up in self.texinputs_nodes:
+								Logs.debug('tex: trying %s%s', path, k)
+								found = up.find_resource(path + k)
+								if found:
+									break
+
+
+							for tsk in self.generator.tasks:
+								if not found or found in tsk.outputs:
+									break
+							else:
+								nodes.append(found)
+								add_name = False
+								for ext in exts_tex:
+									if found.name.endswith(ext):
+										parse_node(found)
+										break
+
+							# multibib stuff
+							if found and multibib and found.name.endswith('.bib'):
+								try:
+									self.multibibs.append(found)
+								except AttributeError:
+									self.multibibs = [found]
+
+							# no break, people are crazy
+						if add_name:
+							names.append(path)
+		parse_node(node)
+
+		for x in nodes:
+			x.parent.get_bld().mkdir()
+
+		Logs.debug("tex: found the following : %s and names %s", nodes, names)
+		return (nodes, names)
+
+	def check_status(self, msg, retcode):
+		"""
+		Checks an exit status and raise an error with a particular message
+
+		:param msg: message to display if the code is non-zero
+		:type msg: string
+		:param retcode: condition
+		:type retcode: boolean
+		"""
+		if retcode != 0:
+			raise Errors.WafError('%r command exit status %r' % (msg, retcode))
+
+	def info(self, *k, **kw):
+		try:
+			info = self.generator.bld.conf.logger.info
+		except AttributeError:
+			info = Logs.info
+		info(*k, **kw)
+
+	def bibfile(self):
+		"""
+		Parses *.aux* files to find bibfiles to process.
+		If present, execute :py:meth:`waflib.Tools.tex.tex.bibtex_fun`
+		"""
+		for aux_node in self.aux_nodes:
+			try:
+				ct = aux_node.read()
+			except EnvironmentError:
+				Logs.error('Error reading %s: %r', aux_node.abspath())
+				continue
+
+			if g_bibtex_re.findall(ct):
+				self.info('calling bibtex')
+
+				self.env.env = {}
+				self.env.env.update(os.environ)
+				self.env.env.update({'BIBINPUTS': self.texinputs(), 'BSTINPUTS': self.texinputs()})
+				self.env.SRCFILE = aux_node.name[:-4]
+				self.check_status('error when calling bibtex', self.bibtex_fun())
+
+		for node in getattr(self, 'multibibs', []):
+			self.env.env = {}
+			self.env.env.update(os.environ)
+			self.env.env.update({'BIBINPUTS': self.texinputs(), 'BSTINPUTS': self.texinputs()})
+			self.env.SRCFILE = node.name[:-4]
+			self.check_status('error when calling bibtex', self.bibtex_fun())
+
+	def bibunits(self):
+		"""
+		Parses *.aux* file to find bibunit files. If there are bibunit files,
+		runs :py:meth:`waflib.Tools.tex.tex.bibtex_fun`.
+		"""
+		try:
+			bibunits = bibunitscan(self)
+		except OSError:
+			Logs.error('error bibunitscan')
+		else:
+			if bibunits:
+				fn  = ['bu' + str(i) for i in range(1, len(bibunits) + 1)]
+				if fn:
+					self.info('calling bibtex on bibunits')
+
+				for f in fn:
+					self.env.env = {'BIBINPUTS': self.texinputs(), 'BSTINPUTS': self.texinputs()}
+					self.env.SRCFILE = f
+					self.check_status('error when calling bibtex', self.bibtex_fun())
+
+	def makeindex(self):
+		"""
+		Searches the filesystem for *.idx* files to process. If present,
+		runs :py:meth:`waflib.Tools.tex.tex.makeindex_fun`
+		"""
+		self.idx_node = self.inputs[0].change_ext('.idx')
+		try:
+			idx_path = self.idx_node.abspath()
+			os.stat(idx_path)
+		except OSError:
+			self.info('index file %s absent, not calling makeindex', idx_path)
+		else:
+			self.info('calling makeindex')
+
+			self.env.SRCFILE = self.idx_node.name
+			self.env.env = {}
+			self.check_status('error when calling makeindex %s' % idx_path, self.makeindex_fun())
+
+	def bibtopic(self):
+		"""
+		Lists additional .aux files from the bibtopic package
+		"""
+		p = self.inputs[0].parent.get_bld()
+		if os.path.exists(os.path.join(p.abspath(), 'btaux.aux')):
+			self.aux_nodes += p.ant_glob('*[0-9].aux')
+
+	def makeglossaries(self):
+		"""
+		Lists additional glossaries from .aux files. If present, runs the makeglossaries program.
+		"""
+		src_file = self.inputs[0].abspath()
+		base_file = os.path.basename(src_file)
+		base, _ = os.path.splitext(base_file)
+		for aux_node in self.aux_nodes:
+			try:
+				ct = aux_node.read()
+			except EnvironmentError:
+				Logs.error('Error reading %s: %r', aux_node.abspath())
+				continue
+
+			if g_glossaries_re.findall(ct):
+				if not self.env.MAKEGLOSSARIES:
+					raise Errors.WafError("The program 'makeglossaries' is missing!")
+				Logs.warn('calling makeglossaries')
+				self.env.SRCFILE = base
+				self.check_status('error when calling makeglossaries %s' % base, self.makeglossaries_fun())
+				return
+
+	def texinputs(self):
+		"""
+		Returns the list of texinput nodes as a string suitable for the TEXINPUTS environment variables
+
+		:rtype: string
+		"""
+		return os.pathsep.join([k.abspath() for k in self.texinputs_nodes]) + os.pathsep
+
+	def run(self):
+		"""
+		Runs the whole TeX build process
+
+		Multiple passes are required depending on the usage of cross-references,
+		bibliographies, glossaries, indexes and additional contents
+		The appropriate TeX compiler is called until the *.aux* files stop changing.
+		"""
+		env = self.env
+
+		if not env.PROMPT_LATEX:
+			env.append_value('LATEXFLAGS', '-interaction=batchmode')
+			env.append_value('PDFLATEXFLAGS', '-interaction=batchmode')
+			env.append_value('XELATEXFLAGS', '-interaction=batchmode')
+
+		# important, set the cwd for everybody
+		self.cwd = self.inputs[0].parent.get_bld()
+
+		self.info('first pass on %s', self.__class__.__name__)
+
+		# Hash .aux files before even calling the LaTeX compiler
+		cur_hash = self.hash_aux_nodes()
+
+		self.call_latex()
+
+		# Find the .aux files again since bibtex processing can require it
+		self.hash_aux_nodes()
+
+		self.bibtopic()
+		self.bibfile()
+		self.bibunits()
+		self.makeindex()
+		self.makeglossaries()
+
+		for i in range(10):
+			# There is no need to call latex again if the .aux hash value has not changed
+			prev_hash = cur_hash
+			cur_hash = self.hash_aux_nodes()
+			if not cur_hash:
+				Logs.error('No aux.h to process')
+			if cur_hash and cur_hash == prev_hash:
+				break
+
+			# run the command
+			self.info('calling %s', self.__class__.__name__)
+			self.call_latex()
+
+	def hash_aux_nodes(self):
+		"""
+		Returns a hash of the .aux file contents
+
+		:rtype: string or bytes
+		"""
+		try:
+			self.aux_nodes
+		except AttributeError:
+			try:
+				self.aux_nodes = self.scan_aux(self.inputs[0].change_ext('.aux'))
+			except IOError:
+				return None
+		return Utils.h_list([Utils.h_file(x.abspath()) for x in self.aux_nodes])
+
+	def call_latex(self):
+		"""
+		Runs the TeX compiler once
+		"""
+		self.env.env = {}
+		self.env.env.update(os.environ)
+		self.env.env.update({'TEXINPUTS': self.texinputs()})
+		self.env.SRCFILE = self.inputs[0].abspath()
+		self.check_status('error when calling latex', self.texfun())
+
+class latex(tex):
+	"Compiles LaTeX files"
+	texfun, vars = Task.compile_fun('${LATEX} ${LATEXFLAGS} ${SRCFILE}', shell=False)
+
+class pdflatex(tex):
+	"Compiles PdfLaTeX files"
+	texfun, vars =  Task.compile_fun('${PDFLATEX} ${PDFLATEXFLAGS} ${SRCFILE}', shell=False)
+
+class xelatex(tex):
+	"XeLaTeX files"
+	texfun, vars = Task.compile_fun('${XELATEX} ${XELATEXFLAGS} ${SRCFILE}', shell=False)
+
+class dvips(Task.Task):
+	"Converts dvi files to postscript"
+	run_str = '${DVIPS} ${DVIPSFLAGS} ${SRC} -o ${TGT}'
+	color   = 'BLUE'
+	after   = ['latex', 'pdflatex', 'xelatex']
+
+class dvipdf(Task.Task):
+	"Converts dvi files to pdf"
+	run_str = '${DVIPDF} ${DVIPDFFLAGS} ${SRC} ${TGT}'
+	color   = 'BLUE'
+	after   = ['latex', 'pdflatex', 'xelatex']
+
+class pdf2ps(Task.Task):
+	"Converts pdf files to postscript"
+	run_str = '${PDF2PS} ${PDF2PSFLAGS} ${SRC} ${TGT}'
+	color   = 'BLUE'
+	after   = ['latex', 'pdflatex', 'xelatex']
+
+@feature('tex')
+@before_method('process_source')
+def apply_tex(self):
+	"""
+	Creates :py:class:`waflib.Tools.tex.tex` objects, and
+	dvips/dvipdf/pdf2ps tasks if necessary (outs='ps', etc).
+	"""
+	if not getattr(self, 'type', None) in ('latex', 'pdflatex', 'xelatex'):
+		self.type = 'pdflatex'
+
+	outs = Utils.to_list(getattr(self, 'outs', []))
+
+	# prompt for incomplete files (else the batchmode is used)
+	try:
+		self.generator.bld.conf
+	except AttributeError:
+		default_prompt = False
+	else:
+		default_prompt = True
+	self.env.PROMPT_LATEX = getattr(self, 'prompt', default_prompt)
+
+	deps_lst = []
+
+	if getattr(self, 'deps', None):
+		deps = self.to_list(self.deps)
+		for dep in deps:
+			if isinstance(dep, str):
+				n = self.path.find_resource(dep)
+				if not n:
+					self.bld.fatal('Could not find %r for %r' % (dep, self))
+				if not n in deps_lst:
+					deps_lst.append(n)
+			elif isinstance(dep, Node.Node):
+				deps_lst.append(dep)
+
+	for node in self.to_nodes(self.source):
+		if self.type == 'latex':
+			task = self.create_task('latex', node, node.change_ext('.dvi'))
+		elif self.type == 'pdflatex':
+			task = self.create_task('pdflatex', node, node.change_ext('.pdf'))
+		elif self.type == 'xelatex':
+			task = self.create_task('xelatex', node, node.change_ext('.pdf'))
+
+		task.env = self.env
+
+		# add the manual dependencies
+		if deps_lst:
+			for n in deps_lst:
+				if not n in task.dep_nodes:
+					task.dep_nodes.append(n)
+
+		# texinputs is a nasty beast
+		if hasattr(self, 'texinputs_nodes'):
+			task.texinputs_nodes = self.texinputs_nodes
+		else:
+			task.texinputs_nodes = [node.parent, node.parent.get_bld(), self.path, self.path.get_bld()]
+			lst = os.environ.get('TEXINPUTS', '')
+			if self.env.TEXINPUTS:
+				lst += os.pathsep + self.env.TEXINPUTS
+			if lst:
+				lst = lst.split(os.pathsep)
+			for x in lst:
+				if x:
+					if os.path.isabs(x):
+						p = self.bld.root.find_node(x)
+						if p:
+							task.texinputs_nodes.append(p)
+						else:
+							Logs.error('Invalid TEXINPUTS folder %s', x)
+					else:
+						Logs.error('Cannot resolve relative paths in TEXINPUTS %s', x)
+
+		if self.type == 'latex':
+			if 'ps' in outs:
+				tsk = self.create_task('dvips', task.outputs, node.change_ext('.ps'))
+				tsk.env.env = dict(os.environ)
+			if 'pdf' in outs:
+				tsk = self.create_task('dvipdf', task.outputs, node.change_ext('.pdf'))
+				tsk.env.env = dict(os.environ)
+		elif self.type == 'pdflatex':
+			if 'ps' in outs:
+				self.create_task('pdf2ps', task.outputs, node.change_ext('.ps'))
+	self.source = []
+
+def configure(self):
+	"""
+	Find the programs tex, latex and others without raising errors.
+	"""
+	v = self.env
+	for p in 'tex latex pdflatex xelatex bibtex dvips dvipdf ps2pdf makeindex pdf2ps makeglossaries'.split():
+		try:
+			self.find_program(p, var=p.upper())
+		except self.errors.ConfigurationError:
+			pass
+	v.DVIPSFLAGS = '-Ppdf'
+
diff --git a/waflib/Tools/vala.py b/waflib/Tools/vala.py
new file mode 100644
index 00000000..822ec502
--- /dev/null
+++ b/waflib/Tools/vala.py
@@ -0,0 +1,355 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Ali Sabil, 2007
+# Radosław Szkodziński, 2010
+
+"""
+At this point, vala is still unstable, so do not expect
+this tool to be too stable either (apis, etc)
+"""
+
+import re
+from waflib import Build, Context, Errors, Logs, Node, Options, Task, Utils
+from waflib.TaskGen import extension, taskgen_method
+from waflib.Configure import conf
+
+class valac(Task.Task):
+	"""
+	Compiles vala files
+	"""
+	#run_str = "${VALAC} ${VALAFLAGS}" # ideally
+	#vars = ['VALAC_VERSION']
+	vars = ["VALAC", "VALAC_VERSION", "VALAFLAGS"]
+	ext_out = ['.h']
+
+	def run(self):
+		cmd = self.env.VALAC + self.env.VALAFLAGS
+		resources = getattr(self, 'vala_exclude', [])
+		cmd.extend([a.abspath() for a in self.inputs if a not in resources])
+		ret = self.exec_command(cmd, cwd=self.vala_dir_node.abspath())
+
+		if ret:
+			return ret
+
+		if self.generator.dump_deps_node:
+			self.generator.dump_deps_node.write('\n'.join(self.generator.packages))
+
+		return ret
+
+@taskgen_method
+def init_vala_task(self):
+	"""
+	Initializes the vala task with the relevant data (acts as a constructor)
+	"""
+	self.profile = getattr(self, 'profile', 'gobject')
+
+	self.packages = packages = Utils.to_list(getattr(self, 'packages', []))
+	self.use = Utils.to_list(getattr(self, 'use', []))
+	if packages and not self.use:
+		self.use = packages[:] # copy
+
+	if self.profile == 'gobject':
+		if not 'GOBJECT' in self.use:
+			self.use.append('GOBJECT')
+
+	def addflags(flags):
+		self.env.append_value('VALAFLAGS', flags)
+
+	if self.profile:
+		addflags('--profile=%s' % self.profile)
+
+	valatask = self.valatask
+
+	# output directory
+	if hasattr(self, 'vala_dir'):
+		if isinstance(self.vala_dir, str):
+			valatask.vala_dir_node = self.path.get_bld().make_node(self.vala_dir)
+			try:
+				valatask.vala_dir_node.mkdir()
+			except OSError:
+				raise self.bld.fatal('Cannot create the vala dir %r' % valatask.vala_dir_node)
+		else:
+			valatask.vala_dir_node = self.vala_dir
+	else:
+		valatask.vala_dir_node = self.path.get_bld()
+	addflags('--directory=%s' % valatask.vala_dir_node.abspath())
+
+	if hasattr(self, 'thread'):
+		if self.profile == 'gobject':
+			if not 'GTHREAD' in self.use:
+				self.use.append('GTHREAD')
+		else:
+			#Vala doesn't have threading support for dova nor posix
+			Logs.warn('Profile %s means no threading support', self.profile)
+			self.thread = False
+
+		if self.thread:
+			addflags('--thread')
+
+	self.is_lib = 'cprogram' not in self.features
+	if self.is_lib:
+		addflags('--library=%s' % self.target)
+
+		h_node = valatask.vala_dir_node.find_or_declare('%s.h' % self.target)
+		valatask.outputs.append(h_node)
+		addflags('--header=%s' % h_node.name)
+
+		valatask.outputs.append(valatask.vala_dir_node.find_or_declare('%s.vapi' % self.target))
+
+		if getattr(self, 'gir', None):
+			gir_node = valatask.vala_dir_node.find_or_declare('%s.gir' % self.gir)
+			addflags('--gir=%s' % gir_node.name)
+			valatask.outputs.append(gir_node)
+
+	self.vala_target_glib = getattr(self, 'vala_target_glib', getattr(Options.options, 'vala_target_glib', None))
+	if self.vala_target_glib:
+		addflags('--target-glib=%s' % self.vala_target_glib)
+
+	addflags(['--define=%s' % x for x in Utils.to_list(getattr(self, 'vala_defines', []))])
+
+	packages_private = Utils.to_list(getattr(self, 'packages_private', []))
+	addflags(['--pkg=%s' % x for x in packages_private])
+
+	def _get_api_version():
+		api_version = '1.0'
+		if hasattr(Context.g_module, 'API_VERSION'):
+			version = Context.g_module.API_VERSION.split(".")
+			if version[0] == "0":
+				api_version = "0." + version[1]
+			else:
+				api_version = version[0] + ".0"
+		return api_version
+
+	self.includes = Utils.to_list(getattr(self, 'includes', []))
+	valatask.install_path = getattr(self, 'install_path', '')
+
+	valatask.vapi_path = getattr(self, 'vapi_path', '${DATAROOTDIR}/vala/vapi')
+	valatask.pkg_name = getattr(self, 'pkg_name', self.env.PACKAGE)
+	valatask.header_path = getattr(self, 'header_path', '${INCLUDEDIR}/%s-%s' % (valatask.pkg_name, _get_api_version()))
+	valatask.install_binding = getattr(self, 'install_binding', True)
+
+	self.vapi_dirs = vapi_dirs = Utils.to_list(getattr(self, 'vapi_dirs', []))
+	#includes =  []
+
+	if hasattr(self, 'use'):
+		local_packages = Utils.to_list(self.use)[:] # make sure to have a copy
+		seen = []
+		while len(local_packages) > 0:
+			package = local_packages.pop()
+			if package in seen:
+				continue
+			seen.append(package)
+
+			# check if the package exists
+			try:
+				package_obj = self.bld.get_tgen_by_name(package)
+			except Errors.WafError:
+				continue
+
+			# in practice the other task is already processed
+			# but this makes it explicit
+			package_obj.post()
+			package_name = package_obj.target
+			task = getattr(package_obj, 'valatask', None)
+			if task:
+				for output in task.outputs:
+					if output.name == package_name + ".vapi":
+						valatask.set_run_after(task)
+						if package_name not in packages:
+							packages.append(package_name)
+						if output.parent not in vapi_dirs:
+							vapi_dirs.append(output.parent)
+						if output.parent not in self.includes:
+							self.includes.append(output.parent)
+
+			if hasattr(package_obj, 'use'):
+				lst = self.to_list(package_obj.use)
+				lst.reverse()
+				local_packages = [pkg for pkg in lst if pkg not in seen] + local_packages
+
+	addflags(['--pkg=%s' % p for p in packages])
+
+	for vapi_dir in vapi_dirs:
+		if isinstance(vapi_dir, Node.Node):
+			v_node = vapi_dir
+		else:
+			v_node = self.path.find_dir(vapi_dir)
+		if not v_node:
+			Logs.warn('Unable to locate Vala API directory: %r', vapi_dir)
+		else:
+			addflags('--vapidir=%s' % v_node.abspath())
+
+	self.dump_deps_node = None
+	if self.is_lib and self.packages:
+		self.dump_deps_node = valatask.vala_dir_node.find_or_declare('%s.deps' % self.target)
+		valatask.outputs.append(self.dump_deps_node)
+
+	if self.is_lib and valatask.install_binding:
+		headers_list = [o for o in valatask.outputs if o.suffix() == ".h"]
+		if headers_list:
+			self.install_vheader = self.add_install_files(install_to=valatask.header_path, install_from=headers_list)
+
+		vapi_list = [o for o in valatask.outputs if (o.suffix() in (".vapi", ".deps"))]
+		if vapi_list:
+			self.install_vapi = self.add_install_files(install_to=valatask.vapi_path, install_from=vapi_list)
+
+		gir_list = [o for o in valatask.outputs if o.suffix() == '.gir']
+		if gir_list:
+			self.install_gir = self.add_install_files(
+				install_to=getattr(self, 'gir_path', '${DATAROOTDIR}/gir-1.0'), install_from=gir_list)
+
+	if hasattr(self, 'vala_resources'):
+		nodes = self.to_nodes(self.vala_resources)
+		valatask.vala_exclude = getattr(valatask, 'vala_exclude', []) + nodes
+		valatask.inputs.extend(nodes)
+		for x in nodes:
+			addflags(['--gresources', x.abspath()])
+
+@extension('.vala', '.gs')
+def vala_file(self, node):
+	"""
+	Compile a vala file and bind the task to *self.valatask*. If an existing vala task is already set, add the node
+	to its inputs. The typical example is::
+
+		def build(bld):
+			bld.program(
+				packages      = 'gtk+-2.0',
+				target        = 'vala-gtk-example',
+				use           = 'GTK GLIB',
+				source        = 'vala-gtk-example.vala foo.vala',
+				vala_defines  = ['DEBUG'] # adds --define=<xyz> values to the command-line
+
+				# the following arguments are for libraries
+				#gir          = 'hello-1.0',
+				#gir_path     = '/tmp',
+				#vapi_path = '/tmp',
+				#pkg_name = 'hello'
+				# disable installing of gir, vapi and header
+				#install_binding = False
+
+				# profile     = 'xyz' # adds --profile=<xyz> to enable profiling
+				# thread      = True, # adds --thread, except if profile is on or not on 'gobject'
+				# vala_target_glib = 'xyz' # adds --target-glib=<xyz>, can be given through the command-line option --vala-target-glib=<xyz>
+			)
+
+
+	:param node: vala file
+	:type node: :py:class:`waflib.Node.Node`
+	"""
+
+	try:
+		valatask = self.valatask
+	except AttributeError:
+		valatask = self.valatask = self.create_task('valac')
+		self.init_vala_task()
+
+	valatask.inputs.append(node)
+	name = node.name[:node.name.rfind('.')] + '.c'
+	c_node = valatask.vala_dir_node.find_or_declare(name)
+	valatask.outputs.append(c_node)
+	self.source.append(c_node)
+
+@extension('.vapi')
+def vapi_file(self, node):
+	try:
+		valatask = self.valatask
+	except AttributeError:
+		valatask = self.valatask = self.create_task('valac')
+		self.init_vala_task()
+	valatask.inputs.append(node)
+
+@conf
+def find_valac(self, valac_name, min_version):
+	"""
+	Find the valac program, and execute it to store the version
+	number in *conf.env.VALAC_VERSION*
+
+	:param valac_name: program name
+	:type valac_name: string or list of string
+	:param min_version: minimum version acceptable
+	:type min_version: tuple of int
+	"""
+	valac = self.find_program(valac_name, var='VALAC')
+	try:
+		output = self.cmd_and_log(valac + ['--version'])
+	except Errors.WafError:
+		valac_version = None
+	else:
+		ver = re.search(r'\d+.\d+.\d+', output).group().split('.')
+		valac_version = tuple([int(x) for x in ver])
+
+	self.msg('Checking for %s version >= %r' % (valac_name, min_version),
+	         valac_version, valac_version and valac_version >= min_version)
+	if valac and valac_version < min_version:
+		self.fatal("%s version %r is too old, need >= %r" % (valac_name, valac_version, min_version))
+
+	self.env.VALAC_VERSION = valac_version
+	return valac
+
+@conf
+def check_vala(self, min_version=(0,8,0), branch=None):
+	"""
+	Check if vala compiler from a given branch exists of at least a given
+	version.
+
+	:param min_version: minimum version acceptable (0.8.0)
+	:type min_version: tuple
+	:param branch: first part of the version number, in case a snapshot is used (0, 8)
+	:type branch: tuple of int
+	"""
+	if self.env.VALA_MINVER:
+		min_version = self.env.VALA_MINVER
+	if self.env.VALA_MINVER_BRANCH:
+		branch = self.env.VALA_MINVER_BRANCH
+	if not branch:
+		branch = min_version[:2]
+	try:
+		find_valac(self, 'valac-%d.%d' % (branch[0], branch[1]), min_version)
+	except self.errors.ConfigurationError:
+		find_valac(self, 'valac', min_version)
+
+@conf
+def check_vala_deps(self):
+	"""
+	Load the gobject and gthread packages if they are missing.
+	"""
+	if not self.env.HAVE_GOBJECT:
+		pkg_args = {'package':      'gobject-2.0',
+		            'uselib_store': 'GOBJECT',
+		            'args':         '--cflags --libs'}
+		if getattr(Options.options, 'vala_target_glib', None):
+			pkg_args['atleast_version'] = Options.options.vala_target_glib
+		self.check_cfg(**pkg_args)
+
+	if not self.env.HAVE_GTHREAD:
+		pkg_args = {'package':      'gthread-2.0',
+		            'uselib_store': 'GTHREAD',
+		            'args':         '--cflags --libs'}
+		if getattr(Options.options, 'vala_target_glib', None):
+			pkg_args['atleast_version'] = Options.options.vala_target_glib
+		self.check_cfg(**pkg_args)
+
+def configure(self):
+	"""
+	Use the following to enforce minimum vala version::
+
+		def configure(conf):
+			conf.env.VALA_MINVER = (0, 10, 0)
+			conf.load('vala')
+	"""
+	self.load('gnu_dirs')
+	self.check_vala_deps()
+	self.check_vala()
+	self.add_os_flags('VALAFLAGS')
+	self.env.append_unique('VALAFLAGS', ['-C'])
+
+def options(opt):
+	"""
+	Load the :py:mod:`waflib.Tools.gnu_dirs` tool and add the ``--vala-target-glib`` command-line option
+	"""
+	opt.load('gnu_dirs')
+	valaopts = opt.add_option_group('Vala Compiler Options')
+	valaopts.add_option('--vala-target-glib', default=None,
+		dest='vala_target_glib', metavar='MAJOR.MINOR',
+		help='Target version of glib for Vala GObject code generation')
+
diff --git a/waflib/Tools/waf_unit_test.py b/waflib/Tools/waf_unit_test.py
index 27cd9a40..a71ed1c0 100644
--- a/waflib/Tools/waf_unit_test.py
+++ b/waflib/Tools/waf_unit_test.py
@@ -1,10 +1,10 @@
 #!/usr/bin/env python
 # encoding: utf-8
 # Carlos Rafael Giani, 2006
-# Thomas Nagy, 2010
+# Thomas Nagy, 2010-2018 (ita)
 
 """
-Unit testing system for C/C++/D providing test execution:
+Unit testing system for C/C++/D and interpreted languages providing test execution:
 
 * in parallel, by using ``waf -j``
 * partial (only the tests that have changed) or full (by using ``waf --alltests``)
@@ -31,31 +31,128 @@ def build(bld):
 		bld(features='cxx cxxprogram test', source='main.c', target='app')
 		from waflib.Tools import waf_unit_test
 		bld.add_post_fun(waf_unit_test.summary)
+
+By passing --dump-test-scripts the build outputs corresponding python files
+(with extension _run.py) that are useful for debugging purposes.
 """
 
-import os
+import os, shlex, sys
 from waflib.TaskGen import feature, after_method, taskgen_method
 from waflib import Utils, Task, Logs, Options
+from waflib.Tools import ccroot
 testlock = Utils.threading.Lock()
 
+SCRIPT_TEMPLATE = """#! %(python)s
+import subprocess, sys
+cmd = %(cmd)r
+# if you want to debug with gdb:
+#cmd = ['gdb', '-args'] + cmd
+env = %(env)r
+status = subprocess.call(cmd, env=env, cwd=%(cwd)r, shell=isinstance(cmd, str))
+sys.exit(status)
+"""
+
+@taskgen_method
+def handle_ut_cwd(self, key):
+	"""
+	Task generator method, used internally to limit code duplication.
+	This method may disappear anytime.
+	"""
+	cwd = getattr(self, key, None)
+	if cwd:
+		if isinstance(cwd, str):
+			# we want a Node instance
+			if os.path.isabs(cwd):
+				self.ut_cwd = self.bld.root.make_node(cwd)
+			else:
+				self.ut_cwd = self.path.make_node(cwd)
+
+@feature('test_scripts')
+def make_interpreted_test(self):
+	"""Create interpreted unit tests."""
+	for x in ['test_scripts_source', 'test_scripts_template']:
+		if not hasattr(self, x):
+			Logs.warn('a test_scripts taskgen i missing %s' % x)
+			return
+
+	self.ut_run, lst = Task.compile_fun(self.test_scripts_template, shell=getattr(self, 'test_scripts_shell', False))
+
+	script_nodes = self.to_nodes(self.test_scripts_source)
+	for script_node in script_nodes:
+		tsk = self.create_task('utest', [script_node])
+		tsk.vars = lst + tsk.vars
+		tsk.env['SCRIPT'] = script_node.path_from(tsk.get_cwd())
+
+	self.handle_ut_cwd('test_scripts_cwd')
+
+	env = getattr(self, 'test_scripts_env', None)
+	if env:
+		self.ut_env = env
+	else:
+		self.ut_env = dict(os.environ)
+
+	paths = getattr(self, 'test_scripts_paths', {})
+	for (k,v) in paths.items():
+		p = self.ut_env.get(k, '').split(os.pathsep)
+		if isinstance(v, str):
+			v = v.split(os.pathsep)
+		self.ut_env[k] = os.pathsep.join(p + v)
+
 @feature('test')
-@after_method('apply_link')
+@after_method('apply_link', 'process_use')
 def make_test(self):
 	"""Create the unit test task. There can be only one unit test task by task generator."""
-	if getattr(self, 'link_task', None):
-		self.create_task('utest', self.link_task.outputs)
-
+	if not getattr(self, 'link_task', None):
+		return
+
+	tsk = self.create_task('utest', self.link_task.outputs)
+	if getattr(self, 'ut_str', None):
+		self.ut_run, lst = Task.compile_fun(self.ut_str, shell=getattr(self, 'ut_shell', False))
+		tsk.vars = lst + tsk.vars
+
+	self.handle_ut_cwd('ut_cwd')
+
+	if not hasattr(self, 'ut_paths'):
+		paths = []
+		for x in self.tmp_use_sorted:
+			try:
+				y = self.bld.get_tgen_by_name(x).link_task
+			except AttributeError:
+				pass
+			else:
+				if not isinstance(y, ccroot.stlink_task):
+					paths.append(y.outputs[0].parent.abspath())
+		self.ut_paths = os.pathsep.join(paths) + os.pathsep
+
+	if not hasattr(self, 'ut_env'):
+		self.ut_env = dct = dict(os.environ)
+		def add_path(var):
+			dct[var] = self.ut_paths + dct.get(var,'')
+		if Utils.is_win32:
+			add_path('PATH')
+		elif Utils.unversioned_sys_platform() == 'darwin':
+			add_path('DYLD_LIBRARY_PATH')
+			add_path('LD_LIBRARY_PATH')
+		else:
+			add_path('LD_LIBRARY_PATH')
+
+	if not hasattr(self, 'ut_cmd'):
+		self.ut_cmd = getattr(Options.options, 'testcmd', False)
 
 @taskgen_method
 def add_test_results(self, tup):
 	"""Override and return tup[1] to interrupt the build immediately if a test does not run"""
 	Logs.debug("ut: %r", tup)
-	self.utest_result = tup
+	try:
+		self.utest_results.append(tup)
+	except AttributeError:
+		self.utest_results = [tup]
 	try:
 		self.bld.utest_results.append(tup)
 	except AttributeError:
 		self.bld.utest_results = [tup]
 
+@Task.deep_inputs
 class utest(Task.Task):
 	"""
 	Execute a unit test
@@ -63,6 +160,7 @@ class utest(Task.Task):
 	color = 'PINK'
 	after = ['vnum', 'inst']
 	vars = []
+
 	def runnable_status(self):
 		"""
 		Always execute the task if `waf --alltests` was used or no
@@ -77,37 +175,17 @@ def runnable_status(self):
 				return Task.RUN_ME
 		return ret
 
-	def add_path(self, dct, path, var):
-		dct[var] = os.pathsep.join(Utils.to_list(path) + [os.environ.get(var, '')])
-
 	def get_test_env(self):
 		"""
 		In general, tests may require any library built anywhere in the project.
 		Override this method if fewer paths are needed
 		"""
-		try:
-			fu = getattr(self.generator.bld, 'all_test_paths')
-		except AttributeError:
-			# this operation may be performed by at most #maxjobs
-			fu = os.environ.copy()
-
-			lst = []
-			for g in self.generator.bld.groups:
-				for tg in g:
-					if getattr(tg, 'link_task', None):
-						s = tg.link_task.outputs[0].parent.abspath()
-						if s not in lst:
-							lst.append(s)
-
-			if Utils.is_win32:
-				self.add_path(fu, lst, 'PATH')
-			elif Utils.unversioned_sys_platform() == 'darwin':
-				self.add_path(fu, lst, 'DYLD_LIBRARY_PATH')
-				self.add_path(fu, lst, 'LD_LIBRARY_PATH')
-			else:
-				self.add_path(fu, lst, 'LD_LIBRARY_PATH')
-			self.generator.bld.all_test_paths = fu
-		return fu
+		return self.generator.ut_env
+
+	def post_run(self):
+		super(utest, self).post_run()
+		if getattr(Options.options, 'clear_failed_tests', False) and self.waf_unit_test_results[1]:
+			self.generator.bld.task_sigs[self.uid()] = None
 
 	def run(self):
 		"""
@@ -116,29 +194,44 @@ def run(self):
 
 		Override ``add_test_results`` to interrupt the build
 		"""
-
-		filename = self.inputs[0].abspath()
-		self.ut_exec = getattr(self.generator, 'ut_exec', [filename])
-		if getattr(self.generator, 'ut_fun', None):
-			self.generator.ut_fun(self)
-
-
-		cwd = getattr(self.generator, 'ut_cwd', '') or self.inputs[0].parent.abspath()
-
-		testcmd = getattr(self.generator, 'ut_cmd', False) or getattr(Options.options, 'testcmd', False)
-		if testcmd:
-			self.ut_exec = (testcmd % self.ut_exec[0]).split(' ')
-
-		proc = Utils.subprocess.Popen(self.ut_exec, cwd=cwd, env=self.get_test_env(), stderr=Utils.subprocess.PIPE, stdout=Utils.subprocess.PIPE)
+		if hasattr(self.generator, 'ut_run'):
+			return self.generator.ut_run(self)
+
+		self.ut_exec = getattr(self.generator, 'ut_exec', [self.inputs[0].abspath()])
+		ut_cmd = getattr(self.generator, 'ut_cmd', False)
+		if ut_cmd:
+			self.ut_exec = shlex.split(ut_cmd % ' '.join(self.ut_exec))
+
+		return self.exec_command(self.ut_exec)
+
+	def exec_command(self, cmd, **kw):
+		Logs.debug('runner: %r', cmd)
+		if getattr(Options.options, 'dump_test_scripts', False):
+			script_code = SCRIPT_TEMPLATE % {
+				'python': sys.executable,
+				'env': self.get_test_env(),
+				'cwd': self.get_cwd().abspath(),
+				'cmd': cmd
+			}
+			script_file = self.inputs[0].abspath() + '_run.py'
+			Utils.writef(script_file, script_code)
+			os.chmod(script_file, Utils.O755)
+			if Logs.verbose > 1:
+				Logs.info('Test debug file written as %r' % script_file)
+
+		proc = Utils.subprocess.Popen(cmd, cwd=self.get_cwd().abspath(), env=self.get_test_env(),
+			stderr=Utils.subprocess.PIPE, stdout=Utils.subprocess.PIPE, shell=isinstance(cmd,str))
 		(stdout, stderr) = proc.communicate()
-
-		tup = (filename, proc.returncode, stdout, stderr)
+		self.waf_unit_test_results = tup = (self.inputs[0].abspath(), proc.returncode, stdout, stderr)
 		testlock.acquire()
 		try:
 			return self.generator.add_test_results(tup)
 		finally:
 			testlock.release()
 
+	def get_cwd(self):
+		return getattr(self.generator, 'ut_cwd', self.inputs[0].parent)
+
 def summary(bld):
 	"""
 	Display an execution summary::
@@ -155,15 +248,15 @@ def build(bld):
 		total = len(lst)
 		tfail = len([x for x in lst if x[1]])
 
-		Logs.pprint('CYAN', '  tests that pass %d/%d' % (total-tfail, total))
+		Logs.pprint('GREEN', '  tests that pass %d/%d' % (total-tfail, total))
 		for (f, code, out, err) in lst:
 			if not code:
-				Logs.pprint('CYAN', '    %s' % f)
+				Logs.pprint('GREEN', '    %s' % f)
 
-		Logs.pprint('CYAN', '  tests that fail %d/%d' % (tfail, total))
+		Logs.pprint('GREEN' if tfail == 0 else 'RED', '  tests that fail %d/%d' % (tfail, total))
 		for (f, code, out, err) in lst:
 			if code:
-				Logs.pprint('CYAN', '    %s' % f)
+				Logs.pprint('RED', '    %s' % f)
 
 def set_exit_code(bld):
 	"""
@@ -194,8 +287,10 @@ def options(opt):
 	"""
 	opt.add_option('--notests', action='store_true', default=False, help='Exec no unit tests', dest='no_tests')
 	opt.add_option('--alltests', action='store_true', default=False, help='Exec all unit tests', dest='all_tests')
-	opt.add_option('--testcmd', action='store', default=False,
-	 help = 'Run the unit tests using the test-cmd string'
-	 ' example "--test-cmd="valgrind --error-exitcode=1'
-	 ' %s" to run under valgrind', dest='testcmd')
+	opt.add_option('--clear-failed', action='store_true', default=False,
+		help='Force failed unit tests to run again next time', dest='clear_failed_tests')
+	opt.add_option('--testcmd', action='store', default=False, dest='testcmd',
+		help='Run the unit tests using the test-cmd string example "--testcmd="valgrind --error-exitcode=1 %s" to run under valgrind')
+	opt.add_option('--dump-test-scripts', action='store_true', default=False,
+		help='Create python scripts to help debug tests', dest='dump_test_scripts')
 
diff --git a/waflib/Tools/winres.py b/waflib/Tools/winres.py
new file mode 100644
index 00000000..586c596c
--- /dev/null
+++ b/waflib/Tools/winres.py
@@ -0,0 +1,78 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Brant Young, 2007
+
+"Process *.rc* files for C/C++: X{.rc -> [.res|.rc.o]}"
+
+import re
+from waflib import Task
+from waflib.TaskGen import extension
+from waflib.Tools import c_preproc
+
+@extension('.rc')
+def rc_file(self, node):
+	"""
+	Binds the .rc extension to a winrc task
+	"""
+	obj_ext = '.rc.o'
+	if self.env.WINRC_TGT_F == '/fo':
+		obj_ext = '.res'
+	rctask = self.create_task('winrc', node, node.change_ext(obj_ext))
+	try:
+		self.compiled_tasks.append(rctask)
+	except AttributeError:
+		self.compiled_tasks = [rctask]
+
+re_lines = re.compile(
+	'(?:^[ \t]*(#|%:)[ \t]*(ifdef|ifndef|if|else|elif|endif|include|import|define|undef|pragma)[ \t]*(.*?)\s*$)|'\
+	'(?:^\w+[ \t]*(ICON|BITMAP|CURSOR|HTML|FONT|MESSAGETABLE|TYPELIB|REGISTRY|D3DFX)[ \t]*(.*?)\s*$)',
+	re.IGNORECASE | re.MULTILINE)
+
+class rc_parser(c_preproc.c_parser):
+	"""
+	Calculates dependencies in .rc files
+	"""
+	def filter_comments(self, node):
+		"""
+		Overrides :py:meth:`waflib.Tools.c_preproc.c_parser.filter_comments`
+		"""
+		code = node.read()
+		if c_preproc.use_trigraphs:
+			for (a, b) in c_preproc.trig_def:
+				code = code.split(a).join(b)
+		code = c_preproc.re_nl.sub('', code)
+		code = c_preproc.re_cpp.sub(c_preproc.repl, code)
+		ret = []
+		for m in re.finditer(re_lines, code):
+			if m.group(2):
+				ret.append((m.group(2), m.group(3)))
+			else:
+				ret.append(('include', m.group(5)))
+		return ret
+
+class winrc(Task.Task):
+	"""
+	Compiles resource files
+	"""
+	run_str = '${WINRC} ${WINRCFLAGS} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} ${WINRC_TGT_F} ${TGT} ${WINRC_SRC_F} ${SRC}'
+	color   = 'BLUE'
+	def scan(self):
+		tmp = rc_parser(self.generator.includes_nodes)
+		tmp.start(self.inputs[0], self.env)
+		return (tmp.nodes, tmp.names)
+
+def configure(conf):
+	"""
+	Detects the programs RC or windres, depending on the C/C++ compiler in use
+	"""
+	v = conf.env
+	if not v.WINRC:
+		if v.CC_NAME == 'msvc':
+			conf.find_program('RC', var='WINRC', path_list=v.PATH)
+			v.WINRC_TGT_F = '/fo'
+			v.WINRC_SRC_F = ''
+		else:
+			conf.find_program('windres', var='WINRC', path_list=v.PATH)
+			v.WINRC_TGT_F = '-o'
+			v.WINRC_SRC_F = '-i'
+
diff --git a/waflib/Tools/xlc.py b/waflib/Tools/xlc.py
index 3bd8d026..134dd415 100644
--- a/waflib/Tools/xlc.py
+++ b/waflib/Tools/xlc.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 # Ralf Habacker, 2006 (rh)
 # Yinon Ehrlich, 2009
 # Michael Kuhn, 2009
@@ -11,7 +11,7 @@
 @conf
 def find_xlc(conf):
 	"""
-	Detect the Aix C compiler
+	Detects the Aix C compiler
 	"""
 	cc = conf.find_program(['xlc_r', 'xlc'], var='CC')
 	conf.get_xlc_version(cc)
@@ -24,38 +24,36 @@ def xlc_common_flags(conf):
 	"""
 	v = conf.env
 
-	v['CC_SRC_F']            = []
-	v['CC_TGT_F']            = ['-c', '-o']
+	v.CC_SRC_F            = []
+	v.CC_TGT_F            = ['-c', '-o']
 
-	# linker
-	if not v['LINK_CC']: v['LINK_CC'] = v['CC']
-	v['CCLNK_SRC_F']         = []
-	v['CCLNK_TGT_F']         = ['-o']
-	v['CPPPATH_ST']          = '-I%s'
-	v['DEFINES_ST']          = '-D%s'
+	if not v.LINK_CC:
+		v.LINK_CC = v.CC
 
-	v['LIB_ST']              = '-l%s' # template for adding libs
-	v['LIBPATH_ST']          = '-L%s' # template for adding libpaths
-	v['STLIB_ST']            = '-l%s'
-	v['STLIBPATH_ST']        = '-L%s'
-	v['RPATH_ST']            = '-Wl,-rpath,%s'
+	v.CCLNK_SRC_F         = []
+	v.CCLNK_TGT_F         = ['-o']
+	v.CPPPATH_ST          = '-I%s'
+	v.DEFINES_ST          = '-D%s'
 
-	v['SONAME_ST']           = []
-	v['SHLIB_MARKER']        = []
-	v['STLIB_MARKER']        = []
+	v.LIB_ST              = '-l%s' # template for adding libs
+	v.LIBPATH_ST          = '-L%s' # template for adding libpaths
+	v.STLIB_ST            = '-l%s'
+	v.STLIBPATH_ST        = '-L%s'
+	v.RPATH_ST            = '-Wl,-rpath,%s'
 
-	# program
-	v['LINKFLAGS_cprogram']  = ['-Wl,-brtl']
-	v['cprogram_PATTERN']    = '%s'
+	v.SONAME_ST           = []
+	v.SHLIB_MARKER        = []
+	v.STLIB_MARKER        = []
 
-	# shared library
-	v['CFLAGS_cshlib']       = ['-fPIC']
-	v['LINKFLAGS_cshlib']    = ['-G', '-Wl,-brtl,-bexpfull']
-	v['cshlib_PATTERN']      = 'lib%s.so'
+	v.LINKFLAGS_cprogram  = ['-Wl,-brtl']
+	v.cprogram_PATTERN    = '%s'
 
-	# static lib
-	v['LINKFLAGS_cstlib']    = []
-	v['cstlib_PATTERN']      = 'lib%s.a'
+	v.CFLAGS_cshlib       = ['-fPIC']
+	v.LINKFLAGS_cshlib    = ['-G', '-Wl,-brtl,-bexpfull']
+	v.cshlib_PATTERN      = 'lib%s.so'
+
+	v.LINKFLAGS_cstlib    = []
+	v.cstlib_PATTERN      = 'lib%s.a'
 
 def configure(conf):
 	conf.find_xlc()
diff --git a/waflib/Tools/xlcxx.py b/waflib/Tools/xlcxx.py
index 150aeaa4..76aa59bc 100644
--- a/waflib/Tools/xlcxx.py
+++ b/waflib/Tools/xlcxx.py
@@ -1,6 +1,6 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2006-2010 (ita)
+# Thomas Nagy, 2006-2018 (ita)
 # Ralf Habacker, 2006 (rh)
 # Yinon Ehrlich, 2009
 # Michael Kuhn, 2009
@@ -11,7 +11,7 @@
 @conf
 def find_xlcxx(conf):
 	"""
-	Detect the Aix C++ compiler
+	Detects the Aix C++ compiler
 	"""
 	cxx = conf.find_program(['xlc++_r', 'xlc++'], var='CXX')
 	conf.get_xlc_version(cxx)
@@ -24,38 +24,36 @@ def xlcxx_common_flags(conf):
 	"""
 	v = conf.env
 
-	v['CXX_SRC_F']           = []
-	v['CXX_TGT_F']           = ['-c', '-o']
+	v.CXX_SRC_F           = []
+	v.CXX_TGT_F           = ['-c', '-o']
 
-	# linker
-	if not v['LINK_CXX']: v['LINK_CXX'] = v['CXX']
-	v['CXXLNK_SRC_F']        = []
-	v['CXXLNK_TGT_F']        = ['-o']
-	v['CPPPATH_ST']          = '-I%s'
-	v['DEFINES_ST']          = '-D%s'
+	if not v.LINK_CXX:
+		v.LINK_CXX = v.CXX
 
-	v['LIB_ST']              = '-l%s' # template for adding libs
-	v['LIBPATH_ST']          = '-L%s' # template for adding libpaths
-	v['STLIB_ST']            = '-l%s'
-	v['STLIBPATH_ST']        = '-L%s'
-	v['RPATH_ST']            = '-Wl,-rpath,%s'
+	v.CXXLNK_SRC_F        = []
+	v.CXXLNK_TGT_F        = ['-o']
+	v.CPPPATH_ST          = '-I%s'
+	v.DEFINES_ST          = '-D%s'
 
-	v['SONAME_ST']           = []
-	v['SHLIB_MARKER']        = []
-	v['STLIB_MARKER']        = []
+	v.LIB_ST              = '-l%s' # template for adding libs
+	v.LIBPATH_ST          = '-L%s' # template for adding libpaths
+	v.STLIB_ST            = '-l%s'
+	v.STLIBPATH_ST        = '-L%s'
+	v.RPATH_ST            = '-Wl,-rpath,%s'
 
-	# program
-	v['LINKFLAGS_cxxprogram']= ['-Wl,-brtl']
-	v['cxxprogram_PATTERN']  = '%s'
+	v.SONAME_ST           = []
+	v.SHLIB_MARKER        = []
+	v.STLIB_MARKER        = []
 
-	# shared library
-	v['CXXFLAGS_cxxshlib']   = ['-fPIC']
-	v['LINKFLAGS_cxxshlib']  = ['-G', '-Wl,-brtl,-bexpfull']
-	v['cxxshlib_PATTERN']    = 'lib%s.so'
+	v.LINKFLAGS_cxxprogram= ['-Wl,-brtl']
+	v.cxxprogram_PATTERN  = '%s'
 
-	# static lib
-	v['LINKFLAGS_cxxstlib']  = []
-	v['cxxstlib_PATTERN']    = 'lib%s.a'
+	v.CXXFLAGS_cxxshlib   = ['-fPIC']
+	v.LINKFLAGS_cxxshlib  = ['-G', '-Wl,-brtl,-bexpfull']
+	v.cxxshlib_PATTERN    = 'lib%s.so'
+
+	v.LINKFLAGS_cxxstlib  = []
+	v.cxxstlib_PATTERN    = 'lib%s.a'
 
 def configure(conf):
 	conf.find_xlcxx()
diff --git a/waflib/Utils.py b/waflib/Utils.py
index 3d981ac7..b4665c4d 100644
--- a/waflib/Utils.py
+++ b/waflib/Utils.py
@@ -1,16 +1,37 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
 
 """
 Utilities and platform-specific fixes
 
 The portability fixes try to provide a consistent behavior of the Waf API
-through Python versions 2.3 to 3.X and across different platforms (win32, linux, etc)
+through Python versions 2.5 to 3.X and across different platforms (win32, linux, etc)
 """
 
-import os, sys, errno, traceback, inspect, re, shutil, datetime, gc, platform
-import subprocess # <- leave this!
+from __future__ import with_statement
+
+import atexit, os, sys, errno, inspect, re, datetime, platform, base64, signal, functools, time
+
+try:
+	import cPickle
+except ImportError:
+	import pickle as cPickle
+
+# leave this
+if os.name == 'posix' and sys.version_info[0] < 3:
+	try:
+		import subprocess32 as subprocess
+	except ImportError:
+		import subprocess
+else:
+	import subprocess
+
+try:
+	TimeoutExpired = subprocess.TimeoutExpired
+except AttributeError:
+	class TimeoutExpired(Exception):
+		pass
 
 from collections import deque, defaultdict
 
@@ -24,11 +45,6 @@
 
 from waflib import Errors
 
-try:
-	from collections import UserDict
-except ImportError:
-	from UserDict import UserDict
-
 try:
 	from hashlib import md5
 except ImportError:
@@ -47,8 +63,8 @@
 
 	class threading(object):
 		"""
-			A fake threading class for platforms lacking the threading module.
-			Use ``waf -j1`` on those platforms
+		A fake threading class for platforms lacking the threading module.
+		Use ``waf -j1`` on those platforms
 		"""
 		pass
 	class Lock(object):
@@ -58,19 +74,9 @@ def acquire(self):
 		def release(self):
 			pass
 	threading.Lock = threading.Thread = Lock
-else:
-	run_old = threading.Thread.run
-	def run(*args, **kwargs):
-		try:
-			run_old(*args, **kwargs)
-		except (KeyboardInterrupt, SystemExit):
-			raise
-		except Exception:
-			sys.excepthook(*sys.exc_info())
-	threading.Thread.run = run
 
-SIG_NIL = 'iluvcuteoverload'.encode()
-"""Arbitrary null value for a md5 hash. This value must be changed when the hash value is replaced (size)"""
+SIG_NIL = 'SIG_NIL_SIG_NIL_'.encode()
+"""Arbitrary null value for hashes. Modify this value according to the hash function in use"""
 
 O644 = 420
 """Constant representing the permissions for regular files (0644 raises a syntax error on python 3)"""
@@ -84,42 +90,126 @@ def run(*args, **kwargs):
 rot_idx = 0
 "Index of the current throbber character (progress bar)"
 
-try:
-	from collections import OrderedDict as ordered_iter_dict
-except ImportError:
-	class ordered_iter_dict(dict):
-		def __init__(self, *k, **kw):
-			self.lst = []
-			dict.__init__(self, *k, **kw)
-		def clear(self):
-			dict.clear(self)
-			self.lst = []
-		def __setitem__(self, key, value):
-			dict.__setitem__(self, key, value)
-			try:
-				self.lst.remove(key)
-			except ValueError:
-				pass
-			self.lst.append(key)
-		def __delitem__(self, key):
-			dict.__delitem__(self, key)
-			try:
-				self.lst.remove(key)
-			except ValueError:
-				pass
-		def __iter__(self):
-			for x in self.lst:
-				yield x
-		def keys(self):
-			return self.lst
+class ordered_iter_dict(dict):
+	"""Ordered dictionary that provides iteration from the most recently inserted keys first"""
+	def __init__(self, *k, **kw):
+		self.lst = deque()
+		dict.__init__(self, *k, **kw)
+	def clear(self):
+		dict.clear(self)
+		self.lst = deque()
+	def __setitem__(self, key, value):
+		if key in dict.keys(self):
+			self.lst.remove(key)
+		dict.__setitem__(self, key, value)
+		self.lst.append(key)
+	def __delitem__(self, key):
+		dict.__delitem__(self, key)
+		try:
+			self.lst.remove(key)
+		except ValueError:
+			pass
+	def __iter__(self):
+		return reversed(self.lst)
+	def keys(self):
+		return reversed(self.lst)
 
-is_win32 = os.sep == '\\' or sys.platform == 'win32' # msys2
+class lru_node(object):
+	"""
+	Used by :py:class:`waflib.Utils.lru_cache`
+	"""
+	__slots__ = ('next', 'prev', 'key', 'val')
+	def __init__(self):
+		self.next = self
+		self.prev = self
+		self.key = None
+		self.val = None
 
-def readf(fname, m='r', encoding='ISO8859-1'):
+class lru_cache(object):
+	"""
+	A simple least-recently used cache with lazy allocation
 	"""
-	Read an entire file into a string, use this function instead of os.open() whenever possible.
+	__slots__ = ('maxlen', 'table', 'head')
+	def __init__(self, maxlen=100):
+		self.maxlen = maxlen
+		"""
+		Maximum amount of elements in the cache
+		"""
+		self.table = {}
+		"""
+		Mapping key-value
+		"""
+		self.head = lru_node()
+		self.head.next = self.head
+		self.head.prev = self.head
+
+	def __getitem__(self, key):
+		node = self.table[key]
+		# assert(key==node.key)
+		if node is self.head:
+			return node.val
+
+		# detach the node found
+		node.prev.next = node.next
+		node.next.prev = node.prev
+
+		# replace the head
+		node.next = self.head.next
+		node.prev = self.head
+		self.head = node.next.prev = node.prev.next = node
+
+		return node.val
+
+	def __setitem__(self, key, val):
+		if key in self.table:
+			# update the value for an existing key
+			node = self.table[key]
+			node.val = val
+			self.__getitem__(key)
+		else:
+			if len(self.table) < self.maxlen:
+				# the very first item is unused until the maximum is reached
+				node = lru_node()
+				node.prev = self.head
+				node.next = self.head.next
+				node.prev.next = node.next.prev = node
+			else:
+				node = self.head = self.head.next
+				try:
+					# that's another key
+					del self.table[node.key]
+				except KeyError:
+					pass
+
+			node.key = key
+			node.val = val
+			self.table[key] = node
+
+class lazy_generator(object):
+	def __init__(self, fun, params):
+		self.fun = fun
+		self.params = params
+
+	def __iter__(self):
+		return self
+
+	def __next__(self):
+		try:
+			it = self.it
+		except AttributeError:
+			it = self.it = self.fun(*self.params)
+		return next(it)
+
+	next = __next__
+
+is_win32 = os.sep == '\\' or sys.platform == 'win32' # msys2
+"""
+Whether this system is a Windows series
+"""
 
-	In practice the wrapper node.read(..) should be preferred to this function::
+def readf(fname, m='r', encoding='latin-1'):
+	"""
+	Reads an entire file into a string. See also :py:meth:`waflib.Node.Node.readf`::
 
 		def build(ctx):
 			from waflib import Utils
@@ -138,28 +228,21 @@ def build(ctx):
 
 	if sys.hexversion > 0x3000000 and not 'b' in m:
 		m += 'b'
-		f = open(fname, m)
-		try:
+		with open(fname, m) as f:
 			txt = f.read()
-		finally:
-			f.close()
 		if encoding:
 			txt = txt.decode(encoding)
 		else:
 			txt = txt.decode()
 	else:
-		f = open(fname, m)
-		try:
+		with open(fname, m) as f:
 			txt = f.read()
-		finally:
-			f.close()
 	return txt
 
-def writef(fname, data, m='w', encoding='ISO8859-1'):
+def writef(fname, data, m='w', encoding='latin-1'):
 	"""
-	Write an entire file from a string, use this function instead of os.open() whenever possible.
-
-	In practice the wrapper node.write(..) should be preferred to this function::
+	Writes an entire file from a string.
+	See also :py:meth:`waflib.Node.Node.writef`::
 
 		def build(ctx):
 			from waflib import Utils
@@ -178,44 +261,27 @@ def build(ctx):
 	if sys.hexversion > 0x3000000 and not 'b' in m:
 		data = data.encode(encoding)
 		m += 'b'
-	f = open(fname, m)
-	try:
+	with open(fname, m) as f:
 		f.write(data)
-	finally:
-		f.close()
 
 def h_file(fname):
 	"""
-	Compute a hash value for a file by using md5. This method may be replaced by
-	a faster version if necessary. The following uses the file size and the timestamp value::
-
-		import stat
-		from waflib import Utils
-		def h_file(fname):
-			st = os.stat(fname)
-			if stat.S_ISDIR(st[stat.ST_MODE]): raise IOError('not a file')
-			m = Utils.md5()
-			m.update(str(st.st_mtime))
-			m.update(str(st.st_size))
-			m.update(fname)
-			return m.digest()
-		Utils.h_file = h_file
+	Computes a hash value for a file by using md5. Use the md5_tstamp
+	extension to get faster build hashes if necessary.
 
 	:type fname: string
 	:param fname: path to the file to hash
 	:return: hash of the file contents
+	:rtype: string or bytes
 	"""
-	f = open(fname, 'rb')
 	m = md5()
-	try:
+	with open(fname, 'rb') as f:
 		while fname:
 			fname = f.read(200000)
 			m.update(fname)
-	finally:
-		f.close()
 	return m.digest()
 
-def readf_win32(f, m='r', encoding='ISO8859-1'):
+def readf_win32(f, m='r', encoding='latin-1'):
 	flags = os.O_NOINHERIT | os.O_RDONLY
 	if 'b' in m:
 		flags |= os.O_BINARY
@@ -228,24 +294,18 @@ def readf_win32(f, m='r', encoding='ISO8859-1'):
 
 	if sys.hexversion > 0x3000000 and not 'b' in m:
 		m += 'b'
-		f = os.fdopen(fd, m)
-		try:
+		with os.fdopen(fd, m) as f:
 			txt = f.read()
-		finally:
-			f.close()
 		if encoding:
 			txt = txt.decode(encoding)
 		else:
 			txt = txt.decode()
 	else:
-		f = os.fdopen(fd, m)
-		try:
+		with os.fdopen(fd, m) as f:
 			txt = f.read()
-		finally:
-			f.close()
 	return txt
 
-def writef_win32(f, data, m='w', encoding='ISO8859-1'):
+def writef_win32(f, data, m='w', encoding='latin-1'):
 	if sys.hexversion > 0x3000000 and not 'b' in m:
 		data = data.encode(encoding)
 		m += 'b'
@@ -257,26 +317,20 @@ def writef_win32(f, data, m='w', encoding='ISO8859-1'):
 	try:
 		fd = os.open(f, flags)
 	except OSError:
-		raise IOError('Cannot write to %r' % f)
-	f = os.fdopen(fd, m)
-	try:
+		raise OSError('Cannot write to %r' % f)
+	with os.fdopen(fd, m) as f:
 		f.write(data)
-	finally:
-		f.close()
 
 def h_file_win32(fname):
 	try:
 		fd = os.open(fname, os.O_BINARY | os.O_RDONLY | os.O_NOINHERIT)
 	except OSError:
-		raise IOError('Cannot read from %r' % fname)
-	f = os.fdopen(fd, 'rb')
+		raise OSError('Cannot read from %r' % fname)
 	m = md5()
-	try:
+	with os.fdopen(fd, 'rb') as f:
 		while fname:
 			fname = f.read(200000)
 			m.update(fname)
-	finally:
-		f.close()
 	return m.digest()
 
 # always save these
@@ -311,8 +365,8 @@ def to_hex(s):
 
 def listdir_win32(s):
 	"""
-	List the contents of a folder in a portable manner.
-	On Win32, return the list of drive letters: ['C:', 'X:', 'Z:']
+	Lists the contents of a folder in a portable manner.
+	On Win32, returns the list of drive letters: ['C:', 'X:', 'Z:'] when an empty string is given.
 
 	:type s: string
 	:param s: a string, which can be empty on Windows
@@ -322,7 +376,7 @@ def listdir_win32(s):
 			import ctypes
 		except ImportError:
 			# there is nothing much we can do
-			return [x + ':\\' for x in list('ABCDEFGHIJKLMNOPQRSTUVWXYZ')]
+			return [x + ':\\' for x in 'ABCDEFGHIJKLMNOPQRSTUVWXYZ']
 		else:
 			dlen = 4 # length of "?:\\x00"
 			maxdrives = 26
@@ -345,7 +399,7 @@ def listdir_win32(s):
 
 def num2ver(ver):
 	"""
-	Convert a string, tuple or version number into an integer. The number is supposed to have at most 4 digits::
+	Converts a string, tuple or version number into an integer. The number is supposed to have at most 4 digits::
 
 		from waflib.Utils import num2ver
 		num2ver('1.3.2') == num2ver((1,3,2)) == num2ver((1,3,2,0))
@@ -363,33 +417,37 @@ def num2ver(ver):
 		return ret
 	return ver
 
-def ex_stack():
-	"""
-	Extract the stack to display exceptions
-
-	:return: a string represening the last exception
-	"""
-	exc_type, exc_value, tb = sys.exc_info()
-	exc_lines = traceback.format_exception(exc_type, exc_value, tb)
-	return ''.join(exc_lines)
-
-def to_list(sth):
+def to_list(val):
 	"""
-	Convert a string argument to a list by splitting on spaces, and pass
-	through a list argument unchanged::
+	Converts a string argument to a list by splitting it by spaces.
+	Returns the object if not a string::
 
 		from waflib.Utils import to_list
-		lst = to_list("a b c d")
+		lst = to_list('a b c d')
 
-	:param sth: List or a string of items separated by spaces
+	:param val: list of string or space-separated string
 	:rtype: list
 	:return: Argument converted to list
-
 	"""
-	if isinstance(sth, str):
-		return sth.split()
+	if isinstance(val, str):
+		return val.split()
 	else:
-		return sth
+		return val
+
+def console_encoding():
+	try:
+		import ctypes
+	except ImportError:
+		pass
+	else:
+		try:
+			codepage = ctypes.windll.kernel32.GetConsoleCP()
+		except AttributeError:
+			pass
+		else:
+			if codepage:
+				return 'cp%d' % codepage
+	return sys.stdout.encoding or ('cp1252' if is_win32 else 'latin-1')
 
 def split_path_unix(path):
 	return path.split('/')
@@ -401,22 +459,24 @@ def split_path_cygwin(path):
 		return ret
 	return path.split('/')
 
-re_sp = re.compile('[/\\\\]')
+re_sp = re.compile('[/\\\\]+')
 def split_path_win32(path):
 	if path.startswith('\\\\'):
-		ret = re.split(re_sp, path)[2:]
-		ret[0] = '\\' + ret[0]
+		ret = re_sp.split(path)[1:]
+		ret[0] = '\\\\' + ret[0]
+		if ret[0] == '\\\\?':
+			return ret[1:]
 		return ret
-	return re.split(re_sp, path)
+	return re_sp.split(path)
 
 msysroot = None
 def split_path_msys(path):
-	if (path.startswith('/') or path.startswith('\\')) and not path.startswith('//') and not path.startswith('\\\\'):
+	if path.startswith(('/', '\\')) and not path.startswith(('//', '\\\\')):
 		# msys paths can be in the form /usr/bin
 		global msysroot
 		if not msysroot:
 			# msys has python 2.7 or 3, so we can use this
-			msysroot = subprocess.check_output(['cygpath', '-w', '/']).decode(sys.stdout.encoding or 'iso8859-1')
+			msysroot = subprocess.check_output(['cygpath', '-w', '/']).decode(sys.stdout.encoding or 'latin-1')
 			msysroot = msysroot.strip()
 		path = os.path.normpath(msysroot + os.sep + path)
 	return split_path_win32(path)
@@ -424,7 +484,7 @@ def split_path_msys(path):
 if sys.platform == 'cygwin':
 	split_path = split_path_cygwin
 elif is_win32:
-	if os.environ.get('MSYSTEM', None):
+	if os.environ.get('MSYSTEM'):
 		split_path = split_path_msys
 	else:
 		split_path = split_path_win32
@@ -432,19 +492,20 @@ def split_path_msys(path):
 	split_path = split_path_unix
 
 split_path.__doc__ = """
-Split a path by / or \\. This function is not like os.path.split
+Splits a path by / or \\; do not confuse this function with with ``os.path.split``
 
 :type  path: string
 :param path: path to split
-:return:     list of strings
+:return:     list of string
 """
 
 def check_dir(path):
 	"""
-	Ensure that a directory exists (similar to ``mkdir -p``).
+	Ensures that a directory exists (similar to ``mkdir -p``).
 
 	:type  path: string
 	:param path: Path to directory
+	:raises: :py:class:`waflib.Errors.WafError` if the folder cannot be added.
 	"""
 	if not os.path.isdir(path):
 		try:
@@ -455,11 +516,14 @@ def check_dir(path):
 
 def check_exe(name, env=None):
 	"""
-	Ensure that a program exists
+	Ensures that a program exists
 
 	:type name: string
-	:param name: name or path to program
+	:param name: path to the program
+	:param env: configuration object
+	:type env: :py:class:`waflib.ConfigSet.ConfigSet`
 	:return: path of the program or None
+	:raises: :py:class:`waflib.Errors.WafError` if the folder cannot be added.
 	"""
 	if not name:
 		raise ValueError('Cannot execute an empty string!')
@@ -471,7 +535,7 @@ def is_exe(fpath):
 		return os.path.abspath(name)
 	else:
 		env = env or os.environ
-		for path in env["PATH"].split(os.pathsep):
+		for path in env['PATH'].split(os.pathsep):
 			path = path.strip('"')
 			exe_file = os.path.join(path, name)
 			if is_exe(exe_file):
@@ -480,7 +544,7 @@ def is_exe(fpath):
 
 def def_attrs(cls, **kw):
 	"""
-	Set default attributes on a class instance
+	Sets default attributes on a class instance
 
 	:type cls: class
 	:param cls: the class to update the given attributes in.
@@ -493,7 +557,7 @@ def def_attrs(cls, **kw):
 
 def quote_define_name(s):
 	"""
-	Convert a string to an identifier suitable for C defines.
+	Converts a string into an identifier suitable for C defines.
 
 	:type  s: string
 	:param s: String to convert
@@ -505,18 +569,32 @@ def quote_define_name(s):
 	fu = fu.upper()
 	return fu
 
+re_sh = re.compile('\\s|\'|"')
+"""
+Regexp used for shell_escape below
+"""
+
+def shell_escape(cmd):
+	"""
+	Escapes a command:
+	['ls', '-l', 'arg space'] -> ls -l 'arg space'
+	"""
+	if isinstance(cmd, str):
+		return cmd
+	return ' '.join(repr(x) if re_sh.search(x) else x for x in cmd)
+
 def h_list(lst):
 	"""
-	Hash lists. For tuples, using hash(tup) is much more efficient,
-	except on python >= 3.3 where hash randomization assumes everybody is running a web application.
+	Hashes lists of ordered data.
+
+	Using hash(tup) for tuples would be much more efficient,
+	but Python now enforces hash randomization
 
 	:param lst: list to hash
 	:type lst: list of strings
 	:return: hash of the list
 	"""
-	m = md5()
-	m.update(str(lst).encode())
-	return m.digest()
+	return md5(repr(lst).encode()).digest()
 
 def h_fun(fun):
 	"""
@@ -525,14 +603,27 @@ def h_fun(fun):
 	:param fun: function to hash
 	:type  fun: function
 	:return: hash of the function
+	:rtype: string or bytes
 	"""
 	try:
 		return fun.code
 	except AttributeError:
+		if isinstance(fun, functools.partial):
+			code = list(fun.args)
+			# The method items() provides a sequence of tuples where the first element
+			# represents an optional argument of the partial function application
+			#
+			# The sorting result outcome will be consistent because:
+			# 1. tuples are compared in order of their elements
+			# 2. optional argument namess are unique
+			code.extend(sorted(fun.keywords.items()))
+			code.append(h_fun(fun.func))
+			fun.code = h_list(code)
+			return fun.code
 		try:
 			h = inspect.getsource(fun)
-		except IOError:
-			h = "nocode"
+		except EnvironmentError:
+			h = 'nocode'
 		try:
 			fun.code = h
 		except AttributeError:
@@ -541,8 +632,11 @@ def h_fun(fun):
 
 def h_cmd(ins):
 	"""
-	Task command hashes are calculated by calling this function. The inputs can be
-	strings, functions, tuples/lists containing strings/functions
+	Hashes objects recursively
+
+	:param ins: input object
+	:type ins: string or list or tuple or function
+	:rtype: string or bytes
 	"""
 	# this function is not meant to be particularly fast
 	if isinstance(ins, str):
@@ -555,13 +649,13 @@ def h_cmd(ins):
 		# or just a python function
 		ret = str(h_fun(ins))
 	if sys.hexversion > 0x3000000:
-		ret = ret.encode('iso8859-1', 'xmlcharrefreplace')
+		ret = ret.encode('latin-1', 'xmlcharrefreplace')
 	return ret
 
 reg_subst = re.compile(r"(\\\\)|(\$\$)|\$\{([^}]+)\}")
 def subst_vars(expr, params):
 	"""
-	Replace ${VAR} with the value of VAR taken from a dict or a config set::
+	Replaces ${VAR} with the value of VAR taken from a dict or a config set::
 
 		from waflib import Utils
 		s = Utils.subst_vars('${PREFIX}/bin', env)
@@ -586,7 +680,8 @@ def repl_var(m):
 
 def destos_to_binfmt(key):
 	"""
-	Return the binary format based on the unversioned platform name.
+	Returns the binary format based on the unversioned platform name,
+	and defaults to ``elf`` if nothing is found.
 
 	:param key: platform name
 	:type  key: string
@@ -600,7 +695,7 @@ def destos_to_binfmt(key):
 
 def unversioned_sys_platform():
 	"""
-	Return the unversioned platform name.
+	Returns the unversioned platform name.
 	Some Python platform names contain versions, that depend on
 	the build environment, e.g. linux2, freebsd6, etc.
 	This returns the name without the version number. Exceptions are
@@ -639,7 +734,7 @@ def unversioned_sys_platform():
 
 def nada(*k, **kw):
 	"""
-	A function that does nothing
+	Does nothing
 
 	:return: None
 	"""
@@ -648,7 +743,7 @@ def nada(*k, **kw):
 class Timer(object):
 	"""
 	Simple object for timing the execution of commands.
-	Its string representation is the current time::
+	Its string representation is the duration::
 
 		from waflib.Utils import Timer
 		timer = Timer()
@@ -656,10 +751,12 @@ class Timer(object):
 		s = str(timer)
 	"""
 	def __init__(self):
-		self.start_time = datetime.datetime.utcnow()
+		self.start_time = self.now()
 
 	def __str__(self):
-		delta = datetime.datetime.utcnow() - self.start_time
+		delta = self.now() - self.start_time
+		if not isinstance(delta, datetime.timedelta):
+			delta = datetime.timedelta(seconds=delta)
 		days = delta.days
 		hours, rem = divmod(delta.seconds, 3600)
 		minutes, seconds = divmod(rem, 60)
@@ -673,28 +770,16 @@ def __str__(self):
 			result += '%dm' % minutes
 		return '%s%.3fs' % (result, seconds)
 
-if is_win32:
-	old = shutil.copy2
-	def copy2(src, dst):
-		"""
-		shutil.copy2 does not copy the file attributes on windows, so we
-		hack into the shutil module to fix the problem
-		"""
-		old(src, dst)
-		shutil.copystat(src, dst)
-	setattr(shutil, 'copy2', copy2)
+	def now(self):
+		return datetime.datetime.utcnow()
 
-if os.name == 'java':
-	# Jython cannot disable the gc but they can enable it ... wtf?
-	try:
-		gc.disable()
-		gc.enable()
-	except NotImplementedError:
-		gc.disable = gc.enable
+	if hasattr(time, 'perf_counter'):
+		def now(self):
+			return time.perf_counter()
 
 def read_la_file(path):
 	"""
-	Read property files, used by msvc.py
+	Reads property files, used by msvc.py
 
 	:param path: file to read
 	:type path: string
@@ -709,25 +794,6 @@ def read_la_file(path):
 			pass
 	return dc
 
-def nogc(fun):
-	"""
-	Decorator: let a function disable the garbage collector during its execution.
-	It is used in the build context when storing/loading the build cache file (pickle)
-
-	:param fun: function to execute
-	:type fun: function
-	:return: the return value of the function executed
-	"""
-	def f(*k, **kw):
-		try:
-			gc.disable()
-			ret = fun(*k, **kw)
-		finally:
-			gc.enable()
-		return ret
-	f.__doc__ = fun.__doc__
-	return f
-
 def run_once(fun):
 	"""
 	Decorator: let a function cache its results, use like this::
@@ -736,16 +802,18 @@ def run_once(fun):
 		def foo(k):
 			return 345*2343
 
+	.. note:: in practice this can cause memory leaks, prefer a :py:class:`waflib.Utils.lru_cache`
+
 	:param fun: function to execute
 	:type fun: function
 	:return: the return value of the function executed
 	"""
 	cache = {}
-	def wrap(k):
+	def wrap(*k):
 		try:
 			return cache[k]
 		except KeyError:
-			ret = fun(k)
+			ret = fun(*k)
 			cache[k] = ret
 			return ret
 	wrap.__cache__ = cache
@@ -753,17 +821,29 @@ def wrap(k):
 	return wrap
 
 def get_registry_app_path(key, filename):
+	"""
+	Returns the value of a registry key for an executable
+
+	:type key: string
+	:type filename: list of string
+	"""
 	if not winreg:
 		return None
 	try:
 		result = winreg.QueryValue(key, "Software\\Microsoft\\Windows\\CurrentVersion\\App Paths\\%s.exe" % filename[0])
-	except WindowsError:
+	except OSError:
 		pass
 	else:
 		if os.path.isfile(result):
 			return result
 
 def lib64():
+	"""
+	Guess the default ``/usr/lib`` extension for 64-bit applications
+
+	:return: '64' or ''
+	:rtype: string
+	"""
 	# default settings for /usr/lib
 	if os.sep == '/':
 		if platform.architecture()[0] == '64bit':
@@ -775,3 +855,167 @@ def sane_path(p):
 	# private function for the time being!
 	return os.path.abspath(os.path.expanduser(p))
 
+process_pool = []
+"""
+List of processes started to execute sub-process commands
+"""
+
+def get_process():
+	"""
+	Returns a process object that can execute commands as sub-processes
+
+	:rtype: subprocess.Popen
+	"""
+	try:
+		return process_pool.pop()
+	except IndexError:
+		filepath = os.path.dirname(os.path.abspath(__file__)) + os.sep + 'processor.py'
+		cmd = [sys.executable, '-c', readf(filepath)]
+		return subprocess.Popen(cmd, stdout=subprocess.PIPE, stdin=subprocess.PIPE, bufsize=0)
+
+def run_prefork_process(cmd, kwargs, cargs):
+	"""
+	Delegates process execution to a pre-forked process instance.
+	"""
+	if not 'env' in kwargs:
+		kwargs['env'] = dict(os.environ)
+	try:
+		obj = base64.b64encode(cPickle.dumps([cmd, kwargs, cargs]))
+	except (TypeError, AttributeError):
+		return run_regular_process(cmd, kwargs, cargs)
+
+	proc = get_process()
+	if not proc:
+		return run_regular_process(cmd, kwargs, cargs)
+
+	proc.stdin.write(obj)
+	proc.stdin.write('\n'.encode())
+	proc.stdin.flush()
+	obj = proc.stdout.readline()
+	if not obj:
+		raise OSError('Preforked sub-process %r died' % proc.pid)
+
+	process_pool.append(proc)
+	lst = cPickle.loads(base64.b64decode(obj))
+	# Jython wrapper failures (bash/execvp)
+	assert len(lst) == 5
+	ret, out, err, ex, trace = lst
+	if ex:
+		if ex == 'OSError':
+			raise OSError(trace)
+		elif ex == 'ValueError':
+			raise ValueError(trace)
+		elif ex == 'TimeoutExpired':
+			exc = TimeoutExpired(cmd, timeout=cargs['timeout'], output=out)
+			exc.stderr = err
+			raise exc
+		else:
+			raise Exception(trace)
+	return ret, out, err
+
+def lchown(path, user=-1, group=-1):
+	"""
+	Change the owner/group of a path, raises an OSError if the
+	ownership change fails.
+
+	:param user: user to change
+	:type user: int or str
+	:param group: group to change
+	:type group: int or str
+	"""
+	if isinstance(user, str):
+		import pwd
+		entry = pwd.getpwnam(user)
+		if not entry:
+			raise OSError('Unknown user %r' % user)
+		user = entry[2]
+	if isinstance(group, str):
+		import grp
+		entry = grp.getgrnam(group)
+		if not entry:
+			raise OSError('Unknown group %r' % group)
+		group = entry[2]
+	return os.lchown(path, user, group)
+
+def run_regular_process(cmd, kwargs, cargs={}):
+	"""
+	Executes a subprocess command by using subprocess.Popen
+	"""
+	proc = subprocess.Popen(cmd, **kwargs)
+	if kwargs.get('stdout') or kwargs.get('stderr'):
+		try:
+			out, err = proc.communicate(**cargs)
+		except TimeoutExpired:
+			if kwargs.get('start_new_session') and hasattr(os, 'killpg'):
+				os.killpg(proc.pid, signal.SIGKILL)
+			else:
+				proc.kill()
+			out, err = proc.communicate()
+			exc = TimeoutExpired(proc.args, timeout=cargs['timeout'], output=out)
+			exc.stderr = err
+			raise exc
+		status = proc.returncode
+	else:
+		out, err = (None, None)
+		try:
+			status = proc.wait(**cargs)
+		except TimeoutExpired as e:
+			if kwargs.get('start_new_session') and hasattr(os, 'killpg'):
+				os.killpg(proc.pid, signal.SIGKILL)
+			else:
+				proc.kill()
+			proc.wait()
+			raise e
+	return status, out, err
+
+def run_process(cmd, kwargs, cargs={}):
+	"""
+	Executes a subprocess by using a pre-forked process when possible
+	or falling back to subprocess.Popen. See :py:func:`waflib.Utils.run_prefork_process`
+	and :py:func:`waflib.Utils.run_regular_process`
+	"""
+	if kwargs.get('stdout') and kwargs.get('stderr'):
+		return run_prefork_process(cmd, kwargs, cargs)
+	else:
+		return run_regular_process(cmd, kwargs, cargs)
+
+def alloc_process_pool(n, force=False):
+	"""
+	Allocates an amount of processes to the default pool so its size is at least *n*.
+	It is useful to call this function early so that the pre-forked
+	processes use as little memory as possible.
+
+	:param n: pool size
+	:type n: integer
+	:param force: if True then *n* more processes are added to the existing pool
+	:type force: bool
+	"""
+	# mandatory on python2, unnecessary on python >= 3.2
+	global run_process, get_process, alloc_process_pool
+	if not force:
+		n = max(n - len(process_pool), 0)
+	try:
+		lst = [get_process() for x in range(n)]
+	except OSError:
+		run_process = run_regular_process
+		get_process = alloc_process_pool = nada
+	else:
+		for x in lst:
+			process_pool.append(x)
+
+def atexit_pool():
+	for k in process_pool:
+		try:
+			os.kill(k.pid, 9)
+		except OSError:
+			pass
+		else:
+			k.wait()
+# see #1889
+if (sys.hexversion<0x207000f and not is_win32) or sys.hexversion>=0x306000f:
+	atexit.register(atexit_pool)
+
+if os.environ.get('WAF_NO_PREFORK') or sys.platform == 'cli' or not sys.executable:
+	run_process = run_regular_process
+	get_process = alloc_process_pool = nada
+
diff --git a/waflib/__init__.py b/waflib/__init__.py
index c8a3c349..079df358 100644
--- a/waflib/__init__.py
+++ b/waflib/__init__.py
@@ -1,3 +1,3 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2005-2010 (ita)
+# Thomas Nagy, 2005-2018 (ita)
diff --git a/waflib/ansiterm.py b/waflib/ansiterm.py
index 64fa1407..0d20c637 100644
--- a/waflib/ansiterm.py
+++ b/waflib/ansiterm.py
@@ -120,7 +120,7 @@ def screen_buffer_info(self):
 		def clear_line(self, param):
 			mode = param and int(param) or 0
 			sbinfo = self.screen_buffer_info()
-			if mode == 1: # Clear from begining of line to cursor position
+			if mode == 1: # Clear from beginning of line to cursor position
 				line_start = COORD(0, sbinfo.CursorPosition.Y)
 				line_length = sbinfo.Size.X
 			elif mode == 2: # Clear entire line
@@ -136,7 +136,7 @@ def clear_line(self, param):
 		def clear_screen(self, param):
 			mode = to_int(param, 0)
 			sbinfo = self.screen_buffer_info()
-			if mode == 1: # Clear from begining of screen to cursor position
+			if mode == 1: # Clear from beginning of screen to cursor position
 				clear_start = COORD(0, 0)
 				clear_length = sbinfo.CursorPosition.X * sbinfo.CursorPosition.Y
 			elif mode == 2: # Clear entire screen and return cursor to home
@@ -320,7 +320,7 @@ def isatty(self):
 		sbinfo = CONSOLE_SCREEN_BUFFER_INFO()
 		def get_term_cols():
 			windll.kernel32.GetConsoleScreenBufferInfo(console, byref(sbinfo))
-			# TODO Issue 1401
+			# Issue 1401 - the progress bar cannot reach the last character
 			return sbinfo.Size.X - 1
 
 # just try and see
diff --git a/waflib/extras/batched_cc.py b/waflib/extras/batched_cc.py
index 0b035e85..aad28722 100644
--- a/waflib/extras/batched_cc.py
+++ b/waflib/extras/batched_cc.py
@@ -3,21 +3,22 @@
 # Thomas Nagy, 2006-2015 (ita)
 
 """
-Build as batches.
-
 Instead of compiling object files one by one, c/c++ compilers are often able to compile at once:
 cc -c ../file1.c ../file2.c ../file3.c
 
 Files are output on the directory where the compiler is called, and dependencies are more difficult
 to track (do not run the command on all source files if only one file changes)
-
 As such, we do as if the files were compiled one by one, but no command is actually run:
 replace each cc/cpp Task by a TaskSlave. A new task called TaskMaster collects the
 signatures from each slave and finds out the command-line to run.
 
-Just import this module in the configuration (no other change required).
-This is provided as an example, for performance unity builds are recommended (fewer tasks and
-fewer jobs to execute). See waflib/extras/unity.py.
+Just import this module to start using it:
+def build(bld):
+	bld.load('batched_cc')
+
+Note that this is provided as an example, unity builds are recommended
+for best performance results (fewer tasks and fewer jobs to execute).
+See waflib/extras/unity.py.
 """
 
 from waflib import Task, Utils
@@ -26,24 +27,21 @@
 
 MAX_BATCH = 50
 
-c_str = '${CC} ${CFLAGS} ${CPPFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} -c ${SRCLST} ${CXX_TGT_F_BATCHED}'
+c_str = '${CC} ${ARCH_ST:ARCH} ${CFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${tsk.batch_incpaths()} ${DEFINES_ST:DEFINES} -c ${SRCLST} ${CXX_TGT_F_BATCHED} ${CPPFLAGS}'
 c_fun, _ = Task.compile_fun_noshell(c_str)
 
-cxx_str = '${CXX} ${CXXFLAGS} ${CPPFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} -c ${SRCLST} ${CXX_TGT_F_BATCHED}'
+cxx_str = '${CXX} ${ARCH_ST:ARCH} ${CXXFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${tsk.batch_incpaths()} ${DEFINES_ST:DEFINES} -c ${SRCLST} ${CXX_TGT_F_BATCHED} ${CPPFLAGS}'
 cxx_fun, _ = Task.compile_fun_noshell(cxx_str)
 
 count = 70000
-class batch_task(Task.Task):
+class batch(Task.Task):
 	color = 'PINK'
 
 	after = ['c', 'cxx']
 	before = ['cprogram', 'cshlib', 'cstlib', 'cxxprogram', 'cxxshlib', 'cxxstlib']
 
 	def uid(self):
-		m = Utils.md5()
-		m.update(Task.Task.uid(self))
-		m.update(str(self.generator.idx).encode())
-		return m.digest()
+		return Utils.h_list([Task.Task.uid(self), self.generator.idx, self.generator.path.abspath(), self.generator.target])
 
 	def __str__(self):
 		return 'Batch compilation for %d slaves' % len(self.slaves)
@@ -74,6 +72,13 @@ def runnable_status(self):
 
 		return Task.SKIP_ME
 
+	def get_cwd(self):
+		return self.slaves[0].outputs[0].parent
+
+	def batch_incpaths(self):
+		st = self.env.CPPPATH_ST
+		return [st % node.abspath() for node in self.generator.includes_nodes]
+
 	def run(self):
 		self.outputs = []
 
@@ -85,7 +90,6 @@ def run(self):
 				srclst.append(t.inputs[0].abspath())
 
 		self.env.SRCLST = srclst
-		self.cwd = slaves[0].outputs[0].parent.abspath()
 
 		if self.slaves[0].__class__.__name__ == 'c':
 			ret = c_fun(self)
diff --git a/waflib/extras/biber.py b/waflib/extras/biber.py
new file mode 100644
index 00000000..fd9db4e0
--- /dev/null
+++ b/waflib/extras/biber.py
@@ -0,0 +1,58 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2011 (ita)
+
+"""
+Latex processing using "biber"
+"""
+
+import os
+from waflib import Task, Logs
+
+from waflib.Tools import tex as texmodule
+
+class tex(texmodule.tex):
+	biber_fun, _ = Task.compile_fun('${BIBER} ${BIBERFLAGS} ${SRCFILE}',shell=False)
+	biber_fun.__doc__ = """
+	Execute the program **biber**
+	"""
+
+	def bibfile(self):
+		return None
+
+	def bibunits(self):
+		self.env.env = {}
+		self.env.env.update(os.environ)
+		self.env.env.update({'BIBINPUTS': self.texinputs(), 'BSTINPUTS': self.texinputs()})
+		self.env.SRCFILE = self.aux_nodes[0].name[:-4]
+
+		if not self.env['PROMPT_LATEX']:
+			self.env.append_unique('BIBERFLAGS', '--quiet')
+
+		path = self.aux_nodes[0].abspath()[:-4] + '.bcf'
+		if os.path.isfile(path):
+			Logs.warn('calling biber')
+			self.check_status('error when calling biber, check %s.blg for errors' % (self.env.SRCFILE), self.biber_fun())
+		else:
+			super(tex, self).bibfile()
+			super(tex, self).bibunits()
+
+class latex(tex):
+	texfun, vars = Task.compile_fun('${LATEX} ${LATEXFLAGS} ${SRCFILE}', shell=False)
+class pdflatex(tex):
+	texfun, vars =  Task.compile_fun('${PDFLATEX} ${PDFLATEXFLAGS} ${SRCFILE}', shell=False)
+class xelatex(tex):
+	texfun, vars = Task.compile_fun('${XELATEX} ${XELATEXFLAGS} ${SRCFILE}', shell=False)
+
+def configure(self):
+	"""
+	Almost the same as in tex.py, but try to detect 'biber'
+	"""
+	v = self.env
+	for p in ' biber tex latex pdflatex xelatex bibtex dvips dvipdf ps2pdf makeindex pdf2ps'.split():
+		try:
+			self.find_program(p, var=p.upper())
+		except self.errors.ConfigurationError:
+			pass
+	v['DVIPSFLAGS'] = '-Ppdf'
+
diff --git a/waflib/extras/bjam.py b/waflib/extras/bjam.py
new file mode 100644
index 00000000..8e04d3a6
--- /dev/null
+++ b/waflib/extras/bjam.py
@@ -0,0 +1,128 @@
+#! /usr/bin/env python
+# per rosengren 2011
+
+from os import sep, readlink
+from waflib import Logs
+from waflib.TaskGen import feature, after_method
+from waflib.Task import Task, always_run
+
+def options(opt):
+	grp = opt.add_option_group('Bjam Options')
+	grp.add_option('--bjam_src', default=None, help='You can find it in <boost root>/tools/jam/src')
+	grp.add_option('--bjam_uname', default='linuxx86_64', help='bjam is built in <src>/bin.<uname>/bjam')
+	grp.add_option('--bjam_config', default=None)
+	grp.add_option('--bjam_toolset', default=None)
+
+def configure(cnf):
+	if not cnf.env.BJAM_SRC:
+		cnf.env.BJAM_SRC = cnf.options.bjam_src
+	if not cnf.env.BJAM_UNAME:
+		cnf.env.BJAM_UNAME = cnf.options.bjam_uname
+	try:
+		cnf.find_program('bjam', path_list=[
+			cnf.env.BJAM_SRC + sep + 'bin.' + cnf.env.BJAM_UNAME
+		])
+	except Exception:
+		cnf.env.BJAM = None
+	if not cnf.env.BJAM_CONFIG:
+		cnf.env.BJAM_CONFIG = cnf.options.bjam_config
+	if not cnf.env.BJAM_TOOLSET:
+		cnf.env.BJAM_TOOLSET = cnf.options.bjam_toolset
+
+@feature('bjam')
+@after_method('process_rule')
+def process_bjam(self):
+	if not self.bld.env.BJAM:
+		self.create_task('bjam_creator')
+	self.create_task('bjam_build')
+	self.create_task('bjam_installer')
+	if getattr(self, 'always', False):
+		always_run(bjam_creator)
+		always_run(bjam_build)
+	always_run(bjam_installer)
+
+class bjam_creator(Task):
+	ext_out = 'bjam_exe'
+	vars=['BJAM_SRC', 'BJAM_UNAME']
+	def run(self):
+		env = self.env
+		gen = self.generator
+		bjam = gen.bld.root.find_dir(env.BJAM_SRC)
+		if not bjam:
+			Logs.error('Can not find bjam source')
+			return -1
+		bjam_exe_relpath = 'bin.' + env.BJAM_UNAME + '/bjam'
+		bjam_exe = bjam.find_resource(bjam_exe_relpath)
+		if bjam_exe:
+			env.BJAM = bjam_exe.srcpath()
+			return 0
+		bjam_cmd = ['./build.sh']
+		Logs.debug('runner: ' + bjam.srcpath() + '> ' + str(bjam_cmd))
+		result = self.exec_command(bjam_cmd, cwd=bjam.srcpath())
+		if not result == 0:
+			Logs.error('bjam failed')
+			return -1
+		bjam_exe = bjam.find_resource(bjam_exe_relpath)
+		if bjam_exe:
+			env.BJAM = bjam_exe.srcpath()
+			return 0
+		Logs.error('bjam failed')
+		return -1
+
+class bjam_build(Task):
+	ext_in = 'bjam_exe'
+	ext_out = 'install'
+	vars = ['BJAM_TOOLSET']
+	def run(self):
+		env = self.env
+		gen = self.generator
+		path = gen.path
+		bld = gen.bld
+		if hasattr(gen, 'root'):
+			build_root = path.find_node(gen.root)
+		else:
+			build_root = path
+		jam = bld.srcnode.find_resource(env.BJAM_CONFIG)
+		if jam:
+			Logs.debug('bjam: Using jam configuration from ' + jam.srcpath())
+			jam_rel = jam.relpath_gen(build_root)
+		else:
+			Logs.warn('No build configuration in build_config/user-config.jam. Using default')
+			jam_rel = None
+		bjam_exe = bld.srcnode.find_node(env.BJAM)
+		if not bjam_exe:
+			Logs.error('env.BJAM is not set')
+			return -1
+		bjam_exe_rel = bjam_exe.relpath_gen(build_root)
+		cmd = ([bjam_exe_rel] +
+			(['--user-config=' + jam_rel] if jam_rel else []) +
+			['--stagedir=' + path.get_bld().path_from(build_root)] +
+			['--debug-configuration'] +
+			['--with-' + lib for lib in self.generator.target] +
+			(['toolset=' + env.BJAM_TOOLSET] if env.BJAM_TOOLSET else []) +
+			['link=' + 'shared'] +
+			['variant=' + 'release']
+		)
+		Logs.debug('runner: ' + build_root.srcpath() + '> ' + str(cmd))
+		ret = self.exec_command(cmd, cwd=build_root.srcpath())
+		if ret != 0:
+			return ret
+		self.set_outputs(path.get_bld().ant_glob('lib/*') + path.get_bld().ant_glob('bin/*'))
+		return 0
+
+class bjam_installer(Task):
+	ext_in = 'install'
+	def run(self):
+		gen = self.generator
+		path = gen.path
+		for idir, pat in (('${LIBDIR}', 'lib/*'), ('${BINDIR}', 'bin/*')):
+			files = []
+			for n in path.get_bld().ant_glob(pat):
+				try:
+					t = readlink(n.srcpath())
+					gen.bld.symlink_as(sep.join([idir, n.name]), t, postpone=False)
+				except OSError:
+					files.append(n)
+			gen.bld.install_files(idir, files, postpone=False)
+		return 0
+
diff --git a/waflib/extras/blender.py b/waflib/extras/blender.py
new file mode 100644
index 00000000..e5efc280
--- /dev/null
+++ b/waflib/extras/blender.py
@@ -0,0 +1,108 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Michal Proszek, 2014 (poxip)
+
+"""
+Detect the version of Blender, path
+and install the extension:
+
+	def options(opt):
+		opt.load('blender')
+	def configure(cnf):
+		cnf.load('blender')
+	def build(bld):
+		bld(name='io_mesh_raw',
+			feature='blender',
+			files=['file1.py', 'file2.py']
+		)
+If name variable is empty, files are installed in scripts/addons, otherwise scripts/addons/name
+Use ./waf configure --system to set the installation directory to system path
+"""
+import os
+import re
+from getpass import getuser
+
+from waflib import Utils
+from waflib.TaskGen import feature
+from waflib.Configure import conf
+
+def options(opt):
+	opt.add_option(
+		'-s', '--system',
+		dest='directory_system',
+		default=False,
+		action='store_true',
+		help='determines installation directory (default: user)'
+	)
+
+@conf
+def find_blender(ctx):
+	'''Return version number of blender, if not exist return None'''
+	blender = ctx.find_program('blender')
+	output = ctx.cmd_and_log(blender + ['--version'])
+	m = re.search(r'Blender\s*((\d+(\.|))*)', output)
+	if not m:
+		ctx.fatal('Could not retrieve blender version')
+
+	try:
+		blender_version = m.group(1)
+	except IndexError:
+		ctx.fatal('Could not retrieve blender version')
+
+	ctx.env['BLENDER_VERSION'] = blender_version
+	return blender
+
+@conf
+def configure_paths(ctx):
+	"""Setup blender paths"""
+	# Get the username
+	user = getuser()
+	_platform = Utils.unversioned_sys_platform()
+	config_path = {'user': '', 'system': ''}
+	if _platform.startswith('linux'):
+		config_path['user'] = '/home/%s/.config/blender/' % user
+		config_path['system'] = '/usr/share/blender/'
+	elif _platform == 'darwin':
+		# MAC OS X
+		config_path['user'] = \
+			'/Users/%s/Library/Application Support/Blender/' % user
+		config_path['system'] = '/Library/Application Support/Blender/'
+	elif Utils.is_win32:
+		# Windows
+		appdata_path = ctx.getenv('APPDATA').replace('\\', '/')
+		homedrive = ctx.getenv('HOMEDRIVE').replace('\\', '/')
+
+		config_path['user'] = '%s/Blender Foundation/Blender/' % appdata_path
+		config_path['system'] = \
+			'%sAll Users/AppData/Roaming/Blender Foundation/Blender/' % homedrive
+	else:
+		ctx.fatal(
+			'Unsupported platform. '
+			'Available platforms: Linux, OSX, MS-Windows.'
+		)
+
+	blender_version = ctx.env['BLENDER_VERSION']
+
+	config_path['user'] += blender_version + '/'
+	config_path['system'] += blender_version + '/'
+
+	ctx.env['BLENDER_CONFIG_DIR'] = os.path.abspath(config_path['user'])
+	if ctx.options.directory_system:
+		ctx.env['BLENDER_CONFIG_DIR'] = config_path['system']
+
+	ctx.env['BLENDER_ADDONS_DIR'] = os.path.join(
+		ctx.env['BLENDER_CONFIG_DIR'], 'scripts/addons'
+	)
+	Utils.check_dir(ctx.env['BLENDER_ADDONS_DIR'])
+
+def configure(ctx):
+	ctx.find_blender()
+	ctx.configure_paths()
+
+@feature('blender_list')
+def blender(self):
+	# Two ways to install a blender extension: as a module or just .py files
+	dest_dir = os.path.join(self.env.BLENDER_ADDONS_DIR, self.get_name())
+	Utils.check_dir(dest_dir)
+	self.add_install_files(install_to=dest_dir, install_from=getattr(self, 'files', '.'))
+
diff --git a/waflib/extras/boo.py b/waflib/extras/boo.py
new file mode 100644
index 00000000..06623d40
--- /dev/null
+++ b/waflib/extras/boo.py
@@ -0,0 +1,81 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Yannick LM 2011
+
+"""
+Support for the boo programming language, for example::
+
+	bld(features = "boo",       # necessary feature
+		source   = "src.boo",   # list of boo files
+		gen      = "world.dll", # target
+		type     = "library",   # library/exe ("-target:xyz" flag)
+		name     = "world"      # necessary if the target is referenced by 'use'
+	)
+"""
+
+from waflib import Task
+from waflib.Configure import conf
+from waflib.TaskGen import feature, after_method, before_method, extension
+
+@extension('.boo')
+def boo_hook(self, node):
+	# Nothing here yet ...
+	# TODO filter the non-boo source files in 'apply_booc' and remove this method
+	pass
+
+@feature('boo')
+@before_method('process_source')
+def apply_booc(self):
+	"""Create a booc task """
+	src_nodes = self.to_nodes(self.source)
+	out_node = self.path.find_or_declare(self.gen)
+
+	self.boo_task = self.create_task('booc', src_nodes, [out_node])
+
+	# Set variables used by the 'booc' task
+	self.boo_task.env.OUT = '-o:%s' % out_node.abspath()
+
+	# type is "exe" by default
+	type = getattr(self, "type", "exe")
+	self.boo_task.env.BOO_TARGET_TYPE = "-target:%s" % type
+
+@feature('boo')
+@after_method('apply_boo')
+def use_boo(self):
+	""""
+	boo applications honor the **use** keyword::
+	"""
+	dep_names = self.to_list(getattr(self, 'use', []))
+	for dep_name in dep_names:
+		dep_task_gen = self.bld.get_tgen_by_name(dep_name)
+		if not dep_task_gen:
+			continue
+		dep_task_gen.post()
+		dep_task = getattr(dep_task_gen, 'boo_task', None)
+		if not dep_task:
+			# Try a cs task:
+			dep_task = getattr(dep_task_gen, 'cs_task', None)
+			if not dep_task:
+				# Try a link task:
+				dep_task = getattr(dep_task, 'link_task', None)
+				if not dep_task:
+					# Abort ...
+					continue
+		self.boo_task.set_run_after(dep_task) # order
+		self.boo_task.dep_nodes.extend(dep_task.outputs) # dependency
+		self.boo_task.env.append_value('BOO_FLAGS', '-reference:%s' % dep_task.outputs[0].abspath())
+
+class booc(Task.Task):
+	"""Compiles .boo files """
+	color   = 'YELLOW'
+	run_str = '${BOOC} ${BOO_FLAGS} ${BOO_TARGET_TYPE} ${OUT} ${SRC}'
+
+@conf
+def check_booc(self):
+	self.find_program('booc', 'BOOC')
+	self.env.BOO_FLAGS = ['-nologo']
+
+def configure(self):
+	"""Check that booc is available """
+	self.check_booc()
+
diff --git a/waflib/extras/boost.py b/waflib/extras/boost.py
new file mode 100644
index 00000000..c2aaaa93
--- /dev/null
+++ b/waflib/extras/boost.py
@@ -0,0 +1,525 @@
+#!/usr/bin/env python
+# encoding: utf-8
+#
+# partially based on boost.py written by Gernot Vormayr
+# written by Ruediger Sonderfeld <ruediger@c-plusplus.de>, 2008
+# modified by Bjoern Michaelsen, 2008
+# modified by Luca Fossati, 2008
+# rewritten for waf 1.5.1, Thomas Nagy, 2008
+# rewritten for waf 1.6.2, Sylvain Rouquette, 2011
+
+'''
+
+This is an extra tool, not bundled with the default waf binary.
+To add the boost tool to the waf file:
+$ ./waf-light --tools=compat15,boost
+	or, if you have waf >= 1.6.2
+$ ./waf update --files=boost
+
+When using this tool, the wscript will look like:
+
+	def options(opt):
+		opt.load('compiler_cxx boost')
+
+	def configure(conf):
+		conf.load('compiler_cxx boost')
+		conf.check_boost(lib='system filesystem')
+
+	def build(bld):
+		bld(source='main.cpp', target='app', use='BOOST')
+
+Options are generated, in order to specify the location of boost includes/libraries.
+The `check_boost` configuration function allows to specify the used boost libraries.
+It can also provide default arguments to the --boost-mt command-line arguments.
+Everything will be packaged together in a BOOST component that you can use.
+
+When using MSVC, a lot of compilation flags need to match your BOOST build configuration:
+ - you may have to add /EHsc to your CXXFLAGS or define boost::throw_exception if BOOST_NO_EXCEPTIONS is defined.
+   Errors: C4530
+ - boost libraries will try to be smart and use the (pretty but often not useful) auto-linking feature of MSVC
+   So before calling `conf.check_boost` you might want to disabling by adding
+		conf.env.DEFINES_BOOST += ['BOOST_ALL_NO_LIB']
+   Errors:
+ - boost might also be compiled with /MT, which links the runtime statically.
+   If you have problems with redefined symbols,
+		self.env['DEFINES_%s' % var] += ['BOOST_ALL_NO_LIB']
+		self.env['CXXFLAGS_%s' % var] += ['/MD', '/EHsc']
+Passing `--boost-linkage_autodetect` might help ensuring having a correct linkage in some basic cases.
+
+'''
+
+import sys
+import re
+from waflib import Utils, Logs, Errors
+from waflib.Configure import conf
+from waflib.TaskGen import feature, after_method
+
+BOOST_LIBS = ['/usr/lib', '/usr/local/lib', '/opt/local/lib', '/sw/lib', '/lib']
+BOOST_INCLUDES = ['/usr/include', '/usr/local/include', '/opt/local/include', '/sw/include']
+BOOST_VERSION_FILE = 'boost/version.hpp'
+BOOST_VERSION_CODE = '''
+#include <iostream>
+#include <boost/version.hpp>
+int main() { std::cout << BOOST_LIB_VERSION << ":" << BOOST_VERSION << std::endl; }
+'''
+
+BOOST_ERROR_CODE = '''
+#include <boost/system/error_code.hpp>
+int main() { boost::system::error_code c; }
+'''
+
+PTHREAD_CODE = '''
+#include <pthread.h>
+static void* f(void*) { return 0; }
+int main() {
+	pthread_t th;
+	pthread_attr_t attr;
+	pthread_attr_init(&attr);
+	pthread_create(&th, &attr, &f, 0);
+	pthread_join(th, 0);
+	pthread_cleanup_push(0, 0);
+	pthread_cleanup_pop(0);
+	pthread_attr_destroy(&attr);
+}
+'''
+
+BOOST_THREAD_CODE = '''
+#include <boost/thread.hpp>
+int main() { boost::thread t; }
+'''
+
+BOOST_LOG_CODE = '''
+#include <boost/log/trivial.hpp>
+#include <boost/log/utility/setup/console.hpp>
+#include <boost/log/utility/setup/common_attributes.hpp>
+int main() {
+	using namespace boost::log;
+	add_common_attributes();
+	add_console_log(std::clog, keywords::format = "%Message%");
+	BOOST_LOG_TRIVIAL(debug) << "log is working" << std::endl;
+}
+'''
+
+# toolsets from {boost_dir}/tools/build/v2/tools/common.jam
+PLATFORM = Utils.unversioned_sys_platform()
+detect_intel = lambda env: (PLATFORM == 'win32') and 'iw' or 'il'
+detect_clang = lambda env: (PLATFORM == 'darwin') and 'clang-darwin' or 'clang'
+detect_mingw = lambda env: (re.search('MinGW', env.CXX[0])) and 'mgw' or 'gcc'
+BOOST_TOOLSETS = {
+	'borland':  'bcb',
+	'clang':	detect_clang,
+	'como':	 'como',
+	'cw':	   'cw',
+	'darwin':   'xgcc',
+	'edg':	  'edg',
+	'g++':	  detect_mingw,
+	'gcc':	  detect_mingw,
+	'icpc':	 detect_intel,
+	'intel':	detect_intel,
+	'kcc':	  'kcc',
+	'kylix':	'bck',
+	'mipspro':  'mp',
+	'mingw':	'mgw',
+	'msvc':	 'vc',
+	'qcc':	  'qcc',
+	'sun':	  'sw',
+	'sunc++':   'sw',
+	'tru64cxx': 'tru',
+	'vacpp':	'xlc'
+}
+
+
+def options(opt):
+	opt = opt.add_option_group('Boost Options')
+	opt.add_option('--boost-includes', type='string',
+				   default='', dest='boost_includes',
+				   help='''path to the directory where the boost includes are,
+				   e.g., /path/to/boost_1_55_0/stage/include''')
+	opt.add_option('--boost-libs', type='string',
+				   default='', dest='boost_libs',
+				   help='''path to the directory where the boost libs are,
+				   e.g., path/to/boost_1_55_0/stage/lib''')
+	opt.add_option('--boost-mt', action='store_true',
+				   default=False, dest='boost_mt',
+				   help='select multi-threaded libraries')
+	opt.add_option('--boost-abi', type='string', default='', dest='boost_abi',
+				   help='''select libraries with tags (gd for debug, static is automatically added),
+				   see doc Boost, Getting Started, chapter 6.1''')
+	opt.add_option('--boost-linkage_autodetect', action="store_true", dest='boost_linkage_autodetect',
+				   help="auto-detect boost linkage options (don't get used to it / might break other stuff)")
+	opt.add_option('--boost-toolset', type='string',
+				   default='', dest='boost_toolset',
+				   help='force a toolset e.g. msvc, vc90, \
+						gcc, mingw, mgw45 (default: auto)')
+	py_version = '%d%d' % (sys.version_info[0], sys.version_info[1])
+	opt.add_option('--boost-python', type='string',
+				   default=py_version, dest='boost_python',
+				   help='select the lib python with this version \
+						(default: %s)' % py_version)
+
+
+@conf
+def __boost_get_version_file(self, d):
+	if not d:
+		return None
+	dnode = self.root.find_dir(d)
+	if dnode:
+		return dnode.find_node(BOOST_VERSION_FILE)
+	return None
+
+@conf
+def boost_get_version(self, d):
+	"""silently retrieve the boost version number"""
+	node = self.__boost_get_version_file(d)
+	if node:
+		try:
+			txt = node.read()
+		except EnvironmentError:
+			Logs.error("Could not read the file %r", node.abspath())
+		else:
+			re_but1 = re.compile('^#define\\s+BOOST_LIB_VERSION\\s+"(.+)"', re.M)
+			m1 = re_but1.search(txt)
+			re_but2 = re.compile('^#define\\s+BOOST_VERSION\\s+(\\d+)', re.M)
+			m2 = re_but2.search(txt)
+			if m1 and m2:
+				return (m1.group(1), m2.group(1))
+	return self.check_cxx(fragment=BOOST_VERSION_CODE, includes=[d], execute=True, define_ret=True).split(":")
+
+@conf
+def boost_get_includes(self, *k, **kw):
+	includes = k and k[0] or kw.get('includes')
+	if includes and self.__boost_get_version_file(includes):
+		return includes
+	for d in self.environ.get('INCLUDE', '').split(';') + BOOST_INCLUDES:
+		if self.__boost_get_version_file(d):
+			return d
+	if includes:
+		self.end_msg('headers not found in %s' % includes)
+		self.fatal('The configuration failed')
+	else:
+		self.end_msg('headers not found, please provide a --boost-includes argument (see help)')
+		self.fatal('The configuration failed')
+
+
+@conf
+def boost_get_toolset(self, cc):
+	toolset = cc
+	if not cc:
+		build_platform = Utils.unversioned_sys_platform()
+		if build_platform in BOOST_TOOLSETS:
+			cc = build_platform
+		else:
+			cc = self.env.CXX_NAME
+	if cc in BOOST_TOOLSETS:
+		toolset = BOOST_TOOLSETS[cc]
+	return isinstance(toolset, str) and toolset or toolset(self.env)
+
+
+@conf
+def __boost_get_libs_path(self, *k, **kw):
+	''' return the lib path and all the files in it '''
+	if 'files' in kw:
+		return self.root.find_dir('.'), Utils.to_list(kw['files'])
+	libs = k and k[0] or kw.get('libs')
+	if libs:
+		path = self.root.find_dir(libs)
+		files = path.ant_glob('*boost_*')
+	if not libs or not files:
+		for d in self.environ.get('LIB', '').split(';') + BOOST_LIBS:
+			if not d:
+				continue
+			path = self.root.find_dir(d)
+			if path:
+				files = path.ant_glob('*boost_*')
+				if files:
+					break
+			path = self.root.find_dir(d + '64')
+			if path:
+				files = path.ant_glob('*boost_*')
+				if files:
+					break
+	if not path:
+		if libs:
+			self.end_msg('libs not found in %s' % libs)
+			self.fatal('The configuration failed')
+		else:
+			self.end_msg('libs not found, please provide a --boost-libs argument (see help)')
+			self.fatal('The configuration failed')
+
+	self.to_log('Found the boost path in %r with the libraries:' % path)
+	for x in files:
+		self.to_log('    %r' % x)
+	return path, files
+
+@conf
+def boost_get_libs(self, *k, **kw):
+	'''
+	return the lib path and the required libs
+	according to the parameters
+	'''
+	path, files = self.__boost_get_libs_path(**kw)
+	files = sorted(files, key=lambda f: (len(f.name), f.name), reverse=True)
+	toolset = self.boost_get_toolset(kw.get('toolset', ''))
+	toolset_pat = '(-%s[0-9]{0,3})' % toolset
+	version = '-%s' % self.env.BOOST_VERSION
+
+	def find_lib(re_lib, files):
+		for file in files:
+			if re_lib.search(file.name):
+				self.to_log('Found boost lib %s' % file)
+				return file
+		return None
+
+	def format_lib_name(name):
+		if name.startswith('lib') and self.env.CC_NAME != 'msvc':
+			name = name[3:]
+		return name[:name.rfind('.')]
+
+	def match_libs(lib_names, is_static):
+		libs = []
+		lib_names = Utils.to_list(lib_names)
+		if not lib_names:
+			return libs
+		t = []
+		if kw.get('mt', False):
+			t.append('-mt')
+		if kw.get('abi'):
+			t.append('%s%s' % (is_static and '-s' or '-', kw['abi']))
+		elif is_static:
+			t.append('-s')
+		tags_pat = t and ''.join(t) or ''
+		ext = is_static and self.env.cxxstlib_PATTERN or self.env.cxxshlib_PATTERN
+		ext = ext.partition('%s')[2] # remove '%s' or 'lib%s' from PATTERN
+
+		for lib in lib_names:
+			if lib == 'python':
+				# for instance, with python='27',
+				# accepts '-py27', '-py2', '27', '-2.7' and '2'
+				# but will reject '-py3', '-py26', '26' and '3'
+				tags = '({0})?((-py{2})|(-py{1}(?=[^0-9]))|({2})|(-{1}.{3})|({1}(?=[^0-9]))|(?=[^0-9])(?!-py))'.format(tags_pat, kw['python'][0], kw['python'], kw['python'][1])
+			else:
+				tags = tags_pat
+			# Trying libraries, from most strict match to least one
+			for pattern in ['boost_%s%s%s%s%s$' % (lib, toolset_pat, tags, version, ext),
+							'boost_%s%s%s%s$' % (lib, tags, version, ext),
+							# Give up trying to find the right version
+							'boost_%s%s%s%s$' % (lib, toolset_pat, tags, ext),
+							'boost_%s%s%s$' % (lib, tags, ext),
+							'boost_%s%s$' % (lib, ext),
+							'boost_%s' % lib]:
+				self.to_log('Trying pattern %s' % pattern)
+				file = find_lib(re.compile(pattern), files)
+				if file:
+					libs.append(format_lib_name(file.name))
+					break
+			else:
+				self.end_msg('lib %s not found in %s' % (lib, path.abspath()))
+				self.fatal('The configuration failed')
+		return libs
+
+	return  path.abspath(), match_libs(kw.get('lib'), False), match_libs(kw.get('stlib'), True)
+
+@conf
+def _check_pthread_flag(self, *k, **kw):
+	'''
+	Computes which flags should be added to CXXFLAGS and LINKFLAGS to compile in multi-threading mode
+
+	Yes, we *need* to put the -pthread thing in CPPFLAGS because with GCC3,
+	boost/thread.hpp will trigger a #error if -pthread isn't used:
+	  boost/config/requires_threads.hpp:47:5: #error "Compiler threading support
+	  is not turned on. Please set the correct command line options for
+	  threading: -pthread (Linux), -pthreads (Solaris) or -mthreads (Mingw32)"
+
+	Based on _BOOST_PTHREAD_FLAG(): https://github.com/tsuna/boost.m4/blob/master/build-aux/boost.m4
+    '''
+
+	var = kw.get('uselib_store', 'BOOST')
+
+	self.start_msg('Checking the flags needed to use pthreads')
+
+	# The ordering *is* (sometimes) important.  Some notes on the
+	# individual items follow:
+	# (none): in case threads are in libc; should be tried before -Kthread and
+	#       other compiler flags to prevent continual compiler warnings
+	# -lpthreads: AIX (must check this before -lpthread)
+	# -Kthread: Sequent (threads in libc, but -Kthread needed for pthread.h)
+	# -kthread: FreeBSD kernel threads (preferred to -pthread since SMP-able)
+	# -llthread: LinuxThreads port on FreeBSD (also preferred to -pthread)
+	# -pthread: GNU Linux/GCC (kernel threads), BSD/GCC (userland threads)
+	# -pthreads: Solaris/GCC
+	# -mthreads: MinGW32/GCC, Lynx/GCC
+	# -mt: Sun Workshop C (may only link SunOS threads [-lthread], but it
+	#      doesn't hurt to check since this sometimes defines pthreads too;
+	#      also defines -D_REENTRANT)
+	#      ... -mt is also the pthreads flag for HP/aCC
+	# -lpthread: GNU Linux, etc.
+	# --thread-safe: KAI C++
+	if Utils.unversioned_sys_platform() == "sunos":
+		# On Solaris (at least, for some versions), libc contains stubbed
+		# (non-functional) versions of the pthreads routines, so link-based
+		# tests will erroneously succeed.  (We need to link with -pthreads/-mt/
+		# -lpthread.)  (The stubs are missing pthread_cleanup_push, or rather
+		# a function called by this macro, so we could check for that, but
+		# who knows whether they'll stub that too in a future libc.)  So,
+		# we'll just look for -pthreads and -lpthread first:
+		boost_pthread_flags = ["-pthreads", "-lpthread", "-mt", "-pthread"]
+	else:
+		boost_pthread_flags = ["", "-lpthreads", "-Kthread", "-kthread", "-llthread", "-pthread",
+							   "-pthreads", "-mthreads", "-lpthread", "--thread-safe", "-mt"]
+
+	for boost_pthread_flag in boost_pthread_flags:
+		try:
+			self.env.stash()
+			self.env.append_value('CXXFLAGS_%s' % var, boost_pthread_flag)
+			self.env.append_value('LINKFLAGS_%s' % var, boost_pthread_flag)
+			self.check_cxx(code=PTHREAD_CODE, msg=None, use=var, execute=False)
+
+			self.end_msg(boost_pthread_flag)
+			return
+		except self.errors.ConfigurationError:
+			self.env.revert()
+	self.end_msg('None')
+
+@conf
+def check_boost(self, *k, **kw):
+	"""
+	Initialize boost libraries to be used.
+
+	Keywords: you can pass the same parameters as with the command line (without "--boost-").
+	Note that the command line has the priority, and should preferably be used.
+	"""
+	if not self.env['CXX']:
+		self.fatal('load a c++ compiler first, conf.load("compiler_cxx")')
+
+	params = {
+		'lib': k and k[0] or kw.get('lib'),
+		'stlib': kw.get('stlib')
+	}
+	for key, value in self.options.__dict__.items():
+		if not key.startswith('boost_'):
+			continue
+		key = key[len('boost_'):]
+		params[key] = value and value or kw.get(key, '')
+
+	var = kw.get('uselib_store', 'BOOST')
+
+	self.find_program('dpkg-architecture', var='DPKG_ARCHITECTURE', mandatory=False)
+	if self.env.DPKG_ARCHITECTURE:
+		deb_host_multiarch = self.cmd_and_log([self.env.DPKG_ARCHITECTURE[0], '-qDEB_HOST_MULTIARCH'])
+		BOOST_LIBS.insert(0, '/usr/lib/%s' % deb_host_multiarch.strip())
+
+	self.start_msg('Checking boost includes')
+	self.env['INCLUDES_%s' % var] = inc = self.boost_get_includes(**params)
+	versions = self.boost_get_version(inc)
+	self.env.BOOST_VERSION = versions[0]
+	self.env.BOOST_VERSION_NUMBER = int(versions[1])
+	self.end_msg("%d.%d.%d" % (int(versions[1]) / 100000,
+							   int(versions[1]) / 100 % 1000,
+							   int(versions[1]) % 100))
+	if Logs.verbose:
+		Logs.pprint('CYAN', '	path : %s' % self.env['INCLUDES_%s' % var])
+
+	if not params['lib'] and not params['stlib']:
+		return
+	if 'static' in kw or 'static' in params:
+		Logs.warn('boost: static parameter is deprecated, use stlib instead.')
+	self.start_msg('Checking boost libs')
+	path, libs, stlibs = self.boost_get_libs(**params)
+	self.env['LIBPATH_%s' % var] = [path]
+	self.env['STLIBPATH_%s' % var] = [path]
+	self.env['LIB_%s' % var] = libs
+	self.env['STLIB_%s' % var] = stlibs
+	self.end_msg('ok')
+	if Logs.verbose:
+		Logs.pprint('CYAN', '	path : %s' % path)
+		Logs.pprint('CYAN', '	shared libs : %s' % libs)
+		Logs.pprint('CYAN', '	static libs : %s' % stlibs)
+
+	def has_shlib(lib):
+		return params['lib'] and lib in params['lib']
+	def has_stlib(lib):
+		return params['stlib'] and lib in params['stlib']
+	def has_lib(lib):
+		return has_shlib(lib) or has_stlib(lib)
+	if has_lib('thread'):
+		# not inside try_link to make check visible in the output
+		self._check_pthread_flag(k, kw)
+
+	def try_link():
+		if has_lib('system'):
+			self.check_cxx(fragment=BOOST_ERROR_CODE, use=var, execute=False)
+		if has_lib('thread'):
+			self.check_cxx(fragment=BOOST_THREAD_CODE, use=var, execute=False)
+		if has_lib('log'):
+			if not has_lib('thread'):
+				self.env['DEFINES_%s' % var] += ['BOOST_LOG_NO_THREADS']
+			if has_shlib('log'):
+				self.env['DEFINES_%s' % var] += ['BOOST_LOG_DYN_LINK']
+			self.check_cxx(fragment=BOOST_LOG_CODE, use=var, execute=False)
+
+	if params.get('linkage_autodetect', False):
+		self.start_msg("Attempting to detect boost linkage flags")
+		toolset = self.boost_get_toolset(kw.get('toolset', ''))
+		if toolset in ('vc',):
+			# disable auto-linking feature, causing error LNK1181
+			# because the code wants to be linked against
+			self.env['DEFINES_%s' % var] += ['BOOST_ALL_NO_LIB']
+
+			# if no dlls are present, we guess the .lib files are not stubs
+			has_dlls = False
+			for x in Utils.listdir(path):
+				if x.endswith(self.env.cxxshlib_PATTERN % ''):
+					has_dlls = True
+					break
+			if not has_dlls:
+				self.env['STLIBPATH_%s' % var] = [path]
+				self.env['STLIB_%s' % var] = libs
+				del self.env['LIB_%s' % var]
+				del self.env['LIBPATH_%s' % var]
+
+			# we attempt to play with some known-to-work CXXFLAGS combinations
+			for cxxflags in (['/MD', '/EHsc'], []):
+				self.env.stash()
+				self.env["CXXFLAGS_%s" % var] += cxxflags
+				try:
+					try_link()
+				except Errors.ConfigurationError as e:
+					self.env.revert()
+					exc = e
+				else:
+					self.end_msg("ok: winning cxxflags combination: %s" % (self.env["CXXFLAGS_%s" % var]))
+					exc = None
+					self.env.commit()
+					break
+
+			if exc is not None:
+				self.end_msg("Could not auto-detect boost linking flags combination, you may report it to boost.py author", ex=exc)
+				self.fatal('The configuration failed')
+		else:
+			self.end_msg("Boost linkage flags auto-detection not implemented (needed ?) for this toolchain")
+			self.fatal('The configuration failed')
+	else:
+		self.start_msg('Checking for boost linkage')
+		try:
+			try_link()
+		except Errors.ConfigurationError as e:
+			self.end_msg("Could not link against boost libraries using supplied options")
+			self.fatal('The configuration failed')
+		self.end_msg('ok')
+
+
+@feature('cxx')
+@after_method('apply_link')
+def install_boost(self):
+	if install_boost.done or not Utils.is_win32 or not self.bld.cmd.startswith('install'):
+		return
+	install_boost.done = True
+	inst_to = getattr(self, 'install_path', '${BINDIR}')
+	for lib in self.env.LIB_BOOST:
+		try:
+			file = self.bld.find_file(self.env.cxxshlib_PATTERN % lib, self.env.LIBPATH_BOOST)
+			self.add_install_files(install_to=inst_to, install_from=self.bld.root.find_node(file))
+		except:
+			continue
+install_boost.done = False
+
diff --git a/waflib/extras/build_file_tracker.py b/waflib/extras/build_file_tracker.py
index 4f9e7b6f..c4f26fd0 100644
--- a/waflib/extras/build_file_tracker.py
+++ b/waflib/extras/build_file_tracker.py
@@ -8,25 +8,21 @@
 
 Note that there is a variety of ways to implement this, one may want use timestamps on source files too for example,
 or one may want to hash the files in the source directory only under certain conditions (md5_tstamp tool)
-or to hash the file in the build directory with its timestamp (similar to 'update_outputs')
+or to hash the file in the build directory with its timestamp
 """
 
 import os
 from waflib import Node, Utils
 
 def get_bld_sig(self):
+	if not self.is_bld() or self.ctx.bldnode is self.ctx.srcnode:
+		return Utils.h_file(self.abspath())
+
 	try:
-		return self.cache_sig
+		# add the creation time to the signature
+		return self.sig + str(os.stat(self.abspath()).st_mtime)
 	except AttributeError:
-		pass
-
-	if not self.is_bld() or self.ctx.bldnode is self.ctx.srcnode:
-		self.sig = Utils.h_file(self.abspath())
-		self.cache_sig = ret = self.sig
-	else:
-		# add the
-		self.cache_sig = ret = self.sig + str(os.stat(self.abspath()).st_mtime)
-	return ret
+		return None
 
 Node.Node.get_bld_sig = get_bld_sig
 
diff --git a/waflib/extras/build_logs.py b/waflib/extras/build_logs.py
index 9209ec33..cdf8ed09 100644
--- a/waflib/extras/build_logs.py
+++ b/waflib/extras/build_logs.py
@@ -17,7 +17,7 @@ def init(ctx):
 	up = os.path.dirname(Context.g_module.__file__)
 except AttributeError:
 	up = '.'
-LOGFILE = os.path.join(up, 'logs', '%s.log' % time.strftime('%Y_%m_%d_%H_%M'))
+LOGFILE = os.path.join(up, 'logs', time.strftime('%Y_%m_%d_%H_%M.log'))
 
 wlock = threading.Lock()
 class log_to_file(object):
@@ -28,7 +28,7 @@ def __init__(self, stream, fileobj, filename):
 		self.filename = filename
 		self.is_valid = True
 	def replace_colors(self, data):
-		for x in Logs.colors_lst.values(): 
+		for x in Logs.colors_lst.values():
 			if isinstance(x, str):
 				data = data.replace(x, '')
 		return data
@@ -96,7 +96,7 @@ def exit_cleanup():
 		fileobj.close()
 		filename = sys.stdout.filename
 
-		Logs.info('Output logged to %r' % filename)
+		Logs.info('Output logged to %r', filename)
 
 		# then copy the log file to "latest.log" if possible
 		up = os.path.dirname(os.path.abspath(filename))
@@ -104,7 +104,6 @@ def exit_cleanup():
 			shutil.copy(filename, os.path.join(up, 'latest.log'))
 		except OSError:
 			# this may fail on windows due to processes spawned
-			# 
 			pass
 
 atexit.register(exit_cleanup)
diff --git a/waflib/extras/buildcopy.py b/waflib/extras/buildcopy.py
new file mode 100644
index 00000000..a6d9ac83
--- /dev/null
+++ b/waflib/extras/buildcopy.py
@@ -0,0 +1,82 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Calle Rosenquist, 2017 (xbreak)
+"""
+Create task that copies source files to the associated build node.
+This is useful to e.g. construct a complete Python package so it can be unit tested
+without installation.
+
+Source files to be copied can be specified either in `buildcopy_source` attribute, or
+`source` attribute. If both are specified `buildcopy_source` has priority.
+
+Examples::
+
+	def build(bld):
+		bld(name             = 'bar',
+			features         = 'py buildcopy',
+			source           = bld.path.ant_glob('src/bar/*.py'))
+
+		bld(name             = 'py baz',
+			features         = 'buildcopy',
+			buildcopy_source = bld.path.ant_glob('src/bar/*.py') + ['src/bar/resource.txt'])
+
+"""
+import os, shutil
+from waflib import Errors, Task, TaskGen, Utils, Node
+
+@TaskGen.before_method('process_source')
+@TaskGen.feature('buildcopy')
+def make_buildcopy(self):
+	"""
+	Creates the buildcopy task.
+	"""
+	def to_src_nodes(lst):
+		"""Find file nodes only in src, TaskGen.to_nodes will not work for this since it gives
+		preference to nodes in build.
+		"""
+		if isinstance(lst, Node.Node):
+			if not lst.is_src():
+				raise Errors.WafError('buildcopy: node %s is not in src'%lst)
+			if not os.path.isfile(lst.abspath()):
+				raise Errors.WafError('buildcopy: Cannot copy directory %s (unsupported action)'%lst)
+			return lst
+
+		if isinstance(lst, str):
+			lst = [x for x in Utils.split_path(lst) if x and x != '.']
+
+		node = self.bld.path.get_src().search_node(lst)
+		if node:
+			if not os.path.isfile(node.abspath()):
+				raise Errors.WafError('buildcopy: Cannot copy directory %s (unsupported action)'%node)
+			return node
+
+		node = self.bld.path.get_src().find_node(lst)
+		if node:
+			if not os.path.isfile(node.abspath()):
+				raise Errors.WafError('buildcopy: Cannot copy directory %s (unsupported action)'%node)
+			return node
+		raise Errors.WafError('buildcopy: File not found in src: %s'%os.path.join(*lst))
+
+	nodes = [ to_src_nodes(n) for n in getattr(self, 'buildcopy_source', getattr(self, 'source', [])) ]
+	node_pairs = [(n, n.get_bld()) for n in nodes]
+	self.create_task('buildcopy', [n[0] for n in node_pairs], [n[1] for n in node_pairs], node_pairs=node_pairs)
+
+
+class buildcopy(Task.Task):
+	"""
+	Copy for each pair `n` in `node_pairs`: n[0] -> n[1].
+
+	Attribute `node_pairs` should contain a list of tuples describing source and target:
+
+		node_pairs = [(in, out), ...]
+
+	"""
+	color = 'PINK'
+
+	def keyword(self):
+		return 'Copying'
+
+	def run(self):
+		for f,t in self.node_pairs:
+			t.parent.mkdir()
+			shutil.copy2(f.abspath(), t.abspath())
diff --git a/waflib/extras/c_dumbpreproc.py b/waflib/extras/c_dumbpreproc.py
new file mode 100644
index 00000000..ce9e1a40
--- /dev/null
+++ b/waflib/extras/c_dumbpreproc.py
@@ -0,0 +1,72 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2010 (ita)
+
+"""
+Dumb C/C++ preprocessor for finding dependencies
+
+It will look at all include files it can find after removing the comments, so the following
+will always add the dependency on both "a.h" and "b.h"::
+
+	#include "a.h"
+	#ifdef B
+		#include "b.h"
+	#endif
+	int main() {
+		return 0;
+	}
+
+To use::
+
+	def configure(conf):
+		conf.load('compiler_c')
+		conf.load('c_dumbpreproc')
+"""
+
+import re
+from waflib.Tools import c_preproc
+
+re_inc = re.compile(
+	'^[ \t]*(#|%:)[ \t]*(include)[ \t]*[<"](.*)[>"]\r*$',
+	re.IGNORECASE | re.MULTILINE)
+
+def lines_includes(node):
+	code = node.read()
+	if c_preproc.use_trigraphs:
+		for (a, b) in c_preproc.trig_def:
+			code = code.split(a).join(b)
+	code = c_preproc.re_nl.sub('', code)
+	code = c_preproc.re_cpp.sub(c_preproc.repl, code)
+	return [(m.group(2), m.group(3)) for m in re.finditer(re_inc, code)]
+
+parser = c_preproc.c_parser
+class dumb_parser(parser):
+	def addlines(self, node):
+		if node in self.nodes[:-1]:
+			return
+		self.currentnode_stack.append(node.parent)
+
+		# Avoid reading the same files again
+		try:
+			lines = self.parse_cache[node]
+		except KeyError:
+			lines = self.parse_cache[node] = lines_includes(node)
+
+		self.lines = lines + [(c_preproc.POPFILE, '')] +  self.lines
+
+	def start(self, node, env):
+		try:
+			self.parse_cache = node.ctx.parse_cache
+		except AttributeError:
+			self.parse_cache = node.ctx.parse_cache = {}
+
+		self.addlines(node)
+		while self.lines:
+			(x, y) = self.lines.pop(0)
+			if x == c_preproc.POPFILE:
+				self.currentnode_stack.pop()
+				continue
+			self.tryfind(y)
+
+c_preproc.c_parser = dumb_parser
+
diff --git a/waflib/extras/c_emscripten.py b/waflib/extras/c_emscripten.py
new file mode 100644
index 00000000..e1ac494f
--- /dev/null
+++ b/waflib/extras/c_emscripten.py
@@ -0,0 +1,87 @@
+#!/usr/bin/env python
+# -*- coding: utf-8 vi:ts=4:noexpandtab
+
+import subprocess, shlex, sys
+
+from waflib.Tools import ccroot, gcc, gxx
+from waflib.Configure import conf
+from waflib.TaskGen import after_method, feature
+
+from waflib.Tools.compiler_c import c_compiler
+from waflib.Tools.compiler_cxx import cxx_compiler
+
+for supported_os in ('linux', 'darwin', 'gnu', 'aix'):
+	c_compiler[supported_os].append('c_emscripten')
+	cxx_compiler[supported_os].append('c_emscripten')
+
+
+@conf
+def get_emscripten_version(conf, cc):
+	"""
+	Emscripten doesn't support processing '-' like clang/gcc
+	"""
+
+	dummy = conf.cachedir.parent.make_node("waf-emscripten.c")
+	dummy.write("")
+	cmd = cc + ['-dM', '-E', '-x', 'c', dummy.abspath()]
+	env = conf.env.env or None
+	try:
+		p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, env=env)
+		out = p.communicate()[0]
+	except Exception as e:
+		conf.fatal('Could not determine emscripten version %r: %s' % (cmd, e))
+
+	if not isinstance(out, str):
+		out = out.decode(sys.stdout.encoding or 'latin-1')
+
+	k = {}
+	out = out.splitlines()
+	for line in out:
+		lst = shlex.split(line)
+		if len(lst)>2:
+			key = lst[1]
+			val = lst[2]
+			k[key] = val
+
+	if not ('__clang__' in k and 'EMSCRIPTEN' in k):
+		conf.fatal('Could not determine the emscripten compiler version.')
+
+	conf.env.DEST_OS = 'generic'
+	conf.env.DEST_BINFMT = 'elf'
+	conf.env.DEST_CPU = 'asm-js'
+	conf.env.CC_VERSION = (k['__clang_major__'], k['__clang_minor__'], k['__clang_patchlevel__'])
+	return k
+
+@conf
+def find_emscripten(conf):
+	cc = conf.find_program(['emcc'], var='CC')
+	conf.get_emscripten_version(cc)
+	conf.env.CC = cc
+	conf.env.CC_NAME = 'emscripten'
+	cxx = conf.find_program(['em++'], var='CXX')
+	conf.env.CXX = cxx
+	conf.env.CXX_NAME = 'emscripten'
+	conf.find_program(['emar'], var='AR')
+
+def configure(conf):
+	conf.find_emscripten()
+	conf.find_ar()
+	conf.gcc_common_flags()
+	conf.gxx_common_flags()
+	conf.cc_load_tools()
+	conf.cc_add_flags()
+	conf.cxx_load_tools()
+	conf.cxx_add_flags()
+	conf.link_add_flags()
+	conf.env.ARFLAGS = ['rcs']
+	conf.env.cshlib_PATTERN = '%s.js'
+	conf.env.cxxshlib_PATTERN = '%s.js'
+	conf.env.cstlib_PATTERN = '%s.a'
+	conf.env.cxxstlib_PATTERN = '%s.a'
+	conf.env.cprogram_PATTERN = '%s.html'
+	conf.env.cxxprogram_PATTERN = '%s.html'
+	conf.env.CXX_TGT_F           = ['-c', '-o', '']
+	conf.env.CC_TGT_F            = ['-c', '-o', '']
+	conf.env.CXXLNK_TGT_F        = ['-o', '']
+	conf.env.CCLNK_TGT_F         = ['-o', '']
+	conf.env.append_value('LINKFLAGS',['-Wl,--enable-auto-import'])
diff --git a/waflib/extras/c_nec.py b/waflib/extras/c_nec.py
index 87e0c055..96bfae4f 100644
--- a/waflib/extras/c_nec.py
+++ b/waflib/extras/c_nec.py
@@ -24,43 +24,46 @@ def find_sxc(conf):
 
 @conf
 def get_sxc_version(conf, fc):
-		version_re = re.compile(r"C\+\+/SX\s*Version\s*(?P<major>\d*)\.(?P<minor>\d*)", re.I).search
-		cmd = fc + ['-V']
-		p = Utils.subprocess.Popen(cmd, stdin=False, stdout=Utils.subprocess.PIPE, stderr=Utils.subprocess.PIPE, env=None)
-		out, err = p.communicate()
+	version_re = re.compile(r"C\+\+/SX\s*Version\s*(?P<major>\d*)\.(?P<minor>\d*)", re.I).search
+	cmd = fc + ['-V']
+	p = Utils.subprocess.Popen(cmd, stdin=False, stdout=Utils.subprocess.PIPE, stderr=Utils.subprocess.PIPE, env=None)
+	out, err = p.communicate()
 
-		if out: match = version_re(out)
-		else: match = version_re(err)
-		if not match:
-				conf.fatal('Could not determine the NEC C compiler version.')
-		k = match.groupdict()
-		conf.env['C_VERSION'] = (k['major'], k['minor'])
+	if out:
+		match = version_re(out)
+	else:
+		match = version_re(err)
+	if not match:
+		conf.fatal('Could not determine the NEC C compiler version.')
+	k = match.groupdict()
+	conf.env['C_VERSION'] = (k['major'], k['minor'])
 
 @conf
 def sxc_common_flags(conf):
-        v=conf.env
-        v['CC_SRC_F']=[]
-        v['CC_TGT_F']=['-c','-o']
-        if not v['LINK_CC']:v['LINK_CC']=v['CC']
-        v['CCLNK_SRC_F']=[]
-        v['CCLNK_TGT_F']=['-o']
-        v['CPPPATH_ST']='-I%s'
-        v['DEFINES_ST']='-D%s'
-        v['LIB_ST']='-l%s'
-        v['LIBPATH_ST']='-L%s'
-        v['STLIB_ST']='-l%s'
-        v['STLIBPATH_ST']='-L%s'
-        v['RPATH_ST']=''
-        v['SONAME_ST']=[]
-        v['SHLIB_MARKER']=[]
-        v['STLIB_MARKER']=[]
-        v['LINKFLAGS_cprogram']=['']
-        v['cprogram_PATTERN']='%s'
-        v['CFLAGS_cshlib']=['-fPIC']
-        v['LINKFLAGS_cshlib']=['']
-        v['cshlib_PATTERN']='lib%s.so'
-        v['LINKFLAGS_cstlib']=[]
-        v['cstlib_PATTERN']='lib%s.a'
+	v=conf.env
+	v['CC_SRC_F']=[]
+	v['CC_TGT_F']=['-c','-o']
+	if not v['LINK_CC']:
+		v['LINK_CC']=v['CC']
+	v['CCLNK_SRC_F']=[]
+	v['CCLNK_TGT_F']=['-o']
+	v['CPPPATH_ST']='-I%s'
+	v['DEFINES_ST']='-D%s'
+	v['LIB_ST']='-l%s'
+	v['LIBPATH_ST']='-L%s'
+	v['STLIB_ST']='-l%s'
+	v['STLIBPATH_ST']='-L%s'
+	v['RPATH_ST']=''
+	v['SONAME_ST']=[]
+	v['SHLIB_MARKER']=[]
+	v['STLIB_MARKER']=[]
+	v['LINKFLAGS_cprogram']=['']
+	v['cprogram_PATTERN']='%s'
+	v['CFLAGS_cshlib']=['-fPIC']
+	v['LINKFLAGS_cshlib']=['']
+	v['cshlib_PATTERN']='lib%s.so'
+	v['LINKFLAGS_cstlib']=[]
+	v['cstlib_PATTERN']='lib%s.a'
 
 def configure(conf):
 	conf.find_sxc()
diff --git a/waflib/extras/cabal.py b/waflib/extras/cabal.py
new file mode 100644
index 00000000..e10a0d11
--- /dev/null
+++ b/waflib/extras/cabal.py
@@ -0,0 +1,152 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Anton Feldmann, 2012
+# "Base for cabal"
+
+from waflib import Task, Utils
+from waflib.TaskGen import extension
+from waflib.Utils import threading
+from shutil import rmtree
+
+lock = threading.Lock()
+registering = False
+
+def configure(self):
+    self.find_program('cabal', var='CABAL')
+    self.find_program('ghc-pkg', var='GHCPKG')
+    pkgconfd = self.bldnode.abspath() + '/package.conf.d'
+    self.env.PREFIX = self.bldnode.abspath() + '/dist'
+    self.env.PKGCONFD = pkgconfd
+    if self.root.find_node(pkgconfd + '/package.cache'):
+        self.msg('Using existing package database', pkgconfd, color='CYAN')
+    else:
+        pkgdir = self.root.find_dir(pkgconfd)
+        if pkgdir:
+            self.msg('Deleting corrupt package database', pkgdir.abspath(), color ='RED')
+            rmtree(pkgdir.abspath())
+            pkgdir = None
+
+        self.cmd_and_log(self.env.GHCPKG + ['init', pkgconfd])
+        self.msg('Created package database', pkgconfd, color = 'YELLOW' if pkgdir else 'GREEN')
+
+@extension('.cabal')
+def process_cabal(self, node):
+    out_dir_node = self.bld.root.find_dir(self.bld.out_dir)
+    package_node = node.change_ext('.package')
+    package_node = out_dir_node.find_or_declare(package_node.name)
+    build_node   = node.parent.get_bld()
+    build_path   = build_node.abspath()
+    config_node  = build_node.find_or_declare('setup-config')
+    inplace_node = build_node.find_or_declare('package.conf.inplace')
+
+    config_task = self.create_task('cabal_configure', node)
+    config_task.cwd = node.parent.abspath()
+    config_task.depends_on = getattr(self, 'depends_on', '')
+    config_task.build_path = build_path
+    config_task.set_outputs(config_node)
+
+    build_task = self.create_task('cabal_build', config_node)
+    build_task.cwd = node.parent.abspath()
+    build_task.build_path = build_path
+    build_task.set_outputs(inplace_node)
+
+    copy_task = self.create_task('cabal_copy', inplace_node)
+    copy_task.cwd = node.parent.abspath()
+    copy_task.depends_on = getattr(self, 'depends_on', '')
+    copy_task.build_path = build_path
+
+    last_task = copy_task
+    task_list = [config_task, build_task, copy_task]
+
+    if (getattr(self, 'register', False)):
+        register_task = self.create_task('cabal_register', inplace_node)
+        register_task.cwd = node.parent.abspath()
+        register_task.set_run_after(copy_task)
+        register_task.build_path = build_path
+
+        pkgreg_task = self.create_task('ghcpkg_register', inplace_node)
+        pkgreg_task.cwd = node.parent.abspath()
+        pkgreg_task.set_run_after(register_task)
+        pkgreg_task.build_path = build_path
+
+        last_task = pkgreg_task
+        task_list += [register_task, pkgreg_task]
+
+    touch_task = self.create_task('cabal_touch', inplace_node)
+    touch_task.set_run_after(last_task)
+    touch_task.set_outputs(package_node)
+    touch_task.build_path = build_path
+
+    task_list += [touch_task]
+
+    return task_list
+
+def get_all_src_deps(node):
+    hs_deps = node.ant_glob('**/*.hs')
+    hsc_deps = node.ant_glob('**/*.hsc')
+    lhs_deps = node.ant_glob('**/*.lhs')
+    c_deps = node.ant_glob('**/*.c')
+    cpp_deps = node.ant_glob('**/*.cpp')
+    proto_deps = node.ant_glob('**/*.proto')
+    return sum([hs_deps, hsc_deps, lhs_deps, c_deps, cpp_deps, proto_deps], [])
+
+class Cabal(Task.Task):
+    def scan(self):
+        return (get_all_src_deps(self.generator.path), ())
+
+class cabal_configure(Cabal):
+    run_str = '${CABAL} configure -v0 --prefix=${PREFIX} --global --user --package-db=${PKGCONFD} --builddir=${tsk.build_path}'
+    shell = True
+
+    def scan(self):
+        out_node = self.generator.bld.root.find_dir(self.generator.bld.out_dir)
+        deps = [out_node.find_or_declare(dep).change_ext('.package') for dep in Utils.to_list(self.depends_on)]
+        return (deps, ())
+
+class cabal_build(Cabal):
+    run_str = '${CABAL} build -v1 --builddir=${tsk.build_path}/'
+    shell = True
+
+class cabal_copy(Cabal):
+    run_str = '${CABAL} copy -v0 --builddir=${tsk.build_path}'
+    shell = True
+
+class cabal_register(Cabal):
+    run_str = '${CABAL} register -v0 --gen-pkg-config=${tsk.build_path}/pkg.config --builddir=${tsk.build_path}'
+    shell = True
+
+class ghcpkg_register(Cabal):
+    run_str = '${GHCPKG} update -v0 --global --user --package-conf=${PKGCONFD} ${tsk.build_path}/pkg.config'
+    shell = True
+
+    def runnable_status(self):
+        global lock, registering
+
+        val = False 
+        lock.acquire()
+        val = registering
+        lock.release()
+
+        if val:
+            return Task.ASK_LATER
+
+        ret = Task.Task.runnable_status(self)
+        if ret == Task.RUN_ME:
+            lock.acquire()
+            registering = True
+            lock.release()
+
+        return ret
+
+    def post_run(self):
+        global lock, registering
+
+        lock.acquire()
+        registering = False
+        lock.release()
+
+        return Task.Task.post_run(self)
+
+class cabal_touch(Cabal):
+    run_str = 'touch ${TGT}'
+
diff --git a/waflib/extras/cfg_altoptions.py b/waflib/extras/cfg_altoptions.py
new file mode 100644
index 00000000..47b1189f
--- /dev/null
+++ b/waflib/extras/cfg_altoptions.py
@@ -0,0 +1,110 @@
+#!/usr/bin/python
+# -*- coding: utf-8 -*-
+# Tool to extend c_config.check_cfg()
+
+__author__ = __maintainer__ = "Jérôme Carretero <cJ-waf@zougloub.eu>"
+__copyright__ = "Jérôme Carretero, 2014"
+
+"""
+
+This tool allows to work around the absence of ``*-config`` programs
+on systems, by keeping the same clean configuration syntax but inferring
+values or permitting their modification via the options interface.
+
+Note that pkg-config can also support setting ``PKG_CONFIG_PATH``,
+so you can put custom files in a folder containing new .pc files.
+This tool could also be implemented by taking advantage of this fact.
+
+Usage::
+
+   def options(opt):
+     opt.load('c_config_alt')
+     opt.add_package_option('package')
+
+   def configure(cfg):
+     conf.load('c_config_alt')
+     conf.check_cfg(...)
+
+Known issues:
+
+- Behavior with different build contexts...
+
+"""
+
+import os
+import functools
+from waflib import Configure, Options, Errors
+
+def name_to_dest(x):
+	return x.lower().replace('-', '_')
+
+
+def options(opt):
+	def x(opt, param):
+		dest = name_to_dest(param)
+		gr = opt.get_option_group("configure options")
+		gr.add_option('--%s-root' % dest,
+		 help="path containing include and lib subfolders for %s" \
+		  % param,
+		)
+
+	opt.add_package_option = functools.partial(x, opt)
+
+
+check_cfg_old = getattr(Configure.ConfigurationContext, 'check_cfg')
+
+@Configure.conf
+def check_cfg(conf, *k, **kw):
+	if k:
+		lst = k[0].split()
+		kw['package'] = lst[0]
+		kw['args'] = ' '.join(lst[1:])
+
+	if not 'package' in kw:
+		return check_cfg_old(conf, **kw)
+
+	package = kw['package']
+
+	package_lo = name_to_dest(package)
+	package_hi = package.upper().replace('-', '_') # TODO FIXME
+	package_hi = kw.get('uselib_store', package_hi)
+
+	def check_folder(path, name):
+		try:
+			assert os.path.isdir(path)
+		except AssertionError:
+			raise Errors.ConfigurationError(
+				"%s_%s (%s) is not a folder!" \
+				% (package_lo, name, path))
+		return path
+
+	root = getattr(Options.options, '%s_root' % package_lo, None)
+
+	if root is None:
+		return check_cfg_old(conf, **kw)
+	else:
+		def add_manual_var(k, v):
+			conf.start_msg('Adding for %s a manual var' % (package))
+			conf.env["%s_%s" % (k, package_hi)] = v
+			conf.end_msg("%s = %s" % (k, v))
+
+
+		check_folder(root, 'root')
+
+		pkg_inc = check_folder(os.path.join(root, "include"), 'inc')
+		add_manual_var('INCLUDES', [pkg_inc])
+		pkg_lib = check_folder(os.path.join(root, "lib"), 'libpath')
+		add_manual_var('LIBPATH', [pkg_lib])
+		add_manual_var('LIB', [package])
+
+		for x in kw.get('manual_deps', []):
+			for k, v in sorted(conf.env.get_merged_dict().items()):
+				if k.endswith('_%s' % x):
+					k = k.replace('_%s' % x, '')
+					conf.start_msg('Adding for %s a manual dep' \
+					 %(package))
+					conf.env["%s_%s" % (k, package_hi)] += v
+					conf.end_msg('%s += %s' % (k, v))
+
+		return True
+
diff --git a/waflib/extras/clang_compilation_database.py b/waflib/extras/clang_compilation_database.py
new file mode 100644
index 00000000..4d9b5e27
--- /dev/null
+++ b/waflib/extras/clang_compilation_database.py
@@ -0,0 +1,85 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Christoph Koke, 2013
+
+"""
+Writes the c and cpp compile commands into build/compile_commands.json
+see http://clang.llvm.org/docs/JSONCompilationDatabase.html
+
+Usage:
+
+    def configure(conf):
+        conf.load('compiler_cxx')
+        ...
+        conf.load('clang_compilation_database')
+"""
+
+import sys, os, json, shlex, pipes
+from waflib import Logs, TaskGen, Task
+
+Task.Task.keep_last_cmd = True
+
+@TaskGen.feature('c', 'cxx')
+@TaskGen.after_method('process_use')
+def collect_compilation_db_tasks(self):
+	"Add a compilation database entry for compiled tasks"
+	try:
+		clang_db = self.bld.clang_compilation_database_tasks
+	except AttributeError:
+		clang_db = self.bld.clang_compilation_database_tasks = []
+		self.bld.add_post_fun(write_compilation_database)
+
+	tup = tuple(y for y in [Task.classes.get(x) for x in ('c', 'cxx')] if y)
+	for task in getattr(self, 'compiled_tasks', []):
+		if isinstance(task, tup):
+			clang_db.append(task)
+
+def write_compilation_database(ctx):
+	"Write the clang compilation database as JSON"
+	database_file = ctx.bldnode.make_node('compile_commands.json')
+	Logs.info('Build commands will be stored in %s', database_file.path_from(ctx.path))
+	try:
+		root = json.load(database_file)
+	except IOError:
+		root = []
+	clang_db = dict((x['file'], x) for x in root)
+	for task in getattr(ctx, 'clang_compilation_database_tasks', []):
+		try:
+			cmd = task.last_cmd
+		except AttributeError:
+			continue
+		directory = getattr(task, 'cwd', ctx.variant_dir)
+		f_node = task.inputs[0]
+		filename = os.path.relpath(f_node.abspath(), directory)
+		entry = {
+			"directory": directory,
+			"arguments": cmd,
+			"file": filename,
+		}
+		clang_db[filename] = entry
+	root = list(clang_db.values())
+	database_file.write(json.dumps(root, indent=2))
+
+# Override the runnable_status function to do a dummy/dry run when the file doesn't need to be compiled.
+# This will make sure compile_commands.json is always fully up to date.
+# Previously you could end up with a partial compile_commands.json if the build failed.
+for x in ('c', 'cxx'):
+	if x not in Task.classes:
+		continue
+
+	t = Task.classes[x]
+
+	def runnable_status(self):
+		def exec_command(cmd, **kw):
+			pass
+
+		run_status = self.old_runnable_status()
+		if run_status == Task.SKIP_ME:
+			setattr(self, 'old_exec_command', getattr(self, 'exec_command', None))
+			setattr(self, 'exec_command', exec_command)
+			self.run()
+			setattr(self, 'exec_command', getattr(self, 'old_exec_command', None))
+		return run_status
+
+	setattr(t, 'old_runnable_status', getattr(t, 'runnable_status', None))
+	setattr(t, 'runnable_status', runnable_status)
diff --git a/waflib/extras/codelite.py b/waflib/extras/codelite.py
new file mode 100644
index 00000000..523302c0
--- /dev/null
+++ b/waflib/extras/codelite.py
@@ -0,0 +1,875 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# CodeLite Project
+# Christian Klein (chrikle@berlios.de)
+# Created: Jan 2012
+# As templete for this file I used the msvs.py
+# I hope this template will work proper
+
+"""
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+
+3. The name of the author may not be used to endorse or promote products
+   derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
+IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
+INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
+STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
+IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+POSSIBILITY OF SUCH DAMAGE.
+"""
+
+"""
+ 
+
+To add this tool to your project:
+def options(conf):
+        opt.load('codelite')
+
+It can be a good idea to add the sync_exec tool too.
+
+To generate solution files:
+$ waf configure codelite
+
+To customize the outputs, provide subclasses in your wscript files:
+
+from waflib.extras import codelite
+class vsnode_target(codelite.vsnode_target):
+        def get_build_command(self, props):
+                # likely to be required
+                return "waf.bat build"
+        def collect_source(self):
+                # likely to be required
+                ...
+class codelite_bar(codelite.codelite_generator):
+        def init(self):
+                codelite.codelite_generator.init(self)
+                self.vsnode_target = vsnode_target
+
+The codelite class re-uses the same build() function for reading the targets (task generators),
+you may therefore specify codelite settings on the context object:
+
+def build(bld):
+        bld.codelite_solution_name = 'foo.workspace'
+        bld.waf_command = 'waf.bat'
+        bld.projects_dir = bld.srcnode.make_node('')
+        bld.projects_dir.mkdir()
+
+
+ASSUMPTIONS:
+* a project can be either a directory or a target, project files are written only for targets that have source files
+* each project is a vcxproj file, therefore the project uuid needs only to be a hash of the absolute path
+"""
+
+import os, re, sys
+import uuid # requires python 2.5
+from waflib.Build import BuildContext
+from waflib import Utils, TaskGen, Logs, Task, Context, Node, Options
+
+HEADERS_GLOB = '**/(*.h|*.hpp|*.H|*.inl)'
+
+PROJECT_TEMPLATE = r'''<?xml version="1.0" encoding="utf-8"?>
+<CodeLite_Project Name="${project.name}" InternalType="Library">
+  <Plugins>
+    <Plugin Name="qmake">
+      <![CDATA[00010001N0005Release000000000000]]>
+    </Plugin>
+  </Plugins>
+  <Description/>
+  <Dependencies/>
+  <VirtualDirectory Name="src">
+  ${for x in project.source}  
+  ${if (project.get_key(x)=="sourcefile")}
+  <File Name="${x.abspath()}"/>
+  ${endif}
+  ${endfor}  
+  </VirtualDirectory>
+  <VirtualDirectory Name="include">  
+  ${for x in project.source}
+  ${if (project.get_key(x)=="headerfile")}
+  <File Name="${x.abspath()}"/>
+  ${endif}
+  ${endfor}
+  </VirtualDirectory>  
+  <Settings Type="Dynamic Library">
+    <GlobalSettings>
+      <Compiler Options="" C_Options="">
+        <IncludePath Value="."/>
+      </Compiler>
+      <Linker Options="">
+        <LibraryPath Value="."/>
+      </Linker>
+      <ResourceCompiler Options=""/>
+    </GlobalSettings>
+    <Configuration Name="Release" CompilerType="gnu gcc" ReleasegerType="GNU gdb Releaseger" Type="Dynamic Library" BuildCmpWithGlobalSettings="append" BuildLnkWithGlobalSettings="append" BuildResWithGlobalSettings="append">
+      <Compiler Options="" C_Options="" Required="yes" PreCompiledHeader="" PCHInCommandLine="no" UseDifferentPCHFlags="no" PCHFlags="">
+        <IncludePath Value="."/>
+        <IncludePath Value="."/>
+      </Compiler>
+      <Linker Options="" Required="yes">
+        <LibraryPath Value=""/>
+      </Linker>
+      <ResourceCompiler Options="" Required="no"/>
+      <General OutputFile="${xml:project.build_properties[0].output_file}" IntermediateDirectory="" Command="" CommandArguments="" PauseExecWhenProcTerminates="yes"/>
+      <Environment EnvVarSetName="&lt;Use Defaults&gt;" DbgSetName="&lt;Use Defaults&gt;">
+        <![CDATA[]]>
+      </Environment>
+      <Releaseger IsRemote="no" RemoteHostName="" RemoteHostPort="" ReleasegerPath="">
+        <PostConnectCommands/>
+        <StartupCommands/>
+      </Releaseger>
+      <PreBuild/>
+      <PostBuild/>
+      <CustomBuild Enabled="yes">
+        $b = project.build_properties[0]}
+        <RebuildCommand>${xml:project.get_rebuild_command(project.build_properties[0])}</RebuildCommand>
+        <CleanCommand>${xml:project.get_clean_command(project.build_properties[0])}</CleanCommand>
+        <BuildCommand>${xml:project.get_build_command(project.build_properties[0])}</BuildCommand> 
+        <Target Name="Install">${xml:project.get_install_command(project.build_properties[0])}</Target>
+        <Target Name="Build and Install">${xml:project.get_build_and_install_command(project.build_properties[0])}</Target>        
+        <Target Name="Build All">${xml:project.get_build_all_command(project.build_properties[0])}</Target>
+        <Target Name="Rebuild All">${xml:project.get_rebuild_all_command(project.build_properties[0])}</Target>
+        <Target Name="Clean All">${xml:project.get_clean_all_command(project.build_properties[0])}</Target>
+        <Target Name="Build and Install All">${xml:project.get_build_and_install_all_command(project.build_properties[0])}</Target>
+        <PreprocessFileCommand/>
+        <SingleFileCommand/>
+        <MakefileGenerationCommand/>
+        <ThirdPartyToolName>None</ThirdPartyToolName>
+        <WorkingDirectory/>
+      </CustomBuild>
+      <AdditionalRules>
+        <CustomPostBuild/>
+        <CustomPreBuild/>
+      </AdditionalRules>
+      <Completion>
+        <ClangCmpFlags/>
+        <ClangPP/>
+        <SearchPaths/>
+      </Completion>
+    </Configuration>
+    <Configuration Name="Release" CompilerType="gnu gcc" ReleasegerType="GNU gdb Releaseger" Type="" BuildCmpWithGlobalSettings="append" BuildLnkWithGlobalSettings="append" BuildResWithGlobalSettings="append">
+      <Compiler Options="" C_Options="" Required="yes" PreCompiledHeader="" PCHInCommandLine="no" UseDifferentPCHFlags="no" PCHFlags="">
+        <IncludePath Value="."/>
+      </Compiler>
+      <Linker Options="" Required="yes"/>
+      <ResourceCompiler Options="" Required="no"/>
+      <General OutputFile="" IntermediateDirectory="./Release" Command="" CommandArguments="" UseSeparateReleaseArgs="no" ReleaseArguments="" WorkingDirectory="$(IntermediateDirectory)" PauseExecWhenProcTerminates="yes"/>
+      <Environment EnvVarSetName="&lt;Use Defaults&gt;" DbgSetName="&lt;Use Defaults&gt;">
+        <![CDATA[
+      
+      
+      
+      ]]>
+      </Environment>
+      <Releaseger IsRemote="no" RemoteHostName="" RemoteHostPort="" ReleasegerPath="">
+        <PostConnectCommands/>
+        <StartupCommands/>
+      </Releaseger>
+      <PreBuild/>
+      <PostBuild/>
+      <CustomBuild Enabled="no">
+        <RebuildCommand/>
+        <CleanCommand/>
+        <BuildCommand/>
+        <PreprocessFileCommand/>
+        <SingleFileCommand/>
+        <MakefileGenerationCommand/>
+        <ThirdPartyToolName/>
+        <WorkingDirectory/>
+      </CustomBuild>
+      <AdditionalRules>
+        <CustomPostBuild/>
+        <CustomPreBuild/>
+      </AdditionalRules>
+      <Completion>
+        <ClangCmpFlags/>
+        <ClangPP/>
+        <SearchPaths/>
+      </Completion>
+    </Configuration>
+  </Settings>
+</CodeLite_Project>'''
+
+
+
+
+SOLUTION_TEMPLATE = '''<?xml version="1.0" encoding="utf-8"?>
+<CodeLite_Workspace Name="${getattr(project, 'codelite_solution_name', None)[:-10]}" Database="./${getattr(project, 'codelite_solution_name', None)[:-10]}.tags">
+${for p in project.all_projects}
+  <Project Name = "${p.name}" Path = "${p.title}" Active="No"/>
+${endfor}
+  <BuildMatrix>
+    <WorkspaceConfiguration Name="Release" Selected="yes">
+${for p in project.all_projects}
+      <Project Name="${p.name}" ConfigName="Release"/>        
+${endfor}
+    </WorkspaceConfiguration>        
+  </BuildMatrix>
+</CodeLite_Workspace>'''
+
+
+
+COMPILE_TEMPLATE = '''def f(project):
+        lst = []
+        def xml_escape(value):
+                return value.replace("&", "&amp;").replace('"', "&quot;").replace("'", "&apos;").replace("<", "&lt;").replace(">", "&gt;")
+
+        %s
+
+        #f = open('cmd.txt', 'w')
+        #f.write(str(lst))
+        #f.close()
+        return ''.join(lst)
+'''
+reg_act = re.compile(r"(?P<backslash>\\)|(?P<dollar>\$\$)|(?P<subst>\$\{(?P<code>[^}]*?)\})", re.M)
+def compile_template(line):
+        """
+        Compile a template expression into a python function (like jsps, but way shorter)
+        """
+        extr = []
+        def repl(match):
+                g = match.group
+                if g('dollar'):
+                        return "$"
+                elif g('backslash'):
+                        return "\\"
+                elif g('subst'):
+                        extr.append(g('code'))
+                        return "<<|@|>>"
+                return None
+
+        line2 = reg_act.sub(repl, line)
+        params = line2.split('<<|@|>>')
+        assert(extr)
+
+
+        indent = 0
+        buf = []
+        app = buf.append
+
+        def app(txt):
+                buf.append(indent * '\t' + txt)
+
+        for x in range(len(extr)):
+                if params[x]:
+                        app("lst.append(%r)" % params[x])
+
+                f = extr[x]
+                if f.startswith(('if', 'for')):
+                        app(f + ':')
+                        indent += 1
+                elif f.startswith('py:'):
+                        app(f[3:])
+                elif f.startswith(('endif', 'endfor')):
+                        indent -= 1
+                elif f.startswith(('else', 'elif')):
+                        indent -= 1
+                        app(f + ':')
+                        indent += 1
+                elif f.startswith('xml:'):
+                        app('lst.append(xml_escape(%s))' % f[4:])
+                else:
+                        #app('lst.append((%s) or "cannot find %s")' % (f, f))
+                        app('lst.append(%s)' % f)
+
+        if extr:
+                if params[-1]:
+                        app("lst.append(%r)" % params[-1])
+
+        fun = COMPILE_TEMPLATE % "\n\t".join(buf)
+        #print(fun)
+        return Task.funex(fun)
+
+
+re_blank = re.compile('(\n|\r|\\s)*\n', re.M)
+def rm_blank_lines(txt):
+        txt = re_blank.sub('\r\n', txt)
+        return txt
+
+BOM = '\xef\xbb\xbf'
+try:
+        BOM = bytes(BOM, 'latin-1') # python 3
+except (TypeError, NameError):
+        pass
+
+def stealth_write(self, data, flags='wb'):
+        try:
+                unicode
+        except NameError:
+                data = data.encode('utf-8') # python 3
+        else:
+                data = data.decode(sys.getfilesystemencoding(), 'replace')
+                data = data.encode('utf-8')
+
+        if self.name.endswith('.project'):
+                data = BOM + data
+
+        try:
+                txt = self.read(flags='rb')
+                if txt != data:
+                        raise ValueError('must write')
+        except (IOError, ValueError):
+                self.write(data, flags=flags)
+        else:
+                Logs.debug('codelite: skipping %r', self)
+Node.Node.stealth_write = stealth_write
+
+re_quote = re.compile("[^a-zA-Z0-9-]")
+def quote(s):
+        return re_quote.sub("_", s)
+
+def xml_escape(value):
+        return value.replace("&", "&amp;").replace('"', "&quot;").replace("'", "&apos;").replace("<", "&lt;").replace(">", "&gt;")
+
+def make_uuid(v, prefix = None):
+        """
+        simple utility function
+        """
+        if isinstance(v, dict):
+                keys = list(v.keys())
+                keys.sort()
+                tmp = str([(k, v[k]) for k in keys])
+        else:
+                tmp = str(v)
+        d = Utils.md5(tmp.encode()).hexdigest().upper()
+        if prefix:
+                d = '%s%s' % (prefix, d[8:])
+        gid = uuid.UUID(d, version = 4)
+        return str(gid).upper()
+
+def diff(node, fromnode):
+        # difference between two nodes, but with "(..)" instead of ".."
+        c1 = node
+        c2 = fromnode
+
+        c1h = c1.height()
+        c2h = c2.height()
+
+        lst = []
+        up = 0
+
+        while c1h > c2h:
+                lst.append(c1.name)
+                c1 = c1.parent
+                c1h -= 1
+
+        while c2h > c1h:
+                up += 1
+                c2 = c2.parent
+                c2h -= 1
+
+        while id(c1) != id(c2):
+                lst.append(c1.name)
+                up += 1
+
+                c1 = c1.parent
+                c2 = c2.parent
+
+        for i in range(up):
+                lst.append('(..)')
+        lst.reverse()
+        return tuple(lst)
+
+class build_property(object):
+        pass
+
+class vsnode(object):
+        """
+        Abstract class representing visual studio elements
+        We assume that all visual studio nodes have a uuid and a parent
+        """
+        def __init__(self, ctx):
+                self.ctx = ctx # codelite context
+                self.name = '' # string, mandatory
+                self.vspath = '' # path in visual studio (name for dirs, absolute path for projects)
+                self.uuid = '' # string, mandatory
+                self.parent = None # parent node for visual studio nesting
+
+        def get_waf(self):
+                """
+                Override in subclasses...
+                """
+                return '%s/%s' % (self.ctx.srcnode.abspath(), getattr(self.ctx, 'waf_command', 'waf'))
+
+        def ptype(self):
+                """
+                Return a special uuid for projects written in the solution file
+                """
+                pass
+
+        def write(self):
+                """
+                Write the project file, by default, do nothing
+                """
+                pass
+
+        def make_uuid(self, val):
+                """
+                Alias for creating uuid values easily (the templates cannot access global variables)
+                """
+                return make_uuid(val)
+
+class vsnode_vsdir(vsnode):
+        """
+        Nodes representing visual studio folders (which do not match the filesystem tree!)
+        """
+        VS_GUID_SOLUTIONFOLDER = "2150E333-8FDC-42A3-9474-1A3956D46DE8"
+        def __init__(self, ctx, uuid, name, vspath=''):
+                vsnode.__init__(self, ctx)
+                self.title = self.name = name
+                self.uuid = uuid
+                self.vspath = vspath or name
+
+        def ptype(self):
+                return self.VS_GUID_SOLUTIONFOLDER
+
+class vsnode_project(vsnode):
+        """
+        Abstract class representing visual studio project elements
+        A project is assumed to be writable, and has a node representing the file to write to
+        """
+        VS_GUID_VCPROJ = "8BC9CEB8-8B4A-11D0-8D11-00A0C91BC942"
+        def ptype(self):
+                return self.VS_GUID_VCPROJ
+
+        def __init__(self, ctx, node):
+                vsnode.__init__(self, ctx)
+                self.path = node
+                self.uuid = make_uuid(node.abspath())
+                self.name = node.name
+                self.title = self.path.abspath()
+                self.source = [] # list of node objects
+                self.build_properties = [] # list of properties (nmake commands, output dir, etc)
+
+        def dirs(self):
+                """
+                Get the list of parent folders of the source files (header files included)
+                for writing the filters
+                """
+                lst = []
+                def add(x):
+                        if x.height() > self.tg.path.height() and x not in lst:
+                                lst.append(x)
+                                add(x.parent)
+                for x in self.source:
+                        add(x.parent)
+                return lst
+
+        def write(self):
+                Logs.debug('codelite: creating %r', self.path)
+                #print "self.name:",self.name
+
+                # first write the project file
+                template1 = compile_template(PROJECT_TEMPLATE)
+                proj_str = template1(self)
+                proj_str = rm_blank_lines(proj_str)
+                self.path.stealth_write(proj_str)
+
+                # then write the filter
+                #template2 = compile_template(FILTER_TEMPLATE)
+                #filter_str = template2(self)
+                #filter_str = rm_blank_lines(filter_str)
+                #tmp = self.path.parent.make_node(self.path.name + '.filters')
+                #tmp.stealth_write(filter_str)
+
+        def get_key(self, node):
+                """
+                required for writing the source files
+                """
+                name = node.name
+                if name.endswith(('.cpp', '.c')):
+                        return 'sourcefile'
+                return 'headerfile'
+
+        def collect_properties(self):
+                """
+                Returns a list of triplet (configuration, platform, output_directory)
+                """
+                ret = []
+                for c in self.ctx.configurations:
+                        for p in self.ctx.platforms:
+                                x = build_property()
+                                x.outdir = ''
+
+                                x.configuration = c
+                                x.platform = p
+
+                                x.preprocessor_definitions = ''
+                                x.includes_search_path = ''
+
+                                # can specify "deploy_dir" too
+                                ret.append(x)
+                self.build_properties = ret
+
+        def get_build_params(self, props):
+                opt = ''
+                return (self.get_waf(), opt)
+
+        def get_build_command(self, props):
+                return "%s build %s" % self.get_build_params(props)
+
+        def get_clean_command(self, props):
+                return "%s clean %s" % self.get_build_params(props)
+
+        def get_rebuild_command(self, props):
+                return "%s clean build %s" % self.get_build_params(props)
+                
+        def get_install_command(self, props):
+                return "%s install %s" % self.get_build_params(props)
+        def get_build_and_install_command(self, props):
+                return "%s build install %s" % self.get_build_params(props)
+                
+        def get_build_and_install_all_command(self, props):
+                return "%s build install" % self.get_build_params(props)[0]
+                
+        def get_clean_all_command(self, props):
+                return "%s clean" % self.get_build_params(props)[0]
+        
+        def get_build_all_command(self, props):
+                return "%s build" % self.get_build_params(props)[0]
+                
+        def get_rebuild_all_command(self, props):
+                return "%s clean build" % self.get_build_params(props)[0]
+
+        def get_filter_name(self, node):
+                lst = diff(node, self.tg.path)
+                return '\\'.join(lst) or '.'
+
+class vsnode_alias(vsnode_project):
+        def __init__(self, ctx, node, name):
+                vsnode_project.__init__(self, ctx, node)
+                self.name = name
+                self.output_file = ''
+
+class vsnode_build_all(vsnode_alias):
+        """
+        Fake target used to emulate the behaviour of "make all" (starting one process by target is slow)
+        This is the only alias enabled by default
+        """
+        def __init__(self, ctx, node, name='build_all_projects'):
+                vsnode_alias.__init__(self, ctx, node, name)
+                self.is_active = True
+
+class vsnode_install_all(vsnode_alias):
+        """
+        Fake target used to emulate the behaviour of "make install"
+        """
+        def __init__(self, ctx, node, name='install_all_projects'):
+                vsnode_alias.__init__(self, ctx, node, name)
+
+        def get_build_command(self, props):
+                return "%s build install %s" % self.get_build_params(props)
+
+        def get_clean_command(self, props):
+                return "%s clean %s" % self.get_build_params(props)
+
+        def get_rebuild_command(self, props):
+                return "%s clean build install %s" % self.get_build_params(props)
+
+class vsnode_project_view(vsnode_alias):
+        """
+        Fake target used to emulate a file system view
+        """
+        def __init__(self, ctx, node, name='project_view'):
+                vsnode_alias.__init__(self, ctx, node, name)
+                self.tg = self.ctx() # fake one, cannot remove
+                self.exclude_files = Node.exclude_regs + '''
+waf-2*
+waf3-2*/**
+.waf-2*
+.waf3-2*/**
+**/*.sdf
+**/*.suo
+**/*.ncb
+**/%s
+                ''' % Options.lockfile
+
+        def collect_source(self):
+                # this is likely to be slow
+                self.source = self.ctx.srcnode.ant_glob('**', excl=self.exclude_files)
+
+        def get_build_command(self, props):
+                params = self.get_build_params(props) + (self.ctx.cmd,)
+                return "%s %s %s" % params
+
+        def get_clean_command(self, props):
+                return ""
+
+        def get_rebuild_command(self, props):
+                return self.get_build_command(props)
+
+class vsnode_target(vsnode_project):
+        """
+        CodeLite project representing a targets (programs, libraries, etc) and bound
+        to a task generator
+        """
+        def __init__(self, ctx, tg):
+                """
+                A project is more or less equivalent to a file/folder
+                """
+                base = getattr(ctx, 'projects_dir', None) or tg.path
+                node = base.make_node(quote(tg.name) + ctx.project_extension) # the project file as a Node
+                vsnode_project.__init__(self, ctx, node)
+                self.name = quote(tg.name)
+                self.tg     = tg  # task generator
+
+        def get_build_params(self, props):
+                """
+                Override the default to add the target name
+                """
+                opt = ''
+                if getattr(self, 'tg', None):
+                        opt += " --targets=%s" % self.tg.name
+                return (self.get_waf(), opt)
+
+        def collect_source(self):
+                tg = self.tg
+                source_files = tg.to_nodes(getattr(tg, 'source', []))
+                include_dirs = Utils.to_list(getattr(tg, 'codelite_includes', []))
+                include_files = []
+                for x in include_dirs:
+                        if isinstance(x, str):
+                                x = tg.path.find_node(x)
+                        if x:
+                                lst = [y for y in x.ant_glob(HEADERS_GLOB, flat=False)]
+                                include_files.extend(lst)
+
+                # remove duplicates
+                self.source.extend(list(set(source_files + include_files)))
+                self.source.sort(key=lambda x: x.abspath())
+
+        def collect_properties(self):
+                """
+                CodeLite projects are associated with platforms and configurations (for building especially)
+                """
+                super(vsnode_target, self).collect_properties()
+                for x in self.build_properties:
+                        x.outdir = self.path.parent.abspath()
+                        x.preprocessor_definitions = ''
+                        x.includes_search_path = ''
+
+                        try:
+                                tsk = self.tg.link_task
+                        except AttributeError:
+                                pass
+                        else:                                
+                                x.output_file = tsk.outputs[0].abspath()
+                                x.preprocessor_definitions = ';'.join(tsk.env.DEFINES)
+                                x.includes_search_path = ';'.join(self.tg.env.INCPATHS)
+
+class codelite_generator(BuildContext):
+        '''generates a CodeLite workspace'''
+        cmd = 'codelite'
+        fun = 'build'
+
+        def init(self):
+                """
+                Some data that needs to be present
+                """
+                if not getattr(self, 'configurations', None):
+                        self.configurations = ['Release'] # LocalRelease, RemoteDebug, etc
+                if not getattr(self, 'platforms', None):
+                        self.platforms = ['Win32']
+                if not getattr(self, 'all_projects', None):
+                        self.all_projects = []
+                if not getattr(self, 'project_extension', None):
+                        self.project_extension = '.project'
+                if not getattr(self, 'projects_dir', None):
+                        self.projects_dir = self.srcnode.make_node('')
+                        self.projects_dir.mkdir()
+
+                # bind the classes to the object, so that subclass can provide custom generators
+                if not getattr(self, 'vsnode_vsdir', None):
+                        self.vsnode_vsdir = vsnode_vsdir
+                if not getattr(self, 'vsnode_target', None):
+                        self.vsnode_target = vsnode_target
+                if not getattr(self, 'vsnode_build_all', None):
+                        self.vsnode_build_all = vsnode_build_all
+                if not getattr(self, 'vsnode_install_all', None):
+                        self.vsnode_install_all = vsnode_install_all
+                if not getattr(self, 'vsnode_project_view', None):
+                        self.vsnode_project_view = vsnode_project_view
+
+                self.numver = '11.00'
+                self.vsver  = '2010'
+
+        def execute(self):
+                """
+                Entry point
+                """
+                self.restore()
+                if not self.all_envs:
+                        self.load_envs()
+                self.recurse([self.run_dir])
+
+                # user initialization
+                self.init()
+
+                # two phases for creating the solution
+                self.collect_projects() # add project objects into "self.all_projects"
+                self.write_files() # write the corresponding project and solution files
+
+        def collect_projects(self):
+                """
+                Fill the list self.all_projects with project objects
+                Fill the list of build targets
+                """
+                self.collect_targets()
+                #self.add_aliases()
+                #self.collect_dirs()
+                default_project = getattr(self, 'default_project', None)
+                def sortfun(x):
+                        if x.name == default_project:
+                                return ''
+                        return getattr(x, 'path', None) and x.path.abspath() or x.name
+                self.all_projects.sort(key=sortfun)
+
+        def write_files(self):
+                """
+                Write the project and solution files from the data collected
+                so far. It is unlikely that you will want to change this
+                """
+                for p in self.all_projects:
+                        p.write()
+
+                # and finally write the solution file
+                node = self.get_solution_node()
+                node.parent.mkdir()
+                Logs.warn('Creating %r', node)
+                #a = dir(self.root)
+                #for b in a:
+                #        print b
+                #print self.group_names
+                #print "Hallo2:   ",self.root.listdir()
+                #print getattr(self, 'codelite_solution_name', None)
+                template1 = compile_template(SOLUTION_TEMPLATE)
+                sln_str = template1(self)
+                sln_str = rm_blank_lines(sln_str)
+                node.stealth_write(sln_str)
+
+        def get_solution_node(self):
+                """
+                The solution filename is required when writing the .vcproj files
+                return self.solution_node and if it does not exist, make one
+                """
+                try:
+                        return self.solution_node
+                except:
+                        pass
+
+                codelite_solution_name = getattr(self, 'codelite_solution_name', None)
+                if not codelite_solution_name:
+                        codelite_solution_name = getattr(Context.g_module, Context.APPNAME, 'project') + '.workspace'
+                        setattr(self, 'codelite_solution_name', codelite_solution_name)
+                if os.path.isabs(codelite_solution_name):
+                        self.solution_node = self.root.make_node(codelite_solution_name)
+                else:
+                        self.solution_node = self.srcnode.make_node(codelite_solution_name)
+                return self.solution_node
+
+        def project_configurations(self):
+                """
+                Helper that returns all the pairs (config,platform)
+                """
+                ret = []
+                for c in self.configurations:
+                        for p in self.platforms:
+                                ret.append((c, p))
+                return ret
+
+        def collect_targets(self):
+                """
+                Process the list of task generators
+                """
+                for g in self.groups:
+                        for tg in g:
+                                if not isinstance(tg, TaskGen.task_gen):
+                                        continue
+
+                                if not hasattr(tg, 'codelite_includes'):
+                                        tg.codelite_includes = tg.to_list(getattr(tg, 'includes', [])) + tg.to_list(getattr(tg, 'export_includes', []))
+                                tg.post()
+                                if not getattr(tg, 'link_task', None):
+                                        continue
+
+                                p = self.vsnode_target(self, tg)
+                                p.collect_source() # delegate this processing
+                                p.collect_properties()                               
+                                self.all_projects.append(p)
+
+        def add_aliases(self):
+                """
+                Add a specific target that emulates the "make all" necessary for Visual studio when pressing F7
+                We also add an alias for "make install" (disabled by default)
+                """
+                base = getattr(self, 'projects_dir', None) or self.tg.path
+
+                node_project = base.make_node('build_all_projects' + self.project_extension) # Node
+                p_build = self.vsnode_build_all(self, node_project)
+                p_build.collect_properties()
+                self.all_projects.append(p_build)
+
+                node_project = base.make_node('install_all_projects' + self.project_extension) # Node
+                p_install = self.vsnode_install_all(self, node_project)
+                p_install.collect_properties()
+                self.all_projects.append(p_install)
+
+                node_project = base.make_node('project_view' + self.project_extension) # Node
+                p_view = self.vsnode_project_view(self, node_project)
+                p_view.collect_source()
+                p_view.collect_properties()
+                self.all_projects.append(p_view)
+
+                n = self.vsnode_vsdir(self, make_uuid(self.srcnode.abspath() + 'build_aliases'), "build_aliases")
+                p_build.parent = p_install.parent = p_view.parent = n
+                self.all_projects.append(n)
+
+        def collect_dirs(self):
+                """
+                Create the folder structure in the CodeLite project view
+                """
+                seen = {}
+                def make_parents(proj):
+                        # look at a project, try to make a parent
+                        if getattr(proj, 'parent', None):
+                                # aliases already have parents
+                                return
+                        x = proj.iter_path
+                        if x in seen:
+                                proj.parent = seen[x]
+                                return
+
+                        # There is not vsnode_vsdir for x.
+                        # So create a project representing the folder "x"
+                        n = proj.parent = seen[x] = self.vsnode_vsdir(self, make_uuid(x.abspath()), x.name)
+                        n.iter_path = x.parent
+                        self.all_projects.append(n)
+
+                        # recurse up to the project directory
+                        if x.height() > self.srcnode.height() + 1:
+                                make_parents(n)
+
+                for p in self.all_projects[:]: # iterate over a copy of all projects
+                        if not getattr(p, 'tg', None):
+                                # but only projects that have a task generator
+                                continue
+
+                        # make a folder for each task generator
+                        p.iter_path = p.tg.path
+                        make_parents(p)
+
diff --git a/waflib/extras/color_gcc.py b/waflib/extras/color_gcc.py
new file mode 100644
index 00000000..b68c5ebf
--- /dev/null
+++ b/waflib/extras/color_gcc.py
@@ -0,0 +1,39 @@
+#!/usr/bin/env python
+# encoding: utf-8
+
+# Replaces the default formatter by one which understands GCC output and colorizes it.
+
+__author__ = __maintainer__ = "Jérôme Carretero <cJ-waf@zougloub.eu>"
+__copyright__ = "Jérôme Carretero, 2012"
+
+import sys
+from waflib import Logs
+
+class ColorGCCFormatter(Logs.formatter):
+	def __init__(self, colors):
+		self.colors = colors
+		Logs.formatter.__init__(self)
+	def format(self, rec):
+		frame = sys._getframe()
+		while frame:
+			func = frame.f_code.co_name
+			if func == 'exec_command':
+				cmd = frame.f_locals.get('cmd')
+				if isinstance(cmd, list) and ('gcc' in cmd[0] or 'g++' in cmd[0]):
+					lines = []
+					for line in rec.msg.splitlines():
+						if 'warning: ' in line:
+							lines.append(self.colors.YELLOW + line)
+						elif 'error: ' in line:
+							lines.append(self.colors.RED + line)
+						elif 'note: ' in line:
+							lines.append(self.colors.CYAN + line)
+						else:
+							lines.append(line)
+					rec.msg = "\n".join(lines)
+			frame = frame.f_back
+		return Logs.formatter.format(self, rec)
+
+def options(opt):
+	Logs.log.handlers[0].setFormatter(ColorGCCFormatter(Logs.colors))
+
diff --git a/waflib/extras/color_rvct.py b/waflib/extras/color_rvct.py
new file mode 100644
index 00000000..f89ccbdb
--- /dev/null
+++ b/waflib/extras/color_rvct.py
@@ -0,0 +1,51 @@
+#!/usr/bin/env python
+# encoding: utf-8
+
+# Replaces the default formatter by one which understands RVCT output and colorizes it.
+
+__author__ = __maintainer__ = "Jérôme Carretero <cJ-waf@zougloub.eu>"
+__copyright__ = "Jérôme Carretero, 2012"
+
+import sys
+import atexit
+from waflib import Logs
+
+errors = []
+
+def show_errors():
+	for i, e in enumerate(errors):
+		if i > 5:
+			break
+		print("Error: %s" % e)
+
+atexit.register(show_errors)
+
+class RcvtFormatter(Logs.formatter):
+	def __init__(self, colors):
+		Logs.formatter.__init__(self)
+		self.colors = colors
+	def format(self, rec):
+		frame = sys._getframe()
+		while frame:
+			func = frame.f_code.co_name
+			if func == 'exec_command':
+				cmd = frame.f_locals['cmd']
+				if isinstance(cmd, list) and ('armcc' in cmd[0] or 'armld' in cmd[0]):
+					lines = []
+					for line in rec.msg.splitlines():
+						if 'Warning: ' in line:
+							lines.append(self.colors.YELLOW + line)
+						elif 'Error: ' in line:
+							lines.append(self.colors.RED + line)
+							errors.append(line)
+						elif 'note: ' in line:
+							lines.append(self.colors.CYAN + line)
+						else:
+							lines.append(line)
+					rec.msg = "\n".join(lines)
+			frame = frame.f_back
+		return Logs.formatter.format(self, rec)
+
+def options(opt):
+	Logs.log.handlers[0].setFormatter(RcvtFormatter(Logs.colors))
+
diff --git a/waflib/extras/compat15.py b/waflib/extras/compat15.py
new file mode 100644
index 00000000..0e74df85
--- /dev/null
+++ b/waflib/extras/compat15.py
@@ -0,0 +1,406 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2010 (ita)
+
+"""
+This file is provided to enable compatibility with waf 1.5
+It was enabled by default in waf 1.6, but it is not used in waf 1.7
+"""
+
+import sys
+from waflib import ConfigSet, Logs, Options, Scripting, Task, Build, Configure, Node, Runner, TaskGen, Utils, Errors, Context
+
+# the following is to bring some compatibility with waf 1.5 "import waflib.Configure → import Configure"
+sys.modules['Environment'] = ConfigSet
+ConfigSet.Environment = ConfigSet.ConfigSet
+
+sys.modules['Logs'] = Logs
+sys.modules['Options'] = Options
+sys.modules['Scripting'] = Scripting
+sys.modules['Task'] = Task
+sys.modules['Build'] = Build
+sys.modules['Configure'] = Configure
+sys.modules['Node'] = Node
+sys.modules['Runner'] = Runner
+sys.modules['TaskGen'] = TaskGen
+sys.modules['Utils'] = Utils
+sys.modules['Constants'] = Context
+Context.SRCDIR = ''
+Context.BLDDIR = ''
+
+from waflib.Tools import c_preproc
+sys.modules['preproc'] = c_preproc
+
+from waflib.Tools import c_config
+sys.modules['config_c'] = c_config
+
+ConfigSet.ConfigSet.copy = ConfigSet.ConfigSet.derive
+ConfigSet.ConfigSet.set_variant = Utils.nada
+
+Utils.pproc = Utils.subprocess
+
+Build.BuildContext.add_subdirs = Build.BuildContext.recurse
+Build.BuildContext.new_task_gen = Build.BuildContext.__call__
+Build.BuildContext.is_install = 0
+Node.Node.relpath_gen = Node.Node.path_from
+
+Utils.pproc = Utils.subprocess
+Utils.get_term_cols = Logs.get_term_cols
+
+def cmd_output(cmd, **kw):
+
+	silent = False
+	if 'silent' in kw:
+		silent = kw['silent']
+		del(kw['silent'])
+
+	if 'e' in kw:
+		tmp = kw['e']
+		del(kw['e'])
+		kw['env'] = tmp
+
+	kw['shell'] = isinstance(cmd, str)
+	kw['stdout'] = Utils.subprocess.PIPE
+	if silent:
+		kw['stderr'] = Utils.subprocess.PIPE
+
+	try:
+		p = Utils.subprocess.Popen(cmd, **kw)
+		output = p.communicate()[0]
+	except OSError as e:
+		raise ValueError(str(e))
+
+	if p.returncode:
+		if not silent:
+			msg = "command execution failed: %s -> %r" % (cmd, str(output))
+			raise ValueError(msg)
+		output = ''
+	return output
+Utils.cmd_output = cmd_output
+
+def name_to_obj(self, s, env=None):
+	if Logs.verbose:
+		Logs.warn('compat: change "name_to_obj(name, env)" by "get_tgen_by_name(name)"')
+	return self.get_tgen_by_name(s)
+Build.BuildContext.name_to_obj = name_to_obj
+
+def env_of_name(self, name):
+	try:
+		return self.all_envs[name]
+	except KeyError:
+		Logs.error('no such environment: '+name)
+		return None
+Build.BuildContext.env_of_name = env_of_name
+
+
+def set_env_name(self, name, env):
+	self.all_envs[name] = env
+	return env
+Configure.ConfigurationContext.set_env_name = set_env_name
+
+def retrieve(self, name, fromenv=None):
+	try:
+		env = self.all_envs[name]
+	except KeyError:
+		env = ConfigSet.ConfigSet()
+		self.prepare_env(env)
+		self.all_envs[name] = env
+	else:
+		if fromenv:
+			Logs.warn('The environment %s may have been configured already', name)
+	return env
+Configure.ConfigurationContext.retrieve = retrieve
+
+Configure.ConfigurationContext.sub_config = Configure.ConfigurationContext.recurse
+Configure.ConfigurationContext.check_tool = Configure.ConfigurationContext.load
+Configure.conftest = Configure.conf
+Configure.ConfigurationError = Errors.ConfigurationError
+Utils.WafError = Errors.WafError
+
+Options.OptionsContext.sub_options = Options.OptionsContext.recurse
+Options.OptionsContext.tool_options = Context.Context.load
+Options.Handler = Options.OptionsContext
+
+Task.simple_task_type = Task.task_type_from_func = Task.task_factory
+Task.Task.classes = Task.classes
+
+def setitem(self, key, value):
+	if key.startswith('CCFLAGS'):
+		key = key[1:]
+	self.table[key] = value
+ConfigSet.ConfigSet.__setitem__ = setitem
+
+@TaskGen.feature('d')
+@TaskGen.before('apply_incpaths')
+def old_importpaths(self):
+	if getattr(self, 'importpaths', []):
+		self.includes = self.importpaths
+
+from waflib import Context
+eld = Context.load_tool
+def load_tool(*k, **kw):
+	ret = eld(*k, **kw)
+	if 'set_options' in ret.__dict__:
+		if Logs.verbose:
+			Logs.warn('compat: rename "set_options" to options')
+		ret.options = ret.set_options
+	if 'detect' in ret.__dict__:
+		if Logs.verbose:
+			Logs.warn('compat: rename "detect" to "configure"')
+		ret.configure = ret.detect
+	return ret
+Context.load_tool = load_tool
+
+def get_curdir(self):
+	return self.path.abspath()
+Context.Context.curdir = property(get_curdir, Utils.nada)
+
+def get_srcdir(self):
+	return self.srcnode.abspath()
+Configure.ConfigurationContext.srcdir = property(get_srcdir, Utils.nada)
+
+def get_blddir(self):
+	return self.bldnode.abspath()
+Configure.ConfigurationContext.blddir = property(get_blddir, Utils.nada)
+
+Configure.ConfigurationContext.check_message_1 = Configure.ConfigurationContext.start_msg
+Configure.ConfigurationContext.check_message_2 = Configure.ConfigurationContext.end_msg
+
+rev = Context.load_module
+def load_module(path, encoding=None):
+	ret = rev(path, encoding)
+	if 'set_options' in ret.__dict__:
+		if Logs.verbose:
+			Logs.warn('compat: rename "set_options" to "options" (%r)', path)
+		ret.options = ret.set_options
+	if 'srcdir' in ret.__dict__:
+		if Logs.verbose:
+			Logs.warn('compat: rename "srcdir" to "top" (%r)', path)
+		ret.top = ret.srcdir
+	if 'blddir' in ret.__dict__:
+		if Logs.verbose:
+			Logs.warn('compat: rename "blddir" to "out" (%r)', path)
+		ret.out = ret.blddir
+	Utils.g_module = Context.g_module
+	Options.launch_dir = Context.launch_dir
+	return ret
+Context.load_module = load_module
+
+old_post = TaskGen.task_gen.post
+def post(self):
+	self.features = self.to_list(self.features)
+	if 'cc' in self.features:
+		if Logs.verbose:
+			Logs.warn('compat: the feature cc does not exist anymore (use "c")')
+		self.features.remove('cc')
+		self.features.append('c')
+	if 'cstaticlib' in self.features:
+		if Logs.verbose:
+			Logs.warn('compat: the feature cstaticlib does not exist anymore (use "cstlib" or "cxxstlib")')
+		self.features.remove('cstaticlib')
+		self.features.append(('cxx' in self.features) and 'cxxstlib' or 'cstlib')
+	if getattr(self, 'ccflags', None):
+		if Logs.verbose:
+			Logs.warn('compat: "ccflags" was renamed to "cflags"')
+		self.cflags = self.ccflags
+	return old_post(self)
+TaskGen.task_gen.post = post
+
+def waf_version(*k, **kw):
+	Logs.warn('wrong version (waf_version was removed in waf 1.6)')
+Utils.waf_version = waf_version
+
+
+import os
+@TaskGen.feature('c', 'cxx', 'd')
+@TaskGen.before('apply_incpaths', 'propagate_uselib_vars')
+@TaskGen.after('apply_link', 'process_source')
+def apply_uselib_local(self):
+	"""
+	process the uselib_local attribute
+	execute after apply_link because of the execution order set on 'link_task'
+	"""
+	env = self.env
+	from waflib.Tools.ccroot import stlink_task
+
+	# 1. the case of the libs defined in the project (visit ancestors first)
+	# the ancestors external libraries (uselib) will be prepended
+	self.uselib = self.to_list(getattr(self, 'uselib', []))
+	self.includes = self.to_list(getattr(self, 'includes', []))
+	names = self.to_list(getattr(self, 'uselib_local', []))
+	get = self.bld.get_tgen_by_name
+	seen = set()
+	seen_uselib = set()
+	tmp = Utils.deque(names) # consume a copy of the list of names
+	if tmp:
+		if Logs.verbose:
+			Logs.warn('compat: "uselib_local" is deprecated, replace by "use"')
+	while tmp:
+		lib_name = tmp.popleft()
+		# visit dependencies only once
+		if lib_name in seen:
+			continue
+
+		y = get(lib_name)
+		y.post()
+		seen.add(lib_name)
+
+		# object has ancestors to process (shared libraries): add them to the end of the list
+		if getattr(y, 'uselib_local', None):
+			for x in self.to_list(getattr(y, 'uselib_local', [])):
+				obj = get(x)
+				obj.post()
+				if getattr(obj, 'link_task', None):
+					if not isinstance(obj.link_task, stlink_task):
+						tmp.append(x)
+
+		# link task and flags
+		if getattr(y, 'link_task', None):
+
+			link_name = y.target[y.target.rfind(os.sep) + 1:]
+			if isinstance(y.link_task, stlink_task):
+				env.append_value('STLIB', [link_name])
+			else:
+				# some linkers can link against programs
+				env.append_value('LIB', [link_name])
+
+			# the order
+			self.link_task.set_run_after(y.link_task)
+
+			# for the recompilation
+			self.link_task.dep_nodes += y.link_task.outputs
+
+			# add the link path too
+			tmp_path = y.link_task.outputs[0].parent.bldpath()
+			if not tmp_path in env['LIBPATH']:
+				env.prepend_value('LIBPATH', [tmp_path])
+
+		# add ancestors uselib too - but only propagate those that have no staticlib defined
+		for v in self.to_list(getattr(y, 'uselib', [])):
+			if v not in seen_uselib:
+				seen_uselib.add(v)
+				if not env['STLIB_' + v]:
+					if not v in self.uselib:
+						self.uselib.insert(0, v)
+
+		# if the library task generator provides 'export_includes', add to the include path
+		# the export_includes must be a list of paths relative to the other library
+		if getattr(y, 'export_includes', None):
+			self.includes.extend(y.to_incnodes(y.export_includes))
+
+@TaskGen.feature('cprogram', 'cxxprogram', 'cstlib', 'cxxstlib', 'cshlib', 'cxxshlib', 'dprogram', 'dstlib', 'dshlib')
+@TaskGen.after('apply_link')
+def apply_objdeps(self):
+	"add the .o files produced by some other object files in the same manner as uselib_local"
+	names = getattr(self, 'add_objects', [])
+	if not names:
+		return
+	names = self.to_list(names)
+
+	get = self.bld.get_tgen_by_name
+	seen = []
+	while names:
+		x = names[0]
+
+		# visit dependencies only once
+		if x in seen:
+			names = names[1:]
+			continue
+
+		# object does not exist ?
+		y = get(x)
+
+		# object has ancestors to process first ? update the list of names
+		if getattr(y, 'add_objects', None):
+			added = 0
+			lst = y.to_list(y.add_objects)
+			lst.reverse()
+			for u in lst:
+				if u in seen:
+					continue
+				added = 1
+				names = [u]+names
+			if added:
+				continue # list of names modified, loop
+
+		# safe to process the current object
+		y.post()
+		seen.append(x)
+
+		for t in getattr(y, 'compiled_tasks', []):
+			self.link_task.inputs.extend(t.outputs)
+
+@TaskGen.after('apply_link')
+def process_obj_files(self):
+	if not hasattr(self, 'obj_files'):
+		return
+	for x in self.obj_files:
+		node = self.path.find_resource(x)
+		self.link_task.inputs.append(node)
+
+@TaskGen.taskgen_method
+def add_obj_file(self, file):
+	"""Small example on how to link object files as if they were source
+	obj = bld.create_obj('cc')
+	obj.add_obj_file('foo.o')"""
+	if not hasattr(self, 'obj_files'):
+		self.obj_files = []
+	if not 'process_obj_files' in self.meths:
+		self.meths.append('process_obj_files')
+	self.obj_files.append(file)
+
+
+old_define = Configure.ConfigurationContext.__dict__['define']
+
+@Configure.conf
+def define(self, key, val, quote=True, comment=''):
+	old_define(self, key, val, quote, comment)
+	if key.startswith('HAVE_'):
+		self.env[key] = 1
+
+old_undefine = Configure.ConfigurationContext.__dict__['undefine']
+
+@Configure.conf
+def undefine(self, key, comment=''):
+	old_undefine(self, key, comment)
+	if key.startswith('HAVE_'):
+		self.env[key] = 0
+
+# some people might want to use export_incdirs, but it was renamed
+def set_incdirs(self, val):
+	Logs.warn('compat: change "export_incdirs" by "export_includes"')
+	self.export_includes = val
+TaskGen.task_gen.export_incdirs = property(None, set_incdirs)
+
+def install_dir(self, path):
+	if not path:
+		return []
+
+	destpath = Utils.subst_vars(path, self.env)
+
+	if self.is_install > 0:
+		Logs.info('* creating %s', destpath)
+		Utils.check_dir(destpath)
+	elif self.is_install < 0:
+		Logs.info('* removing %s', destpath)
+		try:
+			os.remove(destpath)
+		except OSError:
+			pass
+Build.BuildContext.install_dir = install_dir
+
+# before/after names
+repl = {'apply_core': 'process_source',
+	'apply_lib_vars': 'process_source',
+	'apply_obj_vars': 'propagate_uselib_vars',
+	'exec_rule': 'process_rule'
+}
+def after(*k):
+	k = [repl.get(key, key) for key in k]
+	return TaskGen.after_method(*k)
+
+def before(*k):
+	k = [repl.get(key, key) for key in k]
+	return TaskGen.before_method(*k)
+TaskGen.before = before
+
diff --git a/waflib/extras/cppcheck.py b/waflib/extras/cppcheck.py
new file mode 100644
index 00000000..13ff4247
--- /dev/null
+++ b/waflib/extras/cppcheck.py
@@ -0,0 +1,591 @@
+#! /usr/bin/env python
+# -*- encoding: utf-8 -*-
+# Michel Mooij, michel.mooij7@gmail.com
+
+"""
+Tool Description
+================
+This module provides a waf wrapper (i.e. waftool) around the C/C++ source code
+checking tool 'cppcheck'.
+
+See http://cppcheck.sourceforge.net/ for more information on the cppcheck tool
+itself.
+Note that many linux distributions already provide a ready to install version
+of cppcheck. On fedora, for instance, it can be installed using yum:
+
+	'sudo yum install cppcheck'
+
+
+Usage
+=====
+In order to use this waftool simply add it to the 'options' and 'configure'
+functions of your main waf script as shown in the example below:
+
+	def options(opt):
+		opt.load('cppcheck', tooldir='./waftools')
+
+	def configure(conf):
+		conf.load('cppcheck')
+
+Note that example shown above assumes that the cppcheck waftool is located in
+the sub directory named 'waftools'.
+
+When configured as shown in the example above, cppcheck will automatically
+perform a source code analysis on all C/C++ build tasks that have been
+defined in your waf build system.
+
+The example shown below for a C program will be used as input for cppcheck when
+building the task.
+
+	def build(bld):
+		bld.program(name='foo', src='foobar.c')
+
+The result of the source code analysis will be stored both as xml and html
+files in the build location for the task. Should any error be detected by
+cppcheck the build will be aborted and a link to the html report will be shown.
+By default, one index.html file is created for each task generator. A global
+index.html file can be obtained by setting the following variable
+in the configuration section:
+
+	conf.env.CPPCHECK_SINGLE_HTML = False
+
+When needed source code checking by cppcheck can be disabled per task, per
+detected error or warning for a particular task. It can be also be disabled for
+all tasks.
+
+In order to exclude a task from source code checking add the skip option to the
+task as shown below:
+
+	def build(bld):
+		bld.program(
+				name='foo',
+				src='foobar.c'
+				cppcheck_skip=True
+		)
+
+When needed problems detected by cppcheck may be suppressed using a file
+containing a list of suppression rules. The relative or absolute path to this
+file can be added to the build task as shown in the example below:
+
+		bld.program(
+				name='bar',
+				src='foobar.c',
+				cppcheck_suppress='bar.suppress'
+		)
+
+A cppcheck suppress file should contain one suppress rule per line. Each of
+these rules will be passed as an '--suppress=<rule>' argument to cppcheck.
+
+Dependencies
+================
+This waftool depends on the python pygments module, it is used for source code
+syntax highlighting when creating the html reports. see http://pygments.org/ for
+more information on this package.
+
+Remarks
+================
+The generation of the html report is originally based on the cppcheck-htmlreport.py
+script that comes shipped with the cppcheck tool.
+"""
+
+import sys
+import xml.etree.ElementTree as ElementTree
+from waflib import Task, TaskGen, Logs, Context, Options
+
+PYGMENTS_EXC_MSG= '''
+The required module 'pygments' could not be found. Please install it using your
+platform package manager (e.g. apt-get or yum), using 'pip' or 'easy_install',
+see 'http://pygments.org/download/' for installation instructions.
+'''
+
+try:
+	import pygments
+	from pygments import formatters, lexers
+except ImportError as e:
+	Logs.warn(PYGMENTS_EXC_MSG)
+	raise e
+
+
+def options(opt):
+	opt.add_option('--cppcheck-skip', dest='cppcheck_skip',
+		default=False, action='store_true',
+		help='do not check C/C++ sources (default=False)')
+
+	opt.add_option('--cppcheck-err-resume', dest='cppcheck_err_resume',
+		default=False, action='store_true',
+		help='continue in case of errors (default=False)')
+
+	opt.add_option('--cppcheck-bin-enable', dest='cppcheck_bin_enable',
+		default='warning,performance,portability,style,unusedFunction', action='store',
+		help="cppcheck option '--enable=' for binaries (default=warning,performance,portability,style,unusedFunction)")
+
+	opt.add_option('--cppcheck-lib-enable', dest='cppcheck_lib_enable',
+		default='warning,performance,portability,style', action='store',
+		help="cppcheck option '--enable=' for libraries (default=warning,performance,portability,style)")
+
+	opt.add_option('--cppcheck-std-c', dest='cppcheck_std_c',
+		default='c99', action='store',
+		help='cppcheck standard to use when checking C (default=c99)')
+
+	opt.add_option('--cppcheck-std-cxx', dest='cppcheck_std_cxx',
+		default='c++03', action='store',
+		help='cppcheck standard to use when checking C++ (default=c++03)')
+
+	opt.add_option('--cppcheck-check-config', dest='cppcheck_check_config',
+		default=False, action='store_true',
+		help='forced check for missing buildin include files, e.g. stdio.h (default=False)')
+
+	opt.add_option('--cppcheck-max-configs', dest='cppcheck_max_configs',
+		default='20', action='store',
+		help='maximum preprocessor (--max-configs) define iterations (default=20)')
+
+	opt.add_option('--cppcheck-jobs', dest='cppcheck_jobs',
+		default='1', action='store',
+		help='number of jobs (-j) to do the checking work (default=1)')
+
+def configure(conf):
+	if conf.options.cppcheck_skip:
+		conf.env.CPPCHECK_SKIP = [True]
+	conf.env.CPPCHECK_STD_C = conf.options.cppcheck_std_c
+	conf.env.CPPCHECK_STD_CXX = conf.options.cppcheck_std_cxx
+	conf.env.CPPCHECK_MAX_CONFIGS = conf.options.cppcheck_max_configs
+	conf.env.CPPCHECK_BIN_ENABLE = conf.options.cppcheck_bin_enable
+	conf.env.CPPCHECK_LIB_ENABLE = conf.options.cppcheck_lib_enable
+	conf.env.CPPCHECK_JOBS = conf.options.cppcheck_jobs
+	if conf.options.cppcheck_jobs != '1' and ('unusedFunction' in conf.options.cppcheck_bin_enable or 'unusedFunction' in conf.options.cppcheck_lib_enable or 'all' in conf.options.cppcheck_bin_enable or 'all' in conf.options.cppcheck_lib_enable):
+		Logs.warn('cppcheck: unusedFunction cannot be used with multiple threads, cppcheck will disable it automatically')
+	conf.find_program('cppcheck', var='CPPCHECK')
+
+	# set to True to get a single index.html file
+	conf.env.CPPCHECK_SINGLE_HTML = False
+
+@TaskGen.feature('c')
+@TaskGen.feature('cxx')
+def cppcheck_execute(self):
+	if hasattr(self.bld, 'conf'):
+		return
+	if len(self.env.CPPCHECK_SKIP) or Options.options.cppcheck_skip:
+		return
+	if getattr(self, 'cppcheck_skip', False):
+		return
+	task = self.create_task('cppcheck')
+	task.cmd = _tgen_create_cmd(self)
+	task.fatal = []
+	if not Options.options.cppcheck_err_resume:
+		task.fatal.append('error')
+
+
+def _tgen_create_cmd(self):
+	features = getattr(self, 'features', [])
+	std_c = self.env.CPPCHECK_STD_C
+	std_cxx = self.env.CPPCHECK_STD_CXX
+	max_configs = self.env.CPPCHECK_MAX_CONFIGS
+	bin_enable = self.env.CPPCHECK_BIN_ENABLE
+	lib_enable = self.env.CPPCHECK_LIB_ENABLE
+	jobs = self.env.CPPCHECK_JOBS
+
+	cmd  = self.env.CPPCHECK
+	args = ['--inconclusive','--report-progress','--verbose','--xml','--xml-version=2']
+	args.append('--max-configs=%s' % max_configs)
+	args.append('-j %s' % jobs)
+
+	if 'cxx' in features:
+		args.append('--language=c++')
+		args.append('--std=%s' % std_cxx)
+	else:
+		args.append('--language=c')
+		args.append('--std=%s' % std_c)
+
+	if Options.options.cppcheck_check_config:
+		args.append('--check-config')
+
+	if set(['cprogram','cxxprogram']) & set(features):
+		args.append('--enable=%s' % bin_enable)
+	else:
+		args.append('--enable=%s' % lib_enable)
+
+	for src in self.to_list(getattr(self, 'source', [])):
+		if not isinstance(src, str):
+			src = repr(src)
+		args.append(src)
+	for inc in self.to_incnodes(self.to_list(getattr(self, 'includes', []))):
+		if not isinstance(inc, str):
+			inc = repr(inc)
+		args.append('-I%s' % inc)
+	for inc in self.to_incnodes(self.to_list(self.env.INCLUDES)):
+		if not isinstance(inc, str):
+			inc = repr(inc)
+		args.append('-I%s' % inc)
+	return cmd + args
+
+
+class cppcheck(Task.Task):
+	quiet = True
+
+	def run(self):
+		stderr = self.generator.bld.cmd_and_log(self.cmd, quiet=Context.STDERR, output=Context.STDERR)
+		self._save_xml_report(stderr)
+		defects = self._get_defects(stderr)
+		index = self._create_html_report(defects)
+		self._errors_evaluate(defects, index)
+		return 0
+
+	def _save_xml_report(self, s):
+		'''use cppcheck xml result string, add the command string used to invoke cppcheck
+		and save as xml file.
+		'''
+		header = '%s\n' % s.splitlines()[0]
+		root = ElementTree.fromstring(s)
+		cmd = ElementTree.SubElement(root.find('cppcheck'), 'cmd')
+		cmd.text = str(self.cmd)
+		body = ElementTree.tostring(root).decode('us-ascii')
+		body_html_name = 'cppcheck-%s.xml' % self.generator.get_name()
+		if self.env.CPPCHECK_SINGLE_HTML:
+			body_html_name = 'cppcheck.xml'
+		node = self.generator.path.get_bld().find_or_declare(body_html_name)
+		node.write(header + body)
+
+	def _get_defects(self, xml_string):
+		'''evaluate the xml string returned by cppcheck (on sdterr) and use it to create
+		a list of defects.
+		'''
+		defects = []
+		for error in ElementTree.fromstring(xml_string).iter('error'):
+			defect = {}
+			defect['id'] = error.get('id')
+			defect['severity'] = error.get('severity')
+			defect['msg'] = str(error.get('msg')).replace('<','&lt;')
+			defect['verbose'] = error.get('verbose')
+			for location in error.findall('location'):
+				defect['file'] = location.get('file')
+				defect['line'] = str(int(location.get('line')) - 1)
+			defects.append(defect)
+		return defects
+
+	def _create_html_report(self, defects):
+		files, css_style_defs = self._create_html_files(defects)
+		index = self._create_html_index(files)
+		self._create_css_file(css_style_defs)
+		return index
+
+	def _create_html_files(self, defects):
+		sources = {}
+		defects = [defect for defect in defects if 'file' in defect]
+		for defect in defects:
+			name = defect['file']
+			if not name in sources:
+				sources[name] = [defect]
+			else:
+				sources[name].append(defect)
+
+		files = {}
+		css_style_defs = None
+		bpath = self.generator.path.get_bld().abspath()
+		names = list(sources.keys())
+		for i in range(0,len(names)):
+			name = names[i]
+			if self.env.CPPCHECK_SINGLE_HTML:
+				htmlfile = 'cppcheck/%i.html' % (i)
+			else:
+				htmlfile = 'cppcheck/%s%i.html' % (self.generator.get_name(),i)
+			errors = sources[name]
+			files[name] = { 'htmlfile': '%s/%s' % (bpath, htmlfile), 'errors': errors }
+			css_style_defs = self._create_html_file(name, htmlfile, errors)
+		return files, css_style_defs
+
+	def _create_html_file(self, sourcefile, htmlfile, errors):
+		name = self.generator.get_name()
+		root = ElementTree.fromstring(CPPCHECK_HTML_FILE)
+		title = root.find('head/title')
+		title.text = 'cppcheck - report - %s' % name
+
+		body = root.find('body')
+		for div in body.findall('div'):
+			if div.get('id') == 'page':
+				page = div
+				break
+		for div in page.findall('div'):
+			if div.get('id') == 'header':
+				h1 = div.find('h1')
+				h1.text = 'cppcheck report - %s' % name
+			if div.get('id') == 'menu':
+				indexlink = div.find('a')
+				if self.env.CPPCHECK_SINGLE_HTML:
+					indexlink.attrib['href'] = 'index.html'
+				else:
+					indexlink.attrib['href'] = 'index-%s.html' % name
+			if div.get('id') == 'content':
+				content = div
+				srcnode = self.generator.bld.root.find_node(sourcefile)
+				hl_lines = [e['line'] for e in errors if 'line' in e]
+				formatter = CppcheckHtmlFormatter(linenos=True, style='colorful', hl_lines=hl_lines, lineanchors='line')
+				formatter.errors = [e for e in errors if 'line' in e]
+				css_style_defs = formatter.get_style_defs('.highlight')
+				lexer = pygments.lexers.guess_lexer_for_filename(sourcefile, "")
+				s = pygments.highlight(srcnode.read(), lexer, formatter)
+				table = ElementTree.fromstring(s)
+				content.append(table)
+
+		s = ElementTree.tostring(root, method='html').decode('us-ascii')
+		s = CCPCHECK_HTML_TYPE + s
+		node = self.generator.path.get_bld().find_or_declare(htmlfile)
+		node.write(s)
+		return css_style_defs
+
+	def _create_html_index(self, files):
+		name = self.generator.get_name()
+		root = ElementTree.fromstring(CPPCHECK_HTML_FILE)
+		title = root.find('head/title')
+		title.text = 'cppcheck - report - %s' % name
+
+		body = root.find('body')
+		for div in body.findall('div'):
+			if div.get('id') == 'page':
+				page = div
+				break
+		for div in page.findall('div'):
+			if div.get('id') == 'header':
+				h1 = div.find('h1')
+				h1.text = 'cppcheck report - %s' % name
+			if div.get('id') == 'content':
+				content = div
+				self._create_html_table(content, files)
+			if div.get('id') == 'menu':
+				indexlink = div.find('a')
+				if self.env.CPPCHECK_SINGLE_HTML:
+					indexlink.attrib['href'] = 'index.html'
+				else:
+					indexlink.attrib['href'] = 'index-%s.html' % name
+
+		s = ElementTree.tostring(root, method='html').decode('us-ascii')
+		s = CCPCHECK_HTML_TYPE + s
+		index_html_name = 'cppcheck/index-%s.html' % name
+		if self.env.CPPCHECK_SINGLE_HTML:
+			index_html_name = 'cppcheck/index.html'
+		node = self.generator.path.get_bld().find_or_declare(index_html_name)
+		node.write(s)
+		return node
+
+	def _create_html_table(self, content, files):
+		table = ElementTree.fromstring(CPPCHECK_HTML_TABLE)
+		for name, val in files.items():
+			f = val['htmlfile']
+			s = '<tr><td colspan="4"><a href="%s">%s</a></td></tr>\n' % (f,name)
+			row = ElementTree.fromstring(s)
+			table.append(row)
+
+			errors = sorted(val['errors'], key=lambda e: int(e['line']) if 'line' in e else sys.maxint)
+			for e in errors:
+				if not 'line' in e:
+					s = '<tr><td></td><td>%s</td><td>%s</td><td>%s</td></tr>\n' % (e['id'], e['severity'], e['msg'])
+				else:
+					attr = ''
+					if e['severity'] == 'error':
+						attr = 'class="error"'
+					s = '<tr><td><a href="%s#line-%s">%s</a></td>' % (f, e['line'], e['line'])
+					s+= '<td>%s</td><td>%s</td><td %s>%s</td></tr>\n' % (e['id'], e['severity'], attr, e['msg'])
+				row = ElementTree.fromstring(s)
+				table.append(row)
+		content.append(table)
+
+	def _create_css_file(self, css_style_defs):
+		css = str(CPPCHECK_CSS_FILE)
+		if css_style_defs:
+			css = "%s\n%s\n" % (css, css_style_defs)
+		node = self.generator.path.get_bld().find_or_declare('cppcheck/style.css')
+		node.write(css)
+
+	def _errors_evaluate(self, errors, http_index):
+		name = self.generator.get_name()
+		fatal = self.fatal
+		severity = [err['severity'] for err in errors]
+		problems = [err for err in errors if err['severity'] != 'information']
+
+		if set(fatal) & set(severity):
+			exc  = "\n"
+			exc += "\nccpcheck detected fatal error(s) in task '%s', see report for details:" % name
+			exc += "\n    file://%r" % (http_index)
+			exc += "\n"
+			self.generator.bld.fatal(exc)
+
+		elif len(problems):
+			msg =  "\nccpcheck detected (possible) problem(s) in task '%s', see report for details:" % name
+			msg += "\n    file://%r" % http_index
+			msg += "\n"
+			Logs.error(msg)
+
+
+class CppcheckHtmlFormatter(pygments.formatters.HtmlFormatter):
+	errors = []
+
+	def wrap(self, source, outfile):
+		line_no = 1
+		for i, t in super(CppcheckHtmlFormatter, self).wrap(source, outfile):
+			# If this is a source code line we want to add a span tag at the end.
+			if i == 1:
+				for error in self.errors:
+					if int(error['line']) == line_no:
+						t = t.replace('\n', CPPCHECK_HTML_ERROR % error['msg'])
+				line_no += 1
+			yield i, t
+
+
+CCPCHECK_HTML_TYPE = \
+'<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">\n'
+
+CPPCHECK_HTML_FILE = """
+<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd" [<!ENTITY nbsp "&#160;">]>
+<html>
+	<head>
+		<title>cppcheck - report - XXX</title>
+		<link href="style.css" rel="stylesheet" type="text/css" />
+		<style type="text/css">
+		</style>
+	</head>
+	<body class="body">
+		<div id="page-header">&nbsp;</div>
+		<div id="page">
+			<div id="header">
+				<h1>cppcheck report - XXX</h1>
+			</div>
+			<div id="menu">
+				<a href="index.html">Defect list</a>
+			</div>
+			<div id="content">
+			</div>
+			<div id="footer">
+				<div>cppcheck - a tool for static C/C++ code analysis</div>
+				<div>
+				Internet: <a href="http://cppcheck.sourceforge.net">http://cppcheck.sourceforge.net</a><br/>
+          		Forum: <a href="http://apps.sourceforge.net/phpbb/cppcheck/">http://apps.sourceforge.net/phpbb/cppcheck/</a><br/>
+				IRC: #cppcheck at irc.freenode.net
+				</div>
+				&nbsp;
+			</div>
+		&nbsp;
+		</div>
+		<div id="page-footer">&nbsp;</div>
+	</body>
+</html>
+"""
+
+CPPCHECK_HTML_TABLE = """
+<table>
+	<tr>
+		<th>Line</th>
+		<th>Id</th>
+		<th>Severity</th>
+		<th>Message</th>
+	</tr>
+</table>
+"""
+
+CPPCHECK_HTML_ERROR = \
+'<span style="background: #ffaaaa;padding: 3px;">&lt;--- %s</span>\n'
+
+CPPCHECK_CSS_FILE = """
+body.body {
+	font-family: Arial;
+	font-size: 13px;
+	background-color: black;
+	padding: 0px;
+	margin: 0px;
+}
+
+.error {
+	font-family: Arial;
+	font-size: 13px;
+	background-color: #ffb7b7;
+	padding: 0px;
+	margin: 0px;
+}
+
+th, td {
+	min-width: 100px;
+	text-align: left;
+}
+
+#page-header {
+	clear: both;
+	width: 1200px;
+	margin: 20px auto 0px auto;
+	height: 10px;
+	border-bottom-width: 2px;
+	border-bottom-style: solid;
+	border-bottom-color: #aaaaaa;
+}
+
+#page {
+	width: 1160px;
+	margin: auto;
+	border-left-width: 2px;
+	border-left-style: solid;
+	border-left-color: #aaaaaa;
+	border-right-width: 2px;
+	border-right-style: solid;
+	border-right-color: #aaaaaa;
+	background-color: White;
+	padding: 20px;
+}
+
+#page-footer {
+	clear: both;
+	width: 1200px;
+	margin: auto;
+	height: 10px;
+	border-top-width: 2px;
+	border-top-style: solid;
+	border-top-color: #aaaaaa;
+}
+
+#header {
+	width: 100%;
+	height: 70px;
+	background-image: url(logo.png);
+	background-repeat: no-repeat;
+	background-position: left top;
+	border-bottom-style: solid;
+	border-bottom-width: thin;
+	border-bottom-color: #aaaaaa;
+}
+
+#menu {
+	margin-top: 5px;
+	text-align: left;
+	float: left;
+	width: 100px;
+	height: 300px;
+}
+
+#menu > a {
+	margin-left: 10px;
+	display: block;
+}
+
+#content {
+	float: left;
+	width: 1020px;
+	margin: 5px;
+	padding: 0px 10px 10px 10px;
+	border-left-style: solid;
+	border-left-width: thin;
+	border-left-color: #aaaaaa;
+}
+
+#footer {
+	padding-bottom: 5px;
+	padding-top: 5px;
+	border-top-style: solid;
+	border-top-width: thin;
+	border-top-color: #aaaaaa;
+	clear: both;
+	font-size: 10px;
+}
+
+#footer > div {
+	float: left;
+	width: 33%;
+}
+
+"""
+
diff --git a/waflib/extras/cpplint.py b/waflib/extras/cpplint.py
new file mode 100644
index 00000000..e3302e5b
--- /dev/null
+++ b/waflib/extras/cpplint.py
@@ -0,0 +1,209 @@
+#! /usr/bin/env python
+# encoding: utf-8
+#
+# written by Sylvain Rouquette, 2014
+
+'''
+
+This is an extra tool, not bundled with the default waf binary.
+To add the cpplint tool to the waf file:
+$ ./waf-light --tools=compat15,cpplint
+
+this tool also requires cpplint for python.
+If you have PIP, you can install it like this: pip install cpplint
+
+When using this tool, the wscript will look like:
+
+    def options(opt):
+        opt.load('compiler_cxx cpplint')
+
+    def configure(conf):
+        conf.load('compiler_cxx cpplint')
+        # optional, you can also specify them on the command line
+        conf.env.CPPLINT_FILTERS = ','.join((
+            '-whitespace/newline',      # c++11 lambda
+            '-readability/braces',      # c++11 constructor
+            '-whitespace/braces',       # c++11 constructor
+            '-build/storage_class',     # c++11 for-range
+            '-whitespace/blank_line',   # user pref
+            '-whitespace/labels'        # user pref
+            ))
+
+    def build(bld):
+        bld(features='cpplint', source='main.cpp', target='app')
+        # add include files, because they aren't usually built
+        bld(features='cpplint', source=bld.path.ant_glob('**/*.hpp'))
+'''
+
+from __future__ import absolute_import
+import sys, re
+import logging
+from waflib import Errors, Task, TaskGen, Logs, Options, Node, Utils
+
+
+critical_errors = 0
+CPPLINT_FORMAT = '[CPPLINT] %(filename)s:\nline %(linenum)s, severity %(confidence)s, category: %(category)s\n%(message)s\n'
+RE_EMACS = re.compile('(?P<filename>.*):(?P<linenum>\d+):  (?P<message>.*)  \[(?P<category>.*)\] \[(?P<confidence>\d+)\]')
+CPPLINT_RE = {
+    'waf': RE_EMACS,
+    'emacs': RE_EMACS,
+    'vs7': re.compile('(?P<filename>.*)\((?P<linenum>\d+)\):  (?P<message>.*)  \[(?P<category>.*)\] \[(?P<confidence>\d+)\]'),
+    'eclipse': re.compile('(?P<filename>.*):(?P<linenum>\d+): warning: (?P<message>.*)  \[(?P<category>.*)\] \[(?P<confidence>\d+)\]'),
+}
+CPPLINT_STR = ('${CPPLINT} '
+               '--verbose=${CPPLINT_LEVEL} '
+               '--output=${CPPLINT_OUTPUT} '
+               '--filter=${CPPLINT_FILTERS} '
+               '--root=${CPPLINT_ROOT} '
+               '--linelength=${CPPLINT_LINE_LENGTH} ')
+
+
+def options(opt):
+    opt.add_option('--cpplint-filters', type='string',
+                   default='', dest='CPPLINT_FILTERS',
+                   help='add filters to cpplint')
+    opt.add_option('--cpplint-length', type='int',
+                   default=80, dest='CPPLINT_LINE_LENGTH',
+                   help='specify the line length (default: 80)')
+    opt.add_option('--cpplint-level', default=1, type='int', dest='CPPLINT_LEVEL',
+                   help='specify the log level (default: 1)')
+    opt.add_option('--cpplint-break', default=5, type='int', dest='CPPLINT_BREAK',
+                   help='break the build if error >= level (default: 5)')
+    opt.add_option('--cpplint-root', type='string',
+                   default='', dest='CPPLINT_ROOT',
+                   help='root directory used to derive header guard')
+    opt.add_option('--cpplint-skip', action='store_true',
+                   default=False, dest='CPPLINT_SKIP',
+                   help='skip cpplint during build')
+    opt.add_option('--cpplint-output', type='string',
+                   default='waf', dest='CPPLINT_OUTPUT',
+                   help='select output format (waf, emacs, vs7, eclipse)')
+
+
+def configure(conf):
+    try:
+        conf.find_program('cpplint', var='CPPLINT')
+    except Errors.ConfigurationError:
+        conf.env.CPPLINT_SKIP = True
+
+
+class cpplint_formatter(Logs.formatter, object):
+    def __init__(self, fmt):
+        logging.Formatter.__init__(self, CPPLINT_FORMAT)
+        self.fmt = fmt
+
+    def format(self, rec):
+        if self.fmt == 'waf':
+            result = CPPLINT_RE[self.fmt].match(rec.msg).groupdict()
+            rec.msg = CPPLINT_FORMAT % result
+        if rec.levelno <= logging.INFO:
+            rec.c1 = Logs.colors.CYAN
+        return super(cpplint_formatter, self).format(rec)
+
+
+class cpplint_handler(Logs.log_handler, object):
+    def __init__(self, stream=sys.stderr, **kw):
+        super(cpplint_handler, self).__init__(stream, **kw)
+        self.stream = stream
+
+    def emit(self, rec):
+        rec.stream = self.stream
+        self.emit_override(rec)
+        self.flush()
+
+
+class cpplint_wrapper(object):
+    def __init__(self, logger, threshold, fmt):
+        self.logger = logger
+        self.threshold = threshold
+        self.fmt = fmt
+
+    def __enter__(self):
+        return self
+
+    def __exit__(self, exc_type, exc_value, traceback):
+        if isinstance(exc_value, Utils.subprocess.CalledProcessError):
+            messages = [m for m in exc_value.output.splitlines() 
+                        if 'Done processing' not in m 
+                        and 'Total errors found' not in m]
+            for message in messages:
+                self.write(message)
+            return True
+
+    def write(self, message):
+        global critical_errors
+        result = CPPLINT_RE[self.fmt].match(message)
+        if not result:
+            return
+        level = int(result.groupdict()['confidence'])
+        if level >= self.threshold:
+            critical_errors += 1
+        if level <= 2:
+            self.logger.info(message)
+        elif level <= 4:
+            self.logger.warning(message)
+        else:
+            self.logger.error(message)
+
+
+cpplint_logger = None
+def get_cpplint_logger(fmt):
+    global cpplint_logger
+    if cpplint_logger:
+        return cpplint_logger
+    cpplint_logger = logging.getLogger('cpplint')
+    hdlr = cpplint_handler()
+    hdlr.setFormatter(cpplint_formatter(fmt))
+    cpplint_logger.addHandler(hdlr)
+    cpplint_logger.setLevel(logging.DEBUG)
+    return cpplint_logger
+
+
+class cpplint(Task.Task):
+    color = 'PINK'
+
+    def __init__(self, *k, **kw):
+        super(cpplint, self).__init__(*k, **kw)
+
+    def run(self):
+        global critical_errors
+        with cpplint_wrapper(get_cpplint_logger(self.env.CPPLINT_OUTPUT), self.env.CPPLINT_BREAK, self.env.CPPLINT_OUTPUT):
+            params = {key: str(self.env[key]) for key in self.env if 'CPPLINT_' in key}
+            if params['CPPLINT_OUTPUT'] is 'waf':
+                params['CPPLINT_OUTPUT'] = 'emacs'
+            params['CPPLINT'] = self.env.get_flat('CPPLINT')
+            cmd = Utils.subst_vars(CPPLINT_STR, params)
+            env = self.env.env or None
+            Utils.subprocess.check_output(cmd + self.inputs[0].abspath(),
+                                          stderr=Utils.subprocess.STDOUT,
+                                          env=env, shell=True)
+        return critical_errors
+
+@TaskGen.extension('.h', '.hh', '.hpp', '.hxx')
+def cpplint_includes(self, node):
+    pass
+
+@TaskGen.feature('cpplint')
+@TaskGen.before_method('process_source')
+def post_cpplint(self):
+    if not self.env.CPPLINT_INITIALIZED:
+        for key, value in Options.options.__dict__.items():
+            if not key.startswith('CPPLINT_') or self.env[key]:
+                continue
+            self.env[key] = value
+        self.env.CPPLINT_INITIALIZED = True
+
+    if self.env.CPPLINT_SKIP:
+        return
+
+    if not self.env.CPPLINT_OUTPUT in CPPLINT_RE:
+        return
+
+    for src in self.to_list(getattr(self, 'source', [])):
+        if isinstance(src, Node.Node):
+            node = src
+        else:
+            node = self.path.find_or_declare(src)
+        if not node:
+            self.bld.fatal('Could not find %r' % src)
+        self.create_task('cpplint', node)
diff --git a/waflib/extras/cross_gnu.py b/waflib/extras/cross_gnu.py
new file mode 100644
index 00000000..309f53b0
--- /dev/null
+++ b/waflib/extras/cross_gnu.py
@@ -0,0 +1,227 @@
+#!/usr/bin/python
+# -*- coding: utf-8 vi:ts=4:noexpandtab
+# Tool to provide dedicated variables for cross-compilation
+
+__author__ = __maintainer__ = "Jérôme Carretero <cJ-waf@zougloub.eu>"
+__copyright__ = "Jérôme Carretero, 2014"
+
+"""
+This tool allows to use environment variables to define cross-compilation
+variables intended for build variants.
+
+The variables are obtained from the environment in 3 ways:
+
+1. By defining CHOST, they can be derived as ${CHOST}-${TOOL}
+2. By defining HOST_x
+3. By defining ${CHOST//-/_}_x
+
+else one can set ``cfg.env.CHOST`` in ``wscript`` before loading ``cross_gnu``.
+
+Usage:
+
+- In your build script::
+
+	def configure(cfg):
+		...
+		for variant in x_variants:
+			setenv(variant)
+			conf.load('cross_gnu')
+			conf.xcheck_host_var('POUET')
+			...
+
+
+- Then::
+
+	CHOST=arm-hardfloat-linux-gnueabi waf configure
+	env arm-hardfloat-linux-gnueabi-CC="clang -..." waf configure
+	CFLAGS=... CHOST=arm-hardfloat-linux-gnueabi HOST_CFLAGS=-g waf configure
+	HOST_CC="clang -..." waf configure
+
+This example ``wscript`` compiles to Microchip PIC (xc16-gcc-xyz must be in PATH):
+
+.. code:: python
+
+		from waflib import Configure
+
+		#from https://gist.github.com/rpuntaie/2bddfb5d7b77db26415ee14371289971
+		import waf_variants
+
+		variants='pc fw/variant1 fw/variant2'.split()
+
+		top = "."
+		out = "../build"
+
+		PIC = '33FJ128GP804' #dsPICxxx
+
+		@Configure.conf
+		def gcc_modifier_xc16(cfg):
+				v = cfg.env
+				v.cprogram_PATTERN = '%s.elf'
+				v.LINKFLAGS_cprogram = ','.join(['-Wl','','','--defsym=__MPLAB_BUILD=0','','--script=p'+PIC+'.gld',
+						'--stack=16','--check-sections','--data-init','--pack-data','--handles','--isr','--no-gc-sections',
+						'--fill-upper=0','--stackguard=16','--no-force-link','--smart-io']) #,'--report-mem'])
+				v.CFLAGS_cprogram=['-mcpu='+PIC,'-omf=elf','-mlarge-code','-msmart-io=1',
+						'-msfr-warn=off','-mno-override-inline','-finline','-Winline']
+
+		def configure(cfg):
+				if 'fw' in cfg.variant: #firmware
+						cfg.env.DEST_OS = 'xc16' #cfg.env.CHOST = 'xc16' #works too
+						cfg.load('c cross_gnu') #cfg.env.CHOST becomes ['xc16']
+						...
+				else: #configure for pc SW
+						...
+
+		def build(bld):
+				if 'fw' in bld.variant: #firmware
+						bld.program(source='maintst.c', target='maintst');
+						bld(source='maintst.elf', target='maintst.hex', rule="xc16-bin2hex ${SRC} -a -omf=elf")
+				else: #build for pc SW
+						...
+
+"""
+
+import os
+from waflib import Utils, Configure
+from waflib.Tools import ccroot, gcc
+
+try:
+	from shlex import quote
+except ImportError:
+	from pipes import quote
+
+def get_chost_stuff(conf):
+	"""
+	Get the CHOST environment variable contents
+	"""
+	chost = None
+	chost_envar = None
+	if conf.env.CHOST:
+		chost = conf.env.CHOST[0]
+		chost_envar = chost.replace('-', '_')
+	return chost, chost_envar
+
+
+@Configure.conf
+def xcheck_var(conf, name, wafname=None, cross=False):
+	wafname = wafname or name
+
+	if wafname in conf.env:
+		value = conf.env[wafname]
+		if isinstance(value, str):
+			value = [value]
+	else:
+		envar = os.environ.get(name)
+		if not envar:
+			return
+		value = Utils.to_list(envar) if envar != '' else [envar]
+
+	conf.env[wafname] = value
+	if cross:
+		pretty = 'cross-compilation %s' % wafname
+	else:
+		pretty = wafname
+	conf.msg('Will use %s' % pretty, " ".join(quote(x) for x in value))
+
+@Configure.conf
+def xcheck_host_prog(conf, name, tool, wafname=None):
+	wafname = wafname or name
+
+	chost, chost_envar = get_chost_stuff(conf)
+
+	specific = None
+	if chost:
+		specific = os.environ.get('%s_%s' % (chost_envar, name))
+
+	if specific:
+		value = Utils.to_list(specific)
+		conf.env[wafname] += value
+		conf.msg('Will use cross-compilation %s from %s_%s' % (name, chost_envar, name),
+		 " ".join(quote(x) for x in value))
+		return
+	else:
+		envar = os.environ.get('HOST_%s' % name)
+		if envar is not None:
+			value = Utils.to_list(envar)
+			conf.env[wafname] = value
+			conf.msg('Will use cross-compilation %s from HOST_%s' % (name, name),
+			 " ".join(quote(x) for x in value))
+			return
+
+	if conf.env[wafname]:
+		return
+
+	value = None
+	if chost:
+		value = '%s-%s' % (chost, tool)
+
+	if value:
+		conf.env[wafname] = value
+		conf.msg('Will use cross-compilation %s from CHOST' % wafname, value)
+
+@Configure.conf
+def xcheck_host_envar(conf, name, wafname=None):
+	wafname = wafname or name
+
+	chost, chost_envar = get_chost_stuff(conf)
+
+	specific = None
+	if chost:
+		specific = os.environ.get('%s_%s' % (chost_envar, name))
+
+	if specific:
+		value = Utils.to_list(specific)
+		conf.env[wafname] += value
+		conf.msg('Will use cross-compilation %s from %s_%s' \
+		 % (name, chost_envar, name),
+		 " ".join(quote(x) for x in value))
+		return
+
+
+	envar = os.environ.get('HOST_%s' % name)
+	if envar is None:
+		return
+
+	value = Utils.to_list(envar) if envar != '' else [envar]
+
+	conf.env[wafname] = value
+	conf.msg('Will use cross-compilation %s from HOST_%s' % (name, name),
+	 " ".join(quote(x) for x in value))
+
+
+@Configure.conf
+def xcheck_host(conf):
+	conf.xcheck_var('CHOST', cross=True)
+	conf.env.CHOST = conf.env.CHOST or [conf.env.DEST_OS]
+	conf.env.DEST_OS = conf.env.CHOST[0].replace('-','_')
+	conf.xcheck_host_prog('CC', 'gcc')
+	conf.xcheck_host_prog('CXX', 'g++')
+	conf.xcheck_host_prog('LINK_CC', 'gcc')
+	conf.xcheck_host_prog('LINK_CXX', 'g++')
+	conf.xcheck_host_prog('AR', 'ar')
+	conf.xcheck_host_prog('AS', 'as')
+	conf.xcheck_host_prog('LD', 'ld')
+	conf.xcheck_host_envar('CFLAGS')
+	conf.xcheck_host_envar('CXXFLAGS')
+	conf.xcheck_host_envar('LDFLAGS', 'LINKFLAGS')
+	conf.xcheck_host_envar('LIB')
+	conf.xcheck_host_envar('PKG_CONFIG_LIBDIR')
+	conf.xcheck_host_envar('PKG_CONFIG_PATH')
+
+	if not conf.env.env:
+		conf.env.env = {}
+		conf.env.env.update(os.environ)
+	if conf.env.PKG_CONFIG_LIBDIR:
+		conf.env.env['PKG_CONFIG_LIBDIR'] = conf.env.PKG_CONFIG_LIBDIR[0]
+	if conf.env.PKG_CONFIG_PATH:
+		conf.env.env['PKG_CONFIG_PATH'] = conf.env.PKG_CONFIG_PATH[0]
+
+def configure(conf):
+	"""
+	Configuration example for gcc, it will not work for g++/clang/clang++
+	"""
+	conf.xcheck_host()
+	conf.gcc_common_flags()
+	conf.gcc_modifier_platform()
+	conf.cc_load_tools()
+	conf.cc_add_flags()
+	conf.link_add_flags()
diff --git a/waflib/extras/cython.py b/waflib/extras/cython.py
new file mode 100644
index 00000000..481d6f4c
--- /dev/null
+++ b/waflib/extras/cython.py
@@ -0,0 +1,146 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2010-2015
+
+import re
+from waflib import Task, Logs
+from waflib.TaskGen import extension
+
+cy_api_pat = re.compile(r'\s*?cdef\s*?(public|api)\w*')
+re_cyt = re.compile(r"""
+	(?:from\s+(\w+)\s+)?   # optionally match "from foo" and capture foo
+	c?import\s(\w+|[*])    # require "import bar" and capture bar
+	""", re.M | re.VERBOSE)
+
+@extension('.pyx')
+def add_cython_file(self, node):
+	"""
+	Process a *.pyx* file given in the list of source files. No additional
+	feature is required::
+
+		def build(bld):
+			bld(features='c cshlib pyext', source='main.c foo.pyx', target='app')
+	"""
+	ext = '.c'
+	if 'cxx' in self.features:
+		self.env.append_unique('CYTHONFLAGS', '--cplus')
+		ext = '.cc'
+
+	for x in getattr(self, 'cython_includes', []):
+		# TODO re-use these nodes in "scan" below
+		d = self.path.find_dir(x)
+		if d:
+			self.env.append_unique('CYTHONFLAGS', '-I%s' % d.abspath())
+
+	tsk = self.create_task('cython', node, node.change_ext(ext))
+	self.source += tsk.outputs
+
+class cython(Task.Task):
+	run_str = '${CYTHON} ${CYTHONFLAGS} -o ${TGT[0].abspath()} ${SRC}'
+	color   = 'GREEN'
+
+	vars    = ['INCLUDES']
+	"""
+	Rebuild whenever the INCLUDES change. The variables such as CYTHONFLAGS will be appended
+	by the metaclass.
+	"""
+
+	ext_out = ['.h']
+	"""
+	The creation of a .h file is known only after the build has begun, so it is not
+	possible to compute a build order just by looking at the task inputs/outputs.
+	"""
+
+	def runnable_status(self):
+		"""
+		Perform a double-check to add the headers created by cython
+		to the output nodes. The scanner is executed only when the cython task
+		must be executed (optimization).
+		"""
+		ret = super(cython, self).runnable_status()
+		if ret == Task.ASK_LATER:
+			return ret
+		for x in self.generator.bld.raw_deps[self.uid()]:
+			if x.startswith('header:'):
+				self.outputs.append(self.inputs[0].parent.find_or_declare(x.replace('header:', '')))
+		return super(cython, self).runnable_status()
+
+	def post_run(self):
+		for x in self.outputs:
+			if x.name.endswith('.h'):
+				if not x.exists():
+					if Logs.verbose:
+						Logs.warn('Expected %r', x.abspath())
+					x.write('')
+		return Task.Task.post_run(self)
+
+	def scan(self):
+		"""
+		Return the dependent files (.pxd) by looking in the include folders.
+		Put the headers to generate in the custom list "bld.raw_deps".
+		To inspect the scanne results use::
+
+			$ waf clean build --zones=deps
+		"""
+		node = self.inputs[0]
+		txt = node.read()
+
+		mods = []
+		for m in re_cyt.finditer(txt):
+			if m.group(1):  # matches "from foo import bar"
+				mods.append(m.group(1))
+			else:
+				mods.append(m.group(2))
+
+		Logs.debug('cython: mods %r', mods)
+		incs = getattr(self.generator, 'cython_includes', [])
+		incs = [self.generator.path.find_dir(x) for x in incs]
+		incs.append(node.parent)
+
+		found = []
+		missing = []
+		for x in mods:
+			for y in incs:
+				k = y.find_resource(x + '.pxd')
+				if k:
+					found.append(k)
+					break
+			else:
+				missing.append(x)
+
+		# the cython file implicitly depends on a pxd file that might be present
+		implicit = node.parent.find_resource(node.name[:-3] + 'pxd')
+		if implicit:
+			found.append(implicit)
+
+		Logs.debug('cython: found %r', found)
+
+		# Now the .h created - store them in bld.raw_deps for later use
+		has_api = False
+		has_public = False
+		for l in txt.splitlines():
+			if cy_api_pat.match(l):
+				if ' api ' in l:
+					has_api = True
+				if ' public ' in l:
+					has_public = True
+		name = node.name.replace('.pyx', '')
+		if has_api:
+			missing.append('header:%s_api.h' % name)
+		if has_public:
+			missing.append('header:%s.h' % name)
+
+		return (found, missing)
+
+def options(ctx):
+	ctx.add_option('--cython-flags', action='store', default='', help='space separated list of flags to pass to cython')
+
+def configure(ctx):
+	if not ctx.env.CC and not ctx.env.CXX:
+		ctx.fatal('Load a C/C++ compiler first')
+	if not ctx.env.PYTHON:
+		ctx.fatal('Load the python tool first!')
+	ctx.find_program('cython', var='CYTHON')
+	if hasattr(ctx.options, 'cython_flags'):
+		ctx.env.CYTHONFLAGS = ctx.options.cython_flags
+
diff --git a/waflib/extras/dcc.py b/waflib/extras/dcc.py
new file mode 100644
index 00000000..c1a57c04
--- /dev/null
+++ b/waflib/extras/dcc.py
@@ -0,0 +1,72 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Jérôme Carretero, 2011 (zougloub)
+
+from waflib import Options
+from waflib.Tools import ccroot
+from waflib.Configure import conf
+
+@conf
+def find_dcc(conf):
+	conf.find_program(['dcc'], var='CC', path_list=getattr(Options.options, 'diabbindir', ""))
+	conf.env.CC_NAME = 'dcc'
+
+@conf
+def find_dld(conf):
+	conf.find_program(['dld'], var='LINK_CC', path_list=getattr(Options.options, 'diabbindir', ""))
+	conf.env.LINK_CC_NAME = 'dld'
+
+@conf
+def find_dar(conf):
+	conf.find_program(['dar'], var='AR', path_list=getattr(Options.options, 'diabbindir', ""))
+	conf.env.AR_NAME = 'dar'
+	conf.env.ARFLAGS = 'rcs'
+
+@conf
+def find_ddump(conf):
+	conf.find_program(['ddump'], var='DDUMP', path_list=getattr(Options.options, 'diabbindir', ""))
+
+@conf
+def dcc_common_flags(conf):
+	v = conf.env
+	v['CC_SRC_F']            = []
+	v['CC_TGT_F']            = ['-c', '-o']
+
+	# linker
+	if not v['LINK_CC']:
+		v['LINK_CC'] = v['CC']
+	v['CCLNK_SRC_F']         = []
+	v['CCLNK_TGT_F']         = ['-o']
+	v['CPPPATH_ST']          = '-I%s'
+	v['DEFINES_ST']          = '-D%s'
+
+	v['LIB_ST']              = '-l:%s' # template for adding libs
+	v['LIBPATH_ST']          = '-L%s' # template for adding libpaths
+	v['STLIB_ST']            = '-l:%s'
+	v['STLIBPATH_ST']        = '-L%s'
+	v['RPATH_ST']            = '-Wl,-rpath,%s'
+	#v['STLIB_MARKER']        = '-Wl,-Bstatic'
+
+	# program
+	v['cprogram_PATTERN']    = '%s.elf'
+
+	# static lib
+	v['LINKFLAGS_cstlib']    = ['-Wl,-Bstatic']
+	v['cstlib_PATTERN']      = 'lib%s.a'
+
+def configure(conf):
+	conf.find_dcc()
+	conf.find_dar()
+	conf.find_dld()
+	conf.find_ddump()
+	conf.dcc_common_flags()
+	conf.cc_load_tools()
+	conf.cc_add_flags()
+	conf.link_add_flags()
+
+def options(opt):
+	"""
+	Add the ``--with-diab-bindir`` command-line options.
+	"""
+	opt.add_option('--with-diab-bindir', type='string', dest='diabbindir', help = 'Specify alternate diab bin folder', default="")
+
diff --git a/waflib/extras/distnet.py b/waflib/extras/distnet.py
new file mode 100644
index 00000000..09a31a6d
--- /dev/null
+++ b/waflib/extras/distnet.py
@@ -0,0 +1,430 @@
+#! /usr/bin/env python
+# encoding: utf-8
+
+"""
+waf-powered distributed network builds, with a network cache.
+
+Caching files from a server has advantages over a NFS/Samba shared folder:
+
+- builds are much faster because they use local files
+- builds just continue to work in case of a network glitch
+- permissions are much simpler to manage
+"""
+
+import os, urllib, tarfile, re, shutil, tempfile, sys
+from collections import OrderedDict
+from waflib import Context, Utils, Logs
+
+try:
+	from urllib.parse import urlencode
+except ImportError:
+	urlencode = urllib.urlencode
+
+def safe_urlencode(data):
+	x = urlencode(data)
+	try:
+		x = x.encode('utf-8')
+	except Exception:
+		pass
+	return x
+
+try:
+	from urllib.error import URLError
+except ImportError:
+	from urllib2 import URLError
+
+try:
+	from urllib.request import Request, urlopen
+except ImportError:
+	from urllib2 import Request, urlopen
+
+DISTNETCACHE = os.environ.get('DISTNETCACHE', '/tmp/distnetcache')
+DISTNETSERVER = os.environ.get('DISTNETSERVER', 'http://localhost:8000/cgi-bin/')
+TARFORMAT = 'w:bz2'
+TIMEOUT = 60
+REQUIRES = 'requires.txt'
+
+re_com = re.compile('\s*#.*', re.M)
+
+def total_version_order(num):
+	lst = num.split('.')
+	template = '%10s' * len(lst)
+	ret = template % tuple(lst)
+	return ret
+
+def get_distnet_cache():
+	return getattr(Context.g_module, 'DISTNETCACHE', DISTNETCACHE)
+
+def get_server_url():
+	return getattr(Context.g_module, 'DISTNETSERVER', DISTNETSERVER)
+
+def get_download_url():
+	return '%s/download.py' % get_server_url()
+
+def get_upload_url():
+	return '%s/upload.py' % get_server_url()
+
+def get_resolve_url():
+	return '%s/resolve.py' % get_server_url()
+
+def send_package_name():
+	out = getattr(Context.g_module, 'out', 'build')
+	pkgfile = '%s/package_to_upload.tarfile' % out
+	return pkgfile
+
+class package(Context.Context):
+	fun = 'package'
+	cmd = 'package'
+
+	def execute(self):
+		try:
+			files = self.files
+		except AttributeError:
+			files = self.files = []
+
+		Context.Context.execute(self)
+		pkgfile = send_package_name()
+		if not pkgfile in files:
+			if not REQUIRES in files:
+				files.append(REQUIRES)
+			self.make_tarfile(pkgfile, files, add_to_package=False)
+
+	def make_tarfile(self, filename, files, **kw):
+		if kw.get('add_to_package', True):
+			self.files.append(filename)
+
+		with tarfile.open(filename, TARFORMAT) as tar:
+			endname = os.path.split(filename)[-1]
+			endname = endname.split('.')[0] + '/'
+			for x in files:
+				tarinfo = tar.gettarinfo(x, x)
+				tarinfo.uid   = tarinfo.gid   = 0
+				tarinfo.uname = tarinfo.gname = 'root'
+				tarinfo.size = os.stat(x).st_size
+
+				# TODO - more archive creation options?
+				if kw.get('bare', True):
+					tarinfo.name = os.path.split(x)[1]
+				else:
+					tarinfo.name = endname + x # todo, if tuple, then..
+				Logs.debug('distnet: adding %r to %s', tarinfo.name, filename)
+				with open(x, 'rb') as f:
+					tar.addfile(tarinfo, f)
+		Logs.info('Created %s', filename)
+
+class publish(Context.Context):
+	fun = 'publish'
+	cmd = 'publish'
+	def execute(self):
+		if hasattr(Context.g_module, 'publish'):
+			Context.Context.execute(self)
+		mod = Context.g_module
+
+		rfile = getattr(self, 'rfile', send_package_name())
+		if not os.path.isfile(rfile):
+			self.fatal('Create the release file with "waf release" first! %r' % rfile)
+
+		fdata = Utils.readf(rfile, m='rb')
+		data = safe_urlencode([('pkgdata', fdata), ('pkgname', mod.APPNAME), ('pkgver', mod.VERSION)])
+
+		req = Request(get_upload_url(), data)
+		response = urlopen(req, timeout=TIMEOUT)
+		data = response.read().strip()
+
+		if sys.hexversion>0x300000f:
+			data = data.decode('utf-8')
+
+		if data != 'ok':
+			self.fatal('Could not publish the package %r' % data)
+
+class constraint(object):
+	def __init__(self, line=''):
+		self.required_line = line
+		self.info = []
+
+		line = line.strip()
+		if not line:
+			return
+
+		lst = line.split(',')
+		if lst:
+			self.pkgname = lst[0]
+			self.required_version = lst[1]
+			for k in lst:
+				a, b, c = k.partition('=')
+				if a and c:
+					self.info.append((a, c))
+	def __str__(self):
+		buf = []
+		buf.append(self.pkgname)
+		buf.append(self.required_version)
+		for k in self.info:
+			buf.append('%s=%s' % k)
+		return ','.join(buf)
+
+	def __repr__(self):
+		return "requires %s-%s" % (self.pkgname, self.required_version)
+
+	def human_display(self, pkgname, pkgver):
+		return '%s-%s requires %s-%s' % (pkgname, pkgver, self.pkgname, self.required_version)
+
+	def why(self):
+		ret = []
+		for x in self.info:
+			if x[0] == 'reason':
+				ret.append(x[1])
+		return ret
+
+	def add_reason(self, reason):
+		self.info.append(('reason', reason))
+
+def parse_constraints(text):
+	assert(text is not None)
+	constraints = []
+	text = re.sub(re_com, '', text)
+	lines = text.splitlines()
+	for line in lines:
+		line = line.strip()
+		if not line:
+			continue
+		constraints.append(constraint(line))
+	return constraints
+
+def list_package_versions(cachedir, pkgname):
+	pkgdir = os.path.join(cachedir, pkgname)
+	try:
+		versions = os.listdir(pkgdir)
+	except OSError:
+		return []
+	versions.sort(key=total_version_order)
+	versions.reverse()
+	return versions
+
+class package_reader(Context.Context):
+	cmd = 'solver'
+	fun = 'solver'
+
+	def __init__(self, **kw):
+		Context.Context.__init__(self, **kw)
+
+		self.myproject = getattr(Context.g_module, 'APPNAME', 'project')
+		self.myversion = getattr(Context.g_module, 'VERSION', '1.0')
+		self.cache_constraints = {}
+		self.constraints = []
+
+	def compute_dependencies(self, filename=REQUIRES):
+		text = Utils.readf(filename)
+		data = safe_urlencode([('text', text)])
+
+		if '--offline' in sys.argv:
+			self.constraints = self.local_resolve(text)
+		else:
+			req = Request(get_resolve_url(), data)
+			try:
+				response = urlopen(req, timeout=TIMEOUT)
+			except URLError as e:
+				Logs.warn('The package server is down! %r', e)
+				self.constraints = self.local_resolve(text)
+			else:
+				ret = response.read()
+				try:
+					ret = ret.decode('utf-8')
+				except Exception:
+					pass
+				self.trace(ret)
+				self.constraints = parse_constraints(ret)
+		self.check_errors()
+
+	def check_errors(self):
+		errors = False
+		for c in self.constraints:
+			if not c.required_version:
+				errors = True
+
+				reasons = c.why()
+				if len(reasons) == 1:
+					Logs.error('%s but no matching package could be found in this repository', reasons[0])
+				else:
+					Logs.error('Conflicts on package %r:', c.pkgname)
+					for r in reasons:
+						Logs.error('  %s', r)
+		if errors:
+			self.fatal('The package requirements cannot be satisfied!')
+
+	def load_constraints(self, pkgname, pkgver, requires=REQUIRES):
+		try:
+			return self.cache_constraints[(pkgname, pkgver)]
+		except KeyError:
+			text = Utils.readf(os.path.join(get_distnet_cache(), pkgname, pkgver, requires))
+			ret = parse_constraints(text)
+			self.cache_constraints[(pkgname, pkgver)] = ret
+			return ret
+
+	def apply_constraint(self, domain, constraint):
+		vname = constraint.required_version.replace('*', '.*')
+		rev = re.compile(vname, re.M)
+		ret = [x for x in domain if rev.match(x)]
+		return ret
+
+	def trace(self, *k):
+		if getattr(self, 'debug', None):
+			Logs.error(*k)
+
+	def solve(self, packages_to_versions={}, packages_to_constraints={}, pkgname='', pkgver='', todo=[], done=[]):
+		# breadth first search
+		n_packages_to_versions = dict(packages_to_versions)
+		n_packages_to_constraints = dict(packages_to_constraints)
+
+		self.trace("calling solve with %r    %r %r" % (packages_to_versions, todo, done))
+		done = done + [pkgname]
+
+		constraints = self.load_constraints(pkgname, pkgver)
+		self.trace("constraints %r" % constraints)
+
+		for k in constraints:
+			try:
+				domain = n_packages_to_versions[k.pkgname]
+			except KeyError:
+				domain = list_package_versions(get_distnet_cache(), k.pkgname)
+
+
+			self.trace("constraints?")
+			if not k.pkgname in done:
+				todo = todo + [k.pkgname]
+
+			self.trace("domain before %s -> %s, %r" % (pkgname, k.pkgname, domain))
+
+			# apply the constraint
+			domain = self.apply_constraint(domain, k)
+
+			self.trace("domain after %s -> %s, %r" % (pkgname, k.pkgname, domain))
+
+			n_packages_to_versions[k.pkgname] = domain
+
+			# then store the constraint applied
+			constraints = list(packages_to_constraints.get(k.pkgname, []))
+			constraints.append((pkgname, pkgver, k))
+			n_packages_to_constraints[k.pkgname] = constraints
+
+			if not domain:
+				self.trace("no domain while processing constraint %r from %r %r" % (domain, pkgname, pkgver))
+				return (n_packages_to_versions, n_packages_to_constraints)
+
+		# next package on the todo list
+		if not todo:
+			return (n_packages_to_versions, n_packages_to_constraints)
+
+		n_pkgname = todo[0]
+		n_pkgver = n_packages_to_versions[n_pkgname][0]
+		tmp = dict(n_packages_to_versions)
+		tmp[n_pkgname] = [n_pkgver]
+
+		self.trace("fixed point %s" % n_pkgname)
+
+		return self.solve(tmp, n_packages_to_constraints, n_pkgname, n_pkgver, todo[1:], done)
+
+	def get_results(self):
+		return '\n'.join([str(c) for c in self.constraints])
+
+	def solution_to_constraints(self, versions, constraints):
+		solution = []
+		for p in versions:
+			c = constraint()
+			solution.append(c)
+
+			c.pkgname = p
+			if versions[p]:
+				c.required_version = versions[p][0]
+			else:
+				c.required_version = ''
+			for (from_pkgname, from_pkgver, c2) in constraints.get(p, ''):
+				c.add_reason(c2.human_display(from_pkgname, from_pkgver))
+		return solution
+
+	def local_resolve(self, text):
+		self.cache_constraints[(self.myproject, self.myversion)] = parse_constraints(text)
+		p2v = OrderedDict({self.myproject: [self.myversion]})
+		(versions, constraints) = self.solve(p2v, {}, self.myproject, self.myversion, [])
+		return self.solution_to_constraints(versions, constraints)
+
+	def download_to_file(self, pkgname, pkgver, subdir, tmp):
+		data = safe_urlencode([('pkgname', pkgname), ('pkgver', pkgver), ('pkgfile', subdir)])
+		req = urlopen(get_download_url(), data, timeout=TIMEOUT)
+		with open(tmp, 'wb') as f:
+			while True:
+				buf = req.read(8192)
+				if not buf:
+					break
+				f.write(buf)
+
+	def extract_tar(self, subdir, pkgdir, tmpfile):
+		with tarfile.open(tmpfile) as f:
+			temp = tempfile.mkdtemp(dir=pkgdir)
+			try:
+				f.extractall(temp)
+				os.rename(temp, os.path.join(pkgdir, subdir))
+			finally:
+				try:
+					shutil.rmtree(temp)
+				except Exception:
+					pass
+
+	def get_pkg_dir(self, pkgname, pkgver, subdir):
+		pkgdir = os.path.join(get_distnet_cache(), pkgname, pkgver)
+		if not os.path.isdir(pkgdir):
+			os.makedirs(pkgdir)
+
+		target = os.path.join(pkgdir, subdir)
+
+		if os.path.exists(target):
+			return target
+
+		(fd, tmp) = tempfile.mkstemp(dir=pkgdir)
+		try:
+			os.close(fd)
+			self.download_to_file(pkgname, pkgver, subdir, tmp)
+			if subdir == REQUIRES:
+				os.rename(tmp, target)
+			else:
+				self.extract_tar(subdir, pkgdir, tmp)
+		finally:
+			try:
+				os.remove(tmp)
+			except OSError:
+				pass
+
+		return target
+
+	def __iter__(self):
+		if not self.constraints:
+			self.compute_dependencies()
+		for x in self.constraints:
+			if x.pkgname == self.myproject:
+				continue
+			yield x
+
+	def execute(self):
+		self.compute_dependencies()
+
+packages = package_reader()
+
+def load_tools(ctx, extra):
+	global packages
+	for c in packages:
+		packages.get_pkg_dir(c.pkgname, c.required_version, extra)
+		noarchdir = packages.get_pkg_dir(c.pkgname, c.required_version, 'noarch')
+		for x in os.listdir(noarchdir):
+			if x.startswith('waf_') and x.endswith('.py'):
+				ctx.load([x.rstrip('.py')], tooldir=[noarchdir])
+
+def options(opt):
+	opt.add_option('--offline', action='store_true')
+	packages.execute()
+	load_tools(opt, REQUIRES)
+
+def configure(conf):
+	load_tools(conf, conf.variant)
+
+def build(bld):
+	load_tools(bld, bld.variant)
+
diff --git a/waflib/extras/doxygen.py b/waflib/extras/doxygen.py
new file mode 100644
index 00000000..28f56e9c
--- /dev/null
+++ b/waflib/extras/doxygen.py
@@ -0,0 +1,227 @@
+#! /usr/bin/env python
+# encoding: UTF-8
+# Thomas Nagy 2008-2010 (ita)
+
+"""
+
+Doxygen support
+
+Variables passed to bld():
+* doxyfile -- the Doxyfile to use
+* doxy_tar -- destination archive for generated documentation (if desired)
+* install_path -- where to install the documentation
+* pars -- dictionary overriding doxygen configuration settings
+
+When using this tool, the wscript will look like:
+
+	def options(opt):
+		opt.load('doxygen')
+
+	def configure(conf):
+		conf.load('doxygen')
+		# check conf.env.DOXYGEN, if it is mandatory
+
+	def build(bld):
+		if bld.env.DOXYGEN:
+			bld(features="doxygen", doxyfile='Doxyfile', ...)
+"""
+
+import os, os.path, re
+from waflib import Task, Utils, Node
+from waflib.TaskGen import feature
+
+DOXY_STR = '"${DOXYGEN}" - '
+DOXY_FMTS = 'html latex man rft xml'.split()
+DOXY_FILE_PATTERNS = '*.' + ' *.'.join('''
+c cc cxx cpp c++ java ii ixx ipp i++ inl h hh hxx hpp h++ idl odl cs php php3
+inc m mm py f90c cc cxx cpp c++ java ii ixx ipp i++ inl h hh hxx
+'''.split())
+
+re_rl = re.compile('\\\\\r*\n', re.MULTILINE)
+re_nl = re.compile('\r*\n', re.M)
+def parse_doxy(txt):
+	tbl = {}
+	txt   = re_rl.sub('', txt)
+	lines = re_nl.split(txt)
+	for x in lines:
+		x = x.strip()
+		if not x or x.startswith('#') or x.find('=') < 0:
+			continue
+		if x.find('+=') >= 0:
+			tmp = x.split('+=')
+			key = tmp[0].strip()
+			if key in tbl:
+				tbl[key] += ' ' + '+='.join(tmp[1:]).strip()
+			else:
+				tbl[key] = '+='.join(tmp[1:]).strip()
+		else:
+			tmp = x.split('=')
+			tbl[tmp[0].strip()] = '='.join(tmp[1:]).strip()
+	return tbl
+
+class doxygen(Task.Task):
+	vars  = ['DOXYGEN', 'DOXYFLAGS']
+	color = 'BLUE'
+
+	def runnable_status(self):
+		'''
+		self.pars are populated in runnable_status - because this function is being
+		run *before* both self.pars "consumers" - scan() and run()
+
+		set output_dir (node) for the output
+		'''
+
+		for x in self.run_after:
+			if not x.hasrun:
+				return Task.ASK_LATER
+
+		if not getattr(self, 'pars', None):
+			txt = self.inputs[0].read()
+			self.pars = parse_doxy(txt)
+			if self.pars.get('OUTPUT_DIRECTORY'):
+				# Use the path parsed from the Doxyfile as an absolute path
+				output_node = self.inputs[0].parent.get_bld().make_node(self.pars['OUTPUT_DIRECTORY'])
+			else:
+				# If no OUTPUT_PATH was specified in the Doxyfile, build path from the Doxyfile name + '.doxy'
+				output_node = self.inputs[0].parent.get_bld().make_node(self.inputs[0].name + '.doxy')
+			output_node.mkdir()
+			self.pars['OUTPUT_DIRECTORY'] = output_node.abspath()
+
+			# Override with any parameters passed to the task generator
+			if getattr(self.generator, 'pars', None):
+				for k, v in self.generator.pars.items():
+					self.pars[k] = v
+
+			self.doxy_inputs = getattr(self, 'doxy_inputs', [])
+			if not self.pars.get('INPUT'):
+				self.doxy_inputs.append(self.inputs[0].parent)
+			else:
+				for i in self.pars.get('INPUT').split():
+					if os.path.isabs(i):
+						node = self.generator.bld.root.find_node(i)
+					else:
+						node = self.inputs[0].parent.find_node(i)
+					if not node:
+						self.generator.bld.fatal('Could not find the doxygen input %r' % i)
+					self.doxy_inputs.append(node)
+
+		if not getattr(self, 'output_dir', None):
+			bld = self.generator.bld
+			# Output path is always an absolute path as it was transformed above.
+			self.output_dir = bld.root.find_dir(self.pars['OUTPUT_DIRECTORY'])
+
+		self.signature()
+		ret = Task.Task.runnable_status(self)
+		if ret == Task.SKIP_ME:
+			# in case the files were removed
+			self.add_install()
+		return ret
+
+	def scan(self):
+		exclude_patterns = self.pars.get('EXCLUDE_PATTERNS','').split()
+		exclude_patterns = [pattern.replace('*/', '**/') for pattern in exclude_patterns]
+		file_patterns = self.pars.get('FILE_PATTERNS','').split()
+		if not file_patterns:
+			file_patterns = DOXY_FILE_PATTERNS.split()
+		if self.pars.get('RECURSIVE') == 'YES':
+			file_patterns = ["**/%s" % pattern for pattern in file_patterns]
+		nodes = []
+		names = []
+		for node in self.doxy_inputs:
+			if os.path.isdir(node.abspath()):
+				for m in node.ant_glob(incl=file_patterns, excl=exclude_patterns):
+					nodes.append(m)
+			else:
+				nodes.append(node)
+		return (nodes, names)
+
+	def run(self):
+		dct = self.pars.copy()
+		code = '\n'.join(['%s = %s' % (x, dct[x]) for x in self.pars])
+		code = code.encode() # for python 3
+		#fmt = DOXY_STR % (self.inputs[0].parent.abspath())
+		cmd = Utils.subst_vars(DOXY_STR, self.env)
+		env = self.env.env or None
+		proc = Utils.subprocess.Popen(cmd, shell=True, stdin=Utils.subprocess.PIPE, env=env, cwd=self.inputs[0].parent.abspath())
+		proc.communicate(code)
+		return proc.returncode
+
+	def post_run(self):
+		nodes = self.output_dir.ant_glob('**/*', quiet=True)
+		for x in nodes:
+			self.generator.bld.node_sigs[x] = self.uid()
+		self.add_install()
+		return Task.Task.post_run(self)
+
+	def add_install(self):
+		nodes = self.output_dir.ant_glob('**/*', quiet=True)
+		self.outputs += nodes
+		if getattr(self.generator, 'install_path', None):
+			if not getattr(self.generator, 'doxy_tar', None):
+				self.generator.add_install_files(install_to=self.generator.install_path,
+					install_from=self.outputs,
+					postpone=False,
+					cwd=self.output_dir,
+					relative_trick=True)
+
+class tar(Task.Task):
+	"quick tar creation"
+	run_str = '${TAR} ${TAROPTS} ${TGT} ${SRC}'
+	color   = 'RED'
+	after   = ['doxygen']
+	def runnable_status(self):
+		for x in getattr(self, 'input_tasks', []):
+			if not x.hasrun:
+				return Task.ASK_LATER
+
+		if not getattr(self, 'tar_done_adding', None):
+			# execute this only once
+			self.tar_done_adding = True
+			for x in getattr(self, 'input_tasks', []):
+				self.set_inputs(x.outputs)
+			if not self.inputs:
+				return Task.SKIP_ME
+		return Task.Task.runnable_status(self)
+
+	def __str__(self):
+		tgt_str = ' '.join([a.path_from(a.ctx.launch_node()) for a in self.outputs])
+		return '%s: %s\n' % (self.__class__.__name__, tgt_str)
+
+@feature('doxygen')
+def process_doxy(self):
+	if not getattr(self, 'doxyfile', None):
+		self.bld.fatal('no doxyfile variable specified??')
+
+	node = self.doxyfile
+	if not isinstance(node, Node.Node):
+		node = self.path.find_resource(node)
+	if not node:
+		self.bld.fatal('doxygen file %s not found' % self.doxyfile)
+
+	# the task instance
+	dsk = self.create_task('doxygen', node)
+
+	if getattr(self, 'doxy_tar', None):
+		tsk = self.create_task('tar')
+		tsk.input_tasks = [dsk]
+		tsk.set_outputs(self.path.find_or_declare(self.doxy_tar))
+		if self.doxy_tar.endswith('bz2'):
+			tsk.env['TAROPTS'] = ['cjf']
+		elif self.doxy_tar.endswith('gz'):
+			tsk.env['TAROPTS'] = ['czf']
+		else:
+			tsk.env['TAROPTS'] = ['cf']
+		if getattr(self, 'install_path', None):
+			self.add_install_files(install_to=self.install_path, install_from=tsk.outputs)
+
+def configure(conf):
+	'''
+	Check if doxygen and tar commands are present in the system
+
+	If the commands are present, then conf.env.DOXYGEN and conf.env.TAR
+	variables will be set. Detection can be controlled by setting DOXYGEN and
+	TAR environmental variables.
+	'''
+
+	conf.find_program('doxygen', var='DOXYGEN', mandatory=False)
+	conf.find_program('tar', var='TAR', mandatory=False)
diff --git a/waflib/extras/dpapi.py b/waflib/extras/dpapi.py
new file mode 100644
index 00000000..b94d4823
--- /dev/null
+++ b/waflib/extras/dpapi.py
@@ -0,0 +1,87 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Matt Clarkson, 2012
+
+'''
+DPAPI access library (http://msdn.microsoft.com/en-us/library/ms995355.aspx)
+This file uses code originally created by Crusher Joe:
+http://article.gmane.org/gmane.comp.python.ctypes/420
+And modified by Wayne Koorts:
+http://stackoverflow.com/questions/463832/using-dpapi-with-python
+'''
+
+from ctypes import windll, byref, cdll, Structure, POINTER, c_char, c_buffer
+from ctypes.wintypes import DWORD
+from waflib.Configure import conf
+
+LocalFree = windll.kernel32.LocalFree
+memcpy = cdll.msvcrt.memcpy
+CryptProtectData = windll.crypt32.CryptProtectData
+CryptUnprotectData = windll.crypt32.CryptUnprotectData
+CRYPTPROTECT_UI_FORBIDDEN = 0x01
+try:
+	extra_entropy = 'cl;ad13 \0al;323kjd #(adl;k$#ajsd'.encode('ascii')
+except AttributeError:
+	extra_entropy = 'cl;ad13 \0al;323kjd #(adl;k$#ajsd'
+
+class DATA_BLOB(Structure):
+	_fields_ = [
+		('cbData', DWORD),
+		('pbData', POINTER(c_char))
+	]
+
+def get_data(blob_out):
+	cbData = int(blob_out.cbData)
+	pbData = blob_out.pbData
+	buffer = c_buffer(cbData)
+	memcpy(buffer, pbData, cbData)
+	LocalFree(pbData)
+	return buffer.raw
+
+@conf
+def dpapi_encrypt_data(self, input_bytes, entropy = extra_entropy):
+	'''
+	Encrypts data and returns byte string
+
+	:param input_bytes: The data to be encrypted
+	:type input_bytes: String or Bytes
+	:param entropy: Extra entropy to add to the encryption process (optional)
+	:type entropy: String or Bytes
+	'''
+	if not isinstance(input_bytes, bytes) or not isinstance(entropy, bytes):
+		self.fatal('The inputs to dpapi must be bytes')
+	buffer_in      = c_buffer(input_bytes, len(input_bytes))
+	buffer_entropy = c_buffer(entropy, len(entropy))
+	blob_in        = DATA_BLOB(len(input_bytes), buffer_in)
+	blob_entropy   = DATA_BLOB(len(entropy), buffer_entropy)
+	blob_out       = DATA_BLOB()
+
+	if CryptProtectData(byref(blob_in), 'python_data', byref(blob_entropy), 
+		None, None, CRYPTPROTECT_UI_FORBIDDEN, byref(blob_out)):
+		return get_data(blob_out)
+	else:
+		self.fatal('Failed to decrypt data')
+
+@conf
+def dpapi_decrypt_data(self, encrypted_bytes, entropy = extra_entropy):
+	'''
+	Decrypts data and returns byte string
+
+	:param encrypted_bytes: The encrypted data
+	:type encrypted_bytes: Bytes
+	:param entropy: Extra entropy to add to the encryption process (optional)
+	:type entropy: String or Bytes
+	'''
+	if not isinstance(encrypted_bytes, bytes) or not isinstance(entropy, bytes):
+		self.fatal('The inputs to dpapi must be bytes')
+	buffer_in      = c_buffer(encrypted_bytes, len(encrypted_bytes))
+	buffer_entropy = c_buffer(entropy, len(entropy))
+	blob_in        = DATA_BLOB(len(encrypted_bytes), buffer_in)
+	blob_entropy   = DATA_BLOB(len(entropy), buffer_entropy)
+	blob_out       = DATA_BLOB()
+	if CryptUnprotectData(byref(blob_in), None, byref(blob_entropy), None,
+		None, CRYPTPROTECT_UI_FORBIDDEN, byref(blob_out)):
+		return get_data(blob_out)
+	else:
+		self.fatal('Failed to decrypt data')
+
diff --git a/waflib/extras/eclipse.py b/waflib/extras/eclipse.py
new file mode 100644
index 00000000..bb787416
--- /dev/null
+++ b/waflib/extras/eclipse.py
@@ -0,0 +1,431 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Eclipse CDT 5.0 generator for Waf
+# Richard Quirk 2009-1011 (New BSD License)
+# Thomas Nagy 2011 (ported to Waf 1.6)
+
+"""
+Usage:
+
+def options(opt):
+	opt.load('eclipse')
+
+$ waf configure eclipse
+"""
+
+import sys, os
+from waflib import Utils, Logs, Context, Build, TaskGen, Scripting, Errors, Node
+from xml.dom.minidom import Document
+
+STANDARD_INCLUDES = [ '/usr/local/include', '/usr/include' ]
+
+oe_cdt = 'org.eclipse.cdt'
+cdt_mk = oe_cdt + '.make.core'
+cdt_core = oe_cdt + '.core'
+cdt_bld = oe_cdt + '.build.core'
+extbuilder_dir = '.externalToolBuilders'
+extbuilder_name = 'Waf_Builder.launch'
+
+class eclipse(Build.BuildContext):
+	cmd = 'eclipse'
+	fun = Scripting.default_cmd
+
+	def execute(self):
+		"""
+		Entry point
+		"""
+		self.restore()
+		if not self.all_envs:
+			self.load_envs()
+		self.recurse([self.run_dir])
+
+		appname = getattr(Context.g_module, Context.APPNAME, os.path.basename(self.srcnode.abspath()))
+		self.create_cproject(appname, pythonpath=self.env['ECLIPSE_PYTHON_PATH'])
+
+	# Helper to dump the XML document content to XML with UTF-8 encoding
+	def write_conf_to_xml(self, filename, document):
+		self.srcnode.make_node(filename).write(document.toprettyxml(encoding='UTF-8'), flags='wb')
+
+	def create_cproject(self, appname, workspace_includes=[], pythonpath=[]):
+		"""
+		Create the Eclipse CDT .project and .cproject files
+		@param appname The name that will appear in the Project Explorer
+		@param build The BuildContext object to extract includes from
+		@param workspace_includes Optional project includes to prevent
+			  "Unresolved Inclusion" errors in the Eclipse editor
+		@param pythonpath Optional project specific python paths
+		"""
+		hasc = hasjava = haspython = False
+		source_dirs = []
+		cpppath = self.env['CPPPATH']
+		javasrcpath = []
+		javalibpath = []
+		includes = STANDARD_INCLUDES
+		if sys.platform != 'win32':
+			cc = self.env.CC or self.env.CXX
+			if cc:
+				cmd = cc + ['-xc++', '-E', '-Wp,-v', '-']
+				try:
+					gccout = self.cmd_and_log(cmd, output=Context.STDERR, quiet=Context.BOTH, input='\n'.encode()).splitlines()
+				except Errors.WafError:
+					pass
+				else:
+					includes = []
+					for ipath in gccout:
+						if ipath.startswith(' /'):
+							includes.append(ipath[1:])
+			cpppath += includes
+		Logs.warn('Generating Eclipse CDT project files')
+
+		for g in self.groups:
+			for tg in g:
+				if not isinstance(tg, TaskGen.task_gen):
+					continue
+
+				tg.post()
+
+				# Add local Python modules paths to configuration so object resolving will work in IDE
+				# This may also contain generated files (ie. pyqt5 or protoc) that get picked from build
+				if 'py' in tg.features:
+					pypath = tg.path.relpath()
+					py_installfrom = getattr(tg, 'install_from', None)
+					if isinstance(py_installfrom, Node.Node):
+						pypath = py_installfrom.path_from(self.root.make_node(self.top_dir))
+					if pypath not in pythonpath:
+						pythonpath.append(pypath)
+					haspython = True
+
+				# Add Java source directories so object resolving works in IDE
+				# This may also contain generated files (ie. protoc) that get picked from build
+				if 'javac' in tg.features:
+					java_src = tg.path.relpath()
+					java_srcdir = getattr(tg.javac_task, 'srcdir', None)
+					if java_srcdir:
+						if isinstance(java_srcdir, Node.Node):
+							java_srcdir = [java_srcdir]
+						for x in Utils.to_list(java_srcdir):
+							x = x.path_from(self.root.make_node(self.top_dir))
+							if x not in javasrcpath:
+								javasrcpath.append(x)
+					else:
+						if java_src not in javasrcpath:
+							javasrcpath.append(java_src)
+					hasjava = True
+
+					# Check if there are external dependencies and add them as external jar so they will be resolved by Eclipse
+					usedlibs=getattr(tg, 'use', [])
+					for x in Utils.to_list(usedlibs):
+						for cl in Utils.to_list(tg.env['CLASSPATH_'+x]):
+							if cl not in javalibpath:
+								javalibpath.append(cl)
+
+				if not getattr(tg, 'link_task', None):
+					continue
+
+				features = Utils.to_list(getattr(tg, 'features', ''))
+
+				is_cc = 'c' in features or 'cxx' in features
+
+				incnodes = tg.to_incnodes(tg.to_list(getattr(tg, 'includes', [])) + tg.env['INCLUDES'])
+				for p in incnodes:
+					path = p.path_from(self.srcnode)
+
+					if (path.startswith("/")):
+						cpppath.append(path)
+					else:
+						workspace_includes.append(path)
+
+					if is_cc and path not in source_dirs:
+						source_dirs.append(path)
+
+					hasc = True
+
+		waf_executable = os.path.abspath(sys.argv[0])
+		project = self.impl_create_project(sys.executable, appname, hasc, hasjava, haspython, waf_executable)
+		self.write_conf_to_xml('.project', project)
+
+		if hasc:
+			project = self.impl_create_cproject(sys.executable, waf_executable, appname, workspace_includes, cpppath, source_dirs)
+			self.write_conf_to_xml('.cproject', project)
+
+		if haspython:
+			project = self.impl_create_pydevproject(sys.path, pythonpath)
+			self.write_conf_to_xml('.pydevproject', project)
+
+		if hasjava:
+			project = self.impl_create_javaproject(javasrcpath, javalibpath)
+			self.write_conf_to_xml('.classpath', project)
+
+	def impl_create_project(self, executable, appname, hasc, hasjava, haspython, waf_executable):
+		doc = Document()
+		projectDescription = doc.createElement('projectDescription')
+		self.add(doc, projectDescription, 'name', appname)
+		self.add(doc, projectDescription, 'comment')
+		self.add(doc, projectDescription, 'projects')
+		buildSpec = self.add(doc, projectDescription, 'buildSpec')
+		buildCommand = self.add(doc, buildSpec, 'buildCommand')
+		self.add(doc, buildCommand, 'triggers', 'clean,full,incremental,')
+		arguments = self.add(doc, buildCommand, 'arguments')
+		dictionaries = {}
+
+		# If CDT is present, instruct this one to call waf as it is more flexible (separate build/clean ...)
+		if hasc:
+			self.add(doc, buildCommand, 'name', oe_cdt + '.managedbuilder.core.genmakebuilder')
+			# the default make-style targets are overwritten by the .cproject values
+			dictionaries = {
+					cdt_mk + '.contents': cdt_mk + '.activeConfigSettings',
+					cdt_mk + '.enableAutoBuild': 'false',
+					cdt_mk + '.enableCleanBuild': 'true',
+					cdt_mk + '.enableFullBuild': 'true',
+					}
+		else:
+			# Otherwise for Java/Python an external builder tool is created that will call waf build
+			self.add(doc, buildCommand, 'name', 'org.eclipse.ui.externaltools.ExternalToolBuilder')
+			dictionaries = {
+					'LaunchConfigHandle': '<project>/%s/%s'%(extbuilder_dir, extbuilder_name),
+					}
+			# The definition is in a separate directory XML file
+			try:
+				os.mkdir(extbuilder_dir)
+			except OSError:
+				pass	# Ignore error if already exists
+
+			# Populate here the external builder XML calling waf
+			builder = Document()
+			launchConfiguration = doc.createElement('launchConfiguration')
+			launchConfiguration.setAttribute('type', 'org.eclipse.ui.externaltools.ProgramBuilderLaunchConfigurationType')
+			self.add(doc, launchConfiguration, 'booleanAttribute', {'key': 'org.eclipse.debug.ui.ATTR_LAUNCH_IN_BACKGROUND', 'value': 'false'})
+			self.add(doc, launchConfiguration, 'booleanAttribute', {'key': 'org.eclipse.ui.externaltools.ATTR_TRIGGERS_CONFIGURED', 'value': 'true'})
+			self.add(doc, launchConfiguration, 'stringAttribute', {'key': 'org.eclipse.ui.externaltools.ATTR_LOCATION', 'value': waf_executable})
+			self.add(doc, launchConfiguration, 'stringAttribute', {'key': 'org.eclipse.ui.externaltools.ATTR_RUN_BUILD_KINDS', 'value': 'full,incremental,'})
+			self.add(doc, launchConfiguration, 'stringAttribute', {'key': 'org.eclipse.ui.externaltools.ATTR_TOOL_ARGUMENTS', 'value': 'build'})
+			self.add(doc, launchConfiguration, 'stringAttribute', {'key': 'org.eclipse.ui.externaltools.ATTR_WORKING_DIRECTORY', 'value': '${project_loc}'})
+			builder.appendChild(launchConfiguration)
+			# And write the XML to the file references before
+			self.write_conf_to_xml('%s%s%s'%(extbuilder_dir, os.path.sep, extbuilder_name), builder)
+
+
+		for k, v in dictionaries.items():
+			self.addDictionary(doc, arguments, k, v)
+
+		natures = self.add(doc, projectDescription, 'natures')
+
+		if hasc:
+			nature_list = """
+				core.ccnature
+				managedbuilder.core.ScannerConfigNature
+				managedbuilder.core.managedBuildNature
+				core.cnature
+			""".split()
+			for n in nature_list:
+				self.add(doc, natures, 'nature', oe_cdt + '.' + n)
+
+		if haspython:
+			self.add(doc, natures, 'nature', 'org.python.pydev.pythonNature')
+		if hasjava:
+			self.add(doc, natures, 'nature', 'org.eclipse.jdt.core.javanature')
+
+		doc.appendChild(projectDescription)
+		return doc
+
+	def impl_create_cproject(self, executable, waf_executable, appname, workspace_includes, cpppath, source_dirs=[]):
+		doc = Document()
+		doc.appendChild(doc.createProcessingInstruction('fileVersion', '4.0.0'))
+		cconf_id = cdt_core + '.default.config.1'
+		cproject = doc.createElement('cproject')
+		storageModule = self.add(doc, cproject, 'storageModule',
+				{'moduleId': cdt_core + '.settings'})
+		cconf = self.add(doc, storageModule, 'cconfiguration', {'id':cconf_id})
+
+		storageModule = self.add(doc, cconf, 'storageModule',
+				{'buildSystemId': oe_cdt + '.managedbuilder.core.configurationDataProvider',
+				 'id': cconf_id,
+				 'moduleId': cdt_core + '.settings',
+				 'name': 'Default'})
+
+		self.add(doc, storageModule, 'externalSettings')
+
+		extensions = self.add(doc, storageModule, 'extensions')
+		extension_list = """
+			VCErrorParser
+			MakeErrorParser
+			GCCErrorParser
+			GASErrorParser
+			GLDErrorParser
+		""".split()
+		self.add(doc, extensions, 'extension', {'id': cdt_core + '.ELF', 'point':cdt_core + '.BinaryParser'})
+		for e in extension_list:
+			self.add(doc, extensions, 'extension', {'id': cdt_core + '.' + e, 'point':cdt_core + '.ErrorParser'})
+
+		storageModule = self.add(doc, cconf, 'storageModule',
+				{'moduleId': 'cdtBuildSystem', 'version': '4.0.0'})
+		config = self.add(doc, storageModule, 'configuration',
+					{'artifactName': appname,
+					 'id': cconf_id,
+					 'name': 'Default',
+					 'parent': cdt_bld + '.prefbase.cfg'})
+		folderInfo = self.add(doc, config, 'folderInfo',
+							{'id': cconf_id+'.', 'name': '/', 'resourcePath': ''})
+
+		toolChain = self.add(doc, folderInfo, 'toolChain',
+				{'id': cdt_bld + '.prefbase.toolchain.1',
+				 'name': 'No ToolChain',
+				 'resourceTypeBasedDiscovery': 'false',
+				 'superClass': cdt_bld + '.prefbase.toolchain'})
+
+		self.add(doc, toolChain, 'targetPlatform', {'binaryParser': 'org.eclipse.cdt.core.ELF', 'id': cdt_bld + '.prefbase.toolchain.1', 'name': ''})
+
+		waf_build = '"%s" %s'%(waf_executable, eclipse.fun)
+		waf_clean = '"%s" clean'%(waf_executable)
+		self.add(doc, toolChain, 'builder',
+					{'autoBuildTarget': waf_build,
+					 'command': executable,
+					 'enableAutoBuild': 'false',
+					 'cleanBuildTarget': waf_clean,
+					 'enableIncrementalBuild': 'true',
+					 'id': cdt_bld + '.settings.default.builder.1',
+					 'incrementalBuildTarget': waf_build,
+					 'managedBuildOn': 'false',
+					 'name': 'Gnu Make Builder',
+					 'superClass': cdt_bld + '.settings.default.builder'})
+
+		tool_index = 1;
+		for tool_name in ("Assembly", "GNU C++", "GNU C"):
+			tool = self.add(doc, toolChain, 'tool',
+					{'id': cdt_bld + '.settings.holder.' + str(tool_index),
+					 'name': tool_name,
+					 'superClass': cdt_bld + '.settings.holder'})
+			if cpppath or workspace_includes:
+				incpaths = cdt_bld + '.settings.holder.incpaths'
+				option = self.add(doc, tool, 'option',
+						{'id': incpaths + '.' +  str(tool_index),
+						 'name': 'Include Paths',
+						 'superClass': incpaths,
+						 'valueType': 'includePath'})
+				for i in workspace_includes:
+					self.add(doc, option, 'listOptionValue',
+								{'builtIn': 'false',
+								'value': '"${workspace_loc:/%s/%s}"'%(appname, i)})
+				for i in cpppath:
+					self.add(doc, option, 'listOptionValue',
+								{'builtIn': 'false',
+								'value': '"%s"'%(i)})
+			if tool_name == "GNU C++" or tool_name == "GNU C":
+				self.add(doc,tool,'inputType',{ 'id':'org.eclipse.cdt.build.core.settings.holder.inType.' + str(tool_index), \
+					'languageId':'org.eclipse.cdt.core.gcc' if tool_name == "GNU C" else 'org.eclipse.cdt.core.g++','languageName':tool_name, \
+					'sourceContentType':'org.eclipse.cdt.core.cSource,org.eclipse.cdt.core.cHeader', \
+					'superClass':'org.eclipse.cdt.build.core.settings.holder.inType' })
+			tool_index += 1
+
+		if source_dirs:
+			sourceEntries = self.add(doc, config, 'sourceEntries')
+			for i in source_dirs:
+				 self.add(doc, sourceEntries, 'entry',
+							{'excluding': i,
+							'flags': 'VALUE_WORKSPACE_PATH|RESOLVED',
+							'kind': 'sourcePath',
+							'name': ''})
+				 self.add(doc, sourceEntries, 'entry',
+							{
+							'flags': 'VALUE_WORKSPACE_PATH|RESOLVED',
+							'kind': 'sourcePath',
+							'name': i})
+
+		storageModule = self.add(doc, cconf, 'storageModule',
+							{'moduleId': cdt_mk + '.buildtargets'})
+		buildTargets = self.add(doc, storageModule, 'buildTargets')
+		def addTargetWrap(name, runAll):
+			return self.addTarget(doc, buildTargets, executable, name,
+								'"%s" %s'%(waf_executable, name), runAll)
+		addTargetWrap('configure', True)
+		addTargetWrap('dist', False)
+		addTargetWrap('install', False)
+		addTargetWrap('check', False)
+
+		storageModule = self.add(doc, cproject, 'storageModule',
+							{'moduleId': 'cdtBuildSystem',
+							 'version': '4.0.0'})
+
+		self.add(doc, storageModule, 'project', {'id': '%s.null.1'%appname, 'name': appname})
+
+		doc.appendChild(cproject)
+		return doc
+
+	def impl_create_pydevproject(self, system_path, user_path):
+		# create a pydevproject file
+		doc = Document()
+		doc.appendChild(doc.createProcessingInstruction('eclipse-pydev', 'version="1.0"'))
+		pydevproject = doc.createElement('pydev_project')
+		prop = self.add(doc, pydevproject,
+					   'pydev_property',
+					   'python %d.%d'%(sys.version_info[0], sys.version_info[1]))
+		prop.setAttribute('name', 'org.python.pydev.PYTHON_PROJECT_VERSION')
+		prop = self.add(doc, pydevproject, 'pydev_property', 'Default')
+		prop.setAttribute('name', 'org.python.pydev.PYTHON_PROJECT_INTERPRETER')
+		# add waf's paths
+		wafadmin = [p for p in system_path if p.find('wafadmin') != -1]
+		if wafadmin:
+			prop = self.add(doc, pydevproject, 'pydev_pathproperty',
+					{'name':'org.python.pydev.PROJECT_EXTERNAL_SOURCE_PATH'})
+			for i in wafadmin:
+				self.add(doc, prop, 'path', i)
+		if user_path:
+			prop = self.add(doc, pydevproject, 'pydev_pathproperty',
+					{'name':'org.python.pydev.PROJECT_SOURCE_PATH'})
+			for i in user_path:
+				self.add(doc, prop, 'path', '/${PROJECT_DIR_NAME}/'+i)
+
+		doc.appendChild(pydevproject)
+		return doc
+
+	def impl_create_javaproject(self, javasrcpath, javalibpath):
+		# create a .classpath file for java usage
+		doc = Document()
+		javaproject = doc.createElement('classpath')
+		if javasrcpath:
+			for i in javasrcpath:
+				self.add(doc, javaproject, 'classpathentry',
+					{'kind': 'src', 'path': i})
+
+		if javalibpath:
+			for i in javalibpath:
+				self.add(doc, javaproject, 'classpathentry',
+					{'kind': 'lib', 'path': i})
+
+		self.add(doc, javaproject, 'classpathentry', {'kind': 'con', 'path': 'org.eclipse.jdt.launching.JRE_CONTAINER'})
+		self.add(doc, javaproject, 'classpathentry', {'kind': 'output', 'path': self.bldnode.name })
+		doc.appendChild(javaproject)
+		return doc
+
+	def addDictionary(self, doc, parent, k, v):
+		dictionary = self.add(doc, parent, 'dictionary')
+		self.add(doc, dictionary, 'key', k)
+		self.add(doc, dictionary, 'value', v)
+		return dictionary
+
+	def addTarget(self, doc, buildTargets, executable, name, buildTarget, runAllBuilders=True):
+		target = self.add(doc, buildTargets, 'target',
+						{'name': name,
+						 'path': '',
+						 'targetID': oe_cdt + '.build.MakeTargetBuilder'})
+		self.add(doc, target, 'buildCommand', executable)
+		self.add(doc, target, 'buildArguments', None)
+		self.add(doc, target, 'buildTarget', buildTarget)
+		self.add(doc, target, 'stopOnError', 'true')
+		self.add(doc, target, 'useDefaultCommand', 'false')
+		self.add(doc, target, 'runAllBuilders', str(runAllBuilders).lower())
+
+	def add(self, doc, parent, tag, value = None):
+		el = doc.createElement(tag)
+		if (value):
+			if type(value) == type(str()):
+				el.appendChild(doc.createTextNode(value))
+			elif type(value) == type(dict()):
+				self.setAttributes(el, value)
+		parent.appendChild(el)
+		return el
+
+	def setAttributes(self, node, attrs):
+		for k, v in attrs.items():
+			node.setAttribute(k, v)
+
diff --git a/waflib/extras/erlang.py b/waflib/extras/erlang.py
new file mode 100644
index 00000000..49f6d5b4
--- /dev/null
+++ b/waflib/extras/erlang.py
@@ -0,0 +1,110 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2010 (ita)
+# Przemyslaw Rzepecki, 2016
+
+"""
+Erlang support
+"""
+
+import re
+from waflib import Task, TaskGen
+from waflib.TaskGen import feature, after_method, before_method
+# to load the method "to_incnodes" below
+from waflib.Tools import ccroot
+
+# Those flags are required by the Erlang VM to execute/evaluate code in
+# non-interactive mode. It is used in this tool to create Erlang modules
+# documentation and run unit tests. The user can pass additional arguments to the
+# 'erl' command with ERL_FLAGS environment variable.
+EXEC_NON_INTERACTIVE = ['-noshell', '-noinput', '-eval']
+
+def configure(conf):
+	conf.find_program('erlc', var='ERLC')
+	conf.find_program('erl', var='ERL')
+	conf.add_os_flags('ERLC_FLAGS')
+	conf.add_os_flags('ERL_FLAGS')
+	conf.env.ERLC_DEF_PATTERN = '-D%s'
+	conf.env.ERLC_INC_PATTERN = '-I%s'
+
+@TaskGen.extension('.erl')
+def process_erl_node(self, node):
+	tsk = self.create_task('erl', node, node.change_ext('.beam'))
+	tsk.erlc_incnodes = [tsk.outputs[0].parent] + self.to_incnodes(self.includes)
+	tsk.env.append_value('ERLC_INCPATHS', [x.abspath() for x in tsk.erlc_incnodes])
+	tsk.env.append_value('ERLC_DEFINES', self.to_list(getattr(self, 'defines', [])))
+	tsk.env.append_value('ERLC_FLAGS', self.to_list(getattr(self, 'flags', [])))
+	tsk.cwd = tsk.outputs[0].parent
+
+class erl(Task.Task):
+	color = 'GREEN'
+	run_str = '${ERLC} ${ERL_FLAGS} ${ERLC_INC_PATTERN:ERLC_INCPATHS} ${ERLC_DEF_PATTERN:ERLC_DEFINES} ${SRC}'
+
+	def scan(task):
+		node = task.inputs[0]
+
+		deps = []
+		scanned = set([])
+		nodes_to_scan = [node]
+
+		for n in nodes_to_scan:
+			if n.abspath() in scanned:
+				continue
+
+			for i in re.findall('-include\("(.*)"\)\.', n.read()):
+				for d in task.erlc_incnodes:
+					r = d.find_node(i)
+					if r:
+						deps.append(r)
+						nodes_to_scan.append(r)
+						break
+			scanned.add(n.abspath())
+
+		return (deps, [])
+
+@TaskGen.extension('.beam')
+def process(self, node):
+	pass
+
+
+class erl_test(Task.Task):
+	color = 'BLUE'
+	run_str = '${ERL} ${ERL_FLAGS} ${ERL_TEST_FLAGS}'
+
+@feature('eunit')
+@after_method('process_source')
+def add_erl_test_run(self):
+	test_modules = [t.outputs[0] for t in self.tasks]
+	test_task = self.create_task('erl_test')
+	test_task.set_inputs(self.source + test_modules)
+	test_task.cwd = test_modules[0].parent
+
+	test_task.env.append_value('ERL_FLAGS', self.to_list(getattr(self, 'flags', [])))
+
+	test_list = ", ".join([m.change_ext("").path_from(test_task.cwd)+":test()" for m in test_modules])
+	test_flag = 'halt(case lists:all(fun(Elem) -> Elem == ok end, [%s]) of true -> 0; false -> 1 end).' % test_list
+	test_task.env.append_value('ERL_TEST_FLAGS', EXEC_NON_INTERACTIVE)
+	test_task.env.append_value('ERL_TEST_FLAGS', test_flag)
+
+
+class edoc(Task.Task):
+	color = 'BLUE'
+	run_str = "${ERL} ${ERL_FLAGS} ${ERL_DOC_FLAGS}"
+	def keyword(self):
+		return 'Generating edoc'
+
+@feature('edoc')
+@before_method('process_source')
+def add_edoc_task(self):
+	# do not process source, it would create double erl->beam task
+	self.meths.remove('process_source')
+	e = self.path.find_resource(self.source)
+	t = e.change_ext('.html')
+	png = t.parent.make_node('erlang.png')
+	css = t.parent.make_node('stylesheet.css')
+	tsk = self.create_task('edoc', e, [t, png, css])
+	tsk.cwd = tsk.outputs[0].parent
+	tsk.env.append_value('ERL_DOC_FLAGS', EXEC_NON_INTERACTIVE)
+	tsk.env.append_value('ERL_DOC_FLAGS', 'edoc:files(["%s"]), halt(0).' % tsk.inputs[0].abspath())
+	# TODO the above can break if a file path contains '"'
+
diff --git a/waflib/extras/fast_partial.py b/waflib/extras/fast_partial.py
new file mode 100644
index 00000000..b3af513b
--- /dev/null
+++ b/waflib/extras/fast_partial.py
@@ -0,0 +1,518 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2017-2018 (ita)
+
+"""
+A system for fast partial rebuilds
+
+Creating a large amount of task objects up front can take some time.
+By making a few assumptions, it is possible to avoid posting creating
+task objects for targets that are already up-to-date.
+
+On a silly benchmark the gain observed for 1M tasks can be 5m->10s
+for a single file change.
+
+Usage::
+
+	def options(opt):
+		opt.load('fast_partial')
+
+Assuptions:
+* Mostly for C/C++/Fortran targets with link tasks (object-only targets are not handled)
+* For full project builds: no --targets and no pruning from subfolders
+* The installation phase is ignored
+* `use=` dependencies are specified up front even across build groups
+* Task generator source files are not obtained from globs
+
+Implementation details:
+* The first layer obtains file timestamps to recalculate file hashes only
+  when necessary (similar to md5_tstamp); the timestamps are then stored
+  in a dedicated pickle file
+* A second layer associates each task generator to a file set to help
+  detecting changes. Task generators are to create their tasks only when
+  the related files have been modified. A specific db file is created
+  to store such data (5m -> 1m10)
+* A third layer binds build context proxies onto task generators, replacing
+  the default context. While loading data for the full build uses more memory
+  (4GB -> 9GB), partial builds are then much faster (1m10 -> 13s)
+* A fourth layer enables a 2-level cache on file signatures to
+  reduce the size of the main pickle file (13s -> 10s)
+"""
+
+import os
+from waflib import Build, Context, Errors, Logs, Task, TaskGen, Utils
+from waflib.TaskGen import feature, after_method, taskgen_method
+import waflib.Node
+
+DONE = 0
+DIRTY = 1
+NEEDED = 2
+
+SKIPPABLE = ['cshlib', 'cxxshlib', 'cstlib', 'cxxstlib', 'cprogram', 'cxxprogram']
+
+TSTAMP_DB = '.wafpickle_tstamp_db_file'
+
+SAVED_ATTRS = 'root node_sigs task_sigs imp_sigs raw_deps node_deps'.split()
+
+class bld_proxy(object):
+	def __init__(self, bld):
+		object.__setattr__(self, 'bld', bld)
+
+		object.__setattr__(self, 'node_class', type('Nod3', (waflib.Node.Node,), {}))
+		self.node_class.__module__ = 'waflib.Node'
+		self.node_class.ctx = self
+
+		object.__setattr__(self, 'root', self.node_class('', None))
+		for x in SAVED_ATTRS:
+			if x != 'root':
+				object.__setattr__(self, x, {})
+
+		self.fix_nodes()
+
+	def __setattr__(self, name, value):
+		bld = object.__getattribute__(self, 'bld')
+		setattr(bld, name, value)
+
+	def __delattr__(self, name):
+		bld = object.__getattribute__(self, 'bld')
+		delattr(bld, name)
+
+	def __getattribute__(self, name):
+		try:
+			return object.__getattribute__(self, name)
+		except AttributeError:
+			bld = object.__getattribute__(self, 'bld')
+			return getattr(bld, name)
+
+	def __call__(self, *k, **kw):
+		return self.bld(*k, **kw)
+
+	def fix_nodes(self):
+		for x in ('srcnode', 'path', 'bldnode'):
+			node = self.root.find_dir(getattr(self.bld, x).abspath())
+			object.__setattr__(self, x, node)
+
+	def set_key(self, store_key):
+		object.__setattr__(self, 'store_key', store_key)
+
+	def fix_tg_path(self, *tgs):
+		# changing Node objects on task generators is possible
+		# yet, all Node objects must belong to the same parent
+		for tg in tgs:
+			tg.path = self.root.make_node(tg.path.abspath())
+
+	def restore(self):
+		dbfn = os.path.join(self.variant_dir, Context.DBFILE + self.store_key)
+		Logs.debug('rev_use: reading %s', dbfn)
+		try:
+			data = Utils.readf(dbfn, 'rb')
+		except (EnvironmentError, EOFError):
+			# handle missing file/empty file
+			Logs.debug('rev_use: Could not load the build cache %s (missing)', dbfn)
+		else:
+			try:
+				waflib.Node.pickle_lock.acquire()
+				waflib.Node.Nod3 = self.node_class
+				try:
+					data = Build.cPickle.loads(data)
+				except Exception as e:
+					Logs.debug('rev_use: Could not pickle the build cache %s: %r', dbfn, e)
+				else:
+					for x in SAVED_ATTRS:
+						object.__setattr__(self, x, data.get(x, {}))
+			finally:
+				waflib.Node.pickle_lock.release()
+		self.fix_nodes()
+
+	def store(self):
+		data = {}
+		for x in Build.SAVED_ATTRS:
+			data[x] = getattr(self, x)
+		db = os.path.join(self.variant_dir, Context.DBFILE + self.store_key)
+
+		try:
+			waflib.Node.pickle_lock.acquire()
+			waflib.Node.Nod3 = self.node_class
+			x = Build.cPickle.dumps(data, Build.PROTOCOL)
+		finally:
+			waflib.Node.pickle_lock.release()
+
+		Logs.debug('rev_use: storing %s', db)
+		Utils.writef(db + '.tmp', x, m='wb')
+		try:
+			st = os.stat(db)
+			os.remove(db)
+			if not Utils.is_win32:
+				os.chown(db + '.tmp', st.st_uid, st.st_gid)
+		except (AttributeError, OSError):
+			pass
+		os.rename(db + '.tmp', db)
+
+class bld(Build.BuildContext):
+	def __init__(self, **kw):
+		super(bld, self).__init__(**kw)
+		self.hashes_md5_tstamp = {}
+
+	def __call__(self, *k, **kw):
+		# this is one way of doing it, one could use a task generator method too
+		bld = kw['bld'] = bld_proxy(self)
+		ret = TaskGen.task_gen(*k, **kw)
+		self.task_gen_cache_names = {}
+		self.add_to_group(ret, group=kw.get('group'))
+		ret.bld = bld
+		bld.set_key(ret.path.abspath().replace(os.sep, '') + str(ret.idx))
+		return ret
+
+	def is_dirty(self):
+		return True
+
+	def store_tstamps(self):
+		# Called after a build is finished
+		# For each task generator, record all files involved in task objects
+		# optimization: done only if there was something built
+		do_store = False
+		try:
+			f_deps = self.f_deps
+		except AttributeError:
+			f_deps = self.f_deps = {}
+			self.f_tstamps = {}
+
+		allfiles = set()
+		for g in self.groups:
+			for tg in g:
+				try:
+					staleness = tg.staleness
+				except AttributeError:
+					staleness = DIRTY
+
+				if staleness != DIRTY:
+					# DONE case: there was nothing built
+					# NEEDED case: the tg was brought in because of 'use' propagation
+					# but nothing really changed for them, there may be incomplete
+					# tasks (object files) and in this case it is best to let the next build
+					# figure out if an input/output file changed
+					continue
+
+				do_cache = False
+				for tsk in tg.tasks:
+					if tsk.hasrun == Task.SUCCESS:
+						do_cache = True
+						pass
+					elif tsk.hasrun == Task.SKIPPED:
+						pass
+					else:
+						# one failed task, clear the cache for this tg
+						try:
+							del f_deps[(tg.path.abspath(), tg.idx)]
+						except KeyError:
+							pass
+						else:
+							# just store the new state because there is a change
+							do_store = True
+
+						# skip the rest because there is no valid cache possible
+						break
+				else:
+					if not do_cache:
+						# all skipped, but is there anything in cache?
+						try:
+							f_deps[(tg.path.abspath(), tg.idx)]
+						except KeyError:
+							# probably cleared because a wscript file changed
+							# store it
+							do_cache = True
+
+					if do_cache:
+
+						# there was a rebuild, store the data structure too
+						tg.bld.store()
+
+						# all tasks skipped but no cache
+						# or a successful task build
+						do_store = True
+						st = set()
+						for tsk in tg.tasks:
+							st.update(tsk.inputs)
+							st.update(self.node_deps.get(tsk.uid(), []))
+
+						# TODO do last/when loading the tgs?
+						lst = []
+						for k in ('wscript', 'wscript_build'):
+							n = tg.path.find_node(k)
+							if n:
+								n.get_bld_sig()
+								lst.append(n.abspath())
+
+						lst.extend(sorted(x.abspath() for x in st))
+						allfiles.update(lst)
+						f_deps[(tg.path.abspath(), tg.idx)] = lst
+
+		for x in allfiles:
+			# f_tstamps has everything, while md5_tstamp can be relatively empty on partial builds
+			self.f_tstamps[x] = self.hashes_md5_tstamp[x][0]
+
+		if do_store:
+			dbfn = os.path.join(self.variant_dir, TSTAMP_DB)
+			Logs.debug('rev_use: storing %s', dbfn)
+			dbfn_tmp = dbfn + '.tmp'
+			x = Build.cPickle.dumps([self.f_tstamps, f_deps], Build.PROTOCOL)
+			Utils.writef(dbfn_tmp, x, m='wb')
+			os.rename(dbfn_tmp, dbfn)
+			Logs.debug('rev_use: stored %s', dbfn)
+
+	def store(self):
+		self.store_tstamps()
+		if self.producer.dirty:
+			Build.BuildContext.store(self)
+
+	def compute_needed_tgs(self):
+		# assume the 'use' keys are not modified during the build phase
+
+		dbfn = os.path.join(self.variant_dir, TSTAMP_DB)
+		Logs.debug('rev_use: Loading %s', dbfn)
+		try:
+			data = Utils.readf(dbfn, 'rb')
+		except (EnvironmentError, EOFError):
+			Logs.debug('rev_use: Could not load the build cache %s (missing)', dbfn)
+			self.f_deps = {}
+			self.f_tstamps = {}
+		else:
+			try:
+				self.f_tstamps, self.f_deps = Build.cPickle.loads(data)
+			except Exception as e:
+				Logs.debug('rev_use: Could not pickle the build cache %s: %r', dbfn, e)
+				self.f_deps = {}
+				self.f_tstamps = {}
+			else:
+				Logs.debug('rev_use: Loaded %s', dbfn)
+
+
+		# 1. obtain task generators that contain rebuilds
+		# 2. obtain the 'use' graph and its dual
+		stales = set()
+		reverse_use_map = Utils.defaultdict(list)
+		use_map = Utils.defaultdict(list)
+
+		for g in self.groups:
+			for tg in g:
+				if tg.is_stale():
+					stales.add(tg)
+
+				try:
+					lst = tg.use = Utils.to_list(tg.use)
+				except AttributeError:
+					pass
+				else:
+					for x in lst:
+						try:
+							xtg = self.get_tgen_by_name(x)
+						except Errors.WafError:
+							pass
+						else:
+							use_map[tg].append(xtg)
+							reverse_use_map[xtg].append(tg)
+
+		Logs.debug('rev_use: found %r stale tgs', len(stales))
+
+		# 3. dfs to post downstream tg as stale
+		visited = set()
+		def mark_down(tg):
+			if tg in visited:
+				return
+			visited.add(tg)
+			Logs.debug('rev_use: marking down %r as stale', tg.name)
+			tg.staleness = DIRTY
+			for x in reverse_use_map[tg]:
+				mark_down(x)
+		for tg in stales:
+			mark_down(tg)
+
+		# 4. dfs to find ancestors tg to mark as needed
+		self.needed_tgs = needed_tgs = set()
+		def mark_needed(tg):
+			if tg in needed_tgs:
+				return
+			needed_tgs.add(tg)
+			if tg.staleness == DONE:
+				Logs.debug('rev_use: marking up %r as needed', tg.name)
+				tg.staleness = NEEDED
+			for x in use_map[tg]:
+				mark_needed(x)
+		for xx in visited:
+			mark_needed(xx)
+
+		# so we have the whole tg trees to post in the set "needed"
+		# load their build trees
+		for tg in needed_tgs:
+			tg.bld.restore()
+			tg.bld.fix_tg_path(tg)
+
+		# the stale ones should be fully build, while the needed ones
+		# may skip a few tasks, see create_compiled_task and apply_link_after below
+		Logs.debug('rev_use: amount of needed task gens: %r', len(needed_tgs))
+
+	def post_group(self):
+		# assumption: we can ignore the folder/subfolders cuts
+		def tgpost(tg):
+			try:
+				f = tg.post
+			except AttributeError:
+				pass
+			else:
+				f()
+
+		if not self.targets or self.targets == '*':
+			for tg in self.groups[self.current_group]:
+				# this can cut quite a lot of tg objects
+				if tg in self.needed_tgs:
+					tgpost(tg)
+		else:
+			# default implementation
+			return Build.BuildContext.post_group()
+
+	def get_build_iterator(self):
+		if not self.targets or self.targets == '*':
+			self.compute_needed_tgs()
+		return Build.BuildContext.get_build_iterator(self)
+
+@taskgen_method
+def is_stale(self):
+	# assume no globs
+	self.staleness = DIRTY
+
+	# 1. the case of always stale targets
+	if getattr(self, 'always_stale', False):
+		return True
+
+	# 2. check if the db file exists
+	db = os.path.join(self.bld.variant_dir, Context.DBFILE)
+	try:
+		dbstat = os.stat(db).st_mtime
+	except OSError:
+		Logs.debug('rev_use: must post %r because this is a clean build')
+		return True
+
+	# 3. check if the configuration changed
+	if os.stat(self.bld.bldnode.find_node('c4che/build.config.py').abspath()).st_mtime > dbstat:
+		Logs.debug('rev_use: must post %r because the configuration has changed', self.name)
+		return True
+
+	# 3.a any tstamp data?
+	try:
+		f_deps = self.bld.f_deps
+	except AttributeError:
+		Logs.debug('rev_use: must post %r because there is no f_deps', self.name)
+		return True
+
+	# 4. check if this is the first build (no cache)
+	try:
+		lst = f_deps[(self.path.abspath(), self.idx)]
+	except KeyError:
+		Logs.debug('rev_use: must post %r because there it has no cached data', self.name)
+		return True
+
+	try:
+		cache = self.bld.cache_tstamp_rev_use
+	except AttributeError:
+		cache = self.bld.cache_tstamp_rev_use = {}
+
+	# 5. check the timestamp of each dependency files listed is unchanged
+	f_tstamps = self.bld.f_tstamps
+	for x in lst:
+		try:
+			old_ts = f_tstamps[x]
+		except KeyError:
+			Logs.debug('rev_use: must post %r because %r is not in cache', self.name, x)
+			return True
+
+		try:
+			try:
+				ts = cache[x]
+			except KeyError:
+				ts = cache[x] = os.stat(x).st_mtime
+		except OSError:
+			del f_deps[(self.path.abspath(), self.idx)]
+			Logs.debug('rev_use: must post %r because %r does not exist anymore', self.name, x)
+			return True
+		else:
+			if ts != old_ts:
+				Logs.debug('rev_use: must post %r because the timestamp on %r changed %r %r', self.name, x, old_ts, ts)
+				return True
+
+	self.staleness = DONE
+	return False
+
+@taskgen_method
+def create_compiled_task(self, name, node):
+	# skip the creation of object files
+	# assumption: object-only targets are not skippable
+	if self.staleness == NEEDED:
+		# only libraries/programs can skip object files
+		for x in SKIPPABLE:
+			if x in self.features:
+				return None
+
+	out = '%s.%d.o' % (node.name, self.idx)
+	task = self.create_task(name, node, node.parent.find_or_declare(out))
+	try:
+		self.compiled_tasks.append(task)
+	except AttributeError:
+		self.compiled_tasks = [task]
+	return task
+
+@feature(*SKIPPABLE)
+@after_method('apply_link')
+def apply_link_after(self):
+	# cprogram/cxxprogram might be unnecessary
+	if self.staleness != NEEDED:
+		return
+	for tsk in self.tasks:
+		tsk.hasrun = Task.SKIPPED
+
+def path_from(self, node):
+	# handle nodes of distinct types
+	if node.ctx is not self.ctx:
+		node = self.ctx.root.make_node(node.abspath())
+	return self.default_path_from(node)
+waflib.Node.Node.default_path_from = waflib.Node.Node.path_from
+waflib.Node.Node.path_from = path_from
+
+def h_file(self):
+	# similar to md5_tstamp.py, but with 2-layer cache
+	# global_cache for the build context common for all task generators
+	# local_cache for the build context proxy (one by task generator)
+	#
+	# the global cache is not persistent
+	# the local cache is persistent and meant for partial builds
+	#
+	# assume all calls are made from a single thread
+	#
+	filename = self.abspath()
+	st = os.stat(filename)
+
+	global_cache = self.ctx.bld.hashes_md5_tstamp
+	local_cache = self.ctx.hashes_md5_tstamp
+
+	if filename in global_cache:
+		# value already calculated in this build
+		cval = global_cache[filename]
+
+		# the value in global cache is assumed to be calculated once
+		# reverifying it could cause task generators
+		# to get distinct tstamp values, thus missing rebuilds
+		local_cache[filename] = cval
+		return cval[1]
+
+	if filename in local_cache:
+		cval = local_cache[filename]
+		if cval[0] == st.st_mtime:
+			# correct value from a previous build
+			# put it in the global cache
+			global_cache[filename] = cval
+			return cval[1]
+
+	ret = Utils.h_file(filename)
+	local_cache[filename] = global_cache[filename] = (st.st_mtime, ret)
+	return ret
+waflib.Node.Node.h_file = h_file
+
diff --git a/waflib/extras/fc_bgxlf.py b/waflib/extras/fc_bgxlf.py
new file mode 100644
index 00000000..cca18101
--- /dev/null
+++ b/waflib/extras/fc_bgxlf.py
@@ -0,0 +1,32 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# harald at klimachs.de
+
+from waflib.Tools import fc, fc_config, fc_scan
+from waflib.Configure import conf
+
+from waflib.Tools.compiler_fc import fc_compiler
+fc_compiler['linux'].insert(0, 'fc_bgxlf')
+
+@conf
+def find_bgxlf(conf):
+	fc = conf.find_program(['bgxlf2003_r','bgxlf2003'], var='FC')
+	conf.get_xlf_version(fc)
+	conf.env.FC_NAME = 'BGXLF'
+
+@conf
+def bg_flags(self):
+	self.env.SONAME_ST		 = ''
+	self.env.FCSHLIB_MARKER	= ''
+	self.env.FCSTLIB_MARKER	= ''
+	self.env.FCFLAGS_fcshlib   = ['-fPIC']
+	self.env.LINKFLAGS_fcshlib = ['-G', '-Wl,-bexpfull']
+
+def configure(conf):
+	conf.find_bgxlf()
+	conf.find_ar()
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.xlf_flags()
+	conf.bg_flags()
+
diff --git a/waflib/extras/fc_cray.py b/waflib/extras/fc_cray.py
new file mode 100644
index 00000000..da733fad
--- /dev/null
+++ b/waflib/extras/fc_cray.py
@@ -0,0 +1,51 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# harald at klimachs.de
+
+import re
+from waflib.Tools import fc, fc_config, fc_scan
+from waflib.Configure import conf
+
+from waflib.Tools.compiler_fc import fc_compiler
+fc_compiler['linux'].append('fc_cray')
+
+@conf
+def find_crayftn(conf):
+	"""Find the Cray fortran compiler (will look in the environment variable 'FC')"""
+	fc = conf.find_program(['crayftn'], var='FC')
+	conf.get_crayftn_version(fc)
+	conf.env.FC_NAME = 'CRAY'
+	conf.env.FC_MOD_CAPITALIZATION = 'UPPER.mod'
+
+@conf
+def crayftn_flags(conf):
+	v = conf.env
+	v['_FCMODOUTFLAGS']  = ['-em', '-J.'] # enable module files and put them in the current directory
+	v['FCFLAGS_DEBUG'] = ['-m1'] # more verbose compiler warnings
+	v['FCFLAGS_fcshlib']   = ['-h pic']
+	v['LINKFLAGS_fcshlib'] = ['-h shared']
+
+	v['FCSTLIB_MARKER'] = '-h static'
+	v['FCSHLIB_MARKER'] = '-h dynamic'
+
+@conf
+def get_crayftn_version(conf, fc):
+		version_re = re.compile(r"Cray Fortran\s*:\s*Version\s*(?P<major>\d*)\.(?P<minor>\d*)", re.I).search
+		cmd = fc + ['-V']
+		out,err = fc_config.getoutput(conf, cmd, stdin=False)
+		if out:
+			match = version_re(out)
+		else:
+			match = version_re(err)
+		if not match:
+				conf.fatal('Could not determine the Cray Fortran compiler version.')
+		k = match.groupdict()
+		conf.env['FC_VERSION'] = (k['major'], k['minor'])
+
+def configure(conf):
+	conf.find_crayftn()
+	conf.find_ar()
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.crayftn_flags()
+
diff --git a/waflib/extras/fc_nag.py b/waflib/extras/fc_nag.py
new file mode 100644
index 00000000..edcb218b
--- /dev/null
+++ b/waflib/extras/fc_nag.py
@@ -0,0 +1,61 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# harald at klimachs.de
+
+import re
+from waflib import Utils
+from waflib.Tools import fc,fc_config,fc_scan
+from waflib.Configure import conf
+
+from waflib.Tools.compiler_fc import fc_compiler
+fc_compiler['linux'].insert(0, 'fc_nag')
+
+@conf
+def find_nag(conf):
+	"""Find the NAG Fortran Compiler (will look in the environment variable 'FC')"""
+
+	fc = conf.find_program(['nagfor'], var='FC')
+	conf.get_nag_version(fc)
+	conf.env.FC_NAME = 'NAG'
+	conf.env.FC_MOD_CAPITALIZATION = 'lower'
+
+@conf
+def nag_flags(conf):
+	v = conf.env
+	v.FCFLAGS_DEBUG = ['-C=all']
+	v.FCLNK_TGT_F = ['-o', '']
+	v.FC_TGT_F = ['-c', '-o', '']
+
+@conf
+def nag_modifier_platform(conf):
+	dest_os = conf.env['DEST_OS'] or Utils.unversioned_sys_platform()
+	nag_modifier_func = getattr(conf, 'nag_modifier_' + dest_os, None)
+	if nag_modifier_func:
+		nag_modifier_func()
+
+@conf
+def get_nag_version(conf, fc):
+	"""Get the NAG compiler version"""
+
+	version_re = re.compile(r"^NAG Fortran Compiler *Release *(?P<major>\d*)\.(?P<minor>\d*)", re.M).search
+	cmd = fc + ['-V']
+
+	out, err = fc_config.getoutput(conf,cmd,stdin=False)
+	if out:
+		match = version_re(out)
+		if not match:
+			match = version_re(err)
+	else: match = version_re(err)
+	if not match:
+		conf.fatal('Could not determine the NAG version.')
+	k = match.groupdict()
+	conf.env['FC_VERSION'] = (k['major'], k['minor'])
+
+def configure(conf):
+	conf.find_nag()
+	conf.find_ar()
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.nag_flags()
+	conf.nag_modifier_platform()
+
diff --git a/waflib/extras/fc_nec.py b/waflib/extras/fc_nec.py
new file mode 100644
index 00000000..67c86808
--- /dev/null
+++ b/waflib/extras/fc_nec.py
@@ -0,0 +1,60 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# harald at klimachs.de
+
+import re
+from waflib.Tools import fc, fc_config, fc_scan
+from waflib.Configure import conf
+
+from waflib.Tools.compiler_fc import fc_compiler
+fc_compiler['linux'].append('fc_nec')
+
+@conf
+def find_sxfc(conf):
+	"""Find the NEC fortran compiler (will look in the environment variable 'FC')"""
+	fc = conf.find_program(['sxf90','sxf03'], var='FC')
+	conf.get_sxfc_version(fc)
+	conf.env.FC_NAME = 'NEC'
+	conf.env.FC_MOD_CAPITALIZATION = 'lower'
+
+@conf
+def sxfc_flags(conf):
+	v = conf.env
+	v['_FCMODOUTFLAGS']  = [] # enable module files and put them in the current directory
+	v['FCFLAGS_DEBUG'] = [] # more verbose compiler warnings
+	v['FCFLAGS_fcshlib']   = []
+	v['LINKFLAGS_fcshlib'] = []
+
+	v['FCSTLIB_MARKER'] = ''
+	v['FCSHLIB_MARKER'] = ''
+
+@conf
+def get_sxfc_version(conf, fc):
+	version_re = re.compile(r"FORTRAN90/SX\s*Version\s*(?P<major>\d*)\.(?P<minor>\d*)", re.I).search
+	cmd = fc + ['-V']
+	out,err = fc_config.getoutput(conf, cmd, stdin=False)
+	if out:
+		match = version_re(out)
+	else:
+		match = version_re(err)
+	if not match:
+		version_re=re.compile(r"NEC Fortran 2003 Compiler for\s*(?P<major>\S*)\s*\(c\)\s*(?P<minor>\d*)",re.I).search
+		if out:
+			match = version_re(out)
+		else:
+			match = version_re(err)
+		if not match:
+			conf.fatal('Could not determine the NEC Fortran compiler version.')
+	k = match.groupdict()
+	conf.env['FC_VERSION'] = (k['major'], k['minor'])
+
+def configure(conf):
+	conf.find_sxfc()
+	conf.find_program('sxar',var='AR')
+	conf.add_os_flags('ARFLAGS')
+	if not conf.env.ARFLAGS:
+		conf.env.ARFLAGS=['rcs']
+
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.sxfc_flags()
diff --git a/waflib/extras/fc_open64.py b/waflib/extras/fc_open64.py
new file mode 100644
index 00000000..413719f4
--- /dev/null
+++ b/waflib/extras/fc_open64.py
@@ -0,0 +1,58 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# harald at klimachs.de
+
+import re
+from waflib import Utils
+from waflib.Tools import fc,fc_config,fc_scan
+from waflib.Configure import conf
+
+from waflib.Tools.compiler_fc import fc_compiler
+fc_compiler['linux'].insert(0, 'fc_open64')
+
+@conf
+def find_openf95(conf):
+	"""Find the Open64 Fortran Compiler (will look in the environment variable 'FC')"""
+
+	fc = conf.find_program(['openf95', 'openf90'], var='FC')
+	conf.get_open64_version(fc)
+	conf.env.FC_NAME = 'OPEN64'
+	conf.env.FC_MOD_CAPITALIZATION = 'UPPER.mod'
+
+@conf
+def openf95_flags(conf):
+	v = conf.env
+	v['FCFLAGS_DEBUG'] = ['-fullwarn']
+
+@conf
+def openf95_modifier_platform(conf):
+	dest_os = conf.env['DEST_OS'] or Utils.unversioned_sys_platform()
+	openf95_modifier_func = getattr(conf, 'openf95_modifier_' + dest_os, None)
+	if openf95_modifier_func:
+		openf95_modifier_func()
+
+@conf
+def get_open64_version(conf, fc):
+	"""Get the Open64 compiler version"""
+
+	version_re = re.compile(r"Open64 Compiler Suite: *Version *(?P<major>\d*)\.(?P<minor>\d*)", re.I).search
+	cmd = fc + ['-version']
+
+	out, err = fc_config.getoutput(conf,cmd,stdin=False)
+	if out:
+		match = version_re(out)
+	else:
+		match = version_re(err)
+	if not match:
+		conf.fatal('Could not determine the Open64 version.')
+	k = match.groupdict()
+	conf.env['FC_VERSION'] = (k['major'], k['minor'])
+
+def configure(conf):
+	conf.find_openf95()
+	conf.find_ar()
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.openf95_flags()
+	conf.openf95_modifier_platform()
+
diff --git a/waflib/extras/fc_pgfortran.py b/waflib/extras/fc_pgfortran.py
new file mode 100644
index 00000000..afb2817b
--- /dev/null
+++ b/waflib/extras/fc_pgfortran.py
@@ -0,0 +1,68 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# harald at klimachs.de
+
+import re
+from waflib.Tools import fc, fc_config, fc_scan
+from waflib.Configure import conf
+
+from waflib.Tools.compiler_fc import fc_compiler
+fc_compiler['linux'].append('fc_pgfortran')
+
+@conf
+def find_pgfortran(conf):
+	"""Find the PGI fortran compiler (will look in the environment variable 'FC')"""
+	fc = conf.find_program(['pgfortran', 'pgf95', 'pgf90'], var='FC')
+	conf.get_pgfortran_version(fc)
+	conf.env.FC_NAME = 'PGFC'
+
+@conf
+def pgfortran_flags(conf):
+	v = conf.env
+	v['FCFLAGS_fcshlib']   = ['-shared']
+	v['FCFLAGS_DEBUG'] = ['-Minform=inform', '-Mstandard'] # why not
+	v['FCSTLIB_MARKER'] = '-Bstatic'
+	v['FCSHLIB_MARKER'] = '-Bdynamic'
+	v['SONAME_ST']	  = '-soname %s'
+
+@conf
+def get_pgfortran_version(conf,fc):
+		version_re = re.compile(r"The Portland Group", re.I).search
+		cmd = fc + ['-V']
+		out,err = fc_config.getoutput(conf, cmd, stdin=False)
+		if out:
+			match = version_re(out)
+		else:
+			match = version_re(err)
+		if not match:
+				conf.fatal('Could not verify PGI signature')
+		cmd = fc + ['-help=variable']
+		out,err = fc_config.getoutput(conf, cmd, stdin=False)
+		if out.find('COMPVER')<0:
+				conf.fatal('Could not determine the compiler type')
+		k = {}
+		prevk = ''
+		out = out.splitlines()
+		for line in out:
+				lst = line.partition('=')
+				if lst[1] == '=':
+						key = lst[0].rstrip()
+						if key == '':
+							key = prevk
+						val = lst[2].rstrip()
+						k[key] = val
+				else:
+					prevk = line.partition(' ')[0]
+		def isD(var):
+				return var in k
+		def isT(var):
+				return var in k and k[var]!='0'
+		conf.env['FC_VERSION'] = (k['COMPVER'].split('.'))
+
+def configure(conf):
+	conf.find_pgfortran()
+	conf.find_ar()
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.pgfortran_flags()
+
diff --git a/waflib/extras/fc_solstudio.py b/waflib/extras/fc_solstudio.py
new file mode 100644
index 00000000..53766df8
--- /dev/null
+++ b/waflib/extras/fc_solstudio.py
@@ -0,0 +1,62 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# harald at klimachs.de
+
+import re
+from waflib import Utils
+from waflib.Tools import fc,fc_config,fc_scan
+from waflib.Configure import conf
+
+from waflib.Tools.compiler_fc import fc_compiler
+fc_compiler['linux'].append('fc_solstudio')
+
+@conf
+def find_solstudio(conf):
+	"""Find the Solaris Studio compiler (will look in the environment variable 'FC')"""
+
+	fc = conf.find_program(['sunf95', 'f95', 'sunf90', 'f90'], var='FC')
+	conf.get_solstudio_version(fc)
+	conf.env.FC_NAME = 'SOL'
+
+@conf
+def solstudio_flags(conf):
+	v = conf.env
+	v['FCFLAGS_fcshlib'] = ['-Kpic']
+	v['FCFLAGS_DEBUG'] = ['-w3']
+	v['LINKFLAGS_fcshlib'] = ['-G']
+	v['FCSTLIB_MARKER'] = '-Bstatic'
+	v['FCSHLIB_MARKER'] = '-Bdynamic'
+	v['SONAME_ST']      = '-h %s'
+
+@conf
+def solstudio_modifier_platform(conf):
+	dest_os = conf.env['DEST_OS'] or Utils.unversioned_sys_platform()
+	solstudio_modifier_func = getattr(conf, 'solstudio_modifier_' + dest_os, None)
+	if solstudio_modifier_func:
+		solstudio_modifier_func()
+
+@conf
+def get_solstudio_version(conf, fc):
+	"""Get the compiler version"""
+
+	version_re = re.compile(r"Sun Fortran 95 *(?P<major>\d*)\.(?P<minor>\d*)", re.I).search
+	cmd = fc + ['-V']
+
+	out, err = fc_config.getoutput(conf,cmd,stdin=False)
+	if out:
+		match = version_re(out)
+	else:
+		match = version_re(err)
+	if not match:
+		conf.fatal('Could not determine the Sun Studio Fortran version.')
+	k = match.groupdict()
+	conf.env['FC_VERSION'] = (k['major'], k['minor'])
+
+def configure(conf):
+	conf.find_solstudio()
+	conf.find_ar()
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.solstudio_flags()
+	conf.solstudio_modifier_platform()
+
diff --git a/waflib/extras/fc_xlf.py b/waflib/extras/fc_xlf.py
new file mode 100644
index 00000000..5a3da034
--- /dev/null
+++ b/waflib/extras/fc_xlf.py
@@ -0,0 +1,63 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# harald at klimachs.de
+
+import re
+from waflib import Utils,Errors
+from waflib.Tools import fc,fc_config,fc_scan
+from waflib.Configure import conf
+
+from waflib.Tools.compiler_fc import fc_compiler
+fc_compiler['aix'].insert(0, 'fc_xlf')
+
+@conf
+def find_xlf(conf):
+	"""Find the xlf program (will look in the environment variable 'FC')"""
+
+	fc = conf.find_program(['xlf2003_r', 'xlf2003', 'xlf95_r', 'xlf95', 'xlf90_r', 'xlf90', 'xlf_r', 'xlf'], var='FC')
+	conf.get_xlf_version(fc)
+	conf.env.FC_NAME='XLF'
+
+@conf
+def xlf_flags(conf):
+	v = conf.env
+	v['FCDEFINES_ST'] = '-WF,-D%s'
+	v['FCFLAGS_fcshlib'] = ['-qpic=small']
+	v['FCFLAGS_DEBUG'] = ['-qhalt=w']
+	v['LINKFLAGS_fcshlib'] = ['-Wl,-shared']
+
+@conf
+def xlf_modifier_platform(conf):
+	dest_os = conf.env['DEST_OS'] or Utils.unversioned_sys_platform()
+	xlf_modifier_func = getattr(conf, 'xlf_modifier_' + dest_os, None)
+	if xlf_modifier_func:
+		xlf_modifier_func()
+
+@conf
+def get_xlf_version(conf, fc):
+	"""Get the compiler version"""
+
+	cmd = fc + ['-qversion']
+	try:
+		out, err = conf.cmd_and_log(cmd, output=0)
+	except Errors.WafError:
+		conf.fatal('Could not find xlf %r' % cmd)
+
+	for v in (r"IBM XL Fortran.* V(?P<major>\d*)\.(?P<minor>\d*)",):
+		version_re = re.compile(v, re.I).search
+		match = version_re(out or err)
+		if match:
+			k = match.groupdict()
+			conf.env['FC_VERSION'] = (k['major'], k['minor'])
+			break
+	else:
+		conf.fatal('Could not determine the XLF version.')
+
+def configure(conf):
+	conf.find_xlf()
+	conf.find_ar()
+	conf.fc_flags()
+	conf.fc_add_flags()
+	conf.xlf_flags()
+	conf.xlf_modifier_platform()
+
diff --git a/waflib/extras/file_to_object.py b/waflib/extras/file_to_object.py
new file mode 100644
index 00000000..1393b511
--- /dev/null
+++ b/waflib/extras/file_to_object.py
@@ -0,0 +1,137 @@
+#!/usr/bin/python
+# -*- coding: utf-8 -*-
+# Tool to embed file into objects
+
+__author__ = __maintainer__ = "Jérôme Carretero <cJ-waf@zougloub.eu>"
+__copyright__ = "Jérôme Carretero, 2014"
+
+"""
+
+This tool allows to embed file contents in object files (.o).
+It is not exactly portable, and the file contents are reachable
+using various non-portable fashions.
+The goal here is to provide a functional interface to the embedding
+of file data in objects.
+See the ``playground/embedded_resources`` example for an example.
+
+Usage::
+
+   bld(
+    name='pipeline',
+     # ^ Reference this in use="..." for things using the generated code
+    features='file_to_object',
+    source='some.file',
+     # ^ Name of the file to embed in binary section.
+   )
+
+Known issues:
+
+- Destination is named like source, with extension renamed to .o
+  eg. some.file -> some.o
+
+"""
+
+import os
+from waflib import Task, TaskGen, Errors
+
+def filename_c_escape(x):
+	return x.replace("\\", "\\\\")
+
+class file_to_object_s(Task.Task):
+	color = 'CYAN'
+	vars = ['DEST_CPU', 'DEST_BINFMT']
+
+	def run(self):
+		name = []
+		for i, x in enumerate(self.inputs[0].name):
+			if x.isalnum():
+				name.append(x)
+			else:
+				name.append('_')
+		file = self.inputs[0].abspath()
+		size = os.path.getsize(file)
+		if self.env.DEST_CPU in ('x86_64', 'ia', 'aarch64'):
+			unit = 'quad'
+			align = 8
+		elif self.env.DEST_CPU in ('x86','arm', 'thumb', 'm68k'):
+			unit = 'long'
+			align = 4
+		else:
+			raise Errors.WafError("Unsupported DEST_CPU, please report bug!")
+
+		file = filename_c_escape(file)
+		name = "_binary_" + "".join(name)
+		rodata = ".section .rodata"
+		if self.env.DEST_BINFMT == "mac-o":
+			name = "_" + name
+			rodata = ".section __TEXT,__const"
+
+		with open(self.outputs[0].abspath(), 'w') as f:
+			f.write(\
+"""
+	.global %(name)s_start
+	.global %(name)s_end
+	.global %(name)s_size
+	%(rodata)s
+%(name)s_start:
+	.incbin "%(file)s"
+%(name)s_end:
+	.align %(align)d
+%(name)s_size:
+	.%(unit)s 0x%(size)x
+""" % locals())
+
+class file_to_object_c(Task.Task):
+	color = 'CYAN'
+	def run(self):
+		name = []
+		for i, x in enumerate(self.inputs[0].name):
+			if x.isalnum():
+				name.append(x)
+			else:
+				name.append('_')
+		file = self.inputs[0].abspath()
+		size = os.path.getsize(file)
+
+		name = "_binary_" + "".join(name)
+
+		data = self.inputs[0].read('rb')
+		lines, line = [], []
+		for idx_byte, byte in enumerate(data):
+			line.append(byte)
+			if len(line) > 15 or idx_byte == size-1:
+				lines.append(", ".join(("0x%02x" % ord(x)) for x in line))
+				line = []
+		data = ",\n ".join(lines)
+
+		self.outputs[0].write(\
+"""
+unsigned long %(name)s_size = %(size)dL;
+char const %(name)s_start[] = {
+ %(data)s
+};
+char const %(name)s_end[] = {};
+""" % locals())
+
+@TaskGen.feature('file_to_object')
+@TaskGen.before_method('process_source')
+def tg_file_to_object(self):
+	bld = self.bld
+	sources = self.to_nodes(self.source)
+	targets = []
+	for src in sources:
+		if bld.env.F2O_METHOD == ["asm"]:
+			tgt = src.parent.find_or_declare(src.name + '.f2o.s')
+			tsk = self.create_task('file_to_object_s', src, tgt)
+			tsk.cwd = src.parent.abspath() # verify
+		else:
+			tgt = src.parent.find_or_declare(src.name + '.f2o.c')
+			tsk = self.create_task('file_to_object_c', src, tgt)
+			tsk.cwd = src.parent.abspath() # verify
+		targets.append(tgt)
+	self.source = targets
+
+def configure(conf):
+	conf.load('gas')
+	conf.env.F2O_METHOD = ["c"]
+
diff --git a/waflib/extras/fluid.py b/waflib/extras/fluid.py
new file mode 100644
index 00000000..4814a35b
--- /dev/null
+++ b/waflib/extras/fluid.py
@@ -0,0 +1,30 @@
+#!/usr/bin/python
+# encoding: utf-8
+# Grygoriy Fuchedzhy 2009
+
+"""
+Compile fluid files (fltk graphic library). Use the 'fluid' feature in conjunction with the 'cxx' feature.
+"""
+
+from waflib import Task
+from waflib.TaskGen import extension
+
+class fluid(Task.Task):
+	color   = 'BLUE'
+	ext_out = ['.h']
+	run_str = '${FLUID} -c -o ${TGT[0].abspath()} -h ${TGT[1].abspath()} ${SRC}'
+
+@extension('.fl')
+def process_fluid(self, node):
+	"""add the .fl to the source list; the cxx file generated will be compiled when possible"""
+	cpp = node.change_ext('.cpp')
+	hpp = node.change_ext('.hpp')
+	self.create_task('fluid', node, [cpp, hpp])
+
+	if 'cxx' in self.features:
+		self.source.append(cpp)
+
+def configure(conf):
+	conf.find_program('fluid', var='FLUID')
+	conf.check_cfg(path='fltk-config', package='', args='--cxxflags --ldflags', uselib_store='FLTK', mandatory=True)
+
diff --git a/waflib/extras/freeimage.py b/waflib/extras/freeimage.py
new file mode 100644
index 00000000..f27e5258
--- /dev/null
+++ b/waflib/extras/freeimage.py
@@ -0,0 +1,74 @@
+#!/usr/bin/env python
+# encoding: utf-8
+#
+# written by Sylvain Rouquette, 2011
+
+'''
+To add the freeimage tool to the waf file:
+$ ./waf-light --tools=compat15,freeimage
+	or, if you have waf >= 1.6.2
+$ ./waf update --files=freeimage
+
+The wscript will look like:
+
+def options(opt):
+	opt.load('compiler_cxx freeimage')
+
+def configure(conf):
+	conf.load('compiler_cxx freeimage')
+
+	# you can call check_freeimage with some parameters.
+	# It's optional on Linux, it's 'mandatory' on Windows if
+	# you didn't use --fi-path on the command-line
+
+	# conf.check_freeimage(path='FreeImage/Dist', fip=True)
+
+def build(bld):
+	bld(source='main.cpp', target='app', use='FREEIMAGE')
+'''
+
+from waflib import Utils
+from waflib.Configure import conf
+
+
+def options(opt):
+	opt.add_option('--fi-path', type='string', default='', dest='fi_path',
+				   help='''path to the FreeImage directory \
+						where the files are e.g. /FreeImage/Dist''')
+	opt.add_option('--fip', action='store_true', default=False, dest='fip',
+				   help='link with FreeImagePlus')
+	opt.add_option('--fi-static', action='store_true',
+				   default=False, dest='fi_static',
+				   help="link as shared libraries")
+
+
+@conf
+def check_freeimage(self, path=None, fip=False):
+	self.start_msg('Checking FreeImage')
+	if not self.env['CXX']:
+		self.fatal('you must load compiler_cxx before loading freeimage')
+	prefix = self.options.fi_static and 'ST' or ''
+	platform = Utils.unversioned_sys_platform()
+	if platform == 'win32':
+		if not path:
+			self.fatal('you must specify the path to FreeImage. \
+					   use --fi-path=/FreeImage/Dist')
+		else:
+			self.env['INCLUDES_FREEIMAGE'] = path
+			self.env['%sLIBPATH_FREEIMAGE' % prefix] = path
+	libs = ['FreeImage']
+	if self.options.fip:
+		libs.append('FreeImagePlus')
+	if platform == 'win32':
+		self.env['%sLIB_FREEIMAGE' % prefix] = libs
+	else:
+		self.env['%sLIB_FREEIMAGE' % prefix] = [i.lower() for i in libs]
+	self.end_msg('ok')
+
+
+def configure(conf):
+	platform = Utils.unversioned_sys_platform()
+	if platform == 'win32' and not conf.options.fi_path:
+		return
+	conf.check_freeimage(conf.options.fi_path, conf.options.fip)
+
diff --git a/waflib/extras/fsb.py b/waflib/extras/fsb.py
new file mode 100644
index 00000000..1b8f398f
--- /dev/null
+++ b/waflib/extras/fsb.py
@@ -0,0 +1,31 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2011 (ita)
+
+"""
+Fully sequential builds
+
+The previous tasks from task generators are re-processed, and this may lead to speed issues
+Yet, if you are using this, speed is probably a minor concern
+"""
+
+from waflib import Build
+
+def options(opt):
+	pass
+
+def configure(conf):
+	pass
+
+class FSBContext(Build.BuildContext):
+	def __call__(self, *k, **kw):
+		ret = Build.BuildContext.__call__(self, *k, **kw)
+
+		# evaluate the results immediately
+		Build.BuildContext.compile(self)
+
+		return ret
+
+	def compile(self):
+		pass
+
diff --git a/waflib/extras/fsc.py b/waflib/extras/fsc.py
new file mode 100644
index 00000000..c67e70be
--- /dev/null
+++ b/waflib/extras/fsc.py
@@ -0,0 +1,64 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2011 (ita)
+
+"""
+Experimental F# stuff
+
+FSC="mono /path/to/fsc.exe" waf configure build
+"""
+
+from waflib import Utils, Task
+from waflib.TaskGen import before_method, after_method, feature
+from waflib.Tools import ccroot, cs
+
+ccroot.USELIB_VARS['fsc'] = set(['CSFLAGS', 'ASSEMBLIES', 'RESOURCES'])
+
+@feature('fs')
+@before_method('process_source')
+def apply_fsc(self):
+	cs_nodes = []
+	no_nodes = []
+	for x in self.to_nodes(self.source):
+		if x.name.endswith('.fs'):
+			cs_nodes.append(x)
+		else:
+			no_nodes.append(x)
+	self.source = no_nodes
+
+	bintype = getattr(self, 'type', self.gen.endswith('.dll') and 'library' or 'exe')
+	self.cs_task = tsk = self.create_task('fsc', cs_nodes, self.path.find_or_declare(self.gen))
+	tsk.env.CSTYPE = '/target:%s' % bintype
+	tsk.env.OUT    = '/out:%s' % tsk.outputs[0].abspath()
+
+	inst_to = getattr(self, 'install_path', bintype=='exe' and '${BINDIR}' or '${LIBDIR}')
+	if inst_to:
+		# note: we are making a copy, so the files added to cs_task.outputs won't be installed automatically
+		mod = getattr(self, 'chmod', bintype=='exe' and Utils.O755 or Utils.O644)
+		self.install_task = self.add_install_files(install_to=inst_to, install_from=self.cs_task.outputs[:], chmod=mod)
+
+feature('fs')(cs.use_cs)
+after_method('apply_fsc')(cs.use_cs)
+
+feature('fs')(cs.debug_cs)
+after_method('apply_fsc', 'use_cs')(cs.debug_cs)
+
+class fsc(Task.Task):
+	"""
+	Compile F# files
+	"""
+	color   = 'YELLOW'
+	run_str = '${FSC} ${CSTYPE} ${CSFLAGS} ${ASS_ST:ASSEMBLIES} ${RES_ST:RESOURCES} ${OUT} ${SRC}'
+
+def configure(conf):
+	"""
+	Find a F# compiler, set the variable FSC for the compiler and FS_NAME (mono or fsc)
+	"""
+	conf.find_program(['fsc.exe', 'fsharpc'], var='FSC')
+	conf.env.ASS_ST = '/r:%s'
+	conf.env.RES_ST = '/resource:%s'
+
+	conf.env.FS_NAME = 'fsc'
+	if str(conf.env.FSC).lower().find('fsharpc') > -1:
+		conf.env.FS_NAME = 'mono'
+
diff --git a/waflib/extras/gccdeps.py b/waflib/extras/gccdeps.py
new file mode 100644
index 00000000..d9758ab3
--- /dev/null
+++ b/waflib/extras/gccdeps.py
@@ -0,0 +1,214 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2008-2010 (ita)
+
+"""
+Execute the tasks with gcc -MD, read the dependencies from the .d file
+and prepare the dependency calculation for the next run.
+This affects the cxx class, so make sure to load Qt5 after this tool.
+
+Usage::
+
+	def options(opt):
+		opt.load('compiler_cxx')
+	def configure(conf):
+		conf.load('compiler_cxx gccdeps')
+"""
+
+import os, re, threading
+from waflib import Task, Logs, Utils, Errors
+from waflib.Tools import c_preproc
+from waflib.TaskGen import before_method, feature
+
+lock = threading.Lock()
+
+gccdeps_flags = ['-MD']
+if not c_preproc.go_absolute:
+	gccdeps_flags = ['-MMD']
+
+# Third-party tools are allowed to add extra names in here with append()
+supported_compilers = ['gcc', 'icc', 'clang']
+
+def scan(self):
+	if not self.__class__.__name__ in self.env.ENABLE_GCCDEPS:
+		return super(self.derived_gccdeps, self).scan()
+	nodes = self.generator.bld.node_deps.get(self.uid(), [])
+	names = []
+	return (nodes, names)
+
+re_o = re.compile("\.o$")
+re_splitter = re.compile(r'(?<!\\)\s+') # split by space, except when spaces are escaped
+
+def remove_makefile_rule_lhs(line):
+	# Splitting on a plain colon would accidentally match inside a
+	# Windows absolute-path filename, so we must search for a colon
+	# followed by whitespace to find the divider between LHS and RHS
+	# of the Makefile rule.
+	rulesep = ': '
+
+	sep_idx = line.find(rulesep)
+	if sep_idx >= 0:
+		return line[sep_idx + 2:]
+	else:
+		return line
+
+def path_to_node(base_node, path, cached_nodes):
+	# Take the base node and the path and return a node
+	# Results are cached because searching the node tree is expensive
+	# The following code is executed by threads, it is not safe, so a lock is needed...
+	if getattr(path, '__hash__'):
+		node_lookup_key = (base_node, path)
+	else:
+		# Not hashable, assume it is a list and join into a string
+		node_lookup_key = (base_node, os.path.sep.join(path))
+	try:
+		lock.acquire()
+		node = cached_nodes[node_lookup_key]
+	except KeyError:
+		node = base_node.find_resource(path)
+		cached_nodes[node_lookup_key] = node
+	finally:
+		lock.release()
+	return node
+
+def post_run(self):
+	if not self.__class__.__name__ in self.env.ENABLE_GCCDEPS:
+		return super(self.derived_gccdeps, self).post_run()
+
+	name = self.outputs[0].abspath()
+	name = re_o.sub('.d', name)
+	try:
+		txt = Utils.readf(name)
+	except EnvironmentError:
+		Logs.error('Could not find a .d dependency file, are cflags/cxxflags overwritten?')
+		raise
+	#os.remove(name)
+
+	# Compilers have the choice to either output the file's dependencies
+	# as one large Makefile rule:
+	#
+	#   /path/to/file.o: /path/to/dep1.h \
+	#                    /path/to/dep2.h \
+	#                    /path/to/dep3.h \
+	#                    ...
+	#
+	# or as many individual rules:
+	#
+	#   /path/to/file.o: /path/to/dep1.h
+	#   /path/to/file.o: /path/to/dep2.h
+	#   /path/to/file.o: /path/to/dep3.h
+	#   ...
+	#
+	# So the first step is to sanitize the input by stripping out the left-
+	# hand side of all these lines. After that, whatever remains are the
+	# implicit dependencies of task.outputs[0]
+	txt = '\n'.join([remove_makefile_rule_lhs(line) for line in txt.splitlines()])
+
+	# Now join all the lines together
+	txt = txt.replace('\\\n', '')
+
+	val = txt.strip()
+	val = [x.replace('\\ ', ' ') for x in re_splitter.split(val) if x]
+
+	nodes = []
+	bld = self.generator.bld
+
+	# Dynamically bind to the cache
+	try:
+		cached_nodes = bld.cached_nodes
+	except AttributeError:
+		cached_nodes = bld.cached_nodes = {}
+
+	for x in val:
+
+		node = None
+		if os.path.isabs(x):
+			node = path_to_node(bld.root, x, cached_nodes)
+		else:
+			# TODO waf 1.9 - single cwd value
+			path = getattr(bld, 'cwdx', bld.bldnode)
+			# when calling find_resource, make sure the path does not contain '..'
+			x = [k for k in Utils.split_path(x) if k and k != '.']
+			while '..' in x:
+				idx = x.index('..')
+				if idx == 0:
+					x = x[1:]
+					path = path.parent
+				else:
+					del x[idx]
+					del x[idx-1]
+
+			node = path_to_node(path, x, cached_nodes)
+
+		if not node:
+			raise ValueError('could not find %r for %r' % (x, self))
+		if id(node) == id(self.inputs[0]):
+			# ignore the source file, it is already in the dependencies
+			# this way, successful config tests may be retrieved from the cache
+			continue
+		nodes.append(node)
+
+	Logs.debug('deps: gccdeps for %s returned %s', self, nodes)
+
+	bld.node_deps[self.uid()] = nodes
+	bld.raw_deps[self.uid()] = []
+
+	try:
+		del self.cache_sig
+	except AttributeError:
+		pass
+
+	Task.Task.post_run(self)
+
+def sig_implicit_deps(self):
+	if not self.__class__.__name__ in self.env.ENABLE_GCCDEPS:
+		return super(self.derived_gccdeps, self).sig_implicit_deps()
+	try:
+		return Task.Task.sig_implicit_deps(self)
+	except Errors.WafError:
+		return Utils.SIG_NIL
+
+def wrap_compiled_task(classname):
+	derived_class = type(classname, (Task.classes[classname],), {})
+	derived_class.derived_gccdeps = derived_class
+	derived_class.post_run = post_run
+	derived_class.scan = scan
+	derived_class.sig_implicit_deps = sig_implicit_deps
+
+for k in ('c', 'cxx'):
+	if k in Task.classes:
+		wrap_compiled_task(k)
+
+@before_method('process_source')
+@feature('force_gccdeps')
+def force_gccdeps(self):
+	self.env.ENABLE_GCCDEPS = ['c', 'cxx']
+
+def configure(conf):
+	# in case someone provides a --enable-gccdeps command-line option
+	if not getattr(conf.options, 'enable_gccdeps', True):
+		return
+
+	global gccdeps_flags
+	flags = conf.env.GCCDEPS_FLAGS or gccdeps_flags
+	if conf.env.CC_NAME in supported_compilers:
+		try:
+			conf.check(fragment='int main() { return 0; }', features='c force_gccdeps', cflags=flags, msg='Checking for c flags %r' % ''.join(flags))
+		except Errors.ConfigurationError:
+			pass
+		else:
+			conf.env.append_value('CFLAGS', gccdeps_flags)
+			conf.env.append_unique('ENABLE_GCCDEPS', 'c')
+
+	if conf.env.CXX_NAME in supported_compilers:
+		try:
+			conf.check(fragment='int main() { return 0; }', features='cxx force_gccdeps', cxxflags=flags, msg='Checking for cxx flags %r' % ''.join(flags))
+		except Errors.ConfigurationError:
+			pass
+		else:
+			conf.env.append_value('CXXFLAGS', gccdeps_flags)
+			conf.env.append_unique('ENABLE_GCCDEPS', 'cxx')
+
+def options(opt):
+	raise ValueError('Do not load gccdeps options')
+
diff --git a/waflib/extras/gdbus.py b/waflib/extras/gdbus.py
new file mode 100644
index 00000000..0e0476e3
--- /dev/null
+++ b/waflib/extras/gdbus.py
@@ -0,0 +1,87 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Copyright Garmin International or its subsidiaries, 2018
+#
+# Heavily based on dbus.py
+
+"""
+Compiles dbus files with **gdbus-codegen**
+Typical usage::
+	def options(opt):
+		opt.load('compiler_c gdbus')
+	def configure(conf):
+		conf.load('compiler_c gdbus')
+	def build(bld):
+		tg = bld.program(
+			includes = '.',
+			source = bld.path.ant_glob('*.c'),
+			target = 'gnome-hello')
+		tg.add_gdbus_file('test.xml', 'com.example.example.', 'Example')
+"""
+
+from waflib import Task, Errors, Utils
+from waflib.TaskGen import taskgen_method, before_method
+
+@taskgen_method
+def add_gdbus_file(self, filename, prefix, namespace, export=False):
+	"""
+	Adds a dbus file to the list of dbus files to process. Store them in the attribute *dbus_lst*.
+	:param filename: xml file to compile
+	:type filename: string
+	:param prefix: interface prefix (--interface-prefix=prefix)
+	:type prefix: string
+	:param mode: C namespace (--c-namespace=namespace)
+	:type mode: string
+	:param export: Export Headers?
+	:type export: boolean
+	"""
+	if not hasattr(self, 'gdbus_lst'):
+		self.gdbus_lst = []
+	if not 'process_gdbus' in self.meths:
+		self.meths.append('process_gdbus')
+	self.gdbus_lst.append([filename, prefix, namespace, export])
+
+@before_method('process_source')
+def process_gdbus(self):
+	"""
+	Processes the dbus files stored in the attribute *gdbus_lst* to create :py:class:`gdbus_binding_tool` instances.
+	"""
+	output_node = self.path.get_bld().make_node(['gdbus', self.get_name()])
+	sources = []
+
+	for filename, prefix, namespace, export in getattr(self, 'gdbus_lst', []):
+		node = self.path.find_resource(filename)
+		if not node:
+			raise Errors.WafError('file not found ' + filename)
+		c_file = output_node.find_or_declare(node.change_ext('.c').name)
+		h_file = output_node.find_or_declare(node.change_ext('.h').name)
+		tsk = self.create_task('gdbus_binding_tool', node, [c_file, h_file])
+		tsk.cwd = output_node.abspath()
+
+		tsk.env.GDBUS_CODEGEN_INTERFACE_PREFIX = prefix
+		tsk.env.GDBUS_CODEGEN_NAMESPACE = namespace
+		tsk.env.GDBUS_CODEGEN_OUTPUT = node.change_ext('').name
+		sources.append(c_file)
+
+	if sources:
+		output_node.mkdir()
+		self.source = Utils.to_list(self.source) + sources
+		self.includes = [output_node] + self.to_incnodes(getattr(self, 'includes', []))
+		if export:
+			self.export_includes = [output_node] + self.to_incnodes(getattr(self, 'export_includes', []))
+
+class gdbus_binding_tool(Task.Task):
+	"""
+	Compiles a dbus file
+	"""
+	color   = 'BLUE'
+	ext_out = ['.h', '.c']
+	run_str = '${GDBUS_CODEGEN} --interface-prefix ${GDBUS_CODEGEN_INTERFACE_PREFIX} --generate-c-code ${GDBUS_CODEGEN_OUTPUT} --c-namespace ${GDBUS_CODEGEN_NAMESPACE} --c-generate-object-manager ${SRC[0].abspath()}'
+	shell = True
+
+def configure(conf):
+	"""
+	Detects the program gdbus-codegen and sets ``conf.env.GDBUS_CODEGEN``
+	"""
+	conf.find_program('gdbus-codegen', var='GDBUS_CODEGEN')
+
diff --git a/waflib/extras/gob2.py b/waflib/extras/gob2.py
new file mode 100644
index 00000000..b4fa3b9a
--- /dev/null
+++ b/waflib/extras/gob2.py
@@ -0,0 +1,17 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Ali Sabil, 2007
+
+from waflib import TaskGen
+
+TaskGen.declare_chain(
+	name = 'gob2',
+	rule = '${GOB2} -o ${TGT[0].bld_dir()} ${GOB2FLAGS} ${SRC}',
+	ext_in = '.gob',
+	ext_out = '.c'
+)
+
+def configure(conf):
+	conf.find_program('gob2', var='GOB2')
+	conf.env['GOB2FLAGS'] = ''
+
diff --git a/waflib/extras/halide.py b/waflib/extras/halide.py
new file mode 100644
index 00000000..6078e38b
--- /dev/null
+++ b/waflib/extras/halide.py
@@ -0,0 +1,151 @@
+#!/usr/bin/python
+# -*- coding: utf-8 -*-
+# Halide code generation tool
+
+__author__ = __maintainer__ = "Jérôme Carretero <cJ-waf@zougloub.eu>"
+__copyright__ = "Jérôme Carretero, 2014"
+
+"""
+
+Tool to run `Halide <http://halide-lang.org>`_ code generators.
+
+Usage::
+
+   bld(
+    name='pipeline',
+     # ^ Reference this in use="..." for things using the generated code
+    #target=['pipeline.o', 'pipeline.h']
+     # ^ by default, name.{o,h} is added, but you can set the outputs here
+    features='halide',
+    halide_env="HL_TRACE=1 HL_TARGET=host-opencl-gpu_debug",
+     # ^ Environment passed to the generator,
+     # can be a dict, k/v list, or string.
+    args=[],
+     # ^ Command-line arguments to the generator (optional),
+     # eg. to give parameters to the scheduling
+    source='pipeline_gen',
+     # ^ Name of the source executable
+   )
+
+
+Known issues:
+
+
+- Currently only supports Linux (no ".exe")
+
+- Doesn't rerun on input modification when input is part of a build
+  chain, and has been modified externally.
+
+"""
+
+import os
+from waflib import Task, Utils, Options, TaskGen, Errors
+
+class run_halide_gen(Task.Task):
+	color = 'CYAN'
+	vars = ['HALIDE_ENV', 'HALIDE_ARGS']
+	run_str = "${SRC[0].abspath()} ${HALIDE_ARGS}"
+	def __str__(self):
+		stuff = "halide"
+		stuff += ("[%s]" % (",".join(
+		 ('%s=%s' % (k,v)) for k, v in sorted(self.env.env.items()))))
+		return Task.Task.__str__(self).replace(self.__class__.__name__,
+		 stuff)
+
+@TaskGen.feature('halide')
+@TaskGen.before_method('process_source')
+def halide(self):
+	Utils.def_attrs(self,
+	 args=[],
+	 halide_env={},
+	)
+
+	bld = self.bld
+
+	env = self.halide_env
+	try:
+		if isinstance(env, str):
+			env = dict(x.split('=') for x in env.split())
+		elif isinstance(env, list):
+			env = dict(x.split('=') for x in env)
+		assert isinstance(env, dict)
+	except Exception as e:
+		if not isinstance(e, ValueError) \
+		 and not isinstance(e, AssertionError):
+			raise
+		raise Errors.WafError(
+		 "halide_env must be under the form" \
+		 " {'HL_x':'a', 'HL_y':'b'}" \
+		 " or ['HL_x=y', 'HL_y=b']" \
+		 " or 'HL_x=y HL_y=b'")
+
+	src = self.to_nodes(self.source)
+	assert len(src) == 1, "Only one source expected"
+	src = src[0]
+
+	args = Utils.to_list(self.args)
+
+	def change_ext(src, ext):
+		# Return a node with a new extension, in an appropriate folder
+		name = src.name
+		xpos = src.name.rfind('.')
+		if xpos == -1:
+			xpos = len(src.name)
+		newname = name[:xpos] + ext
+		if src.is_child_of(bld.bldnode):
+			node = src.get_src().parent.find_or_declare(newname)
+		else:
+			node = bld.bldnode.find_or_declare(newname)
+		return node
+
+	def to_nodes(self, lst, path=None):
+		tmp = []
+		path = path or self.path
+		find = path.find_or_declare
+
+		if isinstance(lst, self.path.__class__):
+			lst = [lst]
+
+		for x in Utils.to_list(lst):
+			if isinstance(x, str):
+				node = find(x)
+			else:
+				node = x
+			tmp.append(node)
+		return tmp
+
+	tgt = to_nodes(self, self.target)
+	if not tgt:
+		tgt = [change_ext(src, '.o'), change_ext(src, '.h')]
+	cwd = tgt[0].parent.abspath()
+	task = self.create_task('run_halide_gen', src, tgt, cwd=cwd)
+	task.env.append_unique('HALIDE_ARGS', args)
+	if task.env.env == []:
+		task.env.env = {}
+	task.env.env.update(env)
+	task.env.HALIDE_ENV = " ".join(("%s=%s" % (k,v)) for (k,v) in sorted(env.items()))
+	task.env.HALIDE_ARGS = args
+
+	try:
+		self.compiled_tasks.append(task)
+	except AttributeError:
+		self.compiled_tasks = [task]
+	self.source = []
+
+def configure(conf):
+	if Options.options.halide_root is None:
+		conf.check_cfg(package='Halide', args='--cflags --libs')
+	else:
+		halide_root = Options.options.halide_root
+		conf.env.INCLUDES_HALIDE = [ os.path.join(halide_root, "include") ]
+		conf.env.LIBPATH_HALIDE = [ os.path.join(halide_root, "lib") ]
+		conf.env.LIB_HALIDE = ["Halide"]
+
+		# You might want to add this, while upstream doesn't fix it
+		#conf.env.LIB_HALIDE += ['ncurses', 'dl', 'pthread']
+
+def options(opt):
+	opt.add_option('--halide-root',
+	 help="path to Halide include and lib files",
+	)
+
diff --git a/waflib/extras/javatest.py b/waflib/extras/javatest.py
new file mode 100755
index 00000000..979b8d82
--- /dev/null
+++ b/waflib/extras/javatest.py
@@ -0,0 +1,118 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Federico Pellegrin, 2017 (fedepell)
+
+"""
+Provides Java Unit test support using :py:class:`waflib.Tools.waf_unit_test.utest`
+task via the **javatest** feature.
+
+This gives the possibility to run unit test and have them integrated into the
+standard waf unit test environment. It has been tested with TestNG and JUnit
+but should be easily expandable to other frameworks given the flexibility of
+ut_str provided by the standard waf unit test environment.
+
+Example usage:
+
+def options(opt):
+	opt.load('java waf_unit_test javatest')
+
+def configure(conf):
+	conf.load('java javatest')
+
+def build(bld):
+	
+	[ ... mainprog is built here ... ]
+
+	bld(features = 'javac javatest',
+		srcdir     = 'test/', 
+		outdir     = 'test', 
+		sourcepath = ['test'],
+		classpath  = [ 'src' ], 
+		basedir    = 'test', 
+		use = ['JAVATEST', 'mainprog'], # mainprog is the program being tested in src/
+		ut_str = 'java -cp ${CLASSPATH} ${JTRUNNER} ${SRC}',
+		jtest_source = bld.path.ant_glob('test/*.xml'),
+	)
+
+
+At command line the CLASSPATH where to find the testing environment and the
+test runner (default TestNG) that will then be seen in the environment as
+CLASSPATH_JAVATEST (then used for use) and JTRUNNER and can be used for
+dependencies and ut_str generation.
+
+Example configure for TestNG:
+	waf configure --jtpath=/tmp/testng-6.12.jar:/tmp/jcommander-1.71.jar --jtrunner=org.testng.TestNG
+		 or as default runner is TestNG:
+	waf configure --jtpath=/tmp/testng-6.12.jar:/tmp/jcommander-1.71.jar
+
+Example configure for JUnit:
+	waf configure --jtpath=/tmp/junit.jar --jtrunner=org.junit.runner.JUnitCore
+
+The runner class presence on the system is checked for at configuration stage.
+
+"""
+
+import os
+from waflib import Task, TaskGen, Options
+
+@TaskGen.feature('javatest')
+@TaskGen.after_method('apply_java', 'use_javac_files', 'set_classpath')
+def make_javatest(self):
+	"""
+	Creates a ``utest`` task with a populated environment for Java Unit test execution
+
+	"""
+	tsk = self.create_task('utest')
+	tsk.set_run_after(self.javac_task)
+
+	# Put test input files as waf_unit_test relies on that for some prints and log generation
+	# If jtest_source is there, this is specially useful for passing XML for TestNG
+	# that contain test specification, use that as inputs, otherwise test sources
+	if getattr(self, 'jtest_source', None):
+		tsk.inputs = self.to_nodes(self.jtest_source)
+	else:
+		if self.javac_task.srcdir[0].exists():
+			tsk.inputs = self.javac_task.srcdir[0].ant_glob('**/*.java', remove=False)
+
+	if getattr(self, 'ut_str', None):
+		self.ut_run, lst = Task.compile_fun(self.ut_str, shell=getattr(self, 'ut_shell', False))
+		tsk.vars = lst + tsk.vars
+
+	if getattr(self, 'ut_cwd', None):
+		if isinstance(self.ut_cwd, str):
+			# we want a Node instance
+			if os.path.isabs(self.ut_cwd):
+				self.ut_cwd = self.bld.root.make_node(self.ut_cwd)
+			else:
+				self.ut_cwd = self.path.make_node(self.ut_cwd)
+	else:
+		self.ut_cwd = self.bld.bldnode
+
+	# Get parent CLASSPATH and add output dir of test, we run from wscript dir
+	# We have to change it from list to the standard java -cp format (: separated)
+	tsk.env.CLASSPATH = ':'.join(self.env.CLASSPATH) + ':' + self.outdir.abspath()
+
+	if not self.ut_cwd.exists():
+		self.ut_cwd.mkdir()
+
+	if not hasattr(self, 'ut_env'):
+		self.ut_env = dict(os.environ)
+
+def configure(ctx):
+	cp = ctx.env.CLASSPATH or '.'
+	if getattr(Options.options, 'jtpath', None):
+		ctx.env.CLASSPATH_JAVATEST = getattr(Options.options, 'jtpath').split(':')
+		cp += ':' + getattr(Options.options, 'jtpath')
+
+	if getattr(Options.options, 'jtrunner', None):
+		ctx.env.JTRUNNER = getattr(Options.options, 'jtrunner')
+
+	if ctx.check_java_class(ctx.env.JTRUNNER, with_classpath=cp):
+		ctx.fatal('Could not run test class %r' % ctx.env.JTRUNNER)
+
+def options(opt):
+	opt.add_option('--jtpath', action='store', default='', dest='jtpath',
+		help='Path to jar(s) needed for javatest execution, colon separated, if not in the system CLASSPATH')
+	opt.add_option('--jtrunner', action='store', default='org.testng.TestNG', dest='jtrunner',
+		help='Class to run javatest test [default: org.testng.TestNG]')
+
diff --git a/waflib/extras/kde4.py b/waflib/extras/kde4.py
new file mode 100644
index 00000000..e49a9ec0
--- /dev/null
+++ b/waflib/extras/kde4.py
@@ -0,0 +1,93 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2010 (ita)
+
+"""
+Support for the KDE4 libraries and msgfmt
+"""
+
+import os, re
+from waflib import Task, Utils
+from waflib.TaskGen import feature
+
+@feature('msgfmt')
+def apply_msgfmt(self):
+	"""
+	Process all languages to create .mo files and to install them::
+
+		def build(bld):
+			bld(features='msgfmt', langs='es de fr', appname='myapp', install_path='${KDE4_LOCALE_INSTALL_DIR}')
+	"""
+	for lang in self.to_list(self.langs):
+		node = self.path.find_resource(lang+'.po')
+		task = self.create_task('msgfmt', node, node.change_ext('.mo'))
+
+		langname = lang.split('/')
+		langname = langname[-1]
+
+		inst = getattr(self, 'install_path', '${KDE4_LOCALE_INSTALL_DIR}')
+
+		self.add_install_as(
+			inst_to = inst + os.sep + langname + os.sep + 'LC_MESSAGES' + os.sep + getattr(self, 'appname', 'set_your_appname') + '.mo',
+			inst_from = task.outputs[0],
+			chmod = getattr(self, 'chmod', Utils.O644))
+
+class msgfmt(Task.Task):
+	"""
+	Transform .po files into .mo files
+	"""
+	color   = 'BLUE'
+	run_str = '${MSGFMT} ${SRC} -o ${TGT}'
+
+def configure(self):
+	"""
+	Detect kde4-config and set various variables for the *use* system::
+
+		def options(opt):
+			opt.load('compiler_cxx kde4')
+		def configure(conf):
+			conf.load('compiler_cxx kde4')
+		def build(bld):
+			bld.program(source='main.c', target='app', use='KDECORE KIO KHTML')
+	"""
+	kdeconfig = self.find_program('kde4-config')
+	prefix = self.cmd_and_log(kdeconfig + ['--prefix']).strip()
+	fname = '%s/share/apps/cmake/modules/KDELibsDependencies.cmake' % prefix
+	try:
+		os.stat(fname)
+	except OSError:
+		fname = '%s/share/kde4/apps/cmake/modules/KDELibsDependencies.cmake' % prefix
+		try:
+			os.stat(fname)
+		except OSError:
+			self.fatal('could not open %s' % fname)
+
+	try:
+		txt = Utils.readf(fname)
+	except EnvironmentError:
+		self.fatal('could not read %s' % fname)
+
+	txt = txt.replace('\\\n', '\n')
+	fu = re.compile('#(.*)\n')
+	txt = fu.sub('', txt)
+
+	setregexp = re.compile('([sS][eE][tT]\s*\()\s*([^\s]+)\s+\"([^"]+)\"\)')
+	found = setregexp.findall(txt)
+
+	for (_, key, val) in found:
+		#print key, val
+		self.env[key] = val
+
+	# well well, i could just write an interpreter for cmake files
+	self.env['LIB_KDECORE']= ['kdecore']
+	self.env['LIB_KDEUI']  = ['kdeui']
+	self.env['LIB_KIO']    = ['kio']
+	self.env['LIB_KHTML']  = ['khtml']
+	self.env['LIB_KPARTS'] = ['kparts']
+
+	self.env['LIBPATH_KDECORE']  = [os.path.join(self.env.KDE4_LIB_INSTALL_DIR, 'kde4', 'devel'), self.env.KDE4_LIB_INSTALL_DIR]
+	self.env['INCLUDES_KDECORE'] = [self.env['KDE4_INCLUDE_INSTALL_DIR']]
+	self.env.append_value('INCLUDES_KDECORE', [self.env['KDE4_INCLUDE_INSTALL_DIR']+ os.sep + 'KDE'])
+
+	self.find_program('msgfmt', var='MSGFMT')
+
diff --git a/waflib/extras/local_rpath.py b/waflib/extras/local_rpath.py
new file mode 100644
index 00000000..b2507e17
--- /dev/null
+++ b/waflib/extras/local_rpath.py
@@ -0,0 +1,19 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2011 (ita)
+
+from waflib.TaskGen import after_method, feature
+
+@after_method('propagate_uselib_vars')
+@feature('cprogram', 'cshlib', 'cxxprogram', 'cxxshlib', 'fcprogram', 'fcshlib')
+def add_rpath_stuff(self):
+	all = self.to_list(getattr(self, 'use', []))
+	while all:
+		name = all.pop()
+		try:
+			tg = self.bld.get_tgen_by_name(name)
+		except:
+			continue
+		self.env.append_value('RPATH', tg.link_task.outputs[0].parent.abspath())
+		all.extend(self.to_list(getattr(tg, 'use', [])))
+
diff --git a/waflib/extras/make.py b/waflib/extras/make.py
new file mode 100644
index 00000000..933d9cac
--- /dev/null
+++ b/waflib/extras/make.py
@@ -0,0 +1,142 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2011 (ita)
+
+"""
+A make-like way of executing the build, following the relationships between inputs/outputs
+
+This algorithm will lead to slower builds, will not be as flexible as "waf build", but
+it might be useful for building data files (?)
+
+It is likely to break in the following cases:
+- files are created dynamically (no inputs or outputs)
+- headers
+- building two files from different groups
+"""
+
+import re
+from waflib import Options, Task
+from waflib.Build import BuildContext
+
+class MakeContext(BuildContext):
+	'''executes tasks in a step-by-step manner, following dependencies between inputs/outputs'''
+	cmd = 'make'
+	fun = 'build'
+
+	def __init__(self, **kw):
+		super(MakeContext, self).__init__(**kw)
+		self.files = Options.options.files
+
+	def get_build_iterator(self):
+		if not self.files:
+			while 1:
+				yield super(MakeContext, self).get_build_iterator()
+
+		for g in self.groups:
+			for tg in g:
+				try:
+					f = tg.post
+				except AttributeError:
+					pass
+				else:
+					f()
+
+			provides = {}
+			uses = {}
+			all_tasks = []
+			tasks = []
+			for pat in self.files.split(','):
+				matcher = self.get_matcher(pat)
+				for tg in g:
+					if isinstance(tg, Task.Task):
+						lst = [tg]
+					else:
+						lst = tg.tasks
+					for tsk in lst:
+						all_tasks.append(tsk)
+
+						do_exec = False
+						for node in tsk.inputs:
+							try:
+								uses[node].append(tsk)
+							except:
+								uses[node] = [tsk]
+
+							if matcher(node, output=False):
+								do_exec = True
+								break
+
+						for node in tsk.outputs:
+							try:
+								provides[node].append(tsk)
+							except:
+								provides[node] = [tsk]
+
+							if matcher(node, output=True):
+								do_exec = True
+								break
+						if do_exec:
+							tasks.append(tsk)
+
+			# so we have the tasks that we need to process, the list of all tasks,
+			# the map of the tasks providing nodes, and the map of tasks using nodes
+
+			if not tasks:
+				# if there are no tasks matching, return everything in the current group
+				result = all_tasks
+			else:
+				# this is like a big filter...
+				result = set()
+				seen = set()
+				cur = set(tasks)
+				while cur:
+					result |= cur
+					tosee = set()
+					for tsk in cur:
+						for node in tsk.inputs:
+							if node in seen:
+								continue
+							seen.add(node)
+							tosee |= set(provides.get(node, []))
+					cur = tosee
+				result = list(result)
+
+			Task.set_file_constraints(result)
+			Task.set_precedence_constraints(result)
+			yield result
+
+		while 1:
+			yield []
+
+	def get_matcher(self, pat):
+		# this returns a function
+		inn = True
+		out = True
+		if pat.startswith('in:'):
+			out = False
+			pat = pat.replace('in:', '')
+		elif pat.startswith('out:'):
+			inn = False
+			pat = pat.replace('out:', '')
+
+		anode = self.root.find_node(pat)
+		pattern = None
+		if not anode:
+			if not pat.startswith('^'):
+				pat = '^.+?%s' % pat
+			if not pat.endswith('$'):
+				pat = '%s$' % pat
+			pattern = re.compile(pat)
+
+		def match(node, output):
+			if output and not out:
+				return False
+			if not output and not inn:
+				return False
+
+			if anode:
+				return anode == node
+			else:
+				return pattern.match(node.abspath())
+		return match
+
diff --git a/waflib/extras/midl.py b/waflib/extras/midl.py
new file mode 100644
index 00000000..43e6cf91
--- /dev/null
+++ b/waflib/extras/midl.py
@@ -0,0 +1,69 @@
+#!/usr/bin/env python
+# Issue 1185 ultrix gmail com
+
+"""
+Microsoft Interface Definition Language support.  Given ComObject.idl, this tool
+will generate ComObject.tlb ComObject_i.h ComObject_i.c ComObject_p.c and dlldata.c
+
+To declare targets using midl::
+
+	def configure(conf):
+		conf.load('msvc')
+		conf.load('midl')
+
+	def build(bld):
+		bld(
+			features='c cshlib',
+			# Note: ComObject_i.c is generated from ComObject.idl
+			source = 'main.c ComObject.idl ComObject_i.c',
+			target = 'ComObject.dll')
+"""
+
+from waflib import Task, Utils
+from waflib.TaskGen import feature, before_method
+import os
+
+def configure(conf):
+	conf.find_program(['midl'], var='MIDL')
+
+	conf.env.MIDLFLAGS = [
+		'/nologo',
+		'/D',
+		'_DEBUG',
+		'/W1',
+		'/char',
+		'signed',
+		'/Oicf',
+	]
+
+@feature('c', 'cxx')
+@before_method('process_source')
+def idl_file(self):
+	# Do this before process_source so that the generated header can be resolved
+	# when scanning source dependencies.
+	idl_nodes = []
+	src_nodes = []
+	for node in Utils.to_list(self.source):
+		if str(node).endswith('.idl'):
+			idl_nodes.append(node)
+		else:
+			src_nodes.append(node)
+
+	for node in self.to_nodes(idl_nodes):
+		t = node.change_ext('.tlb')
+		h = node.change_ext('_i.h')
+		c = node.change_ext('_i.c')
+		p = node.change_ext('_p.c')
+		d = node.parent.find_or_declare('dlldata.c')
+		self.create_task('midl', node, [t, h, c, p, d])
+
+	self.source = src_nodes
+
+class midl(Task.Task):
+	"""
+	Compile idl files
+	"""
+	color   = 'YELLOW'
+	run_str = '${MIDL} ${MIDLFLAGS} ${CPPPATH_ST:INCLUDES} /tlb ${TGT[0].bldpath()} /header ${TGT[1].bldpath()} /iid ${TGT[2].bldpath()} /proxy ${TGT[3].bldpath()} /dlldata ${TGT[4].bldpath()} ${SRC}'
+	before  = ['winrc']
+
diff --git a/waflib/extras/msvcdeps.py b/waflib/extras/msvcdeps.py
new file mode 100644
index 00000000..fc1ecd4d
--- /dev/null
+++ b/waflib/extras/msvcdeps.py
@@ -0,0 +1,256 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Copyright Garmin International or its subsidiaries, 2012-2013
+
+'''
+Off-load dependency scanning from Python code to MSVC compiler
+
+This tool is safe to load in any environment; it will only activate the
+MSVC exploits when it finds that a particular taskgen uses MSVC to
+compile.
+
+Empirical testing shows about a 10% execution time savings from using
+this tool as compared to c_preproc.
+
+The technique of gutting scan() and pushing the dependency calculation
+down to post_run() is cribbed from gccdeps.py.
+
+This affects the cxx class, so make sure to load Qt5 after this tool.
+
+Usage::
+
+	def options(opt):
+		opt.load('compiler_cxx')
+	def configure(conf):
+		conf.load('compiler_cxx msvcdeps')
+'''
+
+import os, sys, tempfile, threading
+
+from waflib import Context, Errors, Logs, Task, Utils
+from waflib.Tools import c_preproc, c, cxx, msvc
+from waflib.TaskGen import feature, before_method
+
+lock = threading.Lock()
+nodes = {} # Cache the path -> Node lookup
+
+PREPROCESSOR_FLAG = '/showIncludes'
+INCLUDE_PATTERN = 'Note: including file:'
+
+# Extensible by outside tools
+supported_compilers = ['msvc']
+
+@feature('c', 'cxx')
+@before_method('process_source')
+def apply_msvcdeps_flags(taskgen):
+	if taskgen.env.CC_NAME not in supported_compilers:
+		return
+
+	for flag in ('CFLAGS', 'CXXFLAGS'):
+		if taskgen.env.get_flat(flag).find(PREPROCESSOR_FLAG) < 0:
+			taskgen.env.append_value(flag, PREPROCESSOR_FLAG)
+
+	# Figure out what casing conventions the user's shell used when
+	# launching Waf
+	(drive, _) = os.path.splitdrive(taskgen.bld.srcnode.abspath())
+	taskgen.msvcdeps_drive_lowercase = drive == drive.lower()
+
+def path_to_node(base_node, path, cached_nodes):
+	# Take the base node and the path and return a node
+	# Results are cached because searching the node tree is expensive
+	# The following code is executed by threads, it is not safe, so a lock is needed...
+	if getattr(path, '__hash__'):
+		node_lookup_key = (base_node, path)
+	else:
+		# Not hashable, assume it is a list and join into a string
+		node_lookup_key = (base_node, os.path.sep.join(path))
+	try:
+		lock.acquire()
+		node = cached_nodes[node_lookup_key]
+	except KeyError:
+		node = base_node.find_resource(path)
+		cached_nodes[node_lookup_key] = node
+	finally:
+		lock.release()
+	return node
+
+def post_run(self):
+	if self.env.CC_NAME not in supported_compilers:
+		return super(self.derived_msvcdeps, self).post_run()
+
+	# TODO this is unlikely to work with netcache
+	if getattr(self, 'cached', None):
+		return Task.Task.post_run(self)
+
+	bld = self.generator.bld
+	unresolved_names = []
+	resolved_nodes = []
+
+	lowercase = self.generator.msvcdeps_drive_lowercase
+	correct_case_path = bld.path.abspath()
+	correct_case_path_len = len(correct_case_path)
+	correct_case_path_norm = os.path.normcase(correct_case_path)
+
+	# Dynamically bind to the cache
+	try:
+		cached_nodes = bld.cached_nodes
+	except AttributeError:
+		cached_nodes = bld.cached_nodes = {}
+
+	for path in self.msvcdeps_paths:
+		node = None
+		if os.path.isabs(path):
+			# Force drive letter to match conventions of main source tree
+			drive, tail = os.path.splitdrive(path)
+
+			if os.path.normcase(path[:correct_case_path_len]) == correct_case_path_norm:
+				# Path is in the sandbox, force it to be correct.  MSVC sometimes returns a lowercase path.
+				path = correct_case_path + path[correct_case_path_len:]
+			else:
+				# Check the drive letter
+				if lowercase and (drive != drive.lower()):
+					path = drive.lower() + tail
+				elif (not lowercase) and (drive != drive.upper()):
+					path = drive.upper() + tail
+			node = path_to_node(bld.root, path, cached_nodes)
+		else:
+			base_node = bld.bldnode
+			# when calling find_resource, make sure the path does not begin by '..'
+			path = [k for k in Utils.split_path(path) if k and k != '.']
+			while path[0] == '..':
+				path = path[1:]
+				base_node = base_node.parent
+
+			node = path_to_node(base_node, path, cached_nodes)
+
+		if not node:
+			raise ValueError('could not find %r for %r' % (path, self))
+		else:
+			if not c_preproc.go_absolute:
+				if not (node.is_child_of(bld.srcnode) or node.is_child_of(bld.bldnode)):
+					# System library
+					Logs.debug('msvcdeps: Ignoring system include %r', node)
+					continue
+
+			if id(node) == id(self.inputs[0]):
+				# Self-dependency
+				continue
+
+			resolved_nodes.append(node)
+
+	bld.node_deps[self.uid()] = resolved_nodes
+	bld.raw_deps[self.uid()] = unresolved_names
+
+	try:
+		del self.cache_sig
+	except AttributeError:
+		pass
+
+	Task.Task.post_run(self)
+
+def scan(self):
+	if self.env.CC_NAME not in supported_compilers:
+		return super(self.derived_msvcdeps, self).scan()
+
+	resolved_nodes = self.generator.bld.node_deps.get(self.uid(), [])
+	unresolved_names = []
+	return (resolved_nodes, unresolved_names)
+
+def sig_implicit_deps(self):
+	if self.env.CC_NAME not in supported_compilers:
+		return super(self.derived_msvcdeps, self).sig_implicit_deps()
+
+	try:
+		return Task.Task.sig_implicit_deps(self)
+	except Errors.WafError:
+		return Utils.SIG_NIL
+
+def exec_command(self, cmd, **kw):
+	if self.env.CC_NAME not in supported_compilers:
+		return super(self.derived_msvcdeps, self).exec_command(cmd, **kw)
+
+	if not 'cwd' in kw:
+		kw['cwd'] = self.get_cwd()
+
+	if self.env.PATH:
+		env = kw['env'] = dict(kw.get('env') or self.env.env or os.environ)
+		env['PATH'] = self.env.PATH if isinstance(self.env.PATH, str) else os.pathsep.join(self.env.PATH)
+
+	# The Visual Studio IDE adds an environment variable that causes
+	# the MS compiler to send its textual output directly to the
+	# debugging window rather than normal stdout/stderr.
+	#
+	# This is unrecoverably bad for this tool because it will cause
+	# all the dependency scanning to see an empty stdout stream and
+	# assume that the file being compiled uses no headers.
+	#
+	# See http://blogs.msdn.com/b/freik/archive/2006/04/05/569025.aspx
+	#
+	# Attempting to repair the situation by deleting the offending
+	# envvar at this point in tool execution will not be good enough--
+	# its presence poisons the 'waf configure' step earlier. We just
+	# want to put a sanity check here in order to help developers
+	# quickly diagnose the issue if an otherwise-good Waf tree
+	# is then executed inside the MSVS IDE.
+	assert 'VS_UNICODE_OUTPUT' not in kw['env']
+
+	cmd, args = self.split_argfile(cmd)
+	try:
+		(fd, tmp) = tempfile.mkstemp()
+		os.write(fd, '\r\n'.join(args).encode())
+		os.close(fd)
+
+		self.msvcdeps_paths = []
+		kw['env'] = kw.get('env', os.environ.copy())
+		kw['cwd'] = kw.get('cwd', os.getcwd())
+		kw['quiet'] = Context.STDOUT
+		kw['output'] = Context.STDOUT
+
+		out = []
+		if Logs.verbose:
+			Logs.debug('argfile: @%r -> %r', tmp, args)
+		try:
+			raw_out = self.generator.bld.cmd_and_log(cmd + ['@' + tmp], **kw)
+			ret = 0
+		except Errors.WafError as e:
+			raw_out = e.stdout
+			ret = e.returncode
+
+		for line in raw_out.splitlines():
+			if line.startswith(INCLUDE_PATTERN):
+				inc_path = line[len(INCLUDE_PATTERN):].strip()
+				Logs.debug('msvcdeps: Regex matched %s', inc_path)
+				self.msvcdeps_paths.append(inc_path)
+			else:
+				out.append(line)
+
+		# Pipe through the remaining stdout content (not related to /showIncludes)
+		if self.generator.bld.logger:
+			self.generator.bld.logger.debug('out: %s' % os.linesep.join(out))
+		else:
+			sys.stdout.write(os.linesep.join(out) + os.linesep)
+
+		return ret
+	finally:
+		try:
+			os.remove(tmp)
+		except OSError:
+			# anti-virus and indexers can keep files open -_-
+			pass
+
+
+def wrap_compiled_task(classname):
+	derived_class = type(classname, (Task.classes[classname],), {})
+	derived_class.derived_msvcdeps = derived_class
+	derived_class.post_run = post_run
+	derived_class.scan = scan
+	derived_class.sig_implicit_deps = sig_implicit_deps
+	derived_class.exec_command = exec_command
+
+for k in ('c', 'cxx'):
+	if k in Task.classes:
+		wrap_compiled_task(k)
+
+def options(opt):
+	raise ValueError('Do not load msvcdeps options')
+
diff --git a/waflib/extras/msvs.py b/waflib/extras/msvs.py
new file mode 100644
index 00000000..8aa2db0b
--- /dev/null
+++ b/waflib/extras/msvs.py
@@ -0,0 +1,1048 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Avalanche Studios 2009-2011
+# Thomas Nagy 2011
+
+"""
+Redistribution and use in source and binary forms, with or without
+modification, are permitted provided that the following conditions
+are met:
+
+1. Redistributions of source code must retain the above copyright
+   notice, this list of conditions and the following disclaimer.
+
+2. Redistributions in binary form must reproduce the above copyright
+   notice, this list of conditions and the following disclaimer in the
+   documentation and/or other materials provided with the distribution.
+
+3. The name of the author may not be used to endorse or promote products
+   derived from this software without specific prior written permission.
+
+THIS SOFTWARE IS PROVIDED BY THE AUTHOR "AS IS" AND ANY EXPRESS OR
+IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED
+WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
+DISCLAIMED. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT,
+INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES
+(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
+SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)
+HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT,
+STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING
+IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE
+POSSIBILITY OF SUCH DAMAGE.
+"""
+
+"""
+To add this tool to your project:
+def options(conf):
+	opt.load('msvs')
+
+It can be a good idea to add the sync_exec tool too.
+
+To generate solution files:
+$ waf configure msvs
+
+To customize the outputs, provide subclasses in your wscript files::
+
+	from waflib.extras import msvs
+	class vsnode_target(msvs.vsnode_target):
+		def get_build_command(self, props):
+			# likely to be required
+			return "waf.bat build"
+		def collect_source(self):
+			# likely to be required
+			...
+	class msvs_bar(msvs.msvs_generator):
+		def init(self):
+			msvs.msvs_generator.init(self)
+			self.vsnode_target = vsnode_target
+
+The msvs class re-uses the same build() function for reading the targets (task generators),
+you may therefore specify msvs settings on the context object::
+
+	def build(bld):
+		bld.solution_name = 'foo.sln'
+		bld.waf_command = 'waf.bat'
+		bld.projects_dir = bld.srcnode.make_node('.depproj')
+		bld.projects_dir.mkdir()
+
+For visual studio 2008, the command is called 'msvs2008', and the classes
+such as vsnode_target are wrapped by a decorator class 'wrap_2008' to
+provide special functionality.
+
+To customize platform toolsets, pass additional parameters, for example::
+
+	class msvs_2013(msvs.msvs_generator):
+		cmd = 'msvs2013'
+		numver = '13.00'
+		vsver = '2013'
+		platform_toolset_ver = 'v120'
+
+ASSUMPTIONS:
+* a project can be either a directory or a target, vcxproj files are written only for targets that have source files
+* each project is a vcxproj file, therefore the project uuid needs only to be a hash of the absolute path
+"""
+
+import os, re, sys
+import uuid # requires python 2.5
+from waflib.Build import BuildContext
+from waflib import Utils, TaskGen, Logs, Task, Context, Node, Options
+
+HEADERS_GLOB = '**/(*.h|*.hpp|*.H|*.inl)'
+
+PROJECT_TEMPLATE = r'''<?xml version="1.0" encoding="UTF-8"?>
+<Project DefaultTargets="Build" ToolsVersion="4.0"
+	xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
+
+	<ItemGroup Label="ProjectConfigurations">
+		${for b in project.build_properties}
+		<ProjectConfiguration Include="${b.configuration}|${b.platform}">
+			<Configuration>${b.configuration}</Configuration>
+			<Platform>${b.platform}</Platform>
+		</ProjectConfiguration>
+		${endfor}
+	</ItemGroup>
+
+	<PropertyGroup Label="Globals">
+		<ProjectGuid>{${project.uuid}}</ProjectGuid>
+		<Keyword>MakeFileProj</Keyword>
+		<ProjectName>${project.name}</ProjectName>
+	</PropertyGroup>
+	<Import Project="$(VCTargetsPath)\Microsoft.Cpp.Default.props" />
+
+	${for b in project.build_properties}
+	<PropertyGroup Condition="'$(Configuration)|$(Platform)'=='${b.configuration}|${b.platform}'" Label="Configuration">
+		<ConfigurationType>Makefile</ConfigurationType>
+		<OutDir>${b.outdir}</OutDir>
+		<PlatformToolset>${project.platform_toolset_ver}</PlatformToolset>
+	</PropertyGroup>
+	${endfor}
+
+	<Import Project="$(VCTargetsPath)\Microsoft.Cpp.props" />
+	<ImportGroup Label="ExtensionSettings">
+	</ImportGroup>
+
+	${for b in project.build_properties}
+	<ImportGroup Label="PropertySheets" Condition="'$(Configuration)|$(Platform)'=='${b.configuration}|${b.platform}'">
+		<Import Project="$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props" Condition="exists('$(UserRootDir)\Microsoft.Cpp.$(Platform).user.props')" Label="LocalAppDataPlatform" />
+	</ImportGroup>
+	${endfor}
+
+	${for b in project.build_properties}
+	<PropertyGroup Condition="'$(Configuration)|$(Platform)'=='${b.configuration}|${b.platform}'">
+		<NMakeBuildCommandLine>${xml:project.get_build_command(b)}</NMakeBuildCommandLine>
+		<NMakeReBuildCommandLine>${xml:project.get_rebuild_command(b)}</NMakeReBuildCommandLine>
+		<NMakeCleanCommandLine>${xml:project.get_clean_command(b)}</NMakeCleanCommandLine>
+		<NMakeIncludeSearchPath>${xml:b.includes_search_path}</NMakeIncludeSearchPath>
+		<NMakePreprocessorDefinitions>${xml:b.preprocessor_definitions};$(NMakePreprocessorDefinitions)</NMakePreprocessorDefinitions>
+		<IncludePath>${xml:b.includes_search_path}</IncludePath>
+		<ExecutablePath>$(ExecutablePath)</ExecutablePath>
+
+		${if getattr(b, 'output_file', None)}
+		<NMakeOutput>${xml:b.output_file}</NMakeOutput>
+		${endif}
+		${if getattr(b, 'deploy_dir', None)}
+		<RemoteRoot>${xml:b.deploy_dir}</RemoteRoot>
+		${endif}
+	</PropertyGroup>
+	${endfor}
+
+	${for b in project.build_properties}
+		${if getattr(b, 'deploy_dir', None)}
+	<ItemDefinitionGroup Condition="'$(Configuration)|$(Platform)'=='${b.configuration}|${b.platform}'">
+		<Deploy>
+			<DeploymentType>CopyToHardDrive</DeploymentType>
+		</Deploy>
+	</ItemDefinitionGroup>
+		${endif}
+	${endfor}
+
+	<ItemGroup>
+		${for x in project.source}
+		<${project.get_key(x)} Include='${x.win32path()}' />
+		${endfor}
+	</ItemGroup>
+	<Import Project="$(VCTargetsPath)\Microsoft.Cpp.targets" />
+	<ImportGroup Label="ExtensionTargets">
+	</ImportGroup>
+</Project>
+'''
+
+FILTER_TEMPLATE = '''<?xml version="1.0" encoding="UTF-8"?>
+<Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003">
+	<ItemGroup>
+		${for x in project.source}
+			<${project.get_key(x)} Include="${x.win32path()}">
+				<Filter>${project.get_filter_name(x.parent)}</Filter>
+			</${project.get_key(x)}>
+		${endfor}
+	</ItemGroup>
+	<ItemGroup>
+		${for x in project.dirs()}
+			<Filter Include="${project.get_filter_name(x)}">
+				<UniqueIdentifier>{${project.make_uuid(x.win32path())}}</UniqueIdentifier>
+			</Filter>
+		${endfor}
+	</ItemGroup>
+</Project>
+'''
+
+PROJECT_2008_TEMPLATE = r'''<?xml version="1.0" encoding="UTF-8"?>
+<VisualStudioProject ProjectType="Visual C++" Version="9,00"
+	Name="${xml: project.name}" ProjectGUID="{${project.uuid}}"
+	Keyword="MakeFileProj"
+	TargetFrameworkVersion="196613">
+	<Platforms>
+		${if project.build_properties}
+		${for b in project.build_properties}
+		   <Platform Name="${xml: b.platform}" />
+		${endfor}
+		${else}
+		   <Platform Name="Win32" />
+		${endif}
+	</Platforms>
+	<ToolFiles>
+	</ToolFiles>
+	<Configurations>
+		${if project.build_properties}
+		${for b in project.build_properties}
+		<Configuration
+			Name="${xml: b.configuration}|${xml: b.platform}"
+			IntermediateDirectory="$ConfigurationName"
+			OutputDirectory="${xml: b.outdir}"
+			ConfigurationType="0">
+			<Tool
+				Name="VCNMakeTool"
+				BuildCommandLine="${xml: project.get_build_command(b)}"
+				ReBuildCommandLine="${xml: project.get_rebuild_command(b)}"
+				CleanCommandLine="${xml: project.get_clean_command(b)}"
+				${if getattr(b, 'output_file', None)}
+				Output="${xml: b.output_file}"
+				${endif}
+				PreprocessorDefinitions="${xml: b.preprocessor_definitions}"
+				IncludeSearchPath="${xml: b.includes_search_path}"
+				ForcedIncludes=""
+				ForcedUsingAssemblies=""
+				AssemblySearchPath=""
+				CompileAsManaged=""
+			/>
+		</Configuration>
+		${endfor}
+		${else}
+			<Configuration Name="Release|Win32" >
+		</Configuration>
+		${endif}
+	</Configurations>
+	<References>
+	</References>
+	<Files>
+${project.display_filter()}
+	</Files>
+</VisualStudioProject>
+'''
+
+SOLUTION_TEMPLATE = '''Microsoft Visual Studio Solution File, Format Version ${project.numver}
+# Visual Studio ${project.vsver}
+${for p in project.all_projects}
+Project("{${p.ptype()}}") = "${p.name}", "${p.title}", "{${p.uuid}}"
+EndProject${endfor}
+Global
+	GlobalSection(SolutionConfigurationPlatforms) = preSolution
+		${if project.all_projects}
+		${for (configuration, platform) in project.all_projects[0].ctx.project_configurations()}
+		${configuration}|${platform} = ${configuration}|${platform}
+		${endfor}
+		${endif}
+	EndGlobalSection
+	GlobalSection(ProjectConfigurationPlatforms) = postSolution
+		${for p in project.all_projects}
+			${if hasattr(p, 'source')}
+			${for b in p.build_properties}
+		{${p.uuid}}.${b.configuration}|${b.platform}.ActiveCfg = ${b.configuration}|${b.platform}
+			${if getattr(p, 'is_active', None)}
+		{${p.uuid}}.${b.configuration}|${b.platform}.Build.0 = ${b.configuration}|${b.platform}
+			${endif}
+			${if getattr(p, 'is_deploy', None)}
+		{${p.uuid}}.${b.configuration}|${b.platform}.Deploy.0 = ${b.configuration}|${b.platform}
+			${endif}
+			${endfor}
+			${endif}
+		${endfor}
+	EndGlobalSection
+	GlobalSection(SolutionProperties) = preSolution
+		HideSolutionNode = FALSE
+	EndGlobalSection
+	GlobalSection(NestedProjects) = preSolution
+	${for p in project.all_projects}
+		${if p.parent}
+		{${p.uuid}} = {${p.parent.uuid}}
+		${endif}
+	${endfor}
+	EndGlobalSection
+EndGlobal
+'''
+
+COMPILE_TEMPLATE = '''def f(project):
+	lst = []
+	def xml_escape(value):
+		return value.replace("&", "&amp;").replace('"', "&quot;").replace("'", "&apos;").replace("<", "&lt;").replace(">", "&gt;")
+
+	%s
+
+	#f = open('cmd.txt', 'w')
+	#f.write(str(lst))
+	#f.close()
+	return ''.join(lst)
+'''
+reg_act = re.compile(r"(?P<backslash>\\)|(?P<dollar>\$\$)|(?P<subst>\$\{(?P<code>[^}]*?)\})", re.M)
+def compile_template(line):
+	"""
+	Compile a template expression into a python function (like jsps, but way shorter)
+	"""
+	extr = []
+	def repl(match):
+		g = match.group
+		if g('dollar'):
+			return "$"
+		elif g('backslash'):
+			return "\\"
+		elif g('subst'):
+			extr.append(g('code'))
+			return "<<|@|>>"
+		return None
+
+	line2 = reg_act.sub(repl, line)
+	params = line2.split('<<|@|>>')
+	assert(extr)
+
+
+	indent = 0
+	buf = []
+	app = buf.append
+
+	def app(txt):
+		buf.append(indent * '\t' + txt)
+
+	for x in range(len(extr)):
+		if params[x]:
+			app("lst.append(%r)" % params[x])
+
+		f = extr[x]
+		if f.startswith(('if', 'for')):
+			app(f + ':')
+			indent += 1
+		elif f.startswith('py:'):
+			app(f[3:])
+		elif f.startswith(('endif', 'endfor')):
+			indent -= 1
+		elif f.startswith(('else', 'elif')):
+			indent -= 1
+			app(f + ':')
+			indent += 1
+		elif f.startswith('xml:'):
+			app('lst.append(xml_escape(%s))' % f[4:])
+		else:
+			#app('lst.append((%s) or "cannot find %s")' % (f, f))
+			app('lst.append(%s)' % f)
+
+	if extr:
+		if params[-1]:
+			app("lst.append(%r)" % params[-1])
+
+	fun = COMPILE_TEMPLATE % "\n\t".join(buf)
+	#print(fun)
+	return Task.funex(fun)
+
+
+re_blank = re.compile('(\n|\r|\\s)*\n', re.M)
+def rm_blank_lines(txt):
+	txt = re_blank.sub('\r\n', txt)
+	return txt
+
+BOM = '\xef\xbb\xbf'
+try:
+	BOM = bytes(BOM, 'latin-1') # python 3
+except TypeError:
+	pass
+
+def stealth_write(self, data, flags='wb'):
+	try:
+		unicode
+	except NameError:
+		data = data.encode('utf-8') # python 3
+	else:
+		data = data.decode(sys.getfilesystemencoding(), 'replace')
+		data = data.encode('utf-8')
+
+	if self.name.endswith(('.vcproj', '.vcxproj')):
+		data = BOM + data
+
+	try:
+		txt = self.read(flags='rb')
+		if txt != data:
+			raise ValueError('must write')
+	except (IOError, ValueError):
+		self.write(data, flags=flags)
+	else:
+		Logs.debug('msvs: skipping %s', self.win32path())
+Node.Node.stealth_write = stealth_write
+
+re_win32 = re.compile(r'^([/\\]cygdrive)?[/\\]([a-z])([^a-z0-9_-].*)', re.I)
+def win32path(self):
+	p = self.abspath()
+	m = re_win32.match(p)
+	if m:
+		return "%s:%s" % (m.group(2).upper(), m.group(3))
+	return p
+Node.Node.win32path = win32path
+
+re_quote = re.compile("[^a-zA-Z0-9-]")
+def quote(s):
+	return re_quote.sub("_", s)
+
+def xml_escape(value):
+	return value.replace("&", "&amp;").replace('"', "&quot;").replace("'", "&apos;").replace("<", "&lt;").replace(">", "&gt;")
+
+def make_uuid(v, prefix = None):
+	"""
+	simple utility function
+	"""
+	if isinstance(v, dict):
+		keys = list(v.keys())
+		keys.sort()
+		tmp = str([(k, v[k]) for k in keys])
+	else:
+		tmp = str(v)
+	d = Utils.md5(tmp.encode()).hexdigest().upper()
+	if prefix:
+		d = '%s%s' % (prefix, d[8:])
+	gid = uuid.UUID(d, version = 4)
+	return str(gid).upper()
+
+def diff(node, fromnode):
+	# difference between two nodes, but with "(..)" instead of ".."
+	c1 = node
+	c2 = fromnode
+
+	c1h = c1.height()
+	c2h = c2.height()
+
+	lst = []
+	up = 0
+
+	while c1h > c2h:
+		lst.append(c1.name)
+		c1 = c1.parent
+		c1h -= 1
+
+	while c2h > c1h:
+		up += 1
+		c2 = c2.parent
+		c2h -= 1
+
+	while id(c1) != id(c2):
+		lst.append(c1.name)
+		up += 1
+
+		c1 = c1.parent
+		c2 = c2.parent
+
+	for i in range(up):
+		lst.append('(..)')
+	lst.reverse()
+	return tuple(lst)
+
+class build_property(object):
+	pass
+
+class vsnode(object):
+	"""
+	Abstract class representing visual studio elements
+	We assume that all visual studio nodes have a uuid and a parent
+	"""
+	def __init__(self, ctx):
+		self.ctx = ctx # msvs context
+		self.name = '' # string, mandatory
+		self.vspath = '' # path in visual studio (name for dirs, absolute path for projects)
+		self.uuid = '' # string, mandatory
+		self.parent = None # parent node for visual studio nesting
+
+	def get_waf(self):
+		"""
+		Override in subclasses...
+		"""
+		return 'cd /d "%s" & %s' % (self.ctx.srcnode.win32path(), getattr(self.ctx, 'waf_command', 'waf.bat'))
+
+	def ptype(self):
+		"""
+		Return a special uuid for projects written in the solution file
+		"""
+		pass
+
+	def write(self):
+		"""
+		Write the project file, by default, do nothing
+		"""
+		pass
+
+	def make_uuid(self, val):
+		"""
+		Alias for creating uuid values easily (the templates cannot access global variables)
+		"""
+		return make_uuid(val)
+
+class vsnode_vsdir(vsnode):
+	"""
+	Nodes representing visual studio folders (which do not match the filesystem tree!)
+	"""
+	VS_GUID_SOLUTIONFOLDER = "2150E333-8FDC-42A3-9474-1A3956D46DE8"
+	def __init__(self, ctx, uuid, name, vspath=''):
+		vsnode.__init__(self, ctx)
+		self.title = self.name = name
+		self.uuid = uuid
+		self.vspath = vspath or name
+
+	def ptype(self):
+		return self.VS_GUID_SOLUTIONFOLDER
+
+class vsnode_project(vsnode):
+	"""
+	Abstract class representing visual studio project elements
+	A project is assumed to be writable, and has a node representing the file to write to
+	"""
+	VS_GUID_VCPROJ = "8BC9CEB8-8B4A-11D0-8D11-00A0C91BC942"
+	def ptype(self):
+		return self.VS_GUID_VCPROJ
+
+	def __init__(self, ctx, node):
+		vsnode.__init__(self, ctx)
+		self.path = node
+		self.uuid = make_uuid(node.win32path())
+		self.name = node.name
+		self.platform_toolset_ver = getattr(ctx, 'platform_toolset_ver', None)
+		self.title = self.path.win32path()
+		self.source = [] # list of node objects
+		self.build_properties = [] # list of properties (nmake commands, output dir, etc)
+
+	def dirs(self):
+		"""
+		Get the list of parent folders of the source files (header files included)
+		for writing the filters
+		"""
+		lst = []
+		def add(x):
+			if x.height() > self.tg.path.height() and x not in lst:
+				lst.append(x)
+				add(x.parent)
+		for x in self.source:
+			add(x.parent)
+		return lst
+
+	def write(self):
+		Logs.debug('msvs: creating %r', self.path)
+
+		# first write the project file
+		template1 = compile_template(PROJECT_TEMPLATE)
+		proj_str = template1(self)
+		proj_str = rm_blank_lines(proj_str)
+		self.path.stealth_write(proj_str)
+
+		# then write the filter
+		template2 = compile_template(FILTER_TEMPLATE)
+		filter_str = template2(self)
+		filter_str = rm_blank_lines(filter_str)
+		tmp = self.path.parent.make_node(self.path.name + '.filters')
+		tmp.stealth_write(filter_str)
+
+	def get_key(self, node):
+		"""
+		required for writing the source files
+		"""
+		name = node.name
+		if name.endswith(('.cpp', '.c')):
+			return 'ClCompile'
+		return 'ClInclude'
+
+	def collect_properties(self):
+		"""
+		Returns a list of triplet (configuration, platform, output_directory)
+		"""
+		ret = []
+		for c in self.ctx.configurations:
+			for p in self.ctx.platforms:
+				x = build_property()
+				x.outdir = ''
+
+				x.configuration = c
+				x.platform = p
+
+				x.preprocessor_definitions = ''
+				x.includes_search_path = ''
+
+				# can specify "deploy_dir" too
+				ret.append(x)
+		self.build_properties = ret
+
+	def get_build_params(self, props):
+		opt = '--execsolution=%s' % self.ctx.get_solution_node().win32path()
+		return (self.get_waf(), opt)
+
+	def get_build_command(self, props):
+		return "%s build %s" % self.get_build_params(props)
+
+	def get_clean_command(self, props):
+		return "%s clean %s" % self.get_build_params(props)
+
+	def get_rebuild_command(self, props):
+		return "%s clean build %s" % self.get_build_params(props)
+
+	def get_filter_name(self, node):
+		lst = diff(node, self.tg.path)
+		return '\\'.join(lst) or '.'
+
+class vsnode_alias(vsnode_project):
+	def __init__(self, ctx, node, name):
+		vsnode_project.__init__(self, ctx, node)
+		self.name = name
+		self.output_file = ''
+
+class vsnode_build_all(vsnode_alias):
+	"""
+	Fake target used to emulate the behaviour of "make all" (starting one process by target is slow)
+	This is the only alias enabled by default
+	"""
+	def __init__(self, ctx, node, name='build_all_projects'):
+		vsnode_alias.__init__(self, ctx, node, name)
+		self.is_active = True
+
+class vsnode_install_all(vsnode_alias):
+	"""
+	Fake target used to emulate the behaviour of "make install"
+	"""
+	def __init__(self, ctx, node, name='install_all_projects'):
+		vsnode_alias.__init__(self, ctx, node, name)
+
+	def get_build_command(self, props):
+		return "%s build install %s" % self.get_build_params(props)
+
+	def get_clean_command(self, props):
+		return "%s clean %s" % self.get_build_params(props)
+
+	def get_rebuild_command(self, props):
+		return "%s clean build install %s" % self.get_build_params(props)
+
+class vsnode_project_view(vsnode_alias):
+	"""
+	Fake target used to emulate a file system view
+	"""
+	def __init__(self, ctx, node, name='project_view'):
+		vsnode_alias.__init__(self, ctx, node, name)
+		self.tg = self.ctx() # fake one, cannot remove
+		self.exclude_files = Node.exclude_regs + '''
+waf-2*
+waf3-2*/**
+.waf-2*
+.waf3-2*/**
+**/*.sdf
+**/*.suo
+**/*.ncb
+**/%s
+		''' % Options.lockfile
+
+	def collect_source(self):
+		# this is likely to be slow
+		self.source = self.ctx.srcnode.ant_glob('**', excl=self.exclude_files)
+
+	def get_build_command(self, props):
+		params = self.get_build_params(props) + (self.ctx.cmd,)
+		return "%s %s %s" % params
+
+	def get_clean_command(self, props):
+		return ""
+
+	def get_rebuild_command(self, props):
+		return self.get_build_command(props)
+
+class vsnode_target(vsnode_project):
+	"""
+	Visual studio project representing a targets (programs, libraries, etc) and bound
+	to a task generator
+	"""
+	def __init__(self, ctx, tg):
+		"""
+		A project is more or less equivalent to a file/folder
+		"""
+		base = getattr(ctx, 'projects_dir', None) or tg.path
+		node = base.make_node(quote(tg.name) + ctx.project_extension) # the project file as a Node
+		vsnode_project.__init__(self, ctx, node)
+		self.name = quote(tg.name)
+		self.tg     = tg  # task generator
+
+	def get_build_params(self, props):
+		"""
+		Override the default to add the target name
+		"""
+		opt = '--execsolution=%s' % self.ctx.get_solution_node().win32path()
+		if getattr(self, 'tg', None):
+			opt += " --targets=%s" % self.tg.name
+		return (self.get_waf(), opt)
+
+	def collect_source(self):
+		tg = self.tg
+		source_files = tg.to_nodes(getattr(tg, 'source', []))
+		include_dirs = Utils.to_list(getattr(tg, 'msvs_includes', []))
+		include_files = []
+		for x in include_dirs:
+			if isinstance(x, str):
+				x = tg.path.find_node(x)
+			if x:
+				lst = [y for y in x.ant_glob(HEADERS_GLOB, flat=False)]
+				include_files.extend(lst)
+
+		# remove duplicates
+		self.source.extend(list(set(source_files + include_files)))
+		self.source.sort(key=lambda x: x.win32path())
+
+	def collect_properties(self):
+		"""
+		Visual studio projects are associated with platforms and configurations (for building especially)
+		"""
+		super(vsnode_target, self).collect_properties()
+		for x in self.build_properties:
+			x.outdir = self.path.parent.win32path()
+			x.preprocessor_definitions = ''
+			x.includes_search_path = ''
+
+			try:
+				tsk = self.tg.link_task
+			except AttributeError:
+				pass
+			else:
+				x.output_file = tsk.outputs[0].win32path()
+				x.preprocessor_definitions = ';'.join(tsk.env.DEFINES)
+				x.includes_search_path = ';'.join(self.tg.env.INCPATHS)
+
+class msvs_generator(BuildContext):
+	'''generates a visual studio 2010 solution'''
+	cmd = 'msvs'
+	fun = 'build'
+	numver = '11.00' # Visual Studio Version Number
+	vsver  = '2010'  # Visual Studio Version Year
+	platform_toolset_ver = 'v110' # Platform Toolset Version Number
+
+	def init(self):
+		"""
+		Some data that needs to be present
+		"""
+		if not getattr(self, 'configurations', None):
+			self.configurations = ['Release'] # LocalRelease, RemoteDebug, etc
+		if not getattr(self, 'platforms', None):
+			self.platforms = ['Win32']
+		if not getattr(self, 'all_projects', None):
+			self.all_projects = []
+		if not getattr(self, 'project_extension', None):
+			self.project_extension = '.vcxproj'
+		if not getattr(self, 'projects_dir', None):
+			self.projects_dir = self.srcnode.make_node('.depproj')
+			self.projects_dir.mkdir()
+
+		# bind the classes to the object, so that subclass can provide custom generators
+		if not getattr(self, 'vsnode_vsdir', None):
+			self.vsnode_vsdir = vsnode_vsdir
+		if not getattr(self, 'vsnode_target', None):
+			self.vsnode_target = vsnode_target
+		if not getattr(self, 'vsnode_build_all', None):
+			self.vsnode_build_all = vsnode_build_all
+		if not getattr(self, 'vsnode_install_all', None):
+			self.vsnode_install_all = vsnode_install_all
+		if not getattr(self, 'vsnode_project_view', None):
+			self.vsnode_project_view = vsnode_project_view
+
+		self.numver = self.__class__.numver
+		self.vsver  = self.__class__.vsver
+		self.platform_toolset_ver = self.__class__.platform_toolset_ver
+
+	def execute(self):
+		"""
+		Entry point
+		"""
+		self.restore()
+		if not self.all_envs:
+			self.load_envs()
+		self.recurse([self.run_dir])
+
+		# user initialization
+		self.init()
+
+		# two phases for creating the solution
+		self.collect_projects() # add project objects into "self.all_projects"
+		self.write_files() # write the corresponding project and solution files
+
+	def collect_projects(self):
+		"""
+		Fill the list self.all_projects with project objects
+		Fill the list of build targets
+		"""
+		self.collect_targets()
+		self.add_aliases()
+		self.collect_dirs()
+		default_project = getattr(self, 'default_project', None)
+		def sortfun(x):
+			if x.name == default_project:
+				return ''
+			return getattr(x, 'path', None) and x.path.win32path() or x.name
+		self.all_projects.sort(key=sortfun)
+
+	def write_files(self):
+		"""
+		Write the project and solution files from the data collected
+		so far. It is unlikely that you will want to change this
+		"""
+		for p in self.all_projects:
+			p.write()
+
+		# and finally write the solution file
+		node = self.get_solution_node()
+		node.parent.mkdir()
+		Logs.warn('Creating %r', node)
+		template1 = compile_template(SOLUTION_TEMPLATE)
+		sln_str = template1(self)
+		sln_str = rm_blank_lines(sln_str)
+		node.stealth_write(sln_str)
+
+	def get_solution_node(self):
+		"""
+		The solution filename is required when writing the .vcproj files
+		return self.solution_node and if it does not exist, make one
+		"""
+		try:
+			return self.solution_node
+		except AttributeError:
+			pass
+
+		solution_name = getattr(self, 'solution_name', None)
+		if not solution_name:
+			solution_name = getattr(Context.g_module, Context.APPNAME, 'project') + '.sln'
+		if os.path.isabs(solution_name):
+			self.solution_node = self.root.make_node(solution_name)
+		else:
+			self.solution_node = self.srcnode.make_node(solution_name)
+		return self.solution_node
+
+	def project_configurations(self):
+		"""
+		Helper that returns all the pairs (config,platform)
+		"""
+		ret = []
+		for c in self.configurations:
+			for p in self.platforms:
+				ret.append((c, p))
+		return ret
+
+	def collect_targets(self):
+		"""
+		Process the list of task generators
+		"""
+		for g in self.groups:
+			for tg in g:
+				if not isinstance(tg, TaskGen.task_gen):
+					continue
+
+				if not hasattr(tg, 'msvs_includes'):
+					tg.msvs_includes = tg.to_list(getattr(tg, 'includes', [])) + tg.to_list(getattr(tg, 'export_includes', []))
+				tg.post()
+				if not getattr(tg, 'link_task', None):
+					continue
+
+				p = self.vsnode_target(self, tg)
+				p.collect_source() # delegate this processing
+				p.collect_properties()
+				self.all_projects.append(p)
+
+	def add_aliases(self):
+		"""
+		Add a specific target that emulates the "make all" necessary for Visual studio when pressing F7
+		We also add an alias for "make install" (disabled by default)
+		"""
+		base = getattr(self, 'projects_dir', None) or self.tg.path
+
+		node_project = base.make_node('build_all_projects' + self.project_extension) # Node
+		p_build = self.vsnode_build_all(self, node_project)
+		p_build.collect_properties()
+		self.all_projects.append(p_build)
+
+		node_project = base.make_node('install_all_projects' + self.project_extension) # Node
+		p_install = self.vsnode_install_all(self, node_project)
+		p_install.collect_properties()
+		self.all_projects.append(p_install)
+
+		node_project = base.make_node('project_view' + self.project_extension) # Node
+		p_view = self.vsnode_project_view(self, node_project)
+		p_view.collect_source()
+		p_view.collect_properties()
+		self.all_projects.append(p_view)
+
+		n = self.vsnode_vsdir(self, make_uuid(self.srcnode.win32path() + 'build_aliases'), "build_aliases")
+		p_build.parent = p_install.parent = p_view.parent = n
+		self.all_projects.append(n)
+
+	def collect_dirs(self):
+		"""
+		Create the folder structure in the Visual studio project view
+		"""
+		seen = {}
+		def make_parents(proj):
+			# look at a project, try to make a parent
+			if getattr(proj, 'parent', None):
+				# aliases already have parents
+				return
+			x = proj.iter_path
+			if x in seen:
+				proj.parent = seen[x]
+				return
+
+			# There is not vsnode_vsdir for x.
+			# So create a project representing the folder "x"
+			n = proj.parent = seen[x] = self.vsnode_vsdir(self, make_uuid(x.win32path()), x.name)
+			n.iter_path = x.parent
+			self.all_projects.append(n)
+
+			# recurse up to the project directory
+			if x.height() > self.srcnode.height() + 1:
+				make_parents(n)
+
+		for p in self.all_projects[:]: # iterate over a copy of all projects
+			if not getattr(p, 'tg', None):
+				# but only projects that have a task generator
+				continue
+
+			# make a folder for each task generator
+			p.iter_path = p.tg.path
+			make_parents(p)
+
+def wrap_2008(cls):
+	class dec(cls):
+		def __init__(self, *k, **kw):
+			cls.__init__(self, *k, **kw)
+			self.project_template = PROJECT_2008_TEMPLATE
+
+		def display_filter(self):
+
+			root = build_property()
+			root.subfilters = []
+			root.sourcefiles = []
+			root.source = []
+			root.name = ''
+
+			@Utils.run_once
+			def add_path(lst):
+				if not lst:
+					return root
+				child = build_property()
+				child.subfilters = []
+				child.sourcefiles = []
+				child.source = []
+				child.name = lst[-1]
+
+				par = add_path(lst[:-1])
+				par.subfilters.append(child)
+				return child
+
+			for x in self.source:
+				# this crap is for enabling subclasses to override get_filter_name
+				tmp = self.get_filter_name(x.parent)
+				tmp = tmp != '.' and tuple(tmp.split('\\')) or ()
+				par = add_path(tmp)
+				par.source.append(x)
+
+			def display(n):
+				buf = []
+				for x in n.source:
+					buf.append('<File RelativePath="%s" FileType="%s"/>\n' % (xml_escape(x.win32path()), self.get_key(x)))
+				for x in n.subfilters:
+					buf.append('<Filter Name="%s">' % xml_escape(x.name))
+					buf.append(display(x))
+					buf.append('</Filter>')
+				return '\n'.join(buf)
+
+			return display(root)
+
+		def get_key(self, node):
+			"""
+			If you do not want to let visual studio use the default file extensions,
+			override this method to return a value:
+				0: C/C++ Code, 1: C++ Class, 2: C++ Header File, 3: C++ Form,
+				4: C++ Control, 5: Text File, 6: DEF File, 7: IDL File,
+				8: Makefile, 9: RGS File, 10: RC File, 11: RES File, 12: XSD File,
+				13: XML File, 14: HTML File, 15: CSS File, 16: Bitmap, 17: Icon,
+				18: Resx File, 19: BSC File, 20: XSX File, 21: C++ Web Service,
+				22: ASAX File, 23: Asp Page, 24: Document, 25: Discovery File,
+				26: C# File, 27: eFileTypeClassDiagram, 28: MHTML Document,
+				29: Property Sheet, 30: Cursor, 31: Manifest, 32: eFileTypeRDLC
+			"""
+			return ''
+
+		def write(self):
+			Logs.debug('msvs: creating %r', self.path)
+			template1 = compile_template(self.project_template)
+			proj_str = template1(self)
+			proj_str = rm_blank_lines(proj_str)
+			self.path.stealth_write(proj_str)
+
+	return dec
+
+class msvs_2008_generator(msvs_generator):
+	'''generates a visual studio 2008 solution'''
+	cmd = 'msvs2008'
+	fun = msvs_generator.fun
+	numver = '10.00'
+	vsver = '2008'
+
+	def init(self):
+		if not getattr(self, 'project_extension', None):
+			self.project_extension = '_2008.vcproj'
+		if not getattr(self, 'solution_name', None):
+			self.solution_name = getattr(Context.g_module, Context.APPNAME, 'project') + '_2008.sln'
+
+		if not getattr(self, 'vsnode_target', None):
+			self.vsnode_target = wrap_2008(vsnode_target)
+		if not getattr(self, 'vsnode_build_all', None):
+			self.vsnode_build_all = wrap_2008(vsnode_build_all)
+		if not getattr(self, 'vsnode_install_all', None):
+			self.vsnode_install_all = wrap_2008(vsnode_install_all)
+		if not getattr(self, 'vsnode_project_view', None):
+			self.vsnode_project_view = wrap_2008(vsnode_project_view)
+
+		msvs_generator.init(self)
+
+def options(ctx):
+	"""
+	If the msvs option is used, try to detect if the build is made from visual studio
+	"""
+	ctx.add_option('--execsolution', action='store', help='when building with visual studio, use a build state file')
+
+	old = BuildContext.execute
+	def override_build_state(ctx):
+		def lock(rm, add):
+			uns = ctx.options.execsolution.replace('.sln', rm)
+			uns = ctx.root.make_node(uns)
+			try:
+				uns.delete()
+			except OSError:
+				pass
+
+			uns = ctx.options.execsolution.replace('.sln', add)
+			uns = ctx.root.make_node(uns)
+			try:
+				uns.write('')
+			except EnvironmentError:
+				pass
+
+		if ctx.options.execsolution:
+			ctx.launch_dir = Context.top_dir # force a build for the whole project (invalid cwd when called by visual studio)
+			lock('.lastbuildstate', '.unsuccessfulbuild')
+			old(ctx)
+			lock('.unsuccessfulbuild', '.lastbuildstate')
+		else:
+			old(ctx)
+	BuildContext.execute = override_build_state
+
diff --git a/waflib/extras/netcache_client.py b/waflib/extras/netcache_client.py
new file mode 100644
index 00000000..dc490485
--- /dev/null
+++ b/waflib/extras/netcache_client.py
@@ -0,0 +1,390 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2011-2015 (ita)
+
+"""
+A client for the network cache (playground/netcache/). Launch the server with:
+./netcache_server, then use it for the builds by adding the following:
+
+	def build(bld):
+		bld.load('netcache_client')
+
+The parameters should be present in the environment in the form:
+	NETCACHE=host:port waf configure build
+
+Or in a more detailed way:
+	NETCACHE_PUSH=host:port NETCACHE_PULL=host:port waf configure build
+
+where:
+	host: host where the server resides, by default localhost
+	port: by default push on 11001 and pull on 12001
+
+Use the server provided in playground/netcache/Netcache.java
+"""
+
+import os, socket, time, atexit, sys
+from waflib import Task, Logs, Utils, Build, Runner
+from waflib.Configure import conf
+
+BUF = 8192 * 16
+HEADER_SIZE = 128
+MODES = ['PUSH', 'PULL', 'PUSH_PULL']
+STALE_TIME = 30 # seconds
+
+GET = 'GET'
+PUT = 'PUT'
+LST = 'LST'
+BYE = 'BYE'
+
+all_sigs_in_cache = (0.0, [])
+
+def put_data(conn, data):
+	if sys.hexversion > 0x3000000:
+		data = data.encode('latin-1')
+	cnt = 0
+	while cnt < len(data):
+		sent = conn.send(data[cnt:])
+		if sent == 0:
+			raise RuntimeError('connection ended')
+		cnt += sent
+
+push_connections = Runner.Queue(0)
+pull_connections = Runner.Queue(0)
+def get_connection(push=False):
+	# return a new connection... do not forget to release it!
+	try:
+		if push:
+			ret = push_connections.get(block=False)
+		else:
+			ret = pull_connections.get(block=False)
+	except Exception:
+		ret = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
+		if push:
+			ret.connect(Task.push_addr)
+		else:
+			ret.connect(Task.pull_addr)
+	return ret
+
+def release_connection(conn, msg='', push=False):
+	if conn:
+		if push:
+			push_connections.put(conn)
+		else:
+			pull_connections.put(conn)
+
+def close_connection(conn, msg=''):
+	if conn:
+		data = '%s,%s' % (BYE, msg)
+		try:
+			put_data(conn, data.ljust(HEADER_SIZE))
+		except:
+			pass
+		try:
+			conn.close()
+		except:
+			pass
+
+def close_all():
+	for q in (push_connections, pull_connections):
+		while q.qsize():
+			conn = q.get()
+			try:
+				close_connection(conn)
+			except:
+				# ignore errors when cleaning up
+				pass
+atexit.register(close_all)
+
+def read_header(conn):
+	cnt = 0
+	buf = []
+	while cnt < HEADER_SIZE:
+		data = conn.recv(HEADER_SIZE - cnt)
+		if not data:
+			#import traceback
+			#traceback.print_stack()
+			raise ValueError('connection ended when reading a header %r' % buf)
+		buf.append(data)
+		cnt += len(data)
+	if sys.hexversion > 0x3000000:
+		ret = ''.encode('latin-1').join(buf)
+		ret = ret.decode('latin-1')
+	else:
+		ret = ''.join(buf)
+	return ret
+
+def check_cache(conn, ssig):
+	"""
+	List the files on the server, this is an optimization because it assumes that
+	concurrent builds are rare
+	"""
+	global all_sigs_in_cache
+	if not STALE_TIME:
+		return
+	if time.time() - all_sigs_in_cache[0] > STALE_TIME:
+
+		params = (LST,'')
+		put_data(conn, ','.join(params).ljust(HEADER_SIZE))
+
+		# read what is coming back
+		ret = read_header(conn)
+		size = int(ret.split(',')[0])
+
+		buf = []
+		cnt = 0
+		while cnt < size:
+			data = conn.recv(min(BUF, size-cnt))
+			if not data:
+				raise ValueError('connection ended %r %r' % (cnt, size))
+			buf.append(data)
+			cnt += len(data)
+
+		if sys.hexversion > 0x3000000:
+			ret = ''.encode('latin-1').join(buf)
+			ret = ret.decode('latin-1')
+		else:
+			ret = ''.join(buf)
+
+		all_sigs_in_cache = (time.time(), ret.splitlines())
+		Logs.debug('netcache: server cache has %r entries', len(all_sigs_in_cache[1]))
+
+	if not ssig in all_sigs_in_cache[1]:
+		raise ValueError('no file %s in cache' % ssig)
+
+class MissingFile(Exception):
+	pass
+
+def recv_file(conn, ssig, count, p):
+	check_cache(conn, ssig)
+
+	params = (GET, ssig, str(count))
+	put_data(conn, ','.join(params).ljust(HEADER_SIZE))
+	data = read_header(conn)
+
+	size = int(data.split(',')[0])
+
+	if size == -1:
+		raise MissingFile('no file %s - %s in cache' % (ssig, count))
+
+	# get the file, writing immediately
+	# TODO a tmp file would be better
+	f = open(p, 'wb')
+	cnt = 0
+	while cnt < size:
+		data = conn.recv(min(BUF, size-cnt))
+		if not data:
+			raise ValueError('connection ended %r %r' % (cnt, size))
+		f.write(data)
+		cnt += len(data)
+	f.close()
+
+def sock_send(conn, ssig, cnt, p):
+	#print "pushing %r %r %r" % (ssig, cnt, p)
+	size = os.stat(p).st_size
+	params = (PUT, ssig, str(cnt), str(size))
+	put_data(conn, ','.join(params).ljust(HEADER_SIZE))
+	f = open(p, 'rb')
+	cnt = 0
+	while cnt < size:
+		r = f.read(min(BUF, size-cnt))
+		while r:
+			k = conn.send(r)
+			if not k:
+				raise ValueError('connection ended')
+			cnt += k
+			r = r[k:]
+
+def can_retrieve_cache(self):
+	if not Task.pull_addr:
+		return False
+	if not self.outputs:
+		return False
+	self.cached = False
+
+	cnt = 0
+	sig = self.signature()
+	ssig = Utils.to_hex(self.uid() + sig)
+
+	conn = None
+	err = False
+	try:
+		try:
+			conn = get_connection()
+			for node in self.outputs:
+				p = node.abspath()
+				recv_file(conn, ssig, cnt, p)
+				cnt += 1
+		except MissingFile as e:
+			Logs.debug('netcache: file is not in the cache %r', e)
+			err = True
+		except Exception as e:
+			Logs.debug('netcache: could not get the files %r', self.outputs)
+			if Logs.verbose > 1:
+				Logs.debug('netcache: exception %r', e)
+			err = True
+
+			# broken connection? remove this one
+			close_connection(conn)
+			conn = None
+		else:
+			Logs.debug('netcache: obtained %r from cache', self.outputs)
+
+	finally:
+		release_connection(conn)
+	if err:
+		return False
+
+	self.cached = True
+	return True
+
+@Utils.run_once
+def put_files_cache(self):
+	if not Task.push_addr:
+		return
+	if not self.outputs:
+		return
+	if getattr(self, 'cached', None):
+		return
+
+	#print "called put_files_cache", id(self)
+	bld = self.generator.bld
+	sig = self.signature()
+	ssig = Utils.to_hex(self.uid() + sig)
+
+	conn = None
+	cnt = 0
+	try:
+		for node in self.outputs:
+			# We could re-create the signature of the task with the signature of the outputs
+			# in practice, this means hashing the output files
+			# this is unnecessary
+			try:
+				if not conn:
+					conn = get_connection(push=True)
+				sock_send(conn, ssig, cnt, node.abspath())
+				Logs.debug('netcache: sent %r', node)
+			except Exception as e:
+				Logs.debug('netcache: could not push the files %r', e)
+
+				# broken connection? remove this one
+				close_connection(conn)
+				conn = None
+			cnt += 1
+	finally:
+		release_connection(conn, push=True)
+
+	bld.task_sigs[self.uid()] = self.cache_sig
+
+def hash_env_vars(self, env, vars_lst):
+	# reimplement so that the resulting hash does not depend on local paths
+	if not env.table:
+		env = env.parent
+		if not env:
+			return Utils.SIG_NIL
+
+	idx = str(id(env)) + str(vars_lst)
+	try:
+		cache = self.cache_env
+	except AttributeError:
+		cache = self.cache_env = {}
+	else:
+		try:
+			return self.cache_env[idx]
+		except KeyError:
+			pass
+
+	v = str([env[a] for a in vars_lst])
+	v = v.replace(self.srcnode.abspath().__repr__()[:-1], '')
+	m = Utils.md5()
+	m.update(v.encode())
+	ret = m.digest()
+
+	Logs.debug('envhash: %r %r', ret, v)
+
+	cache[idx] = ret
+
+	return ret
+
+def uid(self):
+	# reimplement so that the signature does not depend on local paths
+	try:
+		return self.uid_
+	except AttributeError:
+		m = Utils.md5()
+		src = self.generator.bld.srcnode
+		up = m.update
+		up(self.__class__.__name__.encode())
+		for x in self.inputs + self.outputs:
+			up(x.path_from(src).encode())
+		self.uid_ = m.digest()
+		return self.uid_
+
+
+def make_cached(cls):
+	if getattr(cls, 'nocache', None):
+		return
+
+	m1 = cls.run
+	def run(self):
+		if getattr(self, 'nocache', False):
+			return m1(self)
+		if self.can_retrieve_cache():
+			return 0
+		return m1(self)
+	cls.run = run
+
+	m2 = cls.post_run
+	def post_run(self):
+		if getattr(self, 'nocache', False):
+			return m2(self)
+		bld = self.generator.bld
+		ret = m2(self)
+		if bld.cache_global:
+			self.put_files_cache()
+		if hasattr(self, 'chmod'):
+			for node in self.outputs:
+				os.chmod(node.abspath(), self.chmod)
+		return ret
+	cls.post_run = post_run
+
+@conf
+def setup_netcache(ctx, push_addr, pull_addr):
+	Task.Task.can_retrieve_cache = can_retrieve_cache
+	Task.Task.put_files_cache = put_files_cache
+	Task.Task.uid = uid
+	Task.push_addr = push_addr
+	Task.pull_addr = pull_addr
+	Build.BuildContext.hash_env_vars = hash_env_vars
+	ctx.cache_global = True
+
+	for x in Task.classes.values():
+		make_cached(x)
+
+def build(bld):
+	if not 'NETCACHE' in os.environ and not 'NETCACHE_PULL' in os.environ and not 'NETCACHE_PUSH' in os.environ:
+		Logs.warn('Setting  NETCACHE_PULL=127.0.0.1:11001 and NETCACHE_PUSH=127.0.0.1:12001')
+		os.environ['NETCACHE_PULL'] = '127.0.0.1:12001'
+		os.environ['NETCACHE_PUSH'] = '127.0.0.1:11001'
+
+	if 'NETCACHE' in os.environ:
+		if not 'NETCACHE_PUSH' in os.environ:
+			os.environ['NETCACHE_PUSH'] = os.environ['NETCACHE']
+		if not 'NETCACHE_PULL' in os.environ:
+			os.environ['NETCACHE_PULL'] = os.environ['NETCACHE']
+
+	v = os.environ['NETCACHE_PULL']
+	if v:
+		h, p = v.split(':')
+		pull_addr = (h, int(p))
+	else:
+		pull_addr = None
+
+	v = os.environ['NETCACHE_PUSH']
+	if v:
+		h, p = v.split(':')
+		push_addr = (h, int(p))
+	else:
+		push_addr = None
+
+	setup_netcache(bld, push_addr, pull_addr)
+
diff --git a/waflib/extras/objcopy.py b/waflib/extras/objcopy.py
new file mode 100644
index 00000000..82d8359e
--- /dev/null
+++ b/waflib/extras/objcopy.py
@@ -0,0 +1,50 @@
+#!/usr/bin/python
+# Grygoriy Fuchedzhy 2010
+
+"""
+Support for converting linked targets to ihex, srec or binary files using
+objcopy. Use the 'objcopy' feature in conjunction with the 'cc' or 'cxx'
+feature. The 'objcopy' feature uses the following attributes:
+
+objcopy_bfdname		Target object format name (eg. ihex, srec, binary).
+					   Defaults to ihex.
+objcopy_target		 File name used for objcopy output. This defaults to the
+					   target name with objcopy_bfdname as extension.
+objcopy_install_path   Install path for objcopy_target file. Defaults to ${PREFIX}/fw.
+objcopy_flags		  Additional flags passed to objcopy.
+"""
+
+from waflib.Utils import def_attrs
+from waflib import Task
+from waflib.TaskGen import feature, after_method
+
+class objcopy(Task.Task):
+	run_str = '${OBJCOPY} -O ${TARGET_BFDNAME} ${OBJCOPYFLAGS} ${SRC} ${TGT}'
+	color   = 'CYAN'
+
+@feature('objcopy')
+@after_method('apply_link')
+def map_objcopy(self):
+	def_attrs(self,
+	   objcopy_bfdname = 'ihex',
+	   objcopy_target = None,
+	   objcopy_install_path = "${PREFIX}/firmware",
+	   objcopy_flags = '')
+
+	link_output = self.link_task.outputs[0]
+	if not self.objcopy_target:
+		self.objcopy_target = link_output.change_ext('.' + self.objcopy_bfdname).name
+	task = self.create_task('objcopy', src=link_output, tgt=self.path.find_or_declare(self.objcopy_target))
+
+	task.env.append_unique('TARGET_BFDNAME', self.objcopy_bfdname)
+	try:
+		task.env.append_unique('OBJCOPYFLAGS', getattr(self, 'objcopy_flags'))
+	except AttributeError:
+		pass
+
+	if self.objcopy_install_path:
+		self.add_install_files(install_to=self.objcopy_install_path, install_from=task.outputs[0])
+
+def configure(ctx):
+	ctx.find_program('objcopy', var='OBJCOPY', mandatory=True)
+
diff --git a/waflib/extras/ocaml.py b/waflib/extras/ocaml.py
new file mode 100644
index 00000000..afe73c0c
--- /dev/null
+++ b/waflib/extras/ocaml.py
@@ -0,0 +1,348 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2010 (ita)
+
+"ocaml support"
+
+import os, re
+from waflib import Utils, Task
+from waflib.Logs import error
+from waflib.TaskGen import feature, before_method, after_method, extension
+
+EXT_MLL = ['.mll']
+EXT_MLY = ['.mly']
+EXT_MLI = ['.mli']
+EXT_MLC = ['.c']
+EXT_ML  = ['.ml']
+
+open_re = re.compile('^\s*open\s+([a-zA-Z]+)(;;){0,1}$', re.M)
+foo = re.compile(r"""(\(\*)|(\*\))|("(\\.|[^"\\])*"|'(\\.|[^'\\])*'|.[^()*"'\\]*)""", re.M)
+def filter_comments(txt):
+	meh = [0]
+	def repl(m):
+		if m.group(1):
+			meh[0] += 1
+		elif m.group(2):
+			meh[0] -= 1
+		elif not meh[0]:
+			return m.group()
+		return ''
+	return foo.sub(repl, txt)
+
+def scan(self):
+	node = self.inputs[0]
+	code = filter_comments(node.read())
+
+	global open_re
+	names = []
+	import_iterator = open_re.finditer(code)
+	if import_iterator:
+		for import_match in import_iterator:
+			names.append(import_match.group(1))
+	found_lst = []
+	raw_lst = []
+	for name in names:
+		nd = None
+		for x in self.incpaths:
+			nd = x.find_resource(name.lower()+'.ml')
+			if not nd:
+				nd = x.find_resource(name+'.ml')
+			if nd:
+				found_lst.append(nd)
+				break
+		else:
+			raw_lst.append(name)
+
+	return (found_lst, raw_lst)
+
+native_lst=['native', 'all', 'c_object']
+bytecode_lst=['bytecode', 'all']
+
+@feature('ocaml')
+def init_ml(self):
+	Utils.def_attrs(self,
+		type = 'all',
+		incpaths_lst = [],
+		bld_incpaths_lst = [],
+		mlltasks = [],
+		mlytasks = [],
+		mlitasks = [],
+		native_tasks = [],
+		bytecode_tasks = [],
+		linktasks = [],
+		bytecode_env = None,
+		native_env = None,
+		compiled_tasks = [],
+		includes = '',
+		uselib = '',
+		are_deps_set = 0)
+
+@feature('ocaml')
+@after_method('init_ml')
+def init_envs_ml(self):
+
+	self.islibrary = getattr(self, 'islibrary', False)
+
+	global native_lst, bytecode_lst
+	self.native_env = None
+	if self.type in native_lst:
+		self.native_env = self.env.derive()
+		if self.islibrary:
+			self.native_env['OCALINKFLAGS']   = '-a'
+
+	self.bytecode_env = None
+	if self.type in bytecode_lst:
+		self.bytecode_env = self.env.derive()
+		if self.islibrary:
+			self.bytecode_env['OCALINKFLAGS'] = '-a'
+
+	if self.type == 'c_object':
+		self.native_env.append_unique('OCALINKFLAGS_OPT', '-output-obj')
+
+@feature('ocaml')
+@before_method('apply_vars_ml')
+@after_method('init_envs_ml')
+def apply_incpaths_ml(self):
+	inc_lst = self.includes.split()
+	lst = self.incpaths_lst
+	for dir in inc_lst:
+		node = self.path.find_dir(dir)
+		if not node:
+			error("node not found: " + str(dir))
+			continue
+		if not node in lst:
+			lst.append(node)
+		self.bld_incpaths_lst.append(node)
+	# now the nodes are added to self.incpaths_lst
+
+@feature('ocaml')
+@before_method('process_source')
+def apply_vars_ml(self):
+	for i in self.incpaths_lst:
+		if self.bytecode_env:
+			app = self.bytecode_env.append_value
+			app('OCAMLPATH', ['-I', i.bldpath(), '-I', i.srcpath()])
+
+		if self.native_env:
+			app = self.native_env.append_value
+			app('OCAMLPATH', ['-I', i.bldpath(), '-I', i.srcpath()])
+
+	varnames = ['INCLUDES', 'OCAMLFLAGS', 'OCALINKFLAGS', 'OCALINKFLAGS_OPT']
+	for name in self.uselib.split():
+		for vname in varnames:
+			cnt = self.env[vname+'_'+name]
+			if cnt:
+				if self.bytecode_env:
+					self.bytecode_env.append_value(vname, cnt)
+				if self.native_env:
+					self.native_env.append_value(vname, cnt)
+
+@feature('ocaml')
+@after_method('process_source')
+def apply_link_ml(self):
+
+	if self.bytecode_env:
+		ext = self.islibrary and '.cma' or '.run'
+
+		linktask = self.create_task('ocalink')
+		linktask.bytecode = 1
+		linktask.set_outputs(self.path.find_or_declare(self.target + ext))
+		linktask.env = self.bytecode_env
+		self.linktasks.append(linktask)
+
+	if self.native_env:
+		if self.type == 'c_object':
+			ext = '.o'
+		elif self.islibrary:
+			ext = '.cmxa'
+		else:
+			ext = ''
+
+		linktask = self.create_task('ocalinkx')
+		linktask.set_outputs(self.path.find_or_declare(self.target + ext))
+		linktask.env = self.native_env
+		self.linktasks.append(linktask)
+
+		# we produce a .o file to be used by gcc
+		self.compiled_tasks.append(linktask)
+
+@extension(*EXT_MLL)
+def mll_hook(self, node):
+	mll_task = self.create_task('ocamllex', node, node.change_ext('.ml'))
+	mll_task.env = self.native_env.derive()
+	self.mlltasks.append(mll_task)
+
+	self.source.append(mll_task.outputs[0])
+
+@extension(*EXT_MLY)
+def mly_hook(self, node):
+	mly_task = self.create_task('ocamlyacc', node, [node.change_ext('.ml'), node.change_ext('.mli')])
+	mly_task.env = self.native_env.derive()
+	self.mlytasks.append(mly_task)
+	self.source.append(mly_task.outputs[0])
+
+	task = self.create_task('ocamlcmi', mly_task.outputs[1], mly_task.outputs[1].change_ext('.cmi'))
+	task.env = self.native_env.derive()
+
+@extension(*EXT_MLI)
+def mli_hook(self, node):
+	task = self.create_task('ocamlcmi', node, node.change_ext('.cmi'))
+	task.env = self.native_env.derive()
+	self.mlitasks.append(task)
+
+@extension(*EXT_MLC)
+def mlc_hook(self, node):
+	task = self.create_task('ocamlcc', node, node.change_ext('.o'))
+	task.env = self.native_env.derive()
+	self.compiled_tasks.append(task)
+
+@extension(*EXT_ML)
+def ml_hook(self, node):
+	if self.native_env:
+		task = self.create_task('ocamlx', node, node.change_ext('.cmx'))
+		task.env = self.native_env.derive()
+		task.incpaths = self.bld_incpaths_lst
+		self.native_tasks.append(task)
+
+	if self.bytecode_env:
+		task = self.create_task('ocaml', node, node.change_ext('.cmo'))
+		task.env = self.bytecode_env.derive()
+		task.bytecode = 1
+		task.incpaths = self.bld_incpaths_lst
+		self.bytecode_tasks.append(task)
+
+def compile_may_start(self):
+
+	if not getattr(self, 'flag_deps', ''):
+		self.flag_deps = 1
+
+		# the evil part is that we can only compute the dependencies after the
+		# source files can be read (this means actually producing the source files)
+		if getattr(self, 'bytecode', ''):
+			alltasks = self.generator.bytecode_tasks
+		else:
+			alltasks = self.generator.native_tasks
+
+		self.signature() # ensure that files are scanned - unfortunately
+		tree = self.generator.bld
+		for node in self.inputs:
+			lst = tree.node_deps[self.uid()]
+			for depnode in lst:
+				for t in alltasks:
+					if t == self:
+						continue
+					if depnode in t.inputs:
+						self.set_run_after(t)
+
+		# TODO necessary to get the signature right - for now
+		delattr(self, 'cache_sig')
+		self.signature()
+
+	return Task.Task.runnable_status(self)
+
+class ocamlx(Task.Task):
+	"""native caml compilation"""
+	color   = 'GREEN'
+	run_str = '${OCAMLOPT} ${OCAMLPATH} ${OCAMLFLAGS} ${OCAMLINCLUDES} -c -o ${TGT} ${SRC}'
+	scan    = scan
+	runnable_status = compile_may_start
+
+class ocaml(Task.Task):
+	"""bytecode caml compilation"""
+	color   = 'GREEN'
+	run_str = '${OCAMLC} ${OCAMLPATH} ${OCAMLFLAGS} ${OCAMLINCLUDES} -c -o ${TGT} ${SRC}'
+	scan    = scan
+	runnable_status = compile_may_start
+
+class ocamlcmi(Task.Task):
+	"""interface generator (the .i files?)"""
+	color   = 'BLUE'
+	run_str = '${OCAMLC} ${OCAMLPATH} ${OCAMLINCLUDES} -o ${TGT} -c ${SRC}'
+	before  = ['ocamlcc', 'ocaml', 'ocamlcc']
+
+class ocamlcc(Task.Task):
+	"""ocaml to c interfaces"""
+	color   = 'GREEN'
+	run_str = 'cd ${TGT[0].bld_dir()} && ${OCAMLOPT} ${OCAMLFLAGS} ${OCAMLPATH} ${OCAMLINCLUDES} -c ${SRC[0].abspath()}'
+
+class ocamllex(Task.Task):
+	"""lexical generator"""
+	color   = 'BLUE'
+	run_str = '${OCAMLLEX} ${SRC} -o ${TGT}'
+	before  = ['ocamlcmi', 'ocaml', 'ocamlcc']
+
+class ocamlyacc(Task.Task):
+	"""parser generator"""
+	color   = 'BLUE'
+	run_str = '${OCAMLYACC} -b ${tsk.base()} ${SRC}'
+	before  = ['ocamlcmi', 'ocaml', 'ocamlcc']
+
+	def base(self):
+		node = self.outputs[0]
+		s = os.path.splitext(node.name)[0]
+		return node.bld_dir() + os.sep + s
+
+def link_may_start(self):
+
+	if getattr(self, 'bytecode', 0):
+		alltasks = self.generator.bytecode_tasks
+	else:
+		alltasks = self.generator.native_tasks
+
+	for x in alltasks:
+		if not x.hasrun:
+			return Task.ASK_LATER
+
+	if not getattr(self, 'order', ''):
+
+		# now reorder the inputs given the task dependencies
+		# this part is difficult, we do not have a total order on the tasks
+		# if the dependencies are wrong, this may not stop
+		seen = []
+		pendant = []+alltasks
+		while pendant:
+			task = pendant.pop(0)
+			if task in seen:
+				continue
+			for x in task.run_after:
+				if not x in seen:
+					pendant.append(task)
+					break
+			else:
+				seen.append(task)
+		self.inputs = [x.outputs[0] for x in seen]
+		self.order = 1
+	return Task.Task.runnable_status(self)
+
+class ocalink(Task.Task):
+	"""bytecode caml link"""
+	color   = 'YELLOW'
+	run_str = '${OCAMLC} -o ${TGT} ${OCAMLINCLUDES} ${OCALINKFLAGS} ${SRC}'
+	runnable_status = link_may_start
+	after = ['ocaml', 'ocamlcc']
+
+class ocalinkx(Task.Task):
+	"""native caml link"""
+	color   = 'YELLOW'
+	run_str = '${OCAMLOPT} -o ${TGT} ${OCAMLINCLUDES} ${OCALINKFLAGS_OPT} ${SRC}'
+	runnable_status = link_may_start
+	after = ['ocamlx', 'ocamlcc']
+
+def configure(conf):
+	opt = conf.find_program('ocamlopt', var='OCAMLOPT', mandatory=False)
+	occ = conf.find_program('ocamlc', var='OCAMLC', mandatory=False)
+	if (not opt) or (not occ):
+		conf.fatal('The objective caml compiler was not found:\ninstall it or make it available in your PATH')
+
+	v = conf.env
+	v['OCAMLC']       = occ
+	v['OCAMLOPT']     = opt
+	v['OCAMLLEX']     = conf.find_program('ocamllex', var='OCAMLLEX', mandatory=False)
+	v['OCAMLYACC']    = conf.find_program('ocamlyacc', var='OCAMLYACC', mandatory=False)
+	v['OCAMLFLAGS']   = ''
+	where = conf.cmd_and_log(conf.env.OCAMLC + ['-where']).strip()+os.sep
+	v['OCAMLLIB']     = where
+	v['LIBPATH_OCAML'] = where
+	v['INCLUDES_OCAML'] = where
+	v['LIB_OCAML'] = 'camlrun'
+
diff --git a/waflib/extras/package.py b/waflib/extras/package.py
new file mode 100644
index 00000000..c06498eb
--- /dev/null
+++ b/waflib/extras/package.py
@@ -0,0 +1,76 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2011
+
+"""
+Obtain packages, unpack them in a location, and add associated uselib variables
+(CFLAGS_pkgname, LIBPATH_pkgname, etc).
+
+The default is use a Dependencies.txt file in the source directory.
+
+This is a work in progress.
+
+Usage:
+
+def options(opt):
+	opt.load('package')
+
+def configure(conf):
+	conf.load_packages()
+"""
+
+from waflib import Logs
+from waflib.Configure import conf
+
+try:
+	from urllib import request
+except ImportError:
+	from urllib import urlopen
+else:
+	urlopen = request.urlopen
+
+
+CACHEVAR = 'WAFCACHE_PACKAGE'
+
+@conf
+def get_package_cache_dir(self):
+	cache = None
+	if CACHEVAR in conf.environ:
+		cache = conf.environ[CACHEVAR]
+		cache = self.root.make_node(cache)
+	elif self.env[CACHEVAR]:
+		cache = self.env[CACHEVAR]
+		cache = self.root.make_node(cache)
+	else:
+		cache = self.srcnode.make_node('.wafcache_package')
+	cache.mkdir()
+	return cache
+
+@conf
+def download_archive(self, src, dst):
+	for x in self.env.PACKAGE_REPO:
+		url = '/'.join((x, src))
+		try:
+			web = urlopen(url)
+			try:
+				if web.getcode() != 200:
+					continue
+			except AttributeError:
+				pass
+		except Exception:
+			# on python3 urlopen throws an exception
+			# python 2.3 does not have getcode and throws an exception to fail
+			continue
+		else:
+			tmp = self.root.make_node(dst)
+			tmp.write(web.read())
+			Logs.warn('Downloaded %s from %s', tmp.abspath(), url)
+			break
+	else:
+		self.fatal('Could not get the package %s' % src)
+
+@conf
+def load_packages(self):
+	self.get_package_cache_dir()
+	# read the dependencies, get the archives, ..
+
diff --git a/waflib/extras/parallel_debug.py b/waflib/extras/parallel_debug.py
new file mode 100644
index 00000000..35883a3d
--- /dev/null
+++ b/waflib/extras/parallel_debug.py
@@ -0,0 +1,459 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2007-2010 (ita)
+
+"""
+Debugging helper for parallel compilation, outputs
+a file named pdebug.svg in the source directory::
+
+	def options(opt):
+		opt.load('parallel_debug')
+	def build(bld):
+		...
+"""
+
+import re, sys, threading, time, traceback
+try:
+	from Queue import Queue
+except:
+	from queue import Queue
+from waflib import Runner, Options, Task, Logs, Errors
+
+SVG_TEMPLATE = """<?xml version="1.0" encoding="UTF-8" standalone="no"?>
+<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.0//EN" "http://www.w3.org/TR/2001/REC-SVG-20010904/DTD/svg10.dtd">
+<svg xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" version="1.0"
+   x="${project.x}" y="${project.y}" width="${project.width}" height="${project.height}" id="svg602" xml:space="preserve">
+
+<style type='text/css' media='screen'>
+	g.over rect { stroke:#FF0000; fill-opacity:0.4 }
+</style>
+
+<script type='text/javascript'><![CDATA[
+var svg  = document.getElementsByTagName('svg')[0];
+
+svg.addEventListener('mouseover', function(e) {
+	var g = e.target.parentNode;
+	var x = document.getElementById('r_' + g.id);
+	if (x) {
+		g.setAttribute('class', g.getAttribute('class') + ' over');
+		x.setAttribute('class', x.getAttribute('class') + ' over');
+		showInfo(e, g.id, e.target.attributes.tooltip.value);
+	}
+}, false);
+
+svg.addEventListener('mouseout', function(e) {
+		var g = e.target.parentNode;
+		var x = document.getElementById('r_' + g.id);
+		if (x) {
+			g.setAttribute('class', g.getAttribute('class').replace(' over', ''));
+			x.setAttribute('class', x.getAttribute('class').replace(' over', ''));
+			hideInfo(e);
+		}
+}, false);
+
+function showInfo(evt, txt, details) {
+${if project.tooltip}
+	tooltip = document.getElementById('tooltip');
+
+	var t = document.getElementById('tooltiptext');
+	t.firstChild.data = txt + " " + details;
+
+	var x = evt.clientX + 9;
+	if (x > 250) { x -= t.getComputedTextLength() + 16; }
+	var y = evt.clientY + 20;
+	tooltip.setAttribute("transform", "translate(" + x + "," + y + ")");
+	tooltip.setAttributeNS(null, "visibility", "visible");
+
+	var r = document.getElementById('tooltiprect');
+	r.setAttribute('width', t.getComputedTextLength() + 6);
+${endif}
+}
+
+function hideInfo(evt) {
+	var tooltip = document.getElementById('tooltip');
+	tooltip.setAttributeNS(null,"visibility","hidden");
+}
+]]></script>
+
+<!-- inkscape requires a big rectangle or it will not export the pictures properly -->
+<rect
+   x='${project.x}' y='${project.y}' width='${project.width}' height='${project.height}'
+   style="font-size:10;fill:#ffffff;fill-opacity:0.01;fill-rule:evenodd;stroke:#ffffff;"></rect>
+
+${if project.title}
+  <text x="${project.title_x}" y="${project.title_y}"
+    style="font-size:15px; text-anchor:middle; font-style:normal;font-weight:normal;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;font-family:Bitstream Vera Sans">${project.title}</text>
+${endif}
+
+
+${for cls in project.groups}
+  <g id='${cls.classname}'>
+    ${for rect in cls.rects}
+    <rect x='${rect.x}' y='${rect.y}' width='${rect.width}' height='${rect.height}' tooltip='${rect.name}' style="font-size:10;fill:${rect.color};fill-rule:evenodd;stroke:#000000;stroke-width:0.4;" />
+    ${endfor}
+  </g>
+${endfor}
+
+${for info in project.infos}
+  <g id='r_${info.classname}'>
+   <rect x='${info.x}' y='${info.y}' width='${info.width}' height='${info.height}' style="font-size:10;fill:${info.color};fill-rule:evenodd;stroke:#000000;stroke-width:0.4;" />
+   <text x="${info.text_x}" y="${info.text_y}"
+       style="font-size:12px;font-style:normal;font-weight:normal;fill:#000000;fill-opacity:1;stroke:none;stroke-width:1px;stroke-linecap:butt;stroke-linejoin:miter;stroke-opacity:1;font-family:Bitstream Vera Sans"
+   >${info.text}</text>
+  </g>
+${endfor}
+
+${if project.tooltip}
+  <g transform="translate(0,0)" visibility="hidden" id="tooltip">
+       <rect id="tooltiprect" y="-15" x="-3" width="1" height="20" style="stroke:black;fill:#edefc2;stroke-width:1"/>
+       <text id="tooltiptext" style="font-family:Arial; font-size:12;fill:black;"> </text>
+  </g>
+${endif}
+
+</svg>
+"""
+
+COMPILE_TEMPLATE = '''def f(project):
+	lst = []
+	def xml_escape(value):
+		return value.replace("&", "&amp;").replace('"', "&quot;").replace("'", "&apos;").replace("<", "&lt;").replace(">", "&gt;")
+
+	%s
+	return ''.join(lst)
+'''
+reg_act = re.compile(r"(?P<backslash>\\)|(?P<dollar>\$\$)|(?P<subst>\$\{(?P<code>[^}]*?)\})", re.M)
+def compile_template(line):
+
+	extr = []
+	def repl(match):
+		g = match.group
+		if g('dollar'):
+			return "$"
+		elif g('backslash'):
+			return "\\"
+		elif g('subst'):
+			extr.append(g('code'))
+			return "<<|@|>>"
+		return None
+
+	line2 = reg_act.sub(repl, line)
+	params = line2.split('<<|@|>>')
+	assert(extr)
+
+
+	indent = 0
+	buf = []
+	app = buf.append
+
+	def app(txt):
+		buf.append(indent * '\t' + txt)
+
+	for x in range(len(extr)):
+		if params[x]:
+			app("lst.append(%r)" % params[x])
+
+		f = extr[x]
+		if f.startswith(('if', 'for')):
+			app(f + ':')
+			indent += 1
+		elif f.startswith('py:'):
+			app(f[3:])
+		elif f.startswith(('endif', 'endfor')):
+			indent -= 1
+		elif f.startswith(('else', 'elif')):
+			indent -= 1
+			app(f + ':')
+			indent += 1
+		elif f.startswith('xml:'):
+			app('lst.append(xml_escape(%s))' % f[4:])
+		else:
+			#app('lst.append((%s) or "cannot find %s")' % (f, f))
+			app('lst.append(str(%s))' % f)
+
+	if extr:
+		if params[-1]:
+			app("lst.append(%r)" % params[-1])
+
+	fun = COMPILE_TEMPLATE % "\n\t".join(buf)
+	# uncomment the following to debug the template
+	#for i, x in enumerate(fun.splitlines()):
+	#	print i, x
+	return Task.funex(fun)
+
+# red   #ff4d4d
+# green #4da74d
+# lila  #a751ff
+
+color2code = {
+	'GREEN'  : '#4da74d',
+	'YELLOW' : '#fefe44',
+	'PINK'   : '#a751ff',
+	'RED'    : '#cc1d1d',
+	'BLUE'   : '#6687bb',
+	'CYAN'   : '#34e2e2',
+}
+
+mp = {}
+info = [] # list of (text,color)
+
+def map_to_color(name):
+	if name in mp:
+		return mp[name]
+	try:
+		cls = Task.classes[name]
+	except KeyError:
+		return color2code['RED']
+	if cls.color in mp:
+		return mp[cls.color]
+	if cls.color in color2code:
+		return color2code[cls.color]
+	return color2code['RED']
+
+def process(self):
+	m = self.generator.bld.producer
+	try:
+		# TODO another place for this?
+		del self.generator.bld.task_sigs[self.uid()]
+	except KeyError:
+		pass
+
+	self.generator.bld.producer.set_running(1, self)
+
+	try:
+		ret = self.run()
+	except Exception:
+		self.err_msg = traceback.format_exc()
+		self.hasrun = Task.EXCEPTION
+
+		# TODO cleanup
+		m.error_handler(self)
+		return
+
+	if ret:
+		self.err_code = ret
+		self.hasrun = Task.CRASHED
+	else:
+		try:
+			self.post_run()
+		except Errors.WafError:
+			pass
+		except Exception:
+			self.err_msg = traceback.format_exc()
+			self.hasrun = Task.EXCEPTION
+		else:
+			self.hasrun = Task.SUCCESS
+	if self.hasrun != Task.SUCCESS:
+		m.error_handler(self)
+
+	self.generator.bld.producer.set_running(-1, self)
+
+Task.Task.process_back = Task.Task.process
+Task.Task.process = process
+
+old_start = Runner.Parallel.start
+def do_start(self):
+	try:
+		Options.options.dband
+	except AttributeError:
+		self.bld.fatal('use def options(opt): opt.load("parallel_debug")!')
+
+	self.taskinfo = Queue()
+	old_start(self)
+	if self.dirty:
+		make_picture(self)
+Runner.Parallel.start = do_start
+
+lock_running = threading.Lock()
+def set_running(self, by, tsk):
+	with lock_running:
+		try:
+			cache = self.lock_cache
+		except AttributeError:
+			cache = self.lock_cache = {}
+
+		i = 0
+		if by > 0:
+			vals = cache.values()
+			for i in range(self.numjobs):
+				if i not in vals:
+					cache[tsk] = i
+					break
+		else:
+			i = cache[tsk]
+			del cache[tsk]
+
+		self.taskinfo.put( (i, id(tsk), time.time(), tsk.__class__.__name__, self.processed, self.count, by, ",".join(map(str, tsk.outputs)))  )
+Runner.Parallel.set_running = set_running
+
+def name2class(name):
+	return name.replace(' ', '_').replace('.', '_')
+
+def make_picture(producer):
+	# first, cast the parameters
+	if not hasattr(producer.bld, 'path'):
+		return
+
+	tmp = []
+	try:
+		while True:
+			tup = producer.taskinfo.get(False)
+			tmp.append(list(tup))
+	except:
+		pass
+
+	try:
+		ini = float(tmp[0][2])
+	except:
+		return
+
+	if not info:
+		seen = []
+		for x in tmp:
+			name = x[3]
+			if not name in seen:
+				seen.append(name)
+			else:
+				continue
+
+			info.append((name, map_to_color(name)))
+		info.sort(key=lambda x: x[0])
+
+	thread_count = 0
+	acc = []
+	for x in tmp:
+		thread_count += x[6]
+		acc.append("%d %d %f %r %d %d %d %s" % (x[0], x[1], x[2] - ini, x[3], x[4], x[5], thread_count, x[7]))
+
+	data_node = producer.bld.path.make_node('pdebug.dat')
+	data_node.write('\n'.join(acc))
+
+	tmp = [lst[:2] + [float(lst[2]) - ini] + lst[3:] for lst in tmp]
+
+	st = {}
+	for l in tmp:
+		if not l[0] in st:
+			st[l[0]] = len(st.keys())
+	tmp = [  [st[lst[0]]] + lst[1:] for lst in tmp ]
+	THREAD_AMOUNT = len(st.keys())
+
+	st = {}
+	for l in tmp:
+		if not l[1] in st:
+			st[l[1]] = len(st.keys())
+	tmp = [  [lst[0]] + [st[lst[1]]] + lst[2:] for lst in tmp ]
+
+
+	BAND = Options.options.dband
+
+	seen = {}
+	acc = []
+	for x in range(len(tmp)):
+		line = tmp[x]
+		id = line[1]
+
+		if id in seen:
+			continue
+		seen[id] = True
+
+		begin = line[2]
+		thread_id = line[0]
+		for y in range(x + 1, len(tmp)):
+			line = tmp[y]
+			if line[1] == id:
+				end = line[2]
+				#print id, thread_id, begin, end
+				#acc.append(  ( 10*thread_id, 10*(thread_id+1), 10*begin, 10*end ) )
+				acc.append( (BAND * begin, BAND*thread_id, BAND*end - BAND*begin, BAND, line[3], line[7]) )
+				break
+
+	if Options.options.dmaxtime < 0.1:
+		gwidth = 1
+		for x in tmp:
+			m = BAND * x[2]
+			if m > gwidth:
+				gwidth = m
+	else:
+		gwidth = BAND * Options.options.dmaxtime
+
+	ratio = float(Options.options.dwidth) / gwidth
+	gwidth = Options.options.dwidth
+	gheight = BAND * (THREAD_AMOUNT + len(info) + 1.5)
+
+
+	# simple data model for our template
+	class tobject(object):
+		pass
+
+	model = tobject()
+	model.x = 0
+	model.y = 0
+	model.width = gwidth + 4
+	model.height = gheight + 4
+
+	model.tooltip = not Options.options.dnotooltip
+
+	model.title = Options.options.dtitle
+	model.title_x = gwidth / 2
+	model.title_y = gheight + - 5
+
+	groups = {}
+	for (x, y, w, h, clsname, name) in acc:
+		try:
+			groups[clsname].append((x, y, w, h, name))
+		except:
+			groups[clsname] = [(x, y, w, h, name)]
+
+	# groups of rectangles (else js highlighting is slow)
+	model.groups = []
+	for cls in groups:
+		g = tobject()
+		model.groups.append(g)
+		g.classname = name2class(cls)
+		g.rects = []
+		for (x, y, w, h, name) in groups[cls]:
+			r = tobject()
+			g.rects.append(r)
+			r.x = 2 + x * ratio
+			r.y = 2 + y
+			r.width = w * ratio
+			r.height = h
+			r.name = name
+			r.color = map_to_color(cls)
+
+	cnt = THREAD_AMOUNT
+
+	# caption
+	model.infos = []
+	for (text, color) in info:
+		inf = tobject()
+		model.infos.append(inf)
+		inf.classname = name2class(text)
+		inf.x = 2 + BAND
+		inf.y = 5 + (cnt + 0.5) * BAND
+		inf.width = BAND/2
+		inf.height = BAND/2
+		inf.color = color
+
+		inf.text = text
+		inf.text_x = 2 + 2 * BAND
+		inf.text_y = 5 + (cnt + 0.5) * BAND + 10
+
+		cnt += 1
+
+	# write the file...
+	template1 = compile_template(SVG_TEMPLATE)
+	txt = template1(model)
+
+	node = producer.bld.path.make_node('pdebug.svg')
+	node.write(txt)
+	Logs.warn('Created the diagram %r', node)
+
+def options(opt):
+	opt.add_option('--dtitle', action='store', default='Parallel build representation for %r' % ' '.join(sys.argv),
+		help='title for the svg diagram', dest='dtitle')
+	opt.add_option('--dwidth', action='store', type='int', help='diagram width', default=800, dest='dwidth')
+	opt.add_option('--dtime', action='store', type='float', help='recording interval in seconds', default=0.009, dest='dtime')
+	opt.add_option('--dband', action='store', type='int', help='band width', default=22, dest='dband')
+	opt.add_option('--dmaxtime', action='store', type='float', help='maximum time, for drawing fair comparisons', default=0, dest='dmaxtime')
+	opt.add_option('--dnotooltip', action='store_true', help='disable tooltips', default=False, dest='dnotooltip')
+
diff --git a/waflib/extras/pch.py b/waflib/extras/pch.py
new file mode 100644
index 00000000..103e7528
--- /dev/null
+++ b/waflib/extras/pch.py
@@ -0,0 +1,148 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Alexander Afanasyev (UCLA), 2014
+
+"""
+Enable precompiled C++ header support (currently only clang++ and g++ are supported)
+
+To use this tool, wscript should look like:
+
+	def options(opt):
+		opt.load('pch')
+		# This will add `--with-pch` configure option.
+		# Unless --with-pch during configure stage specified, the precompiled header support is disabled
+
+	def configure(conf):
+		conf.load('pch')
+		# this will set conf.env.WITH_PCH if --with-pch is specified and the supported compiler is used
+		# Unless conf.env.WITH_PCH is set, the precompiled header support is disabled
+
+	def build(bld):
+		bld(features='cxx pch',
+			target='precompiled-headers',
+			name='precompiled-headers',
+			headers='a.h b.h c.h', # headers to pre-compile into `precompiled-headers`
+
+			# Other parameters to compile precompiled headers
+			# includes=...,
+			# export_includes=...,
+			# use=...,
+			# ...
+
+			# Exported parameters will be propagated even if precompiled headers are disabled
+		)
+
+		bld(
+			target='test',
+			features='cxx cxxprogram',
+			source='a.cpp b.cpp d.cpp main.cpp',
+			use='precompiled-headers',
+		)
+
+		# or
+
+		bld(
+			target='test',
+			features='pch cxx cxxprogram',
+			source='a.cpp b.cpp d.cpp main.cpp',
+			headers='a.h b.h c.h',
+		)
+
+Note that precompiled header must have multiple inclusion guards.  If the guards are missing, any benefit of precompiled header will be voided and compilation may fail in some cases.
+"""
+
+import os
+from waflib import Task, TaskGen, Utils
+from waflib.Tools import c_preproc, cxx
+
+
+PCH_COMPILER_OPTIONS = {
+	'clang++': [['-include'], '.pch', ['-x', 'c++-header']],
+	'g++':     [['-include'], '.gch', ['-x', 'c++-header']],
+}
+
+
+def options(opt):
+	opt.add_option('--without-pch', action='store_false', default=True, dest='with_pch', help='''Try to use precompiled header to speed up compilation (only g++ and clang++)''')
+
+def configure(conf):
+	if (conf.options.with_pch and conf.env['COMPILER_CXX'] in PCH_COMPILER_OPTIONS.keys()):
+		conf.env.WITH_PCH = True
+		flags = PCH_COMPILER_OPTIONS[conf.env['COMPILER_CXX']]
+		conf.env.CXXPCH_F = flags[0]
+		conf.env.CXXPCH_EXT = flags[1]
+		conf.env.CXXPCH_FLAGS = flags[2]
+
+
+@TaskGen.feature('pch')
+@TaskGen.before('process_source')
+def apply_pch(self):
+	if not self.env.WITH_PCH:
+		return
+
+	if getattr(self.bld, 'pch_tasks', None) is None:
+		self.bld.pch_tasks = {}
+
+	if getattr(self, 'headers', None) is None:
+		return
+
+	self.headers = self.to_nodes(self.headers)
+
+	if getattr(self, 'name', None):
+		try:
+			task = self.bld.pch_tasks["%s.%s" % (self.name, self.idx)]
+			self.bld.fatal("Duplicated 'pch' task with name %r" % "%s.%s" % (self.name, self.idx))
+		except KeyError:
+			pass
+
+	out = '%s.%d%s' % (self.target, self.idx, self.env['CXXPCH_EXT'])
+	out = self.path.find_or_declare(out)
+	task = self.create_task('gchx', self.headers, out)
+
+	# target should be an absolute path of `out`, but without precompiled header extension
+	task.target = out.abspath()[:-len(out.suffix())]
+
+	self.pch_task = task
+	if getattr(self, 'name', None):
+		self.bld.pch_tasks["%s.%s" % (self.name, self.idx)] = task
+
+@TaskGen.feature('cxx')
+@TaskGen.after_method('process_source', 'propagate_uselib_vars')
+def add_pch(self):
+	if not (self.env['WITH_PCH'] and getattr(self, 'use', None) and getattr(self, 'compiled_tasks', None) and getattr(self.bld, 'pch_tasks', None)):
+		return
+
+	pch = None
+	# find pch task, if any
+
+	if getattr(self, 'pch_task', None):
+		pch = self.pch_task
+	else:
+		for use in Utils.to_list(self.use):
+			try:
+				pch = self.bld.pch_tasks[use]
+			except KeyError:
+				pass
+
+	if pch:
+		for x in self.compiled_tasks:
+			x.env.append_value('CXXFLAGS', self.env['CXXPCH_F'] + [pch.target])
+
+class gchx(Task.Task):
+	run_str = '${CXX} ${ARCH_ST:ARCH} ${CXXFLAGS} ${CXXPCH_FLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} ${CXXPCH_F:SRC} ${CXX_SRC_F}${SRC[0].abspath()} ${CXX_TGT_F}${TGT[0].abspath()} ${CPPFLAGS}'
+	scan    = c_preproc.scan
+	color   = 'BLUE'
+	ext_out=['.h']
+
+	def runnable_status(self):
+		try:
+			node_deps = self.generator.bld.node_deps[self.uid()]
+		except KeyError:
+			node_deps = []
+		ret = Task.Task.runnable_status(self)
+		if ret == Task.SKIP_ME and self.env.CXX_NAME == 'clang':
+			t = os.stat(self.outputs[0].abspath()).st_mtime
+			for n in self.inputs + node_deps:
+				if os.stat(n.abspath()).st_mtime > t:
+					return Task.RUN_ME
+		return ret
diff --git a/waflib/extras/pep8.py b/waflib/extras/pep8.py
new file mode 100644
index 00000000..676beedb
--- /dev/null
+++ b/waflib/extras/pep8.py
@@ -0,0 +1,106 @@
+#! /usr/bin/env python
+# encoding: utf-8
+#
+# written by Sylvain Rouquette, 2011
+
+'''
+Install pep8 module:
+$ easy_install pep8
+	or
+$ pip install pep8
+
+To add the pep8 tool to the waf file:
+$ ./waf-light --tools=compat15,pep8
+	or, if you have waf >= 1.6.2
+$ ./waf update --files=pep8
+
+
+Then add this to your wscript:
+
+[at]extension('.py', 'wscript')
+def run_pep8(self, node):
+	self.create_task('Pep8', node)
+
+'''
+
+import threading
+from waflib import Task, Options
+
+pep8 = __import__('pep8')
+
+
+class Pep8(Task.Task):
+	color = 'PINK'
+	lock = threading.Lock()
+
+	def check_options(self):
+		if pep8.options:
+			return
+		pep8.options = Options.options
+		pep8.options.prog = 'pep8'
+		excl = pep8.options.exclude.split(',')
+		pep8.options.exclude = [s.rstrip('/') for s in excl]
+		if pep8.options.filename:
+			pep8.options.filename = pep8.options.filename.split(',')
+		if pep8.options.select:
+			pep8.options.select = pep8.options.select.split(',')
+		else:
+			pep8.options.select = []
+		if pep8.options.ignore:
+			pep8.options.ignore = pep8.options.ignore.split(',')
+		elif pep8.options.select:
+			# Ignore all checks which are not explicitly selected
+			pep8.options.ignore = ['']
+		elif pep8.options.testsuite or pep8.options.doctest:
+			# For doctest and testsuite, all checks are required
+			pep8.options.ignore = []
+		else:
+			# The default choice: ignore controversial checks
+			pep8.options.ignore = pep8.DEFAULT_IGNORE.split(',')
+		pep8.options.physical_checks = pep8.find_checks('physical_line')
+		pep8.options.logical_checks = pep8.find_checks('logical_line')
+		pep8.options.counters = dict.fromkeys(pep8.BENCHMARK_KEYS, 0)
+		pep8.options.messages = {}
+
+	def run(self):
+		with Pep8.lock:
+			self.check_options()
+		pep8.input_file(self.inputs[0].abspath())
+		return 0 if not pep8.get_count() else -1
+
+
+def options(opt):
+	opt.add_option('-q', '--quiet', default=0, action='count',
+				   help="report only file names, or nothing with -qq")
+	opt.add_option('-r', '--repeat', action='store_true',
+				   help="show all occurrences of the same error")
+	opt.add_option('--exclude', metavar='patterns',
+				   default=pep8.DEFAULT_EXCLUDE,
+				   help="exclude files or directories which match these "
+				   "comma separated patterns (default: %s)" %
+				   pep8.DEFAULT_EXCLUDE,
+				   dest='exclude')
+	opt.add_option('--filename', metavar='patterns', default='*.py',
+				   help="when parsing directories, only check filenames "
+				   "matching these comma separated patterns (default: "
+				   "*.py)")
+	opt.add_option('--select', metavar='errors', default='',
+				   help="select errors and warnings (e.g. E,W6)")
+	opt.add_option('--ignore', metavar='errors', default='',
+				   help="skip errors and warnings (e.g. E4,W)")
+	opt.add_option('--show-source', action='store_true',
+				   help="show source code for each error")
+	opt.add_option('--show-pep8', action='store_true',
+				   help="show text of PEP 8 for each error")
+	opt.add_option('--statistics', action='store_true',
+				   help="count errors and warnings")
+	opt.add_option('--count', action='store_true',
+				   help="print total number of errors and warnings "
+				   "to standard error and set exit code to 1 if "
+				   "total is not null")
+	opt.add_option('--benchmark', action='store_true',
+				   help="measure processing speed")
+	opt.add_option('--testsuite', metavar='dir',
+				   help="run regression tests from dir")
+	opt.add_option('--doctest', action='store_true',
+				   help="run doctest on myself")
diff --git a/waflib/extras/pgicc.py b/waflib/extras/pgicc.py
new file mode 100644
index 00000000..9790b9cf
--- /dev/null
+++ b/waflib/extras/pgicc.py
@@ -0,0 +1,75 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Antoine Dechaume 2011
+
+"""
+Detect the PGI C compiler
+"""
+
+import sys, re
+from waflib import Errors
+from waflib.Configure import conf
+from waflib.Tools.compiler_c import c_compiler
+c_compiler['linux'].append('pgicc')
+
+@conf
+def find_pgi_compiler(conf, var, name):
+	"""
+	Find the program name, and execute it to ensure it really is itself.
+	"""
+	if sys.platform == 'cygwin':
+		conf.fatal('The PGI compiler does not work on Cygwin')
+
+	v = conf.env
+	cc = None
+	if v[var]:
+		cc = v[var]
+	elif var in conf.environ:
+		cc = conf.environ[var]
+	if not cc:
+		cc = conf.find_program(name, var=var)
+	if not cc:
+		conf.fatal('PGI Compiler (%s) was not found' % name)
+
+	v[var + '_VERSION'] = conf.get_pgi_version(cc)
+	v[var] = cc
+	v[var + '_NAME'] = 'pgi'
+
+@conf
+def get_pgi_version(conf, cc):
+	"""Find the version of a pgi compiler."""
+	version_re = re.compile(r"The Portland Group", re.I).search
+	cmd = cc + ['-V', '-E'] # Issue 1078, prevent wrappers from linking
+
+	try:
+		out, err = conf.cmd_and_log(cmd, output=0)
+	except Errors.WafError:
+		conf.fatal('Could not find pgi compiler %r' % cmd)
+
+	if out:
+		match = version_re(out)
+	else:
+		match = version_re(err)
+
+	if not match:
+		conf.fatal('Could not verify PGI signature')
+
+	cmd = cc + ['-help=variable']
+	try:
+		out, err = conf.cmd_and_log(cmd, output=0)
+	except Errors.WafError:
+		conf.fatal('Could not find pgi compiler %r' % cmd)
+
+	version = re.findall('^COMPVER\s*=(.*)', out, re.M)
+	if len(version) != 1:
+		conf.fatal('Could not determine the compiler version')
+	return version[0]
+
+def configure(conf):
+	conf.find_pgi_compiler('CC', 'pgcc')
+	conf.find_ar()
+	conf.gcc_common_flags()
+	conf.cc_load_tools()
+	conf.cc_add_flags()
+	conf.link_add_flags()
+
diff --git a/waflib/extras/pgicxx.py b/waflib/extras/pgicxx.py
new file mode 100644
index 00000000..eae121c4
--- /dev/null
+++ b/waflib/extras/pgicxx.py
@@ -0,0 +1,20 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Antoine Dechaume 2011
+
+"""
+Detect the PGI C++ compiler
+"""
+
+from waflib.Tools.compiler_cxx import cxx_compiler
+cxx_compiler['linux'].append('pgicxx')
+
+from waflib.extras import pgicc
+
+def configure(conf):
+	conf.find_pgi_compiler('CXX', 'pgCC')
+	conf.find_ar()
+	conf.gxx_common_flags()
+	conf.cxx_load_tools()
+	conf.cxx_add_flags()
+	conf.link_add_flags()
diff --git a/waflib/extras/proc.py b/waflib/extras/proc.py
new file mode 100644
index 00000000..764abecf
--- /dev/null
+++ b/waflib/extras/proc.py
@@ -0,0 +1,54 @@
+#! /usr/bin/env python
+# per rosengren 2011
+
+from os import environ, path
+from waflib import TaskGen, Utils
+
+def options(opt):
+	grp = opt.add_option_group('Oracle ProC Options')
+	grp.add_option('--oracle_home', action='store', default=environ.get('PROC_ORACLE'), help='Path to Oracle installation home (has bin/lib)')
+	grp.add_option('--tns_admin', action='store', default=environ.get('TNS_ADMIN'), help='Directory containing server list (TNS_NAMES.ORA)')
+	grp.add_option('--connection', action='store', default='dummy-user/dummy-password@dummy-server', help='Format: user/password@server')
+
+def configure(cnf):
+	env = cnf.env
+	if not env.PROC_ORACLE:
+		env.PROC_ORACLE = cnf.options.oracle_home
+	if not env.PROC_TNS_ADMIN:
+		env.PROC_TNS_ADMIN = cnf.options.tns_admin
+	if not env.PROC_CONNECTION:
+		env.PROC_CONNECTION = cnf.options.connection
+	cnf.find_program('proc', var='PROC', path_list=env.PROC_ORACLE + path.sep + 'bin')
+
+def proc(tsk):
+	env = tsk.env
+	gen = tsk.generator
+	inc_nodes = gen.to_incnodes(Utils.to_list(getattr(gen,'includes',[])) + env['INCLUDES'])
+
+	cmd = (
+		[env.PROC] +
+		['SQLCHECK=SEMANTICS'] +
+		(['SYS_INCLUDE=(' + ','.join(env.PROC_INCLUDES) + ')']
+			if env.PROC_INCLUDES else []) +
+		['INCLUDE=(' + ','.join(
+			[i.bldpath() for i in inc_nodes]
+		) + ')'] +
+		['userid=' + env.PROC_CONNECTION] +
+		['INAME=' + tsk.inputs[0].bldpath()] +
+		['ONAME=' + tsk.outputs[0].bldpath()]
+	)
+	exec_env = {
+		'ORACLE_HOME': env.PROC_ORACLE,
+		'LD_LIBRARY_PATH': env.PROC_ORACLE + path.sep + 'lib',
+	}
+	if env.PROC_TNS_ADMIN:
+		exec_env['TNS_ADMIN'] = env.PROC_TNS_ADMIN
+	return tsk.exec_command(cmd, env=exec_env)
+
+TaskGen.declare_chain(
+	name = 'proc',
+	rule = proc,
+	ext_in = '.pc',
+	ext_out = '.c',
+)
+
diff --git a/waflib/extras/protoc.py b/waflib/extras/protoc.py
new file mode 100644
index 00000000..f3cb4d86
--- /dev/null
+++ b/waflib/extras/protoc.py
@@ -0,0 +1,243 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Philipp Bender, 2012
+# Matt Clarkson, 2012
+
+import re, os
+from waflib.Task import Task
+from waflib.TaskGen import extension
+from waflib import Errors, Context
+
+"""
+A simple tool to integrate protocol buffers into your build system.
+
+Example for C++:
+
+    def configure(conf):
+        conf.load('compiler_cxx cxx protoc')
+
+    def build(bld):
+        bld(
+                features = 'cxx cxxprogram'
+                source   = 'main.cpp file1.proto proto/file2.proto',
+                includes = '. proto',
+                target   = 'executable')
+
+Example for Python:
+
+    def configure(conf):
+        conf.load('python protoc')
+
+    def build(bld):
+        bld(
+                features = 'py'
+                source   = 'main.py file1.proto proto/file2.proto',
+                protoc_includes  = 'proto')
+
+Example for both Python and C++ at same time:
+
+    def configure(conf):
+        conf.load('cxx python protoc')
+
+    def build(bld):
+        bld(
+                features = 'cxx py'
+                source   = 'file1.proto proto/file2.proto',
+                protoc_includes  = 'proto')	# or includes
+
+
+Example for Java:
+
+    def options(opt):
+        opt.load('java')
+
+    def configure(conf):
+        conf.load('python java protoc')
+        # Here you have to point to your protobuf-java JAR and have it in classpath
+        conf.env.CLASSPATH_PROTOBUF = ['protobuf-java-2.5.0.jar']
+
+    def build(bld):
+        bld(
+                features = 'javac protoc',
+                name = 'pbjava',
+                srcdir = 'inc/ src',	# directories used by javac
+                source   = ['inc/message_inc.proto', 'inc/message.proto'],
+					# source is used by protoc for .proto files
+                use = 'PROTOBUF',
+                protoc_includes = ['inc']) # for protoc to search dependencies
+
+
+
+
+Notes when using this tool:
+
+- protoc command line parsing is tricky.
+
+  The generated files can be put in subfolders which depend on
+  the order of the include paths.
+
+  Try to be simple when creating task generators
+  containing protoc stuff.
+
+"""
+
+class protoc(Task):
+	run_str = '${PROTOC} ${PROTOC_FL:PROTOC_FLAGS} ${PROTOC_ST:INCPATHS} ${PROTOC_ST:PROTOC_INCPATHS} ${SRC[0].bldpath()}'
+	color   = 'BLUE'
+	ext_out = ['.h', 'pb.cc', '.py', '.java']
+	def scan(self):
+		"""
+		Scan .proto dependencies
+		"""
+		node = self.inputs[0]
+
+		nodes = []
+		names = []
+		seen = []
+		search_nodes = []
+
+		if not node:
+			return (nodes, names)
+
+		if 'cxx' in self.generator.features:
+			search_nodes = self.generator.includes_nodes
+
+		if 'py' in self.generator.features or 'javac' in self.generator.features:
+			for incpath in getattr(self.generator, 'protoc_includes', []):
+				search_nodes.append(self.generator.bld.path.find_node(incpath))
+
+		def parse_node(node):
+			if node in seen:
+				return
+			seen.append(node)
+			code = node.read().splitlines()
+			for line in code:
+				m = re.search(r'^import\s+"(.*)";.*(//)?.*', line)
+				if m:
+					dep = m.groups()[0]
+					for incnode in search_nodes:
+						found = incnode.find_resource(dep)
+						if found:
+							nodes.append(found)
+							parse_node(found)
+						else:
+							names.append(dep)
+
+		parse_node(node)
+		# Add also dependencies path to INCPATHS so protoc will find the included file
+		for deppath in nodes:
+			self.env.append_value('INCPATHS', deppath.parent.bldpath())
+		return (nodes, names)
+
+@extension('.proto')
+def process_protoc(self, node):
+	incdirs = []
+	out_nodes = []
+	protoc_flags = []
+
+	# ensure PROTOC_FLAGS is a list; a copy is used below anyway
+	self.env.PROTOC_FLAGS = self.to_list(self.env.PROTOC_FLAGS)
+
+	if 'cxx' in self.features:
+		cpp_node = node.change_ext('.pb.cc')
+		hpp_node = node.change_ext('.pb.h')
+		self.source.append(cpp_node)
+		out_nodes.append(cpp_node)
+		out_nodes.append(hpp_node)
+		protoc_flags.append('--cpp_out=%s' % node.parent.get_bld().bldpath())
+
+	if 'py' in self.features:
+		py_node = node.change_ext('_pb2.py')
+		self.source.append(py_node)
+		out_nodes.append(py_node)
+		protoc_flags.append('--python_out=%s' % node.parent.get_bld().bldpath())
+
+	if 'javac' in self.features:
+		pkgname, javapkg, javacn, nodename = None, None, None, None
+		messages = []
+
+		# .java file name is done with some rules depending on .proto file content:
+		#   -) package is either derived from option java_package if present
+		#      or from package directive
+		#   -) file name is either derived from option java_outer_classname if present
+		#      or the .proto file is converted to camelcase. If a message
+		#      is named the same then the behaviour depends on protoc version
+		#
+		# See also: https://developers.google.com/protocol-buffers/docs/reference/java-generated#invocation
+
+		code = node.read().splitlines()
+		for line in code:
+			m = re.search(r'^package\s+(.*);', line)
+			if m:
+				pkgname = m.groups()[0]
+			m = re.search(r'^option\s+(\S*)\s*=\s*"(\S*)";', line)
+			if m:
+				optname = m.groups()[0]
+				if optname == 'java_package':
+					javapkg = m.groups()[1]
+				elif optname == 'java_outer_classname':
+					javacn = m.groups()[1]
+			if self.env.PROTOC_MAJOR > '2':
+				m = re.search(r'^message\s+(\w*)\s*{*', line)
+				if m:
+					messages.append(m.groups()[0])
+
+		if javapkg:
+			nodename = javapkg
+		elif pkgname:
+			nodename = pkgname
+		else:
+			raise Errors.WafError('Cannot derive java name from protoc file')
+
+		nodename = nodename.replace('.',os.sep) + os.sep
+		if javacn:
+			nodename += javacn + '.java'
+		else:
+			if self.env.PROTOC_MAJOR > '2' and node.abspath()[node.abspath().rfind(os.sep)+1:node.abspath().rfind('.')].title() in messages:
+				nodename += node.abspath()[node.abspath().rfind(os.sep)+1:node.abspath().rfind('.')].title().replace('_','') + 'OuterClass.java'
+			else:
+				nodename += node.abspath()[node.abspath().rfind(os.sep)+1:node.abspath().rfind('.')].title().replace('_','') + '.java'
+
+		java_node = node.parent.find_or_declare(nodename)
+		out_nodes.append(java_node)
+		protoc_flags.append('--java_out=%s' % node.parent.get_bld().bldpath())
+
+		# Make javac get also pick java code generated in build
+		if not node.parent.get_bld() in self.javac_task.srcdir:
+			self.javac_task.srcdir.append(node.parent.get_bld())
+
+	if not out_nodes:
+		raise Errors.WafError('Feature %r not supported by protoc extra' % self.features)
+
+	tsk = self.create_task('protoc', node, out_nodes)
+	tsk.env.append_value('PROTOC_FLAGS', protoc_flags)
+
+	if 'javac' in self.features:
+		self.javac_task.set_run_after(tsk)
+
+	# Instruct protoc where to search for .proto included files.
+	# For C++ standard include files dirs are used,
+	# but this doesn't apply to Python for example
+	for incpath in getattr(self, 'protoc_includes', []):
+		incdirs.append(self.path.find_node(incpath).bldpath())
+	tsk.env.PROTOC_INCPATHS = incdirs
+
+	# PR2115: protoc generates output of .proto files in nested
+	# directories  by canonicalizing paths. To avoid this we have to pass
+	# as first include the full directory file of the .proto file
+	tsk.env.prepend_value('INCPATHS', node.parent.bldpath())
+
+	use = getattr(self, 'use', '')
+	if not 'PROTOBUF' in use:
+		self.use = self.to_list(use) + ['PROTOBUF']
+
+def configure(conf):
+	conf.check_cfg(package='protobuf', uselib_store='PROTOBUF', args=['--cflags', '--libs'])
+	conf.find_program('protoc', var='PROTOC')
+	conf.start_msg('Checking for protoc version')
+	protocver = conf.cmd_and_log(conf.env.PROTOC + ['--version'], output=Context.BOTH)
+	protocver = ''.join(protocver).strip()[protocver[0].rfind(' ')+1:]
+	conf.end_msg(protocver)
+	conf.env.PROTOC_MAJOR = protocver[:protocver.find('.')]
+	conf.env.PROTOC_ST = '-I%s'
+	conf.env.PROTOC_FL = '%s'
diff --git a/waflib/extras/pyqt5.py b/waflib/extras/pyqt5.py
new file mode 100644
index 00000000..c21dfa72
--- /dev/null
+++ b/waflib/extras/pyqt5.py
@@ -0,0 +1,241 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Federico Pellegrin, 2016-2018 (fedepell) adapted for Python
+
+"""
+This tool helps with finding Python Qt5 tools and libraries,
+and provides translation from QT5 files to Python code.
+
+The following snippet illustrates the tool usage::
+
+	def options(opt):
+		opt.load('py pyqt5')
+
+	def configure(conf):
+		conf.load('py pyqt5')
+
+	def build(bld):
+		bld(
+			features = 'py pyqt5',
+			source   = 'main.py textures.qrc aboutDialog.ui',
+		)
+
+Here, the UI description and resource files will be processed
+to generate code.
+
+Usage
+=====
+
+Load the "pyqt5" tool.
+
+Add into the sources list also the qrc resources files or ui5
+definition files and they will be translated into python code
+with the system tools (PyQt5, pyside2, PyQt4 are searched in this
+order) and then compiled
+"""
+
+try:
+	from xml.sax import make_parser
+	from xml.sax.handler import ContentHandler
+except ImportError:
+	has_xml = False
+	ContentHandler = object
+else:
+	has_xml = True
+
+import os
+from waflib.Tools import python
+from waflib import Task, Options
+from waflib.TaskGen import feature, extension
+from waflib.Configure import conf
+from waflib import Logs
+
+EXT_RCC = ['.qrc']
+"""
+File extension for the resource (.qrc) files
+"""
+
+EXT_UI  = ['.ui']
+"""
+File extension for the user interface (.ui) files
+"""
+
+
+class XMLHandler(ContentHandler):
+	"""
+	Parses ``.qrc`` files
+	"""
+	def __init__(self):
+		self.buf = []
+		self.files = []
+	def startElement(self, name, attrs):
+		if name == 'file':
+			self.buf = []
+	def endElement(self, name):
+		if name == 'file':
+			self.files.append(str(''.join(self.buf)))
+	def characters(self, cars):
+		self.buf.append(cars)
+
+@extension(*EXT_RCC)
+def create_pyrcc_task(self, node):
+	"Creates rcc and py task for ``.qrc`` files"
+	rcnode = node.change_ext('.py')
+	self.create_task('pyrcc', node, rcnode)
+	if getattr(self, 'install_from', None):
+		self.install_from = self.install_from.get_bld()
+	else:
+		self.install_from = self.path.get_bld()
+	self.install_path = getattr(self, 'install_path', '${PYTHONDIR}')
+	self.process_py(rcnode)
+
+@extension(*EXT_UI)
+def create_pyuic_task(self, node):
+	"Create uic tasks and py for user interface ``.ui`` definition files"
+	uinode = node.change_ext('.py')
+	self.create_task('ui5py', node, uinode)
+	if getattr(self, 'install_from', None):
+		self.install_from = self.install_from.get_bld()
+	else:
+		self.install_from = self.path.get_bld()
+	self.install_path = getattr(self, 'install_path', '${PYTHONDIR}')
+	self.process_py(uinode)
+
+@extension('.ts')
+def add_pylang(self, node):
+	"""Adds all the .ts file into ``self.lang``"""
+	self.lang = self.to_list(getattr(self, 'lang', [])) + [node]
+
+@feature('pyqt5')
+def apply_pyqt5(self):
+	"""
+	The additional parameters are:
+
+	:param lang: list of translation files (\*.ts) to process
+	:type lang: list of :py:class:`waflib.Node.Node` or string without the .ts extension
+	:param langname: if given, transform the \*.ts files into a .qrc files to include in the binary file
+	:type langname: :py:class:`waflib.Node.Node` or string without the .qrc extension
+	"""
+	if getattr(self, 'lang', None):
+		qmtasks = []
+		for x in self.to_list(self.lang):
+			if isinstance(x, str):
+				x = self.path.find_resource(x + '.ts')
+			qmtasks.append(self.create_task('ts2qm', x, x.change_ext('.qm')))
+
+
+		if getattr(self, 'langname', None):
+			qmnodes = [k.outputs[0] for k in qmtasks]
+			rcnode = self.langname
+			if isinstance(rcnode, str):
+				rcnode = self.path.find_or_declare(rcnode + '.qrc')
+			t = self.create_task('qm2rcc', qmnodes, rcnode)
+			create_pyrcc_task(self, t.outputs[0])
+
+class pyrcc(Task.Task):
+	"""
+	Processes ``.qrc`` files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_PYRCC} ${SRC} -o ${TGT}'
+	ext_out = ['.py']
+
+	def rcname(self):
+		return os.path.splitext(self.inputs[0].name)[0]
+
+	def scan(self):
+		"""Parse the *.qrc* files"""
+		if not has_xml:
+			Logs.error('No xml.sax support was found, rcc dependencies will be incomplete!')
+			return ([], [])
+
+		parser = make_parser()
+		curHandler = XMLHandler()
+		parser.setContentHandler(curHandler)
+		fi = open(self.inputs[0].abspath(), 'r')
+		try:
+			parser.parse(fi)
+		finally:
+			fi.close()
+
+		nodes = []
+		names = []
+		root = self.inputs[0].parent
+		for x in curHandler.files:
+			nd = root.find_resource(x)
+			if nd:
+				nodes.append(nd)
+			else:
+				names.append(x)
+		return (nodes, names)
+
+
+class ui5py(Task.Task):
+	"""
+	Processes ``.ui`` files for python
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_PYUIC} ${SRC} -o ${TGT}'
+	ext_out = ['.py']
+
+class ts2qm(Task.Task):
+	"""
+	Generates ``.qm`` files from ``.ts`` files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_LRELEASE} ${QT_LRELEASE_FLAGS} ${SRC} -qm ${TGT}'
+
+class qm2rcc(Task.Task):
+	"""
+	Generates ``.qrc`` files from ``.qm`` files
+	"""
+	color = 'BLUE'
+	after = 'ts2qm'
+	def run(self):
+		"""Create a qrc file including the inputs"""
+		txt = '\n'.join(['<file>%s</file>' % k.path_from(self.outputs[0].parent) for k in self.inputs])
+		code = '<!DOCTYPE RCC><RCC version="1.0">\n<qresource>\n%s\n</qresource>\n</RCC>' % txt
+		self.outputs[0].write(code)
+
+def configure(self):
+	self.find_pyqt5_binaries()
+
+	# warn about this during the configuration too
+	if not has_xml:
+		Logs.error('No xml.sax support was found, rcc dependencies will be incomplete!')
+
+@conf
+def find_pyqt5_binaries(self):
+	"""
+	Detects PyQt5 or pyside2 programs such as pyuic5/pyside2-uic, pyrcc5/pyside2-rcc
+	"""
+	env = self.env
+
+	if getattr(Options.options, 'want_pyside2', True):
+		self.find_program(['pyside2-uic'], var='QT_PYUIC')
+		self.find_program(['pyside2-rcc'], var='QT_PYRCC')
+		self.find_program(['pyside2-lupdate'], var='QT_PYLUPDATE')
+	elif getattr(Options.options, 'want_pyqt4', True):
+		self.find_program(['pyuic4'], var='QT_PYUIC')
+		self.find_program(['pyrcc4'], var='QT_PYRCC')
+		self.find_program(['pylupdate4'], var='QT_PYLUPDATE')
+	else:
+		self.find_program(['pyuic5','pyside2-uic','pyuic4'], var='QT_PYUIC')
+		self.find_program(['pyrcc5','pyside2-rcc','pyrcc4'], var='QT_PYRCC')
+		self.find_program(['pylupdate5', 'pyside2-lupdate','pylupdate4'], var='QT_PYLUPDATE')
+
+	if not env.QT_PYUIC:
+		self.fatal('cannot find the uic compiler for python for qt5')
+
+	if not env.QT_PYUIC:
+		self.fatal('cannot find the rcc compiler for python for qt5')
+
+	self.find_program(['lrelease-qt5', 'lrelease'], var='QT_LRELEASE')
+
+def options(opt):
+	"""
+	Command-line options
+	"""
+	pyqt5opt=opt.add_option_group("Python QT5 Options")
+	pyqt5opt.add_option('--pyqt5-pyside2', action='store_true', default=False, dest='want_pyside2', help='use pyside2 bindings as python QT5 bindings (default PyQt5 is searched first, PySide2 after)')
+	pyqt5opt.add_option('--pyqt5-pyqt4', action='store_true', default=False, dest='want_pyqt4', help='use PyQt4 bindings as python QT5 bindings (default PyQt5 is searched first, PySide2 after, PyQt4 last)')
diff --git a/waflib/extras/pytest.py b/waflib/extras/pytest.py
new file mode 100644
index 00000000..7dd5a1a0
--- /dev/null
+++ b/waflib/extras/pytest.py
@@ -0,0 +1,225 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Calle Rosenquist, 2016-2018 (xbreak)
+
+"""
+Provides Python unit test support using :py:class:`waflib.Tools.waf_unit_test.utest`
+task via the **pytest** feature.
+
+To use pytest the following is needed:
+
+1. Load `pytest` and the dependency `waf_unit_test` tools.
+2. Create a task generator with feature `pytest` (not `test`) and customize behaviour with
+   the following attributes:
+
+   - `pytest_source`: Test input files.
+   - `ut_str`: Test runner command, e.g. ``${PYTHON} -B -m unittest discover`` or
+               if nose is used: ``${NOSETESTS} --no-byte-compile ${SRC}``.
+   - `ut_shell`: Determines if ``ut_str`` is executed in a shell. Default: False.
+   - `ut_cwd`: Working directory for test runner. Defaults to directory of
+               first ``pytest_source`` file.
+
+   Additionally the following `pytest` specific attributes are used in dependent taskgens:
+
+   - `pytest_path`: Node or string list of additional Python paths.
+   - `pytest_libpath`: Node or string list of additional library paths.
+
+The `use` dependencies are used for both update calculation and to populate
+the following environment variables for the `pytest` test runner:
+
+1. `PYTHONPATH` (`sys.path`) of any dependent taskgen that has the feature `py`:
+
+   - `install_from` attribute is used to determine where the root of the Python sources
+      are located. If `install_from` is not specified the default is to use the taskgen path
+      as the root.
+
+   - `pytest_path` attribute is used to manually specify additional Python paths.
+
+2. Dynamic linker search path variable (e.g. `LD_LIBRARY_PATH`) of any dependent taskgen with
+   non-static link_task.
+
+   - `pytest_libpath` attribute is used to manually specify additional linker paths.
+
+Note: `pytest` cannot automatically determine the correct `PYTHONPATH` for `pyext` taskgens
+      because the extension might be part of a Python package or used standalone:
+
+      - When used as part of another `py` package, the `PYTHONPATH` is provided by
+      that taskgen so no additional action is required.
+
+      - When used as a standalone module, the user needs to specify the `PYTHONPATH` explicitly
+      via the `pytest_path` attribute on the `pyext` taskgen.
+
+      For details c.f. the pytest playground examples.
+
+
+For example::
+
+    # A standalone Python C extension that demonstrates unit test environment population
+    # of PYTHONPATH and LD_LIBRARY_PATH/PATH/DYLD_LIBRARY_PATH.
+    #
+    # Note: `pytest_path` is provided here because pytest cannot automatically determine
+    # if the extension is part of another Python package or is used standalone.
+    bld(name         = 'foo_ext',
+        features     = 'c cshlib pyext',
+        source       = 'src/foo_ext.c',
+        target       = 'foo_ext',
+        pytest_path  = [ bld.path.get_bld() ])
+
+    # Python package under test that also depend on the Python module `foo_ext`
+    #
+    # Note: `install_from` is added automatically to `PYTHONPATH`.
+    bld(name         = 'foo',
+        features     = 'py',
+        use          = 'foo_ext',
+        source       = bld.path.ant_glob('src/foo/*.py'),
+        install_from = 'src')
+
+    # Unit test example using the built in module unittest and let that discover
+    # any test cases.
+    bld(name          = 'foo_test',
+        features      = 'pytest',
+        use           = 'foo',
+        pytest_source = bld.path.ant_glob('test/*.py'),
+        ut_str        = '${PYTHON} -B -m unittest discover')
+
+"""
+
+import os
+from waflib import Task, TaskGen, Errors, Utils, Logs
+from waflib.Tools import ccroot
+
+def _process_use_rec(self, name):
+	"""
+	Recursively process ``use`` for task generator with name ``name``..
+	Used by pytest_process_use.
+	"""
+	if name in self.pytest_use_not or name in self.pytest_use_seen:
+		return
+	try:
+		tg = self.bld.get_tgen_by_name(name)
+	except Errors.WafError:
+		self.pytest_use_not.add(name)
+		return
+
+	self.pytest_use_seen.append(name)
+	tg.post()
+
+	for n in self.to_list(getattr(tg, 'use', [])):
+		_process_use_rec(self, n)
+
+
+@TaskGen.feature('pytest')
+@TaskGen.after_method('process_source', 'apply_link')
+def pytest_process_use(self):
+	"""
+	Process the ``use`` attribute which contains a list of task generator names and store
+	paths that later is used to populate the unit test runtime environment.
+	"""
+	self.pytest_use_not = set()
+	self.pytest_use_seen = []
+	self.pytest_paths = [] # strings or Nodes
+	self.pytest_libpaths = [] # strings or Nodes
+	self.pytest_dep_nodes = []
+
+	names = self.to_list(getattr(self, 'use', []))
+	for name in names:
+		_process_use_rec(self, name)
+	
+	def extend_unique(lst, varlst):
+		ext = []
+		for x in varlst:
+			if x not in lst:
+				ext.append(x)
+		lst.extend(ext)
+
+	# Collect type specific info needed to construct a valid runtime environment
+	# for the test.
+	for name in self.pytest_use_seen:
+		tg = self.bld.get_tgen_by_name(name)
+
+		extend_unique(self.pytest_paths, Utils.to_list(getattr(tg, 'pytest_path', [])))
+		extend_unique(self.pytest_libpaths, Utils.to_list(getattr(tg, 'pytest_libpath', [])))
+
+		if 'py' in tg.features:
+			# Python dependencies are added to PYTHONPATH
+			pypath = getattr(tg, 'install_from', tg.path)
+
+			if 'buildcopy' in tg.features:
+				# Since buildcopy is used we assume that PYTHONPATH in build should be used,
+				# not source
+				extend_unique(self.pytest_paths, [pypath.get_bld().abspath()])
+
+				# Add buildcopy output nodes to dependencies
+				extend_unique(self.pytest_dep_nodes, [o for task in getattr(tg, 'tasks', []) \
+														for o in getattr(task, 'outputs', [])])
+			else:
+				# If buildcopy is not used, depend on sources instead
+				extend_unique(self.pytest_dep_nodes, tg.source)
+				extend_unique(self.pytest_paths, [pypath.abspath()])
+
+		if getattr(tg, 'link_task', None):
+			# For tasks with a link_task (C, C++, D et.c.) include their library paths:
+			if not isinstance(tg.link_task, ccroot.stlink_task):
+				extend_unique(self.pytest_dep_nodes, tg.link_task.outputs)
+				extend_unique(self.pytest_libpaths, tg.link_task.env.LIBPATH)
+
+				if 'pyext' in tg.features:
+					# If the taskgen is extending Python we also want to add the interpreter libpath.
+					extend_unique(self.pytest_libpaths, tg.link_task.env.LIBPATH_PYEXT)
+				else:
+					# Only add to libpath if the link task is not a Python extension
+					extend_unique(self.pytest_libpaths, [tg.link_task.outputs[0].parent.abspath()])
+
+
+@TaskGen.feature('pytest')
+@TaskGen.after_method('pytest_process_use')
+def make_pytest(self):
+	"""
+	Creates a ``utest`` task with a populated environment for Python if not specified in ``ut_env``:
+
+	- Paths in `pytest_paths` attribute are used to populate PYTHONPATH
+	- Paths in `pytest_libpaths` attribute are used to populate the system library path (e.g. LD_LIBRARY_PATH)
+	"""
+	nodes = self.to_nodes(self.pytest_source)
+	tsk = self.create_task('utest', nodes)
+	
+	tsk.dep_nodes.extend(self.pytest_dep_nodes)
+	if getattr(self, 'ut_str', None):
+		self.ut_run, lst = Task.compile_fun(self.ut_str, shell=getattr(self, 'ut_shell', False))
+		tsk.vars = lst + tsk.vars
+
+	if getattr(self, 'ut_cwd', None):
+		if isinstance(self.ut_cwd, str):
+			# we want a Node instance
+			if os.path.isabs(self.ut_cwd):
+				self.ut_cwd = self.bld.root.make_node(self.ut_cwd)
+			else:
+				self.ut_cwd = self.path.make_node(self.ut_cwd)
+	else:
+		if tsk.inputs:
+			self.ut_cwd = tsk.inputs[0].parent
+		else:
+			raise Errors.WafError("no valid input files for pytest task, check pytest_source value")
+
+	if not self.ut_cwd.exists():
+		self.ut_cwd.mkdir()
+
+	if not hasattr(self, 'ut_env'):
+		self.ut_env = dict(os.environ)
+		def add_paths(var, lst):
+			# Add list of paths to a variable, lst can contain strings or nodes
+			lst = [ str(n) for n in lst ]
+			Logs.debug("ut: %s: Adding paths %s=%s", self, var, lst)
+			self.ut_env[var] = os.pathsep.join(lst) + os.pathsep + self.ut_env.get(var, '')
+
+		# Prepend dependency paths to PYTHONPATH and LD_LIBRARY_PATH
+		add_paths('PYTHONPATH', self.pytest_paths)
+
+		if Utils.is_win32:
+			add_paths('PATH', self.pytest_libpaths)
+		elif Utils.unversioned_sys_platform() == 'darwin':
+			add_paths('DYLD_LIBRARY_PATH', self.pytest_libpaths)
+			add_paths('LD_LIBRARY_PATH', self.pytest_libpaths)
+		else:
+			add_paths('LD_LIBRARY_PATH', self.pytest_libpaths)
+
diff --git a/waflib/extras/qnxnto.py b/waflib/extras/qnxnto.py
new file mode 100644
index 00000000..1158124d
--- /dev/null
+++ b/waflib/extras/qnxnto.py
@@ -0,0 +1,72 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Jérôme Carretero 2011 (zougloub)
+# QNX neutrino compatibility functions
+
+import sys, os
+from waflib import Utils
+
+class Popen(object):
+	"""
+	Popen cannot work on QNX from a threaded program:
+	Forking in threads is not implemented in neutrino.
+
+	Python's os.popen / spawn / fork won't work when running in threads (they will if in the main program thread)
+
+	In waf, this happens mostly in build.
+	And the use cases can be replaced by os.system() calls.
+	"""
+	__slots__ = ["prog", "kw", "popen", "verbose"]
+	verbose = 0
+	def __init__(self, prog, **kw):
+		try:
+			self.prog = prog
+			self.kw = kw
+			self.popen = None
+			if Popen.verbose:
+				sys.stdout.write("Popen created: %r, kw=%r..." % (prog, kw))
+
+			do_delegate = kw.get('stdout') == -1 and kw.get('stderr') == -1
+			if do_delegate:
+				if Popen.verbose:
+					print("Delegating to real Popen")
+				self.popen = self.real_Popen(prog, **kw)
+			else:
+				if Popen.verbose:
+					print("Emulating")
+		except Exception as e:
+			if Popen.verbose:
+				print("Exception: %s" % e)
+			raise
+
+	def __getattr__(self, name):
+		if Popen.verbose:
+			sys.stdout.write("Getattr: %s..." % name)
+		if name in Popen.__slots__:
+			return object.__getattribute__(self, name)
+		else:
+			if self.popen is not None:
+				if Popen.verbose:
+					print("from Popen")
+				return getattr(self.popen, name)
+			else:
+				if name == "wait":
+					return self.emu_wait
+				else:
+					raise Exception("subprocess emulation: not implemented: %s" % name)
+
+	def emu_wait(self):
+		if Popen.verbose:
+			print("emulated wait (%r kw=%r)" % (self.prog, self.kw))
+		if isinstance(self.prog, str):
+			cmd = self.prog
+		else:
+			cmd = " ".join(self.prog)
+		if 'cwd' in self.kw:
+			cmd = 'cd "%s" && %s' % (self.kw['cwd'], cmd)
+		return os.system(cmd)
+
+if sys.platform == "qnx6":
+	Popen.real_Popen = Utils.subprocess.Popen
+	Utils.subprocess.Popen = Popen
+
diff --git a/waflib/extras/qt4.py b/waflib/extras/qt4.py
new file mode 100644
index 00000000..90cae7e0
--- /dev/null
+++ b/waflib/extras/qt4.py
@@ -0,0 +1,695 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2006-2010 (ita)
+
+"""
+
+Tool Description
+================
+
+This tool helps with finding Qt4 tools and libraries,
+and also provides syntactic sugar for using Qt4 tools.
+
+The following snippet illustrates the tool usage::
+
+	def options(opt):
+		opt.load('compiler_cxx qt4')
+
+	def configure(conf):
+		conf.load('compiler_cxx qt4')
+
+	def build(bld):
+		bld(
+			features = 'qt4 cxx cxxprogram',
+			uselib   = 'QTCORE QTGUI QTOPENGL QTSVG',
+			source   = 'main.cpp textures.qrc aboutDialog.ui',
+			target   = 'window',
+		)
+
+Here, the UI description and resource files will be processed
+to generate code.
+
+Usage
+=====
+
+Load the "qt4" tool.
+
+You also need to edit your sources accordingly:
+
+- the normal way of doing things is to have your C++ files
+  include the .moc file.
+  This is regarded as the best practice (and provides much faster
+  compilations).
+  It also implies that the include paths have beenset properly.
+
+- to have the include paths added automatically, use the following::
+
+     from waflib.TaskGen import feature, before_method, after_method
+     @feature('cxx')
+     @after_method('process_source')
+     @before_method('apply_incpaths')
+     def add_includes_paths(self):
+        incs = set(self.to_list(getattr(self, 'includes', '')))
+        for x in self.compiled_tasks:
+            incs.add(x.inputs[0].parent.path_from(self.path))
+        self.includes = sorted(incs)
+
+Note: another tool provides Qt processing that does not require
+.moc includes, see 'playground/slow_qt/'.
+
+A few options (--qt{dir,bin,...}) and environment variables
+(QT4_{ROOT,DIR,MOC,UIC,XCOMPILE}) allow finer tuning of the tool,
+tool path selection, etc; please read the source for more info.
+
+"""
+
+try:
+	from xml.sax import make_parser
+	from xml.sax.handler import ContentHandler
+except ImportError:
+	has_xml = False
+	ContentHandler = object
+else:
+	has_xml = True
+
+import os, sys
+from waflib.Tools import cxx
+from waflib import Task, Utils, Options, Errors, Context
+from waflib.TaskGen import feature, after_method, extension
+from waflib.Configure import conf
+from waflib import Logs
+
+MOC_H = ['.h', '.hpp', '.hxx', '.hh']
+"""
+File extensions associated to the .moc files
+"""
+
+EXT_RCC = ['.qrc']
+"""
+File extension for the resource (.qrc) files
+"""
+
+EXT_UI  = ['.ui']
+"""
+File extension for the user interface (.ui) files
+"""
+
+EXT_QT4 = ['.cpp', '.cc', '.cxx', '.C']
+"""
+File extensions of C++ files that may require a .moc processing
+"""
+
+QT4_LIBS = "QtCore QtGui QtUiTools QtNetwork QtOpenGL QtSql QtSvg QtTest QtXml QtXmlPatterns QtWebKit Qt3Support QtHelp QtScript QtDeclarative QtDesigner"
+
+class qxx(Task.classes['cxx']):
+	"""
+	Each C++ file can have zero or several .moc files to create.
+	They are known only when the files are scanned (preprocessor)
+	To avoid scanning the c++ files each time (parsing C/C++), the results
+	are retrieved from the task cache (bld.node_deps/bld.raw_deps).
+	The moc tasks are also created *dynamically* during the build.
+	"""
+
+	def __init__(self, *k, **kw):
+		Task.Task.__init__(self, *k, **kw)
+		self.moc_done = 0
+
+	def runnable_status(self):
+		"""
+		Compute the task signature to make sure the scanner was executed. Create the
+		moc tasks by using :py:meth:`waflib.Tools.qt4.qxx.add_moc_tasks` (if necessary),
+		then postpone the task execution (there is no need to recompute the task signature).
+		"""
+		if self.moc_done:
+			return Task.Task.runnable_status(self)
+		else:
+			for t in self.run_after:
+				if not t.hasrun:
+					return Task.ASK_LATER
+			self.add_moc_tasks()
+			return Task.Task.runnable_status(self)
+
+	def create_moc_task(self, h_node, m_node):
+		"""
+		If several libraries use the same classes, it is possible that moc will run several times (Issue 1318)
+		It is not possible to change the file names, but we can assume that the moc transformation will be identical,
+		and the moc tasks can be shared in a global cache.
+
+		The defines passed to moc will then depend on task generator order. If this is not acceptable, then
+		use the tool slow_qt4 instead (and enjoy the slow builds... :-( )
+		"""
+		try:
+			moc_cache = self.generator.bld.moc_cache
+		except AttributeError:
+			moc_cache = self.generator.bld.moc_cache = {}
+
+		try:
+			return moc_cache[h_node]
+		except KeyError:
+			tsk = moc_cache[h_node] = Task.classes['moc'](env=self.env, generator=self.generator)
+			tsk.set_inputs(h_node)
+			tsk.set_outputs(m_node)
+
+			if self.generator:
+				self.generator.tasks.append(tsk)
+
+			# direct injection in the build phase (safe because called from the main thread)
+			gen = self.generator.bld.producer
+			gen.outstanding.append(tsk)
+			gen.total += 1
+
+			return tsk
+
+	def moc_h_ext(self):
+		ext = []
+		try:
+			ext = Options.options.qt_header_ext.split()
+		except AttributeError:
+			pass
+		if not ext:
+			ext = MOC_H
+		return ext
+
+	def add_moc_tasks(self):
+		"""
+		Create the moc tasks by looking in ``bld.raw_deps[self.uid()]``
+		"""
+		node = self.inputs[0]
+		bld = self.generator.bld
+
+		try:
+			# compute the signature once to know if there is a moc file to create
+			self.signature()
+		except KeyError:
+			# the moc file may be referenced somewhere else
+			pass
+		else:
+			# remove the signature, it must be recomputed with the moc task
+			delattr(self, 'cache_sig')
+
+		include_nodes = [node.parent] + self.generator.includes_nodes
+
+		moctasks = []
+		mocfiles = set()
+		for d in bld.raw_deps.get(self.uid(), []):
+			if not d.endswith('.moc'):
+				continue
+
+			# process that base.moc only once
+			if d in mocfiles:
+				continue
+			mocfiles.add(d)
+
+			# find the source associated with the moc file
+			h_node = None
+
+			base2 = d[:-4]
+			for x in include_nodes:
+				for e in self.moc_h_ext():
+					h_node = x.find_node(base2 + e)
+					if h_node:
+						break
+				if h_node:
+					m_node = h_node.change_ext('.moc')
+					break
+			else:
+				# foo.cpp -> foo.cpp.moc
+				for k in EXT_QT4:
+					if base2.endswith(k):
+						for x in include_nodes:
+							h_node = x.find_node(base2)
+							if h_node:
+								break
+						if h_node:
+							m_node = h_node.change_ext(k + '.moc')
+							break
+
+			if not h_node:
+				raise Errors.WafError('No source found for %r which is a moc file' % d)
+
+			# create the moc task
+			task = self.create_moc_task(h_node, m_node)
+			moctasks.append(task)
+
+		# simple scheduler dependency: run the moc task before others
+		self.run_after.update(set(moctasks))
+		self.moc_done = 1
+
+class trans_update(Task.Task):
+	"""Update a .ts files from a list of C++ files"""
+	run_str = '${QT_LUPDATE} ${SRC} -ts ${TGT}'
+	color   = 'BLUE'
+
+class XMLHandler(ContentHandler):
+	"""
+	Parser for *.qrc* files
+	"""
+	def __init__(self):
+		self.buf = []
+		self.files = []
+	def startElement(self, name, attrs):
+		if name == 'file':
+			self.buf = []
+	def endElement(self, name):
+		if name == 'file':
+			self.files.append(str(''.join(self.buf)))
+	def characters(self, cars):
+		self.buf.append(cars)
+
+@extension(*EXT_RCC)
+def create_rcc_task(self, node):
+	"Create rcc and cxx tasks for *.qrc* files"
+	rcnode = node.change_ext('_rc.cpp')
+	self.create_task('rcc', node, rcnode)
+	cpptask = self.create_task('cxx', rcnode, rcnode.change_ext('.o'))
+	try:
+		self.compiled_tasks.append(cpptask)
+	except AttributeError:
+		self.compiled_tasks = [cpptask]
+	return cpptask
+
+@extension(*EXT_UI)
+def create_uic_task(self, node):
+	"hook for uic tasks"
+	uictask = self.create_task('ui4', node)
+	uictask.outputs = [self.path.find_or_declare(self.env['ui_PATTERN'] % node.name[:-3])]
+
+@extension('.ts')
+def add_lang(self, node):
+	"""add all the .ts file into self.lang"""
+	self.lang = self.to_list(getattr(self, 'lang', [])) + [node]
+
+@feature('qt4')
+@after_method('apply_link')
+def apply_qt4(self):
+	"""
+	Add MOC_FLAGS which may be necessary for moc::
+
+		def build(bld):
+			bld.program(features='qt4', source='main.cpp', target='app', use='QTCORE')
+
+	The additional parameters are:
+
+	:param lang: list of translation files (\*.ts) to process
+	:type lang: list of :py:class:`waflib.Node.Node` or string without the .ts extension
+	:param update: whether to process the C++ files to update the \*.ts files (use **waf --translate**)
+	:type update: bool
+	:param langname: if given, transform the \*.ts files into a .qrc files to include in the binary file
+	:type langname: :py:class:`waflib.Node.Node` or string without the .qrc extension
+	"""
+	if getattr(self, 'lang', None):
+		qmtasks = []
+		for x in self.to_list(self.lang):
+			if isinstance(x, str):
+				x = self.path.find_resource(x + '.ts')
+			qmtasks.append(self.create_task('ts2qm', x, x.change_ext('.qm')))
+
+		if getattr(self, 'update', None) and Options.options.trans_qt4:
+			cxxnodes = [a.inputs[0] for a in self.compiled_tasks] + [
+				a.inputs[0] for a in self.tasks if getattr(a, 'inputs', None) and a.inputs[0].name.endswith('.ui')]
+			for x in qmtasks:
+				self.create_task('trans_update', cxxnodes, x.inputs)
+
+		if getattr(self, 'langname', None):
+			qmnodes = [x.outputs[0] for x in qmtasks]
+			rcnode = self.langname
+			if isinstance(rcnode, str):
+				rcnode = self.path.find_or_declare(rcnode + '.qrc')
+			t = self.create_task('qm2rcc', qmnodes, rcnode)
+			k = create_rcc_task(self, t.outputs[0])
+			self.link_task.inputs.append(k.outputs[0])
+
+	lst = []
+	for flag in self.to_list(self.env['CXXFLAGS']):
+		if len(flag) < 2:
+			continue
+		f = flag[0:2]
+		if f in ('-D', '-I', '/D', '/I'):
+			if (f[0] == '/'):
+				lst.append('-' + flag[1:])
+			else:
+				lst.append(flag)
+	self.env.append_value('MOC_FLAGS', lst)
+
+@extension(*EXT_QT4)
+def cxx_hook(self, node):
+	"""
+	Re-map C++ file extensions to the :py:class:`waflib.Tools.qt4.qxx` task.
+	"""
+	return self.create_compiled_task('qxx', node)
+
+class rcc(Task.Task):
+	"""
+	Process *.qrc* files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_RCC} -name ${tsk.rcname()} ${SRC[0].abspath()} ${RCC_ST} -o ${TGT}'
+	ext_out = ['.h']
+
+	def rcname(self):
+		return os.path.splitext(self.inputs[0].name)[0]
+
+	def scan(self):
+		"""Parse the *.qrc* files"""
+		if not has_xml:
+			Logs.error('no xml support was found, the rcc dependencies will be incomplete!')
+			return ([], [])
+
+		parser = make_parser()
+		curHandler = XMLHandler()
+		parser.setContentHandler(curHandler)
+		fi = open(self.inputs[0].abspath(), 'r')
+		try:
+			parser.parse(fi)
+		finally:
+			fi.close()
+
+		nodes = []
+		names = []
+		root = self.inputs[0].parent
+		for x in curHandler.files:
+			nd = root.find_resource(x)
+			if nd:
+				nodes.append(nd)
+			else:
+				names.append(x)
+		return (nodes, names)
+
+class moc(Task.Task):
+	"""
+	Create *.moc* files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_MOC} ${MOC_FLAGS} ${MOCCPPPATH_ST:INCPATHS} ${MOCDEFINES_ST:DEFINES} ${SRC} ${MOC_ST} ${TGT}'
+	def keyword(self):
+		return "Creating"
+	def __str__(self):
+		return self.outputs[0].path_from(self.generator.bld.launch_node())
+
+class ui4(Task.Task):
+	"""
+	Process *.ui* files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_UIC} ${SRC} -o ${TGT}'
+	ext_out = ['.h']
+
+class ts2qm(Task.Task):
+	"""
+	Create *.qm* files from *.ts* files
+	"""
+	color   = 'BLUE'
+	run_str = '${QT_LRELEASE} ${QT_LRELEASE_FLAGS} ${SRC} -qm ${TGT}'
+
+class qm2rcc(Task.Task):
+	"""
+	Transform *.qm* files into *.rc* files
+	"""
+	color = 'BLUE'
+	after = 'ts2qm'
+
+	def run(self):
+		"""Create a qrc file including the inputs"""
+		txt = '\n'.join(['<file>%s</file>' % k.path_from(self.outputs[0].parent) for k in self.inputs])
+		code = '<!DOCTYPE RCC><RCC version="1.0">\n<qresource>\n%s\n</qresource>\n</RCC>' % txt
+		self.outputs[0].write(code)
+
+def configure(self):
+	"""
+	Besides the configuration options, the environment variable QT4_ROOT may be used
+	to give the location of the qt4 libraries (absolute path).
+
+	The detection will use the program *pkg-config* through :py:func:`waflib.Tools.config_c.check_cfg`
+	"""
+	self.find_qt4_binaries()
+	self.set_qt4_libs_to_check()
+	self.set_qt4_defines()
+	self.find_qt4_libraries()
+	self.add_qt4_rpath()
+	self.simplify_qt4_libs()
+
+@conf
+def find_qt4_binaries(self):
+	env = self.env
+	opt = Options.options
+
+	qtdir = getattr(opt, 'qtdir', '')
+	qtbin = getattr(opt, 'qtbin', '')
+
+	paths = []
+
+	if qtdir:
+		qtbin = os.path.join(qtdir, 'bin')
+
+	# the qt directory has been given from QT4_ROOT - deduce the qt binary path
+	if not qtdir:
+		qtdir = os.environ.get('QT4_ROOT', '')
+		qtbin = os.environ.get('QT4_BIN') or os.path.join(qtdir, 'bin')
+
+	if qtbin:
+		paths = [qtbin]
+
+	# no qtdir, look in the path and in /usr/local/Trolltech
+	if not qtdir:
+		paths = os.environ.get('PATH', '').split(os.pathsep)
+		paths.append('/usr/share/qt4/bin/')
+		try:
+			lst = Utils.listdir('/usr/local/Trolltech/')
+		except OSError:
+			pass
+		else:
+			if lst:
+				lst.sort()
+				lst.reverse()
+
+				# keep the highest version
+				qtdir = '/usr/local/Trolltech/%s/' % lst[0]
+				qtbin = os.path.join(qtdir, 'bin')
+				paths.append(qtbin)
+
+	# at the end, try to find qmake in the paths given
+	# keep the one with the highest version
+	cand = None
+	prev_ver = ['4', '0', '0']
+	for qmk in ('qmake-qt4', 'qmake4', 'qmake'):
+		try:
+			qmake = self.find_program(qmk, path_list=paths)
+		except self.errors.ConfigurationError:
+			pass
+		else:
+			try:
+				version = self.cmd_and_log(qmake + ['-query', 'QT_VERSION']).strip()
+			except self.errors.WafError:
+				pass
+			else:
+				if version:
+					new_ver = version.split('.')
+					if new_ver > prev_ver:
+						cand = qmake
+						prev_ver = new_ver
+	if cand:
+		self.env.QMAKE = cand
+	else:
+		self.fatal('Could not find qmake for qt4')
+
+	qtbin = self.cmd_and_log(self.env.QMAKE + ['-query', 'QT_INSTALL_BINS']).strip() + os.sep
+
+	def find_bin(lst, var):
+		if var in env:
+			return
+		for f in lst:
+			try:
+				ret = self.find_program(f, path_list=paths)
+			except self.errors.ConfigurationError:
+				pass
+			else:
+				env[var]=ret
+				break
+
+	find_bin(['uic-qt3', 'uic3'], 'QT_UIC3')
+	find_bin(['uic-qt4', 'uic'], 'QT_UIC')
+	if not env.QT_UIC:
+		self.fatal('cannot find the uic compiler for qt4')
+
+	self.start_msg('Checking for uic version')
+	uicver = self.cmd_and_log(env.QT_UIC + ["-version"], output=Context.BOTH)
+	uicver = ''.join(uicver).strip()
+	uicver = uicver.replace('Qt User Interface Compiler ','').replace('User Interface Compiler for Qt', '')
+	self.end_msg(uicver)
+	if uicver.find(' 3.') != -1:
+		self.fatal('this uic compiler is for qt3, add uic for qt4 to your path')
+
+	find_bin(['moc-qt4', 'moc'], 'QT_MOC')
+	find_bin(['rcc-qt4', 'rcc'], 'QT_RCC')
+	find_bin(['lrelease-qt4', 'lrelease'], 'QT_LRELEASE')
+	find_bin(['lupdate-qt4', 'lupdate'], 'QT_LUPDATE')
+
+	env['UIC3_ST']= '%s -o %s'
+	env['UIC_ST'] = '%s -o %s'
+	env['MOC_ST'] = '-o'
+	env['ui_PATTERN'] = 'ui_%s.h'
+	env['QT_LRELEASE_FLAGS'] = ['-silent']
+	env.MOCCPPPATH_ST = '-I%s'
+	env.MOCDEFINES_ST = '-D%s'
+
+@conf
+def find_qt4_libraries(self):
+	qtlibs = getattr(Options.options, 'qtlibs', None) or os.environ.get("QT4_LIBDIR")
+	if not qtlibs:
+		try:
+			qtlibs = self.cmd_and_log(self.env.QMAKE + ['-query', 'QT_INSTALL_LIBS']).strip()
+		except Errors.WafError:
+			qtdir = self.cmd_and_log(self.env.QMAKE + ['-query', 'QT_INSTALL_PREFIX']).strip() + os.sep
+			qtlibs = os.path.join(qtdir, 'lib')
+	self.msg('Found the Qt4 libraries in', qtlibs)
+
+	qtincludes =  os.environ.get("QT4_INCLUDES") or self.cmd_and_log(self.env.QMAKE + ['-query', 'QT_INSTALL_HEADERS']).strip()
+	env = self.env
+	if not 'PKG_CONFIG_PATH' in os.environ:
+		os.environ['PKG_CONFIG_PATH'] = '%s:%s/pkgconfig:/usr/lib/qt4/lib/pkgconfig:/opt/qt4/lib/pkgconfig:/usr/lib/qt4/lib:/opt/qt4/lib' % (qtlibs, qtlibs)
+
+	try:
+		if os.environ.get("QT4_XCOMPILE"):
+			raise self.errors.ConfigurationError()
+		self.check_cfg(atleast_pkgconfig_version='0.1')
+	except self.errors.ConfigurationError:
+		for i in self.qt4_vars:
+			uselib = i.upper()
+			if Utils.unversioned_sys_platform() == "darwin":
+				# Since at least qt 4.7.3 each library locates in separate directory
+				frameworkName = i + ".framework"
+				qtDynamicLib = os.path.join(qtlibs, frameworkName, i)
+				if os.path.exists(qtDynamicLib):
+					env.append_unique('FRAMEWORK_' + uselib, i)
+					self.msg('Checking for %s' % i, qtDynamicLib, 'GREEN')
+				else:
+					self.msg('Checking for %s' % i, False, 'YELLOW')
+				env.append_unique('INCLUDES_' + uselib, os.path.join(qtlibs, frameworkName, 'Headers'))
+			elif env.DEST_OS != "win32":
+				qtDynamicLib = os.path.join(qtlibs, "lib" + i + ".so")
+				qtStaticLib = os.path.join(qtlibs, "lib" + i + ".a")
+				if os.path.exists(qtDynamicLib):
+					env.append_unique('LIB_' + uselib, i)
+					self.msg('Checking for %s' % i, qtDynamicLib, 'GREEN')
+				elif os.path.exists(qtStaticLib):
+					env.append_unique('LIB_' + uselib, i)
+					self.msg('Checking for %s' % i, qtStaticLib, 'GREEN')
+				else:
+					self.msg('Checking for %s' % i, False, 'YELLOW')
+
+				env.append_unique('LIBPATH_' + uselib, qtlibs)
+				env.append_unique('INCLUDES_' + uselib, qtincludes)
+				env.append_unique('INCLUDES_' + uselib, os.path.join(qtincludes, i))
+			else:
+				# Release library names are like QtCore4
+				for k in ("lib%s.a", "lib%s4.a", "%s.lib", "%s4.lib"):
+					lib = os.path.join(qtlibs, k % i)
+					if os.path.exists(lib):
+						env.append_unique('LIB_' + uselib, i + k[k.find("%s") + 2 : k.find('.')])
+						self.msg('Checking for %s' % i, lib, 'GREEN')
+						break
+				else:
+					self.msg('Checking for %s' % i, False, 'YELLOW')
+
+				env.append_unique('LIBPATH_' + uselib, qtlibs)
+				env.append_unique('INCLUDES_' + uselib, qtincludes)
+				env.append_unique('INCLUDES_' + uselib, os.path.join(qtincludes, i))
+
+				# Debug library names are like QtCore4d
+				uselib = i.upper() + "_debug"
+				for k in ("lib%sd.a", "lib%sd4.a", "%sd.lib", "%sd4.lib"):
+					lib = os.path.join(qtlibs, k % i)
+					if os.path.exists(lib):
+						env.append_unique('LIB_' + uselib, i + k[k.find("%s") + 2 : k.find('.')])
+						self.msg('Checking for %s' % i, lib, 'GREEN')
+						break
+				else:
+					self.msg('Checking for %s' % i, False, 'YELLOW')
+
+				env.append_unique('LIBPATH_' + uselib, qtlibs)
+				env.append_unique('INCLUDES_' + uselib, qtincludes)
+				env.append_unique('INCLUDES_' + uselib, os.path.join(qtincludes, i))
+	else:
+		for i in self.qt4_vars_debug + self.qt4_vars:
+			self.check_cfg(package=i, args='--cflags --libs', mandatory=False)
+
+@conf
+def simplify_qt4_libs(self):
+	# the libpaths make really long command-lines
+	# remove the qtcore ones from qtgui, etc
+	env = self.env
+	def process_lib(vars_, coreval):
+		for d in vars_:
+			var = d.upper()
+			if var == 'QTCORE':
+				continue
+
+			value = env['LIBPATH_'+var]
+			if value:
+				core = env[coreval]
+				accu = []
+				for lib in value:
+					if lib in core:
+						continue
+					accu.append(lib)
+				env['LIBPATH_'+var] = accu
+
+	process_lib(self.qt4_vars,       'LIBPATH_QTCORE')
+	process_lib(self.qt4_vars_debug, 'LIBPATH_QTCORE_DEBUG')
+
+@conf
+def add_qt4_rpath(self):
+	# rpath if wanted
+	env = self.env
+	if getattr(Options.options, 'want_rpath', False):
+		def process_rpath(vars_, coreval):
+			for d in vars_:
+				var = d.upper()
+				value = env['LIBPATH_'+var]
+				if value:
+					core = env[coreval]
+					accu = []
+					for lib in value:
+						if var != 'QTCORE':
+							if lib in core:
+								continue
+						accu.append('-Wl,--rpath='+lib)
+					env['RPATH_'+var] = accu
+		process_rpath(self.qt4_vars,       'LIBPATH_QTCORE')
+		process_rpath(self.qt4_vars_debug, 'LIBPATH_QTCORE_DEBUG')
+
+@conf
+def set_qt4_libs_to_check(self):
+	if not hasattr(self, 'qt4_vars'):
+		self.qt4_vars = QT4_LIBS
+	self.qt4_vars = Utils.to_list(self.qt4_vars)
+	if not hasattr(self, 'qt4_vars_debug'):
+		self.qt4_vars_debug = [a + '_debug' for a in self.qt4_vars]
+	self.qt4_vars_debug = Utils.to_list(self.qt4_vars_debug)
+
+@conf
+def set_qt4_defines(self):
+	if sys.platform != 'win32':
+		return
+	for x in self.qt4_vars:
+		y = x[2:].upper()
+		self.env.append_unique('DEFINES_%s' % x.upper(), 'QT_%s_LIB' % y)
+		self.env.append_unique('DEFINES_%s_DEBUG' % x.upper(), 'QT_%s_LIB' % y)
+
+def options(opt):
+	"""
+	Command-line options
+	"""
+	opt.add_option('--want-rpath', action='store_true', default=False, dest='want_rpath', help='enable the rpath for qt libraries')
+
+	opt.add_option('--header-ext',
+		type='string',
+		default='',
+		help='header extension for moc files',
+		dest='qt_header_ext')
+
+	for i in 'qtdir qtbin qtlibs'.split():
+		opt.add_option('--'+i, type='string', default='', dest=i)
+
+	opt.add_option('--translate', action="store_true", help="collect translation strings", dest="trans_qt4", default=False)
+
diff --git a/waflib/extras/relocation.py b/waflib/extras/relocation.py
new file mode 100644
index 00000000..7e821f41
--- /dev/null
+++ b/waflib/extras/relocation.py
@@ -0,0 +1,85 @@
+#! /usr/bin/env python
+# encoding: utf-8
+
+"""
+Waf 1.6
+
+Try to detect if the project directory was relocated, and if it was,
+change the node representing the project directory. Just call:
+
+ waf configure build
+
+Note that if the project directory name changes, the signatures for the tasks using
+files in that directory will change, causing a partial build.
+"""
+
+import os
+from waflib import Build, ConfigSet, Task, Utils, Errors
+from waflib.TaskGen import feature, after_method
+
+EXTRA_LOCK = '.old_srcdir'
+
+old1 = Build.BuildContext.store
+def store(self):
+	old1(self)
+	db = os.path.join(self.variant_dir, EXTRA_LOCK)
+	env = ConfigSet.ConfigSet()
+	env.SRCDIR = self.srcnode.abspath()
+	env.store(db)
+Build.BuildContext.store = store
+
+old2 = Build.BuildContext.init_dirs
+def init_dirs(self):
+
+	if not (os.path.isabs(self.top_dir) and os.path.isabs(self.out_dir)):
+		raise Errors.WafError('The project was not configured: run "waf configure" first!')
+
+	srcdir = None
+	db = os.path.join(self.variant_dir, EXTRA_LOCK)
+	env = ConfigSet.ConfigSet()
+	try:
+		env.load(db)
+		srcdir = env.SRCDIR
+	except:
+		pass
+
+	if srcdir:
+		d = self.root.find_node(srcdir)
+		if d and srcdir != self.top_dir and getattr(d, 'children', ''):
+			srcnode = self.root.make_node(self.top_dir)
+			print("relocating the source directory %r -> %r" % (srcdir, self.top_dir))
+			srcnode.children = {}
+
+			for (k, v) in d.children.items():
+				srcnode.children[k] = v
+				v.parent = srcnode
+			d.children = {}
+
+	old2(self)
+
+Build.BuildContext.init_dirs = init_dirs
+
+
+def uid(self):
+	try:
+		return self.uid_
+	except AttributeError:
+		# this is not a real hot zone, but we want to avoid surprises here
+		m = Utils.md5()
+		up = m.update
+		up(self.__class__.__name__.encode())
+		for x in self.inputs + self.outputs:
+			up(x.path_from(x.ctx.srcnode).encode())
+		self.uid_ = m.digest()
+		return self.uid_
+Task.Task.uid = uid
+
+@feature('c', 'cxx', 'd', 'go', 'asm', 'fc', 'includes')
+@after_method('propagate_uselib_vars', 'process_source')
+def apply_incpaths(self):
+	lst = self.to_incnodes(self.to_list(getattr(self, 'includes', [])) + self.env['INCLUDES'])
+	self.includes_nodes = lst
+	bld = self.bld
+	self.env['INCPATHS'] = [x.is_child_of(bld.srcnode) and x.path_from(bld.bldnode) or x.abspath() for x in lst]
+
+
diff --git a/waflib/extras/remote.py b/waflib/extras/remote.py
new file mode 100644
index 00000000..3b038f77
--- /dev/null
+++ b/waflib/extras/remote.py
@@ -0,0 +1,327 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Remote Builds tool using rsync+ssh
+
+__author__ = "Jérôme Carretero <cJ-waf@zougloub.eu>"
+__copyright__ = "Jérôme Carretero, 2013"
+
+"""
+Simple Remote Builds
+********************
+
+This tool is an *experimental* tool (meaning, do not even try to pollute
+the waf bug tracker with bugs in here, contact me directly) providing simple
+remote builds.
+
+It uses rsync and ssh to perform the remote builds.
+It is intended for performing cross-compilation on platforms where
+a cross-compiler is either unavailable (eg. MacOS, QNX) a specific product
+does not exist (eg. Windows builds using Visual Studio) or simply not installed.
+This tool sends the sources and the waf script to the remote host,
+and commands the usual waf execution.
+
+There are alternatives to using this tool, such as setting up shared folders,
+logging on to remote machines, and building on the shared folders.
+Electing one method or another depends on the size of the program.
+
+
+Usage
+=====
+
+1. Set your wscript file so it includes a list of variants,
+   e.g.::
+
+     from waflib import Utils
+     top = '.'
+     out = 'build'
+
+     variants = [
+      'linux_64_debug',
+      'linux_64_release',
+      'linux_32_debug',
+      'linux_32_release',
+      ]
+
+     from waflib.extras import remote
+
+     def options(opt):
+         # normal stuff from here on
+         opt.load('compiler_c')
+
+     def configure(conf):
+         if not conf.variant:
+             return
+         # normal stuff from here on
+         conf.load('compiler_c')
+
+     def build(bld):
+         if not bld.variant:
+             return
+         # normal stuff from here on
+         bld(features='c cprogram', target='app', source='main.c')
+
+
+2. Build the waf file, so it includes this tool, and put it in the current
+   directory
+
+   .. code:: bash
+
+      ./waf-light --tools=remote
+
+3. Set the host names to access the hosts:
+
+   .. code:: bash
+
+      export REMOTE_QNX=user@kiunix
+
+4. Setup the ssh server and ssh keys
+
+   The ssh key should not be protected by a password, or it will prompt for it every time.
+   Create the key on the client:
+
+   .. code:: bash
+
+      ssh-keygen -t rsa -f foo.rsa
+
+   Then copy foo.rsa.pub to the remote machine (user@kiunix:/home/user/.ssh/authorized_keys),
+   and make sure the permissions are correct (chmod go-w ~ ~/.ssh ~/.ssh/authorized_keys)
+
+   A separate key for the build processes can be set in the environment variable WAF_SSH_KEY.
+   The tool will then use 'ssh-keyscan' to avoid prompting for remote hosts, so
+   be warned to use this feature on internal networks only (MITM).
+
+   .. code:: bash
+
+      export WAF_SSH_KEY=~/foo.rsa
+
+5. Perform the build:
+
+   .. code:: bash
+
+      waf configure_all build_all --remote
+
+"""
+
+
+import getpass, os, re, sys
+from collections import OrderedDict
+from waflib import Context, Options, Utils, ConfigSet
+
+from waflib.Build import BuildContext, CleanContext, InstallContext, UninstallContext
+from waflib.Configure import ConfigurationContext
+
+
+is_remote = False
+if '--remote' in sys.argv:
+	is_remote = True
+	sys.argv.remove('--remote')
+
+class init(Context.Context):
+	"""
+	Generates the *_all commands
+	"""
+	cmd = 'init'
+	fun = 'init'
+	def execute(self):
+		for x in list(Context.g_module.variants):
+			self.make_variant(x)
+		lst = ['remote']
+		for k in Options.commands:
+			if k.endswith('_all'):
+				name = k.replace('_all', '')
+				for x in Context.g_module.variants:
+					lst.append('%s_%s' % (name, x))
+			else:
+				lst.append(k)
+		del Options.commands[:]
+		Options.commands += lst
+
+	def make_variant(self, x):
+		for y in (BuildContext, CleanContext, InstallContext, UninstallContext):
+			name = y.__name__.replace('Context','').lower()
+			class tmp(y):
+				cmd = name + '_' + x
+				fun = 'build'
+				variant = x
+		class tmp(ConfigurationContext):
+			cmd = 'configure_' + x
+			fun = 'configure'
+			variant = x
+			def __init__(self, **kw):
+				ConfigurationContext.__init__(self, **kw)
+				self.setenv(x)
+
+class remote(BuildContext):
+	cmd = 'remote'
+	fun = 'build'
+
+	def get_ssh_hosts(self):
+		lst = []
+		for v in Context.g_module.variants:
+			self.env.HOST = self.login_to_host(self.variant_to_login(v))
+			cmd = Utils.subst_vars('${SSH_KEYSCAN} -t rsa,ecdsa ${HOST}', self.env)
+			out, err = self.cmd_and_log(cmd, output=Context.BOTH, quiet=Context.BOTH)
+			lst.append(out.strip())
+		return lst
+
+	def setup_private_ssh_key(self):
+		"""
+		When WAF_SSH_KEY points to a private key, a .ssh directory will be created in the build directory
+		Make sure that the ssh key does not prompt for a password
+		"""
+		key = os.environ.get('WAF_SSH_KEY', '')
+		if not key:
+			return
+		if not os.path.isfile(key):
+			self.fatal('Key in WAF_SSH_KEY must point to a valid file')
+		self.ssh_dir = os.path.join(self.path.abspath(), 'build', '.ssh')
+		self.ssh_hosts = os.path.join(self.ssh_dir, 'known_hosts')
+		self.ssh_key = os.path.join(self.ssh_dir, os.path.basename(key))
+		self.ssh_config = os.path.join(self.ssh_dir, 'config')
+		for x in self.ssh_hosts, self.ssh_key, self.ssh_config:
+			if not os.path.isfile(x):
+				if not os.path.isdir(self.ssh_dir):
+					os.makedirs(self.ssh_dir)
+				Utils.writef(self.ssh_key, Utils.readf(key), 'wb')
+				os.chmod(self.ssh_key, 448)
+
+				Utils.writef(self.ssh_hosts, '\n'.join(self.get_ssh_hosts()))
+				os.chmod(self.ssh_key, 448)
+
+				Utils.writef(self.ssh_config, 'UserKnownHostsFile %s' % self.ssh_hosts, 'wb')
+				os.chmod(self.ssh_config, 448)
+		self.env.SSH_OPTS = ['-F', self.ssh_config, '-i', self.ssh_key]
+		self.env.append_value('RSYNC_SEND_OPTS', '--exclude=build/.ssh')
+
+	def skip_unbuildable_variant(self):
+		# skip variants that cannot be built on this OS
+		for k in Options.commands:
+			a, _, b = k.partition('_')
+			if b in Context.g_module.variants:
+				c, _, _ = b.partition('_')
+				if c != Utils.unversioned_sys_platform():
+					Options.commands.remove(k)
+
+	def login_to_host(self, login):
+		return re.sub('(\w+@)', '', login)
+
+	def variant_to_login(self, variant):
+		"""linux_32_debug -> search env.LINUX_32 and then env.LINUX"""
+		x = variant[:variant.rfind('_')]
+		ret = os.environ.get('REMOTE_' + x.upper(), '')
+		if not ret:
+			x = x[:x.find('_')]
+			ret = os.environ.get('REMOTE_' + x.upper(), '')
+		if not ret:
+			ret = '%s@localhost' % getpass.getuser()
+		return ret
+
+	def execute(self):
+		global is_remote
+		if not is_remote:
+			self.skip_unbuildable_variant()
+		else:
+			BuildContext.execute(self)
+
+	def restore(self):
+		self.top_dir = os.path.abspath(Context.g_module.top)
+		self.srcnode = self.root.find_node(self.top_dir)
+		self.path = self.srcnode
+
+		self.out_dir = os.path.join(self.top_dir, Context.g_module.out)
+		self.bldnode = self.root.make_node(self.out_dir)
+		self.bldnode.mkdir()
+
+		self.env = ConfigSet.ConfigSet()
+
+	def extract_groups_of_builds(self):
+		"""Return a dict mapping each variants to the commands to build"""
+		self.vgroups = {}
+		for x in reversed(Options.commands):
+			_, _, variant = x.partition('_')
+			if variant in Context.g_module.variants:
+				try:
+					dct = self.vgroups[variant]
+				except KeyError:
+					dct = self.vgroups[variant] = OrderedDict()
+				try:
+					dct[variant].append(x)
+				except KeyError:
+					dct[variant] = [x]
+				Options.commands.remove(x)
+
+	def custom_options(self, login):
+		try:
+			return Context.g_module.host_options[login]
+		except (AttributeError, KeyError):
+			return {}
+
+	def recurse(self, *k, **kw):
+		self.env.RSYNC = getattr(Context.g_module, 'rsync', 'rsync -a --chmod=u+rwx')
+		self.env.SSH = getattr(Context.g_module, 'ssh', 'ssh')
+		self.env.SSH_KEYSCAN = getattr(Context.g_module, 'ssh_keyscan', 'ssh-keyscan')
+		try:
+			self.env.WAF = getattr(Context.g_module, 'waf')
+		except AttributeError:
+			try:
+				os.stat('waf')
+			except KeyError:
+				self.fatal('Put a waf file in the directory (./waf-light --tools=remote)')
+			else:
+				self.env.WAF = './waf'
+
+		self.extract_groups_of_builds()
+		self.setup_private_ssh_key()
+		for k, v in self.vgroups.items():
+			task = self(rule=rsync_and_ssh, always=True)
+			task.env.login = self.variant_to_login(k)
+
+			task.env.commands = []
+			for opt, value in v.items():
+				task.env.commands += value
+			task.env.variant = task.env.commands[0].partition('_')[2]
+			for opt, value in self.custom_options(k):
+				task.env[opt] = value
+		self.jobs = len(self.vgroups)
+
+	def make_mkdir_command(self, task):
+		return Utils.subst_vars('${SSH} ${SSH_OPTS} ${login} "rm -fr ${remote_dir} && mkdir -p ${remote_dir}"', task.env)
+
+	def make_send_command(self, task):
+		return Utils.subst_vars('${RSYNC} ${RSYNC_SEND_OPTS} -e "${SSH} ${SSH_OPTS}" ${local_dir} ${login}:${remote_dir}', task.env)
+
+	def make_exec_command(self, task):
+		txt = '''${SSH} ${SSH_OPTS} ${login} "cd ${remote_dir} && ${WAF} ${commands}"'''
+		return Utils.subst_vars(txt, task.env)
+
+	def make_save_command(self, task):
+		return Utils.subst_vars('${RSYNC} ${RSYNC_SAVE_OPTS} -e "${SSH} ${SSH_OPTS}" ${login}:${remote_dir_variant} ${build_dir}', task.env)
+
+def rsync_and_ssh(task):
+
+	# remove a warning
+	task.uid_ = id(task)
+
+	bld = task.generator.bld
+
+	task.env.user, _, _ = task.env.login.partition('@')
+	task.env.hdir = Utils.to_hex(Utils.h_list((task.generator.path.abspath(), task.env.variant)))
+	task.env.remote_dir = '~%s/wafremote/%s' % (task.env.user, task.env.hdir)
+	task.env.local_dir = bld.srcnode.abspath() + '/'
+
+	task.env.remote_dir_variant = '%s/%s/%s' % (task.env.remote_dir, Context.g_module.out, task.env.variant)
+	task.env.build_dir = bld.bldnode.abspath()
+
+	ret = task.exec_command(bld.make_mkdir_command(task))
+	if ret:
+		return ret
+	ret = task.exec_command(bld.make_send_command(task))
+	if ret:
+		return ret
+	ret = task.exec_command(bld.make_exec_command(task))
+	if ret:
+		return ret
+	ret = task.exec_command(bld.make_save_command(task))
+	if ret:
+		return ret
+
diff --git a/waflib/extras/resx.py b/waflib/extras/resx.py
new file mode 100644
index 00000000..caf4d318
--- /dev/null
+++ b/waflib/extras/resx.py
@@ -0,0 +1,35 @@
+#! /usr/bin/env python
+# encoding: utf-8
+
+import os
+from waflib import Task
+from waflib.TaskGen import extension
+
+def configure(conf):
+	conf.find_program(['resgen'], var='RESGEN')
+	conf.env.RESGENFLAGS = '/useSourcePath'
+
+@extension('.resx')
+def resx_file(self, node):
+	"""
+	Bind the .resx extension to a resgen task
+	"""
+	if not getattr(self, 'cs_task', None):
+		self.bld.fatal('resx_file has no link task for use %r' % self)
+
+	# Given assembly 'Foo' and file 'Sub/Dir/File.resx', create 'Foo.Sub.Dir.File.resources'
+	assembly = getattr(self, 'namespace', os.path.splitext(self.gen)[0])
+	res = os.path.splitext(node.path_from(self.path))[0].replace('/', '.').replace('\\', '.')
+	out = self.path.find_or_declare(assembly + '.' + res + '.resources')
+
+	tsk = self.create_task('resgen', node, out)
+
+	self.cs_task.dep_nodes.extend(tsk.outputs) # dependency
+	self.env.append_value('RESOURCES', tsk.outputs[0].bldpath())
+
+class resgen(Task.Task):
+	"""
+	Compile C# resource files
+	"""
+	color   = 'YELLOW'
+	run_str = '${RESGEN} ${RESGENFLAGS} ${SRC} ${TGT}'
diff --git a/waflib/extras/review.py b/waflib/extras/review.py
new file mode 100644
index 00000000..561e0621
--- /dev/null
+++ b/waflib/extras/review.py
@@ -0,0 +1,325 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Laurent Birtz, 2011
+# moved the code into a separate tool (ita)
+
+"""
+There are several things here:
+- a different command-line option management making options persistent
+- the review command to display the options set
+
+Assumptions:
+- configuration options are not always added to the right group (and do not count on the users to do it...)
+- the options are persistent between the executions (waf options are NOT persistent by design), even for the configuration
+- when the options change, the build is invalidated (forcing a reconfiguration)
+"""
+
+import os, textwrap, shutil
+from waflib import Logs, Context, ConfigSet, Options, Build, Configure
+
+class Odict(dict):
+	"""Ordered dictionary"""
+	def __init__(self, data=None):
+		self._keys = []
+		dict.__init__(self)
+		if data:
+			# we were provided a regular dict
+			if isinstance(data, dict):
+				self.append_from_dict(data)
+
+			# we were provided a tuple list
+			elif type(data) == list:
+				self.append_from_plist(data)
+
+			# we were provided invalid input
+			else:
+				raise Exception("expected a dict or a tuple list")
+
+	def append_from_dict(self, dict):
+		map(self.__setitem__, dict.keys(), dict.values())
+
+	def append_from_plist(self, plist):
+		for pair in plist:
+			if len(pair) != 2:
+				raise Exception("invalid pairs list")
+		for (k, v) in plist:
+			self.__setitem__(k, v)
+
+	def __delitem__(self, key):
+		if not key in self._keys:
+			raise KeyError(key)
+		dict.__delitem__(self, key)
+		self._keys.remove(key)
+
+	def __setitem__(self, key, item):
+		dict.__setitem__(self, key, item)
+		if key not in self._keys:
+			self._keys.append(key)
+
+	def clear(self):
+		dict.clear(self)
+		self._keys = []
+
+	def copy(self):
+		return Odict(self.plist())
+
+	def items(self):
+		return zip(self._keys, self.values())
+
+	def keys(self):
+		return list(self._keys) # return a copy of the list
+
+	def values(self):
+		return map(self.get, self._keys)
+
+	def plist(self):
+		p = []
+		for k, v in self.items():
+			p.append( (k, v) )
+		return p
+
+	def __str__(self):
+		buf = []
+		buf.append("{ ")
+		for k, v in self.items():
+			buf.append('%r : %r, ' % (k, v))
+		buf.append("}")
+		return ''.join(buf)
+
+review_options = Odict()
+"""
+Ordered dictionary mapping configuration option names to their optparse option.
+"""
+
+review_defaults = {}
+"""
+Dictionary mapping configuration option names to their default value.
+"""
+
+old_review_set = None
+"""
+Review set containing the configuration values before parsing the command line.
+"""
+
+new_review_set = None
+"""
+Review set containing the configuration values after parsing the command line.
+"""
+
+class OptionsReview(Options.OptionsContext):
+	def __init__(self, **kw):
+		super(self.__class__, self).__init__(**kw)
+
+	def prepare_config_review(self):
+		"""
+		Find the configuration options that are reviewable, detach
+		their default value from their optparse object and store them
+		into the review dictionaries.
+		"""
+		gr = self.get_option_group('configure options')
+		for opt in gr.option_list:
+			if opt.action != 'store' or opt.dest in ("out", "top"):
+				continue
+			review_options[opt.dest] = opt
+			review_defaults[opt.dest] = opt.default
+			if gr.defaults.has_key(opt.dest):
+				del gr.defaults[opt.dest]
+			opt.default = None
+
+	def parse_args(self):
+		self.prepare_config_review()
+		self.parser.get_option('--prefix').help = 'installation prefix'
+		super(OptionsReview, self).parse_args()
+		Context.create_context('review').refresh_review_set()
+
+class ReviewContext(Context.Context):
+	'''reviews the configuration values'''
+
+	cmd = 'review'
+
+	def __init__(self, **kw):
+		super(self.__class__, self).__init__(**kw)
+
+		out = Options.options.out
+		if not out:
+			out = getattr(Context.g_module, Context.OUT, None)
+		if not out:
+			out = Options.lockfile.replace('.lock-waf', '')
+		self.build_path = (os.path.isabs(out) and self.root or self.path).make_node(out).abspath()
+		"""Path to the build directory"""
+
+		self.cache_path = os.path.join(self.build_path, Build.CACHE_DIR)
+		"""Path to the cache directory"""
+
+		self.review_path = os.path.join(self.cache_path, 'review.cache')
+		"""Path to the review cache file"""
+
+	def execute(self):
+		"""
+		Display and store the review set. Invalidate the cache as required.
+		"""
+		if not self.compare_review_set(old_review_set, new_review_set):
+			self.invalidate_cache()
+		self.store_review_set(new_review_set)
+		print(self.display_review_set(new_review_set))
+
+	def invalidate_cache(self):
+		"""Invalidate the cache to prevent bad builds."""
+		try:
+			Logs.warn("Removing the cached configuration since the options have changed")
+			shutil.rmtree(self.cache_path)
+		except:
+			pass
+
+	def refresh_review_set(self):
+		"""
+		Obtain the old review set and the new review set, and import the new set.
+		"""
+		global old_review_set, new_review_set
+		old_review_set = self.load_review_set()
+		new_review_set = self.update_review_set(old_review_set)
+		self.import_review_set(new_review_set)
+
+	def load_review_set(self):
+		"""
+		Load and return the review set from the cache if it exists.
+		Otherwise, return an empty set.
+		"""
+		if os.path.isfile(self.review_path):
+			return ConfigSet.ConfigSet(self.review_path)
+		return ConfigSet.ConfigSet()
+
+	def store_review_set(self, review_set):
+		"""
+		Store the review set specified in the cache.
+		"""
+		if not os.path.isdir(self.cache_path):
+			os.makedirs(self.cache_path)
+		review_set.store(self.review_path)
+
+	def update_review_set(self, old_set):
+		"""
+		Merge the options passed on the command line with those imported
+		from the previous review set and return the corresponding
+		preview set.
+		"""
+
+		# Convert value to string. It's important that 'None' maps to
+		# the empty string.
+		def val_to_str(val):
+			if val == None or val == '':
+				return ''
+			return str(val)
+
+		new_set = ConfigSet.ConfigSet()
+		opt_dict = Options.options.__dict__
+
+		for name in review_options.keys():
+			# the option is specified explicitly on the command line
+			if name in opt_dict:
+				# if the option is the default, pretend it was never specified
+				if val_to_str(opt_dict[name]) != val_to_str(review_defaults[name]):
+					new_set[name] = opt_dict[name]
+			# the option was explicitly specified in a previous command
+			elif name in old_set:
+				new_set[name] = old_set[name]
+
+		return new_set
+
+	def import_review_set(self, review_set):
+		"""
+		Import the actual value of the reviewable options in the option
+		dictionary, given the current review set.
+		"""
+		for name in review_options.keys():
+			if name in review_set:
+				value = review_set[name]
+			else:
+				value = review_defaults[name]
+			setattr(Options.options, name, value)
+
+	def compare_review_set(self, set1, set2):
+		"""
+		Return true if the review sets specified are equal.
+		"""
+		if len(set1.keys()) != len(set2.keys()):
+			return False
+		for key in set1.keys():
+			if not key in set2 or set1[key] != set2[key]:
+				return False
+		return True
+
+	def display_review_set(self, review_set):
+		"""
+		Return the string representing the review set specified.
+		"""
+		term_width = Logs.get_term_cols()
+		lines = []
+		for dest in review_options.keys():
+			opt = review_options[dest]
+			name = ", ".join(opt._short_opts + opt._long_opts)
+			help = opt.help
+			actual = None
+			if dest in review_set:
+				actual = review_set[dest]
+			default = review_defaults[dest]
+			lines.append(self.format_option(name, help, actual, default, term_width))
+		return "Configuration:\n\n" + "\n\n".join(lines) + "\n"
+
+	def format_option(self, name, help, actual, default, term_width):
+		"""
+		Return the string representing the option specified.
+		"""
+		def val_to_str(val):
+			if val == None or val == '':
+				return "(void)"
+			return str(val)
+
+		max_name_len = 20
+		sep_len = 2
+
+		w = textwrap.TextWrapper()
+		w.width = term_width - 1
+		if w.width < 60:
+			w.width = 60
+
+		out = ""
+
+		# format the help
+		out += w.fill(help) + "\n"
+
+		# format the name
+		name_len = len(name)
+		out += Logs.colors.CYAN + name + Logs.colors.NORMAL
+
+		# set the indentation used when the value wraps to the next line
+		w.subsequent_indent = " ".rjust(max_name_len + sep_len)
+		w.width -= (max_name_len + sep_len)
+
+		# the name string is too long, switch to the next line
+		if name_len > max_name_len:
+			out += "\n" + w.subsequent_indent
+
+		# fill the remaining of the line with spaces
+		else:
+			out += " ".rjust(max_name_len + sep_len - name_len)
+
+		# format the actual value, if there is one
+		if actual != None:
+			out += Logs.colors.BOLD + w.fill(val_to_str(actual)) + Logs.colors.NORMAL + "\n" + w.subsequent_indent
+
+		# format the default value
+		default_fmt = val_to_str(default)
+		if actual != None:
+			default_fmt = "default: " + default_fmt
+		out += Logs.colors.NORMAL + w.fill(default_fmt) + Logs.colors.NORMAL
+
+		return out
+
+# Monkey-patch ConfigurationContext.execute() to have it store the review set.
+old_configure_execute = Configure.ConfigurationContext.execute
+def new_configure_execute(self):
+	old_configure_execute(self)
+	Context.create_context('review').store_review_set(new_review_set)
+Configure.ConfigurationContext.execute = new_configure_execute
+
diff --git a/waflib/extras/rst.py b/waflib/extras/rst.py
new file mode 100644
index 00000000..f3c3a5eb
--- /dev/null
+++ b/waflib/extras/rst.py
@@ -0,0 +1,260 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Jérôme Carretero, 2013 (zougloub)
+
+"""
+reStructuredText support (experimental)
+
+Example::
+
+	def configure(conf):
+		conf.load('rst')
+		if not conf.env.RST2HTML:
+			conf.fatal('The program rst2html is required')
+
+	def build(bld):
+		bld(
+		 features = 'rst',
+		 type     = 'rst2html', # rst2html, rst2pdf, ...
+		 source   = 'index.rst', # mandatory, the source
+		 deps     = 'image.png', # to give additional non-trivial dependencies
+		)
+
+By default the tool looks for a set of programs in PATH.
+The tools are defined in `rst_progs`.
+To configure with a special program use::
+
+	$ RST2HTML=/path/to/rst2html waf configure
+
+This tool is experimental; don't hesitate to contribute to it.
+
+"""
+
+import re
+from waflib import Node, Utils, Task, Errors, Logs
+from waflib.TaskGen import feature, before_method
+
+rst_progs = "rst2html rst2xetex rst2latex rst2xml rst2pdf rst2s5 rst2man rst2odt rst2rtf".split()
+
+def parse_rst_node(task, node, nodes, names, seen, dirs=None):
+	# TODO add extensibility, to handle custom rst include tags...
+	if dirs is None:
+		dirs = (node.parent,node.get_bld().parent)
+
+	if node in seen:
+		return
+	seen.append(node)
+	code = node.read()
+	re_rst = re.compile(r'^\s*.. ((?P<subst>\|\S+\|) )?(?P<type>include|image|figure):: (?P<file>.*)$', re.M)
+	for match in re_rst.finditer(code):
+		ipath = match.group('file')
+		itype = match.group('type')
+		Logs.debug('rst: visiting %s: %s', itype, ipath)
+		found = False
+		for d in dirs:
+			Logs.debug('rst: looking for %s in %s', ipath, d.abspath())
+			found = d.find_node(ipath)
+			if found:
+				Logs.debug('rst: found %s as %s', ipath, found.abspath())
+				nodes.append((itype, found))
+				if itype == 'include':
+					parse_rst_node(task, found, nodes, names, seen)
+				break
+		if not found:
+			names.append((itype, ipath))
+
+class docutils(Task.Task):
+	"""
+	Compile a rst file.
+	"""
+
+	def scan(self):
+		"""
+		A recursive regex-based scanner that finds rst dependencies.
+		"""
+
+		nodes = []
+		names = []
+		seen = []
+
+		node = self.inputs[0]
+
+		if not node:
+			return (nodes, names)
+
+		parse_rst_node(self, node, nodes, names, seen)
+
+		Logs.debug('rst: %r: found the following file deps: %r', self, nodes)
+		if names:
+			Logs.warn('rst: %r: could not find the following file deps: %r', self, names)
+
+		return ([v for (t,v) in nodes], [v for (t,v) in names])
+
+	def check_status(self, msg, retcode):
+		"""
+		Check an exit status and raise an error with a particular message
+
+		:param msg: message to display if the code is non-zero
+		:type msg: string
+		:param retcode: condition
+		:type retcode: boolean
+		"""
+		if retcode != 0:
+			raise Errors.WafError('%r command exit status %r' % (msg, retcode))
+
+	def run(self):
+		"""
+		Runs the rst compilation using docutils
+		"""
+		raise NotImplementedError()
+
+class rst2html(docutils):
+	color = 'BLUE'
+
+	def __init__(self, *args, **kw):
+		docutils.__init__(self, *args, **kw)
+		self.command = self.generator.env.RST2HTML
+		self.attributes = ['stylesheet']
+
+	def scan(self):
+		nodes, names = docutils.scan(self)
+
+		for attribute in self.attributes:
+			stylesheet = getattr(self.generator, attribute, None)
+			if stylesheet is not None:
+				ssnode = self.generator.to_nodes(stylesheet)[0]
+				nodes.append(ssnode)
+				Logs.debug('rst: adding dep to %s %s', attribute, stylesheet)
+
+		return nodes, names
+
+	def run(self):
+		cwdn = self.outputs[0].parent
+		src = self.inputs[0].path_from(cwdn)
+		dst = self.outputs[0].path_from(cwdn)
+
+		cmd = self.command + [src, dst]
+		cmd += Utils.to_list(getattr(self.generator, 'options', []))
+		for attribute in self.attributes:
+			stylesheet = getattr(self.generator, attribute, None)
+			if stylesheet is not None:
+				stylesheet = self.generator.to_nodes(stylesheet)[0]
+				cmd += ['--%s' % attribute, stylesheet.path_from(cwdn)]
+
+		return self.exec_command(cmd, cwd=cwdn.abspath())
+
+class rst2s5(rst2html):
+	def __init__(self, *args, **kw):
+		rst2html.__init__(self, *args, **kw)
+		self.command = self.generator.env.RST2S5
+		self.attributes = ['stylesheet']
+
+class rst2latex(rst2html):
+	def __init__(self, *args, **kw):
+		rst2html.__init__(self, *args, **kw)
+		self.command = self.generator.env.RST2LATEX
+		self.attributes = ['stylesheet']
+
+class rst2xetex(rst2html):
+	def __init__(self, *args, **kw):
+		rst2html.__init__(self, *args, **kw)
+		self.command = self.generator.env.RST2XETEX
+		self.attributes = ['stylesheet']
+
+class rst2pdf(docutils):
+	color = 'BLUE'
+	def run(self):
+		cwdn = self.outputs[0].parent
+		src = self.inputs[0].path_from(cwdn)
+		dst = self.outputs[0].path_from(cwdn)
+
+		cmd = self.generator.env.RST2PDF + [src, '-o', dst]
+		cmd += Utils.to_list(getattr(self.generator, 'options', []))
+
+		return self.exec_command(cmd, cwd=cwdn.abspath())
+
+
+@feature('rst')
+@before_method('process_source')
+def apply_rst(self):
+	"""
+	Create :py:class:`rst` or other rst-related task objects
+	"""
+
+	if self.target:
+		if isinstance(self.target, Node.Node):
+			tgt = self.target
+		elif isinstance(self.target, str):
+			tgt = self.path.get_bld().make_node(self.target)
+		else:
+			self.bld.fatal("rst: Don't know how to build target name %s which is not a string or Node for %s" % (self.target, self))
+	else:
+		tgt = None
+
+	tsk_type = getattr(self, 'type', None)
+
+	src = self.to_nodes(self.source)
+	assert len(src) == 1
+	src = src[0]
+
+	if tsk_type is not None and tgt is None:
+		if tsk_type.startswith('rst2'):
+			ext = tsk_type[4:]
+		else:
+			self.bld.fatal("rst: Could not detect the output file extension for %s" % self)
+		tgt = src.change_ext('.%s' % ext)
+	elif tsk_type is None and tgt is not None:
+		out = tgt.name
+		ext = out[out.rfind('.')+1:]
+		self.type = 'rst2' + ext
+	elif tsk_type is not None and tgt is not None:
+		# the user knows what he wants
+		pass
+	else:
+		self.bld.fatal("rst: Need to indicate task type or target name for %s" % self)
+
+	deps_lst = []
+
+	if getattr(self, 'deps', None):
+		deps = self.to_list(self.deps)
+		for filename in deps:
+			n = self.path.find_resource(filename)
+			if not n:
+				self.bld.fatal('Could not find %r for %r' % (filename, self))
+			if not n in deps_lst:
+				deps_lst.append(n)
+
+	try:
+		task = self.create_task(self.type, src, tgt)
+	except KeyError:
+		self.bld.fatal("rst: Task of type %s not implemented (created by %s)" % (self.type, self))
+
+	task.env = self.env
+
+	# add the manual dependencies
+	if deps_lst:
+		try:
+			lst = self.bld.node_deps[task.uid()]
+			for n in deps_lst:
+				if not n in lst:
+					lst.append(n)
+		except KeyError:
+			self.bld.node_deps[task.uid()] = deps_lst
+
+	inst_to = getattr(self, 'install_path', None)
+	if inst_to:
+		self.install_task = self.add_install_files(install_to=inst_to, install_from=task.outputs[:])
+
+	self.source = []
+
+def configure(self):
+	"""
+	Try to find the rst programs.
+
+	Do not raise any error if they are not found.
+	You'll have to use additional code in configure() to die
+	if programs were not found.
+	"""
+	for p in rst_progs:
+		self.find_program(p, mandatory=False)
+
diff --git a/waflib/extras/run_do_script.py b/waflib/extras/run_do_script.py
new file mode 100644
index 00000000..f3c58122
--- /dev/null
+++ b/waflib/extras/run_do_script.py
@@ -0,0 +1,139 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Hans-Martin von Gaudecker, 2012
+
+"""
+Run a Stata do-script in the directory specified by **ctx.bldnode**. The
+first and only argument will be the name of the do-script (no extension),
+which can be accessed inside the do-script by the local macro `1'. Useful
+for keeping a log file.
+
+The tool uses the log file that is automatically kept by Stata only 
+for error-catching purposes, it will be destroyed if the task finished
+without error. In case of an error in **some_script.do**, you can inspect
+it as **some_script.log** in the **ctx.bldnode** directory.
+
+Note that Stata will not return an error code if it exits abnormally -- 
+catching errors relies on parsing the log file mentioned before. Should
+the parser behave incorrectly please send an email to hmgaudecker [at] gmail.
+
+**WARNING**
+
+    The tool will not work if multiple do-scripts of the same name---but in
+    different directories---are run at the same time! Avoid this situation.
+
+Usage::
+
+    ctx(features='run_do_script', 
+        source='some_script.do',
+        target=['some_table.tex', 'some_figure.eps'],
+        deps='some_data.csv')
+"""
+
+
+import os, re, sys
+from waflib import Task, TaskGen, Logs
+
+if sys.platform == 'darwin':
+	STATA_COMMANDS = ['Stata64MP', 'StataMP',
+								'Stata64SE', 'StataSE', 
+								'Stata64', 'Stata']
+	STATAFLAGS = '-e -q do'
+	STATAENCODING = 'MacRoman'
+elif sys.platform.startswith('linux'):
+	STATA_COMMANDS = ['stata-mp', 'stata-se', 'stata']
+	STATAFLAGS = '-b -q do'
+	# Not sure whether this is correct...
+	STATAENCODING = 'Latin-1'
+elif sys.platform.lower().startswith('win'):
+	STATA_COMMANDS = ['StataMP-64', 'StataMP-ia',
+								'StataMP', 'StataSE-64',
+								'StataSE-ia', 'StataSE',
+								'Stata-64', 'Stata-ia',
+								'Stata.e', 'WMPSTATA',
+								'WSESTATA', 'WSTATA']
+	STATAFLAGS = '/e do'
+	STATAENCODING = 'Latin-1'
+else:
+	raise Exception("Unknown sys.platform: %s " % sys.platform)
+
+def configure(ctx):
+	ctx.find_program(STATA_COMMANDS, var='STATACMD', errmsg="""\n
+No Stata executable found!\n\n
+If Stata is needed:\n
+	1) Check the settings of your system path.
+	2) Note we are looking for Stata executables called: %s
+	   If yours has a different name, please report to hmgaudecker [at] gmail\n
+Else:\n
+	Do not load the 'run_do_script' tool in the main wscript.\n\n""" % STATA_COMMANDS)
+	ctx.env.STATAFLAGS = STATAFLAGS
+	ctx.env.STATAENCODING = STATAENCODING
+
+class run_do_script_base(Task.Task):
+	"""Run a Stata do-script from the bldnode directory."""
+	run_str = '"${STATACMD}" ${STATAFLAGS} "${SRC[0].abspath()}" "${DOFILETRUNK}"'
+	shell = True
+
+class run_do_script(run_do_script_base):
+	"""Use the log file automatically kept by Stata for error-catching.
+	Erase it if the task finished without error. If not, it will show 
+	up as do_script.log in the bldnode directory.
+	"""
+	def run(self):
+		run_do_script_base.run(self)
+		ret, log_tail  = self.check_erase_log_file()
+		if ret:
+			Logs.error("""Running Stata on %r failed with code %r.\n\nCheck the log file %s, last 10 lines\n\n%s\n\n\n""",
+				self.inputs[0], ret, self.env.LOGFILEPATH, log_tail)
+		return ret
+
+	def check_erase_log_file(self):
+		"""Parse Stata's default log file and erase it if everything okay.
+
+		Parser is based on Brendan Halpin's shell script found here:
+			http://teaching.sociology.ul.ie/bhalpin/wordpress/?p=122
+		"""
+
+		if sys.version_info.major >= 3:
+			kwargs = {'file': self.env.LOGFILEPATH, 'mode': 'r', 'encoding': self.env.STATAENCODING}
+		else:
+			kwargs = {'name': self.env.LOGFILEPATH, 'mode': 'r'}
+		with open(**kwargs) as log:
+			log_tail = log.readlines()[-10:]
+			for line in log_tail:
+				error_found = re.match("r\(([0-9]+)\)", line)
+				if error_found:
+					return error_found.group(1), ''.join(log_tail)
+				else:
+					pass
+		# Only end up here if the parser did not identify an error.
+		os.remove(self.env.LOGFILEPATH)
+		return None, None
+
+
+@TaskGen.feature('run_do_script')
+@TaskGen.before_method('process_source')
+def apply_run_do_script(tg):
+	"""Task generator customising the options etc. to call Stata in batch
+	mode for running a do-script.
+	"""
+
+	# Convert sources and targets to nodes
+	src_node = tg.path.find_resource(tg.source)
+	tgt_nodes = [tg.path.find_or_declare(t) for t in tg.to_list(tg.target)]
+
+	tsk = tg.create_task('run_do_script', src=src_node, tgt=tgt_nodes)
+	tsk.env.DOFILETRUNK = os.path.splitext(src_node.name)[0]
+	tsk.env.LOGFILEPATH = os.path.join(tg.bld.bldnode.abspath(), '%s.log' % (tsk.env.DOFILETRUNK))
+
+	# dependencies (if the attribute 'deps' changes, trigger a recompilation)
+	for x in tg.to_list(getattr(tg, 'deps', [])):
+		node = tg.path.find_resource(x)
+		if not node:
+			tg.bld.fatal('Could not find dependency %r for running %r' % (x, src_node.abspath()))
+		tsk.dep_nodes.append(node)
+	Logs.debug('deps: found dependencies %r for running %r', tsk.dep_nodes, src_node.abspath())
+
+	# Bypass the execution of process_source by setting the source to an empty list
+	tg.source = []
+
diff --git a/waflib/extras/run_m_script.py b/waflib/extras/run_m_script.py
new file mode 100644
index 00000000..b5f27ebe
--- /dev/null
+++ b/waflib/extras/run_m_script.py
@@ -0,0 +1,88 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Hans-Martin von Gaudecker, 2012
+
+"""
+Run a Matlab script.
+
+Note that the script is run in the directory where it lives -- Matlab won't
+allow it any other way.
+
+For error-catching purposes, keep an own log-file that is destroyed if the
+task finished without error. If not, it will show up as mscript_[index].log 
+in the bldnode directory.
+
+Usage::
+
+    ctx(features='run_m_script', 
+        source='some_script.m',
+        target=['some_table.tex', 'some_figure.eps'],
+        deps='some_data.mat')
+"""
+
+import os, sys
+from waflib import Task, TaskGen, Logs
+
+MATLAB_COMMANDS = ['matlab']
+
+def configure(ctx):
+	ctx.find_program(MATLAB_COMMANDS, var='MATLABCMD', errmsg = """\n
+No Matlab executable found!\n\n
+If Matlab is needed:\n
+    1) Check the settings of your system path.
+    2) Note we are looking for Matlab executables called: %s
+       If yours has a different name, please report to hmgaudecker [at] gmail\n
+Else:\n
+    Do not load the 'run_m_script' tool in the main wscript.\n\n"""  % MATLAB_COMMANDS)
+	ctx.env.MATLABFLAGS = '-wait -nojvm -nosplash -minimize'
+
+class run_m_script_base(Task.Task):
+	"""Run a Matlab script."""
+	run_str = '"${MATLABCMD}" ${MATLABFLAGS} -logfile "${LOGFILEPATH}" -r "try, ${MSCRIPTTRUNK}, exit(0), catch err, disp(err.getReport()), exit(1), end"'
+	shell = True
+
+class run_m_script(run_m_script_base):
+	"""Erase the Matlab overall log file if everything went okay, else raise an
+	error and print its 10 last lines.
+	"""
+	def run(self):
+		ret = run_m_script_base.run(self)
+		logfile = self.env.LOGFILEPATH
+		if ret:
+			mode = 'r'
+			if sys.version_info.major >= 3:
+				mode = 'rb'
+			with open(logfile, mode=mode) as f:
+				tail = f.readlines()[-10:]
+			Logs.error("""Running Matlab on %r returned the error %r\n\nCheck the log file %s, last 10 lines\n\n%s\n\n\n""",
+				self.inputs[0], ret, logfile, '\n'.join(tail))
+		else:
+			os.remove(logfile)
+		return ret
+
+@TaskGen.feature('run_m_script')
+@TaskGen.before_method('process_source')
+def apply_run_m_script(tg):
+	"""Task generator customising the options etc. to call Matlab in batch
+	mode for running a m-script.
+	"""
+
+	# Convert sources and targets to nodes 
+	src_node = tg.path.find_resource(tg.source)
+	tgt_nodes = [tg.path.find_or_declare(t) for t in tg.to_list(tg.target)]
+
+	tsk = tg.create_task('run_m_script', src=src_node, tgt=tgt_nodes)
+	tsk.cwd = src_node.parent.abspath()
+	tsk.env.MSCRIPTTRUNK = os.path.splitext(src_node.name)[0]
+	tsk.env.LOGFILEPATH = os.path.join(tg.bld.bldnode.abspath(), '%s_%d.log' % (tsk.env.MSCRIPTTRUNK, tg.idx))
+
+	# dependencies (if the attribute 'deps' changes, trigger a recompilation)
+	for x in tg.to_list(getattr(tg, 'deps', [])):
+		node = tg.path.find_resource(x)
+		if not node:
+			tg.bld.fatal('Could not find dependency %r for running %r' % (x, src_node.abspath()))
+		tsk.dep_nodes.append(node)
+	Logs.debug('deps: found dependencies %r for running %r', tsk.dep_nodes, src_node.abspath())
+
+	# Bypass the execution of process_source by setting the source to an empty list
+	tg.source = []
diff --git a/waflib/extras/run_py_script.py b/waflib/extras/run_py_script.py
new file mode 100644
index 00000000..36703811
--- /dev/null
+++ b/waflib/extras/run_py_script.py
@@ -0,0 +1,104 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Hans-Martin von Gaudecker, 2012
+
+"""
+Run a Python script in the directory specified by **ctx.bldnode**.
+
+Select a Python version by specifying the **version** keyword for
+the task generator instance as integer 2 or 3. Default is 3.
+
+If the build environment has an attribute "PROJECT_PATHS" with
+a key "PROJECT_ROOT", its value will be appended to the PYTHONPATH.
+Same a string passed to the optional **add_to_pythonpath**
+keyword (appended after the PROJECT_ROOT).
+
+Usage::
+
+    ctx(features='run_py_script', version=3,
+        source='some_script.py',
+        target=['some_table.tex', 'some_figure.eps'],
+        deps='some_data.csv',
+        add_to_pythonpath='src/some/library')
+"""
+
+import os, re
+from waflib import Task, TaskGen, Logs
+
+
+def configure(conf):
+	"""TODO: Might need to be updated for Windows once
+	"PEP 397":http://www.python.org/dev/peps/pep-0397/ is settled.
+	"""
+	conf.find_program('python', var='PY2CMD', mandatory=False)
+	conf.find_program('python3', var='PY3CMD', mandatory=False)
+	if not conf.env.PY2CMD and not conf.env.PY3CMD:
+		conf.fatal("No Python interpreter found!")
+
+class run_py_2_script(Task.Task):
+	"""Run a Python 2 script."""
+	run_str = '${PY2CMD} ${SRC[0].abspath()}'
+	shell=True
+
+class run_py_3_script(Task.Task):
+	"""Run a Python 3 script."""
+	run_str = '${PY3CMD} ${SRC[0].abspath()}'
+	shell=True
+
+@TaskGen.feature('run_py_script')
+@TaskGen.before_method('process_source')
+def apply_run_py_script(tg):
+	"""Task generator for running either Python 2 or Python 3 on a single
+	script.
+
+	Attributes:
+
+		* source -- A **single** source node or string. (required)
+		* target -- A single target or list of targets (nodes or strings)
+		* deps -- A single dependency or list of dependencies (nodes or strings)
+		* add_to_pythonpath -- A string that will be appended to the PYTHONPATH environment variable
+
+	If the build environment has an attribute "PROJECT_PATHS" with
+	a key "PROJECT_ROOT", its value will be appended to the PYTHONPATH.
+	"""
+
+	# Set the Python version to use, default to 3.
+	v = getattr(tg, 'version', 3)
+	if v not in (2, 3):
+		raise ValueError("Specify the 'version' attribute for run_py_script task generator as integer 2 or 3.\n Got: %s" %v)
+
+	# Convert sources and targets to nodes
+	src_node = tg.path.find_resource(tg.source)
+	tgt_nodes = [tg.path.find_or_declare(t) for t in tg.to_list(tg.target)]
+
+	# Create the task.
+	tsk = tg.create_task('run_py_%d_script' %v, src=src_node, tgt=tgt_nodes)
+
+	# custom execution environment
+	# TODO use a list and  os.sep.join(lst) at the end instead of concatenating strings
+	tsk.env.env = dict(os.environ)
+	tsk.env.env['PYTHONPATH'] = tsk.env.env.get('PYTHONPATH', '')
+	project_paths = getattr(tsk.env, 'PROJECT_PATHS', None)
+	if project_paths and 'PROJECT_ROOT' in project_paths:
+		tsk.env.env['PYTHONPATH'] += os.pathsep + project_paths['PROJECT_ROOT'].abspath()
+	if getattr(tg, 'add_to_pythonpath', None):
+		tsk.env.env['PYTHONPATH'] += os.pathsep + tg.add_to_pythonpath
+
+	# Clean up the PYTHONPATH -- replace double occurrences of path separator
+	tsk.env.env['PYTHONPATH'] = re.sub(os.pathsep + '+', os.pathsep, tsk.env.env['PYTHONPATH'])
+
+	# Clean up the PYTHONPATH -- doesn't like starting with path separator
+	if tsk.env.env['PYTHONPATH'].startswith(os.pathsep):
+		 tsk.env.env['PYTHONPATH'] = tsk.env.env['PYTHONPATH'][1:]
+
+	# dependencies (if the attribute 'deps' changes, trigger a recompilation)
+	for x in tg.to_list(getattr(tg, 'deps', [])):
+		node = tg.path.find_resource(x)
+		if not node:
+			tg.bld.fatal('Could not find dependency %r for running %r' % (x, src_node.abspath()))
+		tsk.dep_nodes.append(node)
+	Logs.debug('deps: found dependencies %r for running %r', tsk.dep_nodes, src_node.abspath())
+
+	# Bypass the execution of process_source by setting the source to an empty list
+	tg.source = []
+
diff --git a/waflib/extras/run_r_script.py b/waflib/extras/run_r_script.py
new file mode 100644
index 00000000..b0d8f2b2
--- /dev/null
+++ b/waflib/extras/run_r_script.py
@@ -0,0 +1,86 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Hans-Martin von Gaudecker, 2012
+
+"""
+Run a R script in the directory specified by **ctx.bldnode**.
+
+For error-catching purposes, keep an own log-file that is destroyed if the
+task finished without error. If not, it will show up as rscript_[index].log
+in the bldnode directory.
+
+Usage::
+
+    ctx(features='run_r_script',
+        source='some_script.r',
+        target=['some_table.tex', 'some_figure.eps'],
+        deps='some_data.csv')
+"""
+
+
+import os, sys
+from waflib import Task, TaskGen, Logs
+
+R_COMMANDS = ['RTerm', 'R', 'r']
+
+def configure(ctx):
+	ctx.find_program(R_COMMANDS, var='RCMD', errmsg = """\n
+No R executable found!\n\n
+If R is needed:\n
+	1) Check the settings of your system path.
+	2) Note we are looking for R executables called: %s
+	   If yours has a different name, please report to hmgaudecker [at] gmail\n
+Else:\n
+	Do not load the 'run_r_script' tool in the main wscript.\n\n"""  % R_COMMANDS)
+	ctx.env.RFLAGS = 'CMD BATCH --slave'
+
+class run_r_script_base(Task.Task):
+	"""Run a R script."""
+	run_str = '"${RCMD}" ${RFLAGS} "${SRC[0].abspath()}" "${LOGFILEPATH}"'
+	shell = True
+
+class run_r_script(run_r_script_base):
+	"""Erase the R overall log file if everything went okay, else raise an
+	error and print its 10 last lines.
+	"""
+	def run(self):
+		ret = run_r_script_base.run(self)
+		logfile = self.env.LOGFILEPATH
+		if ret:
+			mode = 'r'
+			if sys.version_info.major >= 3:
+				mode = 'rb'
+			with open(logfile, mode=mode) as f:
+				tail = f.readlines()[-10:]
+			Logs.error("""Running R on %r returned the error %r\n\nCheck the log file %s, last 10 lines\n\n%s\n\n\n""",
+				self.inputs[0], ret, logfile, '\n'.join(tail))
+		else:
+			os.remove(logfile)
+		return ret
+
+
+@TaskGen.feature('run_r_script')
+@TaskGen.before_method('process_source')
+def apply_run_r_script(tg):
+	"""Task generator customising the options etc. to call R in batch
+	mode for running a R script.
+	"""
+
+	# Convert sources and targets to nodes
+	src_node = tg.path.find_resource(tg.source)
+	tgt_nodes = [tg.path.find_or_declare(t) for t in tg.to_list(tg.target)]
+
+	tsk = tg.create_task('run_r_script', src=src_node, tgt=tgt_nodes)
+	tsk.env.LOGFILEPATH = os.path.join(tg.bld.bldnode.abspath(), '%s_%d.log' % (os.path.splitext(src_node.name)[0], tg.idx))
+
+	# dependencies (if the attribute 'deps' changes, trigger a recompilation)
+	for x in tg.to_list(getattr(tg, 'deps', [])):
+		node = tg.path.find_resource(x)
+		if not node:
+			tg.bld.fatal('Could not find dependency %r for running %r' % (x, src_node.abspath()))
+		tsk.dep_nodes.append(node)
+	Logs.debug('deps: found dependencies %r for running %r', tsk.dep_nodes, src_node.abspath())
+
+	# Bypass the execution of process_source by setting the source to an empty list
+	tg.source = []
+
diff --git a/waflib/extras/sas.py b/waflib/extras/sas.py
new file mode 100644
index 00000000..754c6148
--- /dev/null
+++ b/waflib/extras/sas.py
@@ -0,0 +1,71 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Mark Coggeshall, 2010
+
+"SAS support"
+
+import os
+from waflib import Task, Errors, Logs
+from waflib.TaskGen import feature, before_method
+
+sas_fun, _ = Task.compile_fun('sas -sysin ${SRCFILE} -log ${LOGFILE} -print ${LSTFILE}', shell=False)
+
+class sas(Task.Task):
+	vars = ['SAS', 'SASFLAGS']
+	def run(task):
+		command = 'SAS'
+		fun = sas_fun
+
+		node = task.inputs[0]
+		logfilenode = node.change_ext('.log')
+		lstfilenode = node.change_ext('.lst')
+
+		# set the cwd
+		task.cwd = task.inputs[0].parent.get_src().abspath()
+		Logs.debug('runner: %r on %r', command, node)
+
+		SASINPUTS = node.parent.get_bld().abspath() + os.pathsep + node.parent.get_src().abspath() + os.pathsep
+		task.env.env = {'SASINPUTS': SASINPUTS}
+
+		task.env.SRCFILE = node.abspath()
+		task.env.LOGFILE = logfilenode.abspath()
+		task.env.LSTFILE = lstfilenode.abspath()
+		ret = fun(task)
+		if ret:
+			Logs.error('Running %s on %r returned a non-zero exit', command, node)
+			Logs.error('SRCFILE = %r', node)
+			Logs.error('LOGFILE = %r', logfilenode)
+			Logs.error('LSTFILE = %r', lstfilenode)
+		return ret
+
+@feature('sas')
+@before_method('process_source')
+def apply_sas(self):
+	if not getattr(self, 'type', None) in ('sas',):
+		self.type = 'sas'
+
+	self.env['logdir'] = getattr(self, 'logdir', 'log')
+	self.env['lstdir'] = getattr(self, 'lstdir', 'lst')
+
+	deps_lst = []
+
+	if getattr(self, 'deps', None):
+		deps = self.to_list(self.deps)
+		for filename in deps:
+			n = self.path.find_resource(filename)
+			if not n:
+				n = self.bld.root.find_resource(filename)
+			if not n:
+				raise Errors.WafError('cannot find input file %s for processing' % filename)
+			if not n in deps_lst:
+				deps_lst.append(n)
+
+	for node in self.to_nodes(self.source):
+		if self.type == 'sas':
+			task = self.create_task('sas', src=node)
+		task.dep_nodes = deps_lst
+	self.source = []
+
+def configure(self):
+	self.find_program('sas', var='SAS', mandatory=False)
+
diff --git a/waflib/extras/satellite_assembly.py b/waflib/extras/satellite_assembly.py
new file mode 100644
index 00000000..005eb074
--- /dev/null
+++ b/waflib/extras/satellite_assembly.py
@@ -0,0 +1,57 @@
+#!/usr/bin/python
+# encoding: utf-8
+# vim: tabstop=4 noexpandtab
+
+"""
+Create a satellite assembly from "*.??.txt" files. ?? stands for a language code.
+
+The projects Resources subfolder contains resources.??.txt string files for several languages.
+The build folder will hold the satellite assemblies as ./??/ExeName.resources.dll
+
+#gen becomes template (It is called gen because it also uses resx.py).
+bld(source='Resources/resources.de.txt',gen=ExeName)
+"""
+
+import os, re
+from waflib import Task
+from waflib.TaskGen import feature,before_method
+
+class al(Task.Task):
+	run_str = '${AL} ${ALFLAGS}'
+
+@feature('satellite_assembly')
+@before_method('process_source')
+def satellite_assembly(self):
+	if not getattr(self, 'gen', None):
+		self.bld.fatal('satellite_assembly needs a template assembly provided with the "gen" parameter')
+	res_lang = re.compile(r'(.*)\.(\w\w)\.(?:resx|txt)',flags=re.I)
+
+	# self.source can contain node objects, so this will break in one way or another
+	self.source = self.to_list(self.source)
+	for i, x in enumerate(self.source):
+		#x = 'resources/resources.de.resx'
+		#x = 'resources/resources.de.txt'
+		mo = res_lang.match(x)
+		if mo:
+			template = os.path.splitext(self.gen)[0]
+			templatedir, templatename = os.path.split(template)
+			res = mo.group(1)
+			lang = mo.group(2)
+			#./Resources/resources.de.resources
+			resources = self.path.find_or_declare(res+ '.' + lang + '.resources')
+			self.create_task('resgen', self.to_nodes(x), [resources])
+			#./de/Exename.resources.dll
+			satellite = self.path.find_or_declare(os.path.join(templatedir,lang,templatename) + '.resources.dll')
+			tsk = self.create_task('al',[resources],[satellite])
+			tsk.env.append_value('ALFLAGS','/template:'+os.path.join(self.path.relpath(),self.gen))
+			tsk.env.append_value('ALFLAGS','/embed:'+resources.relpath())
+			tsk.env.append_value('ALFLAGS','/culture:'+lang)
+			tsk.env.append_value('ALFLAGS','/out:'+satellite.relpath())
+			self.source[i] = None
+	# remove the None elements that we just substituted
+	self.source = list(filter(lambda x:x, self.source))
+
+def configure(ctx):
+	ctx.find_program('al', var='AL', mandatory=True)
+	ctx.load('resx')
+
diff --git a/waflib/extras/scala.py b/waflib/extras/scala.py
new file mode 100644
index 00000000..a9880f02
--- /dev/null
+++ b/waflib/extras/scala.py
@@ -0,0 +1,128 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2010 (ita)
+
+"""
+Scala support
+
+scalac outputs files a bit where it wants to
+"""
+
+import os
+from waflib import Task, Utils, Node
+from waflib.TaskGen import feature, before_method, after_method
+
+from waflib.Tools import ccroot
+ccroot.USELIB_VARS['scalac'] = set(['CLASSPATH', 'SCALACFLAGS'])
+
+from waflib.Tools import javaw
+
+@feature('scalac')
+@before_method('process_source')
+def apply_scalac(self):
+
+	Utils.def_attrs(self, jarname='', classpath='',
+		sourcepath='.', srcdir='.',
+		jar_mf_attributes={}, jar_mf_classpath=[])
+
+	outdir = getattr(self, 'outdir', None)
+	if outdir:
+		if not isinstance(outdir, Node.Node):
+			outdir = self.path.get_bld().make_node(self.outdir)
+	else:
+		outdir = self.path.get_bld()
+	outdir.mkdir()
+	self.env['OUTDIR'] = outdir.abspath()
+
+	self.scalac_task = tsk = self.create_task('scalac')
+	tmp = []
+
+	srcdir = getattr(self, 'srcdir', '')
+	if isinstance(srcdir, Node.Node):
+		srcdir = [srcdir]
+	for x in Utils.to_list(srcdir):
+		if isinstance(x, Node.Node):
+			y = x
+		else:
+			y = self.path.find_dir(x)
+			if not y:
+				self.bld.fatal('Could not find the folder %s from %s' % (x, self.path))
+		tmp.append(y)
+	tsk.srcdir = tmp
+
+# reuse some code
+feature('scalac')(javaw.use_javac_files)
+after_method('apply_scalac')(javaw.use_javac_files)
+
+feature('scalac')(javaw.set_classpath)
+after_method('apply_scalac', 'use_scalac_files')(javaw.set_classpath)
+
+
+SOURCE_RE = '**/*.scala'
+class scalac(javaw.javac):
+	color = 'GREEN'
+	vars    = ['CLASSPATH', 'SCALACFLAGS', 'SCALAC', 'OUTDIR']
+
+	def runnable_status(self):
+		"""
+		Wait for dependent tasks to be complete, then read the file system to find the input nodes.
+		"""
+		for t in self.run_after:
+			if not t.hasrun:
+				return Task.ASK_LATER
+
+		if not self.inputs:
+			global SOURCE_RE
+			self.inputs  = []
+			for x in self.srcdir:
+				self.inputs.extend(x.ant_glob(SOURCE_RE, remove=False))
+		return super(javaw.javac, self).runnable_status()
+
+	def run(self):
+		"""
+		Execute the scalac compiler
+		"""
+		env = self.env
+		gen = self.generator
+		bld = gen.bld
+		wd = bld.bldnode.abspath()
+		def to_list(xx):
+			if isinstance(xx, str):
+				return [xx]
+			return xx
+		self.last_cmd = lst = []
+		lst.extend(to_list(env['SCALAC']))
+		lst.extend(['-classpath'])
+		lst.extend(to_list(env['CLASSPATH']))
+		lst.extend(['-d'])
+		lst.extend(to_list(env['OUTDIR']))
+		lst.extend(to_list(env['SCALACFLAGS']))
+		lst.extend([a.abspath() for a in self.inputs])
+		lst = [x for x in lst if x]
+		try:
+			self.out = self.generator.bld.cmd_and_log(lst, cwd=wd, env=env.env or None, output=0, quiet=0)[1]
+		except:
+			self.generator.bld.cmd_and_log(lst, cwd=wd, env=env.env or None)
+
+def configure(self):
+	"""
+	Detect the scalac program
+	"""
+	# If SCALA_HOME is set, we prepend it to the path list
+	java_path = self.environ['PATH'].split(os.pathsep)
+	v = self.env
+
+	if 'SCALA_HOME' in self.environ:
+		java_path = [os.path.join(self.environ['SCALA_HOME'], 'bin')] + java_path
+		self.env['SCALA_HOME'] = [self.environ['SCALA_HOME']]
+
+	for x in 'scalac scala'.split():
+		self.find_program(x, var=x.upper(), path_list=java_path)
+
+	if 'CLASSPATH' in self.environ:
+		v['CLASSPATH'] = self.environ['CLASSPATH']
+
+	v.SCALACFLAGS = ['-verbose']
+	if not v['SCALAC']:
+		self.fatal('scalac is required for compiling scala classes')
+
diff --git a/waflib/extras/slow_qt4.py b/waflib/extras/slow_qt4.py
new file mode 100644
index 00000000..ec7880bf
--- /dev/null
+++ b/waflib/extras/slow_qt4.py
@@ -0,0 +1,96 @@
+#! /usr/bin/env python
+# Thomas Nagy, 2011 (ita)
+
+"""
+Create _moc.cpp files
+
+The builds are 30-40% faster when .moc files are included,
+you should NOT use this tool. If you really
+really want it:
+
+def configure(conf):
+	conf.load('compiler_cxx qt4')
+	conf.load('slow_qt4')
+
+See playground/slow_qt/wscript for a complete example.
+"""
+
+from waflib.TaskGen import extension
+from waflib import Task
+import waflib.Tools.qt4
+import waflib.Tools.cxx
+
+@extension(*waflib.Tools.qt4.EXT_QT4)
+def cxx_hook(self, node):
+	return self.create_compiled_task('cxx_qt', node)
+
+class cxx_qt(Task.classes['cxx']):
+	def runnable_status(self):
+		ret = Task.classes['cxx'].runnable_status(self)
+		if ret != Task.ASK_LATER and not getattr(self, 'moc_done', None):
+
+			try:
+				cache = self.generator.moc_cache
+			except AttributeError:
+				cache = self.generator.moc_cache = {}
+
+			deps = self.generator.bld.node_deps[self.uid()]
+			for x in [self.inputs[0]] + deps:
+				if x.read().find('Q_OBJECT') > 0:
+
+					# process "foo.h -> foo.moc" only if "foo.cpp" is in the sources for the current task generator
+					# this code will work because it is in the main thread (runnable_status)
+					if x.name.rfind('.') > -1: # a .h file...
+						name = x.name[:x.name.rfind('.')]
+						for tsk in self.generator.compiled_tasks:
+							if tsk.inputs and tsk.inputs[0].name.startswith(name):
+								break
+						else:
+							# no corresponding file, continue
+							continue
+
+					# the file foo.cpp could be compiled for a static and a shared library - hence the %number in the name
+					cxx_node = x.parent.get_bld().make_node(x.name.replace('.', '_') + '_%d_moc.cpp' % self.generator.idx)
+					if cxx_node in cache:
+						continue
+					cache[cxx_node] = self
+
+					tsk = Task.classes['moc'](env=self.env, generator=self.generator)
+					tsk.set_inputs(x)
+					tsk.set_outputs(cxx_node)
+
+					if x.name.endswith('.cpp'):
+						# moc is trying to be too smart but it is too dumb:
+						# why forcing the #include when Q_OBJECT is in the cpp file?
+						gen = self.generator.bld.producer
+						gen.outstanding.append(tsk)
+						gen.total += 1
+						self.set_run_after(tsk)
+					else:
+						cxxtsk = Task.classes['cxx'](env=self.env, generator=self.generator)
+						cxxtsk.set_inputs(tsk.outputs)
+						cxxtsk.set_outputs(cxx_node.change_ext('.o'))
+						cxxtsk.set_run_after(tsk)
+
+						try:
+							self.more_tasks.extend([tsk, cxxtsk])
+						except AttributeError:
+							self.more_tasks = [tsk, cxxtsk]
+
+						try:
+							link = self.generator.link_task
+						except AttributeError:
+							pass
+						else:
+							link.set_run_after(cxxtsk)
+							link.inputs.extend(cxxtsk.outputs)
+							link.inputs.sort(key=lambda x: x.abspath())
+
+			self.moc_done = True
+
+		for t in self.run_after:
+			if not t.hasrun:
+				return Task.ASK_LATER
+
+		return ret
+
diff --git a/waflib/extras/softlink_libs.py b/waflib/extras/softlink_libs.py
new file mode 100644
index 00000000..50c777f2
--- /dev/null
+++ b/waflib/extras/softlink_libs.py
@@ -0,0 +1,76 @@
+#! /usr/bin/env python
+# per rosengren 2011
+
+from waflib.TaskGen import feature, after_method
+from waflib.Task import Task, always_run
+from os.path import basename, isabs
+from os import tmpfile, linesep
+
+def options(opt):
+	grp = opt.add_option_group('Softlink Libraries Options')
+	grp.add_option('--exclude', default='/usr/lib,/lib', help='No symbolic links are created for libs within [%default]')
+
+def configure(cnf):
+	cnf.find_program('ldd')
+	if not cnf.env.SOFTLINK_EXCLUDE:
+		cnf.env.SOFTLINK_EXCLUDE = cnf.options.exclude.split(',')
+
+@feature('softlink_libs')
+@after_method('process_rule')
+def add_finder(self):
+	tgt = self.path.find_or_declare(self.target)
+	self.create_task('sll_finder', tgt=tgt)
+	self.create_task('sll_installer', tgt=tgt)
+	always_run(sll_installer)
+
+class sll_finder(Task):
+	ext_out = 'softlink_libs'
+	def run(self):
+		bld = self.generator.bld
+		linked=[]
+		target_paths = []
+		for g in bld.groups:
+			for tgen in g:
+				# FIXME it might be better to check if there is a link_task (getattr?)
+				target_paths += [tgen.path.get_bld().bldpath()]
+				linked += [t.outputs[0].bldpath()
+					for t in getattr(tgen, 'tasks', [])
+					if t.__class__.__name__ in
+					['cprogram', 'cshlib', 'cxxprogram', 'cxxshlib']]
+		lib_list = []
+		if len(linked):
+			cmd = [self.env.LDD] + linked
+			# FIXME add DYLD_LIBRARY_PATH+PATH for osx+win32
+			ldd_env = {'LD_LIBRARY_PATH': ':'.join(target_paths + self.env.LIBPATH)}
+			# FIXME the with syntax will not work in python 2
+			with tmpfile() as result:
+				self.exec_command(cmd, env=ldd_env, stdout=result)
+				result.seek(0)
+				for line in result.readlines():
+					words = line.split()
+					if len(words) < 3 or words[1] != '=>':
+						continue
+					lib = words[2]
+					if lib == 'not':
+						continue
+					if any([lib.startswith(p) for p in
+							[bld.bldnode.abspath(), '('] +
+							self.env.SOFTLINK_EXCLUDE]):
+						continue
+					if not isabs(lib):
+						continue
+					lib_list.append(lib)
+			lib_list = sorted(set(lib_list))
+		self.outputs[0].write(linesep.join(lib_list + self.env.DYNAMIC_LIBS))
+		return 0
+
+class sll_installer(Task):
+	ext_in = 'softlink_libs'
+	def run(self):
+		tgt = self.outputs[0]
+		self.generator.bld.install_files('${LIBDIR}', tgt, postpone=False)
+		lib_list=tgt.read().split()
+		for lib in lib_list:
+			self.generator.bld.symlink_as('${LIBDIR}/'+basename(lib), lib, postpone=False)
+		return 0
+
diff --git a/waflib/extras/stale.py b/waflib/extras/stale.py
new file mode 100644
index 00000000..cac3f469
--- /dev/null
+++ b/waflib/extras/stale.py
@@ -0,0 +1,98 @@
+#! /usr/bin/env python
+# encoding: UTF-8
+# Thomas Nagy, 2006-2015 (ita)
+
+"""
+Add a pre-build hook to remove build files (declared in the system)
+that do not have a corresponding target
+
+This can be used for example to remove the targets
+that have changed name without performing
+a full 'waf clean'
+
+Of course, it will only work if there are no dynamically generated
+nodes/tasks, in which case the method will have to be modified
+to exclude some folders for example.
+
+Make sure to set bld.post_mode = waflib.Build.POST_AT_ONCE
+"""
+
+from waflib import Logs, Build
+from waflib.Runner import Parallel
+
+DYNAMIC_EXT = [] # add your non-cleanable files/extensions here
+MOC_H_EXTS = '.cpp .cxx .hpp .hxx .h'.split()
+
+def can_delete(node):
+	"""Imperfect moc cleanup which does not look for a Q_OBJECT macro in the files"""
+	if not node.name.endswith('.moc'):
+		return True
+	base = node.name[:-4]
+	p1 = node.parent.get_src()
+	p2 = node.parent.get_bld()
+	for k in MOC_H_EXTS:
+		h_name = base + k
+		n = p1.search_node(h_name)
+		if n:
+			return False
+		n = p2.search_node(h_name)
+		if n:
+			return False
+
+		# foo.cpp.moc, foo.h.moc, etc.
+		if base.endswith(k):
+			return False
+
+	return True
+
+# recursion over the nodes to find the stale files
+def stale_rec(node, nodes):
+	if node.abspath() in node.ctx.env[Build.CFG_FILES]:
+		return
+
+	if getattr(node, 'children', []):
+		for x in node.children.values():
+			if x.name != "c4che":
+				stale_rec(x, nodes)
+	else:
+		for ext in DYNAMIC_EXT:
+			if node.name.endswith(ext):
+				break
+		else:
+			if not node in nodes:
+				if can_delete(node):
+					Logs.warn('Removing stale file -> %r', node)
+					node.delete()
+
+old = Parallel.refill_task_list
+def refill_task_list(self):
+	iit = old(self)
+	bld = self.bld
+
+	# execute this operation only once
+	if getattr(self, 'stale_done', False):
+		return iit
+	self.stale_done = True
+
+	# this does not work in partial builds
+	if bld.targets != '*':
+		return iit
+
+	# this does not work in dynamic builds
+	if getattr(bld, 'post_mode') == Build.POST_AT_ONCE:
+		return iit
+
+	# obtain the nodes to use during the build
+	nodes = []
+	for tasks in bld.groups:
+		for x in tasks:
+			try:
+				nodes.extend(x.outputs)
+			except AttributeError:
+				pass
+
+	stale_rec(bld.bldnode, nodes)
+	return iit
+
+Parallel.refill_task_list = refill_task_list
+
diff --git a/waflib/extras/stracedeps.py b/waflib/extras/stracedeps.py
new file mode 100644
index 00000000..37d82cbb
--- /dev/null
+++ b/waflib/extras/stracedeps.py
@@ -0,0 +1,174 @@
+#!/usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2015 (ita)
+
+"""
+Execute tasks through strace to obtain dependencies after the process is run. This
+scheme is similar to that of the Fabricate script.
+
+To use::
+
+  def configure(conf):
+     conf.load('strace')
+
+WARNING:
+* This will not work when advanced scanners are needed (qt4/qt5)
+* The overhead of running 'strace' is significant (56s -> 1m29s)
+* It will not work on Windows :-)
+"""
+
+import os, re, threading
+from waflib import Task, Logs, Utils
+
+#TRACECALLS = 'trace=access,chdir,clone,creat,execve,exit_group,fork,lstat,lstat64,mkdir,open,rename,stat,stat64,symlink,vfork'
+TRACECALLS = 'trace=process,file'
+
+BANNED = ('/tmp', '/proc', '/sys', '/dev')
+
+s_process = r'(?:clone|fork|vfork)\(.*?(?P<npid>\d+)'
+s_file = r'(?P<call>\w+)\("(?P<path>([^"\\]|\\.)*)"(.*)'
+re_lines = re.compile(r'^(?P<pid>\d+)\s+(?:(?:%s)|(?:%s))\r*$' % (s_file, s_process), re.IGNORECASE | re.MULTILINE)
+strace_lock = threading.Lock()
+
+def configure(conf):
+	conf.find_program('strace')
+
+def task_method(func):
+	# Decorator function to bind/replace methods on the base Task class
+	#
+	# The methods Task.exec_command and Task.sig_implicit_deps already exists and are rarely overridden
+	# we thus expect that we are the only ones doing this
+	try:
+		setattr(Task.Task, 'nostrace_%s' % func.__name__, getattr(Task.Task, func.__name__))
+	except AttributeError:
+		pass
+	setattr(Task.Task, func.__name__, func)
+	return func
+
+@task_method
+def get_strace_file(self):
+	try:
+		return self.strace_file
+	except AttributeError:
+		pass
+
+	if self.outputs:
+		ret = self.outputs[0].abspath() + '.strace'
+	else:
+		ret = '%s%s%d%s' % (self.generator.bld.bldnode.abspath(), os.sep, id(self), '.strace')
+	self.strace_file = ret
+	return ret
+
+@task_method
+def get_strace_args(self):
+	return (self.env.STRACE or ['strace']) + ['-e', TRACECALLS, '-f', '-o', self.get_strace_file()]
+
+@task_method
+def exec_command(self, cmd, **kw):
+	bld = self.generator.bld
+	if not 'cwd' in kw:
+		kw['cwd'] = self.get_cwd()
+
+	args = self.get_strace_args()
+	fname = self.get_strace_file()
+	if isinstance(cmd, list):
+		cmd = args + cmd
+	else:
+		cmd = '%s %s' % (' '.join(args), cmd)
+
+	try:
+		ret = bld.exec_command(cmd, **kw)
+	finally:
+		if not ret:
+			self.parse_strace_deps(fname, kw['cwd'])
+	return ret
+
+@task_method
+def sig_implicit_deps(self):
+	# bypass the scanner functions
+	return
+
+@task_method
+def parse_strace_deps(self, path, cwd):
+	# uncomment the following line to disable the dependencies and force a file scan
+	# return
+	try:
+		cnt = Utils.readf(path)
+	finally:
+		try:
+			os.remove(path)
+		except OSError:
+			pass
+
+	if not isinstance(cwd, str):
+		cwd = cwd.abspath()
+
+	nodes = []
+	bld = self.generator.bld
+	try:
+		cache = bld.strace_cache
+	except AttributeError:
+		cache = bld.strace_cache = {}
+
+	# chdir and relative paths
+	pid_to_cwd = {}
+
+	global BANNED
+	done = set()
+	for m in re.finditer(re_lines, cnt):
+		# scraping the output of strace
+		pid = m.group('pid')
+		if m.group('npid'):
+			npid = m.group('npid')
+			pid_to_cwd[npid] = pid_to_cwd.get(pid, cwd)
+			continue
+
+		p = m.group('path').replace('\\"', '"')
+
+		if p == '.' or m.group().find('= -1 ENOENT') > -1:
+			# just to speed it up a bit
+			continue
+
+		if not os.path.isabs(p):
+			p = os.path.join(pid_to_cwd.get(pid, cwd), p)
+
+		call = m.group('call')
+		if call == 'chdir':
+			pid_to_cwd[pid] = p
+			continue
+
+		if p in done:
+			continue
+		done.add(p)
+
+		for x in BANNED:
+			if p.startswith(x):
+				break
+		else:
+			if p.endswith('/') or os.path.isdir(p):
+				continue
+
+			try:
+				node = cache[p]
+			except KeyError:
+				strace_lock.acquire()
+				try:
+					cache[p] = node = bld.root.find_node(p)
+					if not node:
+						continue
+				finally:
+					strace_lock.release()
+			nodes.append(node)
+
+	# record the dependencies then force the task signature recalculation for next time
+	if Logs.verbose:
+		Logs.debug('deps: real scanner for %r returned %r', self, nodes)
+	bld = self.generator.bld
+	bld.node_deps[self.uid()] = nodes
+	bld.raw_deps[self.uid()] = []
+	try:
+		del self.cache_sig
+	except AttributeError:
+		pass
+	self.signature()
+
diff --git a/waflib/extras/swig.py b/waflib/extras/swig.py
new file mode 100644
index 00000000..fd3d6d2c
--- /dev/null
+++ b/waflib/extras/swig.py
@@ -0,0 +1,237 @@
+#! /usr/bin/env python
+# encoding: UTF-8
+# Petar Forai
+# Thomas Nagy 2008-2010 (ita)
+
+import re
+from waflib import Task, Logs
+from waflib.TaskGen import extension, feature, after_method
+from waflib.Configure import conf
+from waflib.Tools import c_preproc
+
+"""
+tasks have to be added dynamically:
+- swig interface files may be created at runtime
+- the module name may be unknown in advance
+"""
+
+SWIG_EXTS = ['.swig', '.i']
+
+re_module = re.compile('%module(?:\s*\(.*\))?\s+(.+)', re.M)
+
+re_1 = re.compile(r'^%module.*?\s+([\w]+)\s*?$', re.M)
+re_2 = re.compile('[#%]include [<"](.*)[">]', re.M)
+
+class swig(Task.Task):
+	color   = 'BLUE'
+	run_str = '${SWIG} ${SWIGFLAGS} ${SWIGPATH_ST:INCPATHS} ${SWIGDEF_ST:DEFINES} ${SRC}'
+	ext_out = ['.h'] # might produce .h files although it is not mandatory
+	vars = ['SWIG_VERSION', 'SWIGDEPS']
+
+	def runnable_status(self):
+		for t in self.run_after:
+			if not t.hasrun:
+				return Task.ASK_LATER
+
+		if not getattr(self, 'init_outputs', None):
+			self.init_outputs = True
+			if not getattr(self, 'module', None):
+				# search the module name
+				txt = self.inputs[0].read()
+				m = re_module.search(txt)
+				if not m:
+					raise ValueError("could not find the swig module name")
+				self.module = m.group(1)
+
+			swig_c(self)
+
+			# add the language-specific output files as nodes
+			# call funs in the dict swig_langs
+			for x in self.env['SWIGFLAGS']:
+				# obtain the language
+				x = x[1:]
+				try:
+					fun = swig_langs[x]
+				except KeyError:
+					pass
+				else:
+					fun(self)
+
+		return super(swig, self).runnable_status()
+
+	def scan(self):
+		"scan for swig dependencies, climb the .i files"
+		lst_src = []
+
+		seen = []
+		missing = []
+		to_see = [self.inputs[0]]
+
+		while to_see:
+			node = to_see.pop(0)
+			if node in seen:
+				continue
+			seen.append(node)
+			lst_src.append(node)
+
+			# read the file
+			code = node.read()
+			code = c_preproc.re_nl.sub('', code)
+			code = c_preproc.re_cpp.sub(c_preproc.repl, code)
+
+			# find .i files and project headers
+			names = re_2.findall(code)
+			for n in names:
+				for d in self.generator.includes_nodes + [node.parent]:
+					u = d.find_resource(n)
+					if u:
+						to_see.append(u)
+						break
+				else:
+					missing.append(n)
+		return (lst_src, missing)
+
+# provide additional language processing
+swig_langs = {}
+def swigf(fun):
+	swig_langs[fun.__name__.replace('swig_', '')] = fun
+	return fun
+swig.swigf = swigf
+
+def swig_c(self):
+	ext = '.swigwrap_%d.c' % self.generator.idx
+	flags = self.env['SWIGFLAGS']
+	if '-c++' in flags:
+		ext += 'xx'
+	out_node = self.inputs[0].parent.find_or_declare(self.module + ext)
+
+	if '-c++' in flags:
+		c_tsk = self.generator.cxx_hook(out_node)
+	else:
+		c_tsk = self.generator.c_hook(out_node)
+
+	c_tsk.set_run_after(self)
+
+	# transfer weights from swig task to c task
+	if getattr(self, 'weight', None):
+		c_tsk.weight = self.weight
+	if getattr(self, 'tree_weight', None):
+		c_tsk.tree_weight = self.tree_weight
+
+	try:
+		self.more_tasks.append(c_tsk)
+	except AttributeError:
+		self.more_tasks = [c_tsk]
+
+	try:
+		ltask = self.generator.link_task
+	except AttributeError:
+		pass
+	else:
+		ltask.set_run_after(c_tsk)
+		# setting input nodes does not declare the build order
+		# because the build already started, but it sets
+		# the dependency to enable rebuilds
+		ltask.inputs.append(c_tsk.outputs[0])
+
+	self.outputs.append(out_node)
+
+	if not '-o' in self.env['SWIGFLAGS']:
+		self.env.append_value('SWIGFLAGS', ['-o', self.outputs[0].abspath()])
+
+@swigf
+def swig_python(tsk):
+	node = tsk.inputs[0].parent
+	if tsk.outdir:
+		node = tsk.outdir
+	tsk.set_outputs(node.find_or_declare(tsk.module+'.py'))
+
+@swigf
+def swig_ocaml(tsk):
+	node = tsk.inputs[0].parent
+	if tsk.outdir:
+		node = tsk.outdir
+	tsk.set_outputs(node.find_or_declare(tsk.module+'.ml'))
+	tsk.set_outputs(node.find_or_declare(tsk.module+'.mli'))
+
+@extension(*SWIG_EXTS)
+def i_file(self, node):
+	# the task instance
+	tsk = self.create_task('swig')
+	tsk.set_inputs(node)
+	tsk.module = getattr(self, 'swig_module', None)
+
+	flags = self.to_list(getattr(self, 'swig_flags', []))
+	tsk.env.append_value('SWIGFLAGS', flags)
+
+	tsk.outdir = None
+	if '-outdir' in flags:
+		outdir = flags[flags.index('-outdir')+1]
+		outdir = tsk.generator.bld.bldnode.make_node(outdir)
+		outdir.mkdir()
+		tsk.outdir = outdir
+
+@feature('c', 'cxx', 'd', 'fc', 'asm')
+@after_method('apply_link', 'process_source')
+def enforce_swig_before_link(self):
+	try:
+		link_task = self.link_task
+	except AttributeError:
+		pass
+	else:
+		for x in self.tasks:
+			if x.__class__.__name__ == 'swig':
+				link_task.run_after.add(x)
+
+@conf
+def check_swig_version(conf, minver=None):
+	"""
+	Check if the swig tool is found matching a given minimum version.
+	minver should be a tuple, eg. to check for swig >= 1.3.28 pass (1,3,28) as minver.
+
+	If successful, SWIG_VERSION is defined as 'MAJOR.MINOR'
+	(eg. '1.3') of the actual swig version found.
+
+	:param minver: minimum version
+	:type minver: tuple of int
+	:return: swig version
+	:rtype: tuple of int
+	"""
+	assert minver is None or isinstance(minver, tuple)
+	swigbin = conf.env['SWIG']
+	if not swigbin:
+		conf.fatal('could not find the swig executable')
+
+	# Get swig version string
+	cmd = swigbin + ['-version']
+	Logs.debug('swig: Running swig command %r', cmd)
+	reg_swig = re.compile(r'SWIG Version\s(.*)', re.M)
+	swig_out = conf.cmd_and_log(cmd)
+	swigver_tuple = tuple([int(s) for s in reg_swig.findall(swig_out)[0].split('.')])
+
+	# Compare swig version with the minimum required
+	result = (minver is None) or (swigver_tuple >= minver)
+
+	if result:
+		# Define useful environment variables
+		swigver = '.'.join([str(x) for x in swigver_tuple[:2]])
+		conf.env['SWIG_VERSION'] = swigver
+
+	# Feedback
+	swigver_full = '.'.join(map(str, swigver_tuple[:3]))
+	if minver is None:
+		conf.msg('Checking for swig version', swigver_full)
+	else:
+		minver_str = '.'.join(map(str, minver))
+		conf.msg('Checking for swig version >= %s' % (minver_str,), swigver_full, color=result and 'GREEN' or 'YELLOW')
+
+	if not result:
+		conf.fatal('The swig version is too old, expecting %r' % (minver,))
+
+	return swigver_tuple
+
+def configure(conf):
+	conf.find_program('swig', var='SWIG')
+	conf.env.SWIGPATH_ST = '-I%s'
+	conf.env.SWIGDEF_ST = '-D%s'
+
diff --git a/waflib/extras/syms.py b/waflib/extras/syms.py
new file mode 100644
index 00000000..dfa00593
--- /dev/null
+++ b/waflib/extras/syms.py
@@ -0,0 +1,84 @@
+#! /usr/bin/env python
+# encoding: utf-8
+
+"""
+This tool supports the export_symbols_regex to export the symbols in a shared library.
+by default, all symbols are exported by gcc, and nothing by msvc.
+to use the tool, do something like:
+
+def build(ctx):
+	ctx(features='c cshlib syms', source='a.c b.c', export_symbols_regex='mylib_.*', target='testlib')
+
+only the symbols starting with 'mylib_' will be exported.
+"""
+
+import re
+from waflib.Context import STDOUT
+from waflib.Task import Task
+from waflib.Errors import WafError
+from waflib.TaskGen import feature, after_method
+
+class gen_sym(Task):
+	def run(self):
+		obj = self.inputs[0]
+		kw = {}
+
+		reg = getattr(self.generator, 'export_symbols_regex', '.+?')
+		if 'msvc' in (self.env.CC_NAME, self.env.CXX_NAME):
+			re_nm = re.compile(r'External\s+\|\s+_(?P<symbol>%s)\b' % reg)
+			cmd = (self.env.DUMPBIN or ['dumpbin']) + ['/symbols', obj.abspath()]
+		else:
+			if self.env.DEST_BINFMT == 'pe': #gcc uses nm, and has a preceding _ on windows
+				re_nm = re.compile(r'(T|D)\s+_(?P<symbol>%s)\b' % reg)
+			elif self.env.DEST_BINFMT=='mac-o':
+				re_nm=re.compile(r'(T|D)\s+(?P<symbol>_?%s)\b' % reg)
+			else:
+				re_nm = re.compile(r'(T|D)\s+(?P<symbol>%s)\b' % reg)
+			cmd = (self.env.NM or ['nm']) + ['-g', obj.abspath()]
+		syms = [m.group('symbol') for m in re_nm.finditer(self.generator.bld.cmd_and_log(cmd, quiet=STDOUT, **kw))]
+		self.outputs[0].write('%r' % syms)
+
+class compile_sym(Task):
+	def run(self):
+		syms = {}
+		for x in self.inputs:
+			slist = eval(x.read())
+			for s in slist:
+				syms[s] = 1
+		lsyms = list(syms.keys())
+		lsyms.sort()
+		if self.env.DEST_BINFMT == 'pe':
+			self.outputs[0].write('EXPORTS\n' + '\n'.join(lsyms))
+		elif self.env.DEST_BINFMT == 'elf':
+			self.outputs[0].write('{ global:\n' + ';\n'.join(lsyms) + ";\nlocal: *; };\n")
+		elif self.env.DEST_BINFMT=='mac-o':
+			self.outputs[0].write('\n'.join(lsyms) + '\n')
+		else:
+			raise WafError('NotImplemented')
+
+@feature('syms')
+@after_method('process_source', 'process_use', 'apply_link', 'process_uselib_local', 'propagate_uselib_vars')
+def do_the_symbol_stuff(self):
+	def_node = self.path.find_or_declare(getattr(self, 'sym_file', self.target + '.def'))
+	compiled_tasks = getattr(self, 'compiled_tasks', None)
+	if compiled_tasks:
+		ins = [x.outputs[0] for x in compiled_tasks]
+		self.gen_sym_tasks = [self.create_task('gen_sym', x, x.change_ext('.%d.sym' % self.idx)) for x in ins]
+		self.create_task('compile_sym', [x.outputs[0] for x in self.gen_sym_tasks], def_node)
+
+	link_task = getattr(self, 'link_task', None)
+	if link_task:
+		self.link_task.dep_nodes.append(def_node)
+
+		if 'msvc' in (self.env.CC_NAME, self.env.CXX_NAME):
+			self.link_task.env.append_value('LINKFLAGS', ['/def:' + def_node.bldpath()])
+		elif self.env.DEST_BINFMT == 'pe':
+			# gcc on windows takes *.def as an additional input
+			self.link_task.inputs.append(def_node)
+		elif self.env.DEST_BINFMT == 'elf':
+			self.link_task.env.append_value('LINKFLAGS', ['-Wl,-version-script', '-Wl,' + def_node.bldpath()])
+		elif self.env.DEST_BINFMT=='mac-o':
+			self.link_task.env.append_value('LINKFLAGS',['-Wl,-exported_symbols_list,' + def_node.bldpath()])
+		else:
+			raise WafError('NotImplemented')
+
diff --git a/waflib/extras/ticgt.py b/waflib/extras/ticgt.py
new file mode 100644
index 00000000..f43a7ea5
--- /dev/null
+++ b/waflib/extras/ticgt.py
@@ -0,0 +1,300 @@
+#!/usr/bin/env python
+# encoding: utf-8
+
+# Texas Instruments code generator support (experimental)
+# When reporting issues, please directly assign the bug to the maintainer.
+
+__author__ = __maintainer__ = "Jérôme Carretero <cJ-waf@zougloub.eu>"
+__copyright__ = "Jérôme Carretero, 2012"
+
+"""
+TI cgt6x is a compiler suite for TI DSPs.
+
+The toolchain does pretty weird things, and I'm sure I'm missing some of them.
+But still, the tool saves time.
+
+What this tool does is:
+
+- create a TI compiler environment
+- create TI compiler features, to handle some specifics about this compiler
+  It has a few idiosyncracies, such as not giving the liberty of the .o file names
+- automatically activate them when using the TI compiler
+- handle the tconf tool
+  The tool
+
+TODO:
+
+- the set_platform_flags() function is not nice
+- more tests
+- broaden tool scope, if needed
+
+"""
+
+import os, re
+
+from waflib import Options, Utils, Task, TaskGen
+from waflib.Tools import c, ccroot, c_preproc
+from waflib.Configure import conf
+from waflib.TaskGen import feature, before_method
+from waflib.Tools.c import cprogram
+
+opj = os.path.join
+
+@conf
+def find_ticc(conf):
+	conf.find_program(['cl6x'], var='CC', path_list=opj(getattr(Options.options, 'ti-cgt-dir', ""), 'bin'))
+	conf.env.CC_NAME = 'ticc'
+
+@conf
+def find_tild(conf):
+	conf.find_program(['lnk6x'], var='LINK_CC', path_list=opj(getattr(Options.options, 'ti-cgt-dir', ""), 'bin'))
+	conf.env.LINK_CC_NAME = 'tild'
+
+@conf
+def find_tiar(conf):
+	conf.find_program(['ar6x'], var='AR', path_list=opj(getattr(Options.options, 'ti-cgt-dir', ""), 'bin'))
+	conf.env.AR_NAME = 'tiar'
+	conf.env.ARFLAGS = 'qru'
+
+@conf
+def ticc_common_flags(conf):
+	v = conf.env
+
+	if not v['LINK_CC']:
+		v['LINK_CC'] = v['CC']
+	v['CCLNK_SRC_F']	 = []
+	v['CCLNK_TGT_F']	 = ['-o']
+	v['CPPPATH_ST']	  = '-I%s'
+	v['DEFINES_ST']	  = '-d%s'
+
+	v['LIB_ST']	      = '-l%s' # template for adding libs
+	v['LIBPATH_ST']	  = '-i%s' # template for adding libpaths
+	v['STLIB_ST']	    = '-l=%s.lib'
+	v['STLIBPATH_ST']	= '-i%s'
+
+	# program
+	v['cprogram_PATTERN']    = '%s.out'
+
+	# static lib
+	#v['LINKFLAGS_cstlib']    = ['-Wl,-Bstatic']
+	v['cstlib_PATTERN']      = '%s.lib'
+
+def configure(conf):
+	v = conf.env
+	v.TI_CGT_DIR = getattr(Options.options, 'ti-cgt-dir', "")
+	v.TI_DSPLINK_DIR = getattr(Options.options, 'ti-dsplink-dir', "")
+	v.TI_BIOSUTILS_DIR = getattr(Options.options, 'ti-biosutils-dir', "")
+	v.TI_DSPBIOS_DIR = getattr(Options.options, 'ti-dspbios-dir', "")
+	v.TI_XDCTOOLS_DIR = getattr(Options.options, 'ti-xdctools-dir', "")
+	conf.find_ticc()
+	conf.find_tiar()
+	conf.find_tild()
+	conf.ticc_common_flags()
+	conf.cc_load_tools()
+	conf.cc_add_flags()
+	conf.link_add_flags()
+	conf.find_program(['tconf'], var='TCONF', path_list=v.TI_XDCTOOLS_DIR)
+
+	conf.env.TCONF_INCLUDES += [
+	 opj(conf.env.TI_DSPBIOS_DIR, 'packages'),
+	]
+
+	conf.env.INCLUDES += [
+	 opj(conf.env.TI_CGT_DIR, 'include'),
+	]
+
+	conf.env.LIBPATH += [
+	 opj(conf.env.TI_CGT_DIR, "lib"),
+	]
+
+	conf.env.INCLUDES_DSPBIOS += [
+	 opj(conf.env.TI_DSPBIOS_DIR, 'packages', 'ti', 'bios', 'include'),
+	]
+
+	conf.env.LIBPATH_DSPBIOS += [
+	 opj(conf.env.TI_DSPBIOS_DIR, 'packages', 'ti', 'bios', 'lib'),
+	]
+
+	conf.env.INCLUDES_DSPLINK += [
+	 opj(conf.env.TI_DSPLINK_DIR, 'dsplink', 'dsp', 'inc'),
+	]
+
+@conf
+def ti_set_debug(cfg, debug=1):
+	"""
+	Sets debug flags for the compiler.
+
+	TODO:
+	- for each TI CFLAG/INCLUDES/LINKFLAGS/LIBPATH replace RELEASE by DEBUG
+	- -g --no_compress
+	"""
+	if debug:
+		cfg.env.CFLAGS += "-d_DEBUG -dDEBUG -dDDSP_DEBUG".split()
+
+@conf
+def ti_dsplink_set_platform_flags(cfg, splat, dsp, dspbios_ver, board):
+	"""
+	Sets the INCLUDES, LINKFLAGS for DSPLINK and TCONF_INCLUDES
+	For the specific hardware.
+
+	Assumes that DSPLINK was built in its own folder.
+
+	:param splat: short platform name (eg. OMAPL138)
+	:param dsp: DSP name (eg. 674X)
+	:param dspbios_ver: string identifying DspBios version (eg. 5.XX)
+	:param board: board name (eg. OMAPL138GEM)
+
+	"""
+	d1 = opj(cfg.env.TI_DSPLINK_DIR, 'dsplink', 'dsp', 'inc', 'DspBios', dspbios_ver)
+	d = opj(cfg.env.TI_DSPLINK_DIR, 'dsplink', 'dsp', 'inc', 'DspBios', dspbios_ver, board)
+	cfg.env.TCONF_INCLUDES += [d1, d]
+	cfg.env.INCLUDES_DSPLINK += [
+	 opj(cfg.env.TI_DSPLINK_DIR, 'dsplink', 'dsp', 'inc', dsp),
+	 d,
+	]
+
+	cfg.env.LINKFLAGS_DSPLINK += [
+	 opj(cfg.env.TI_DSPLINK_DIR, 'dsplink', 'dsp', 'export', 'BIN', 'DspBios', splat, board+'_0', 'RELEASE', 'dsplink%s.lib' % x)
+	 for x in ('', 'pool', 'mpcs', 'mplist', 'msg', 'data', 'notify', 'ringio')
+	]
+
+
+def options(opt):
+	opt.add_option('--with-ti-cgt', type='string', dest='ti-cgt-dir', help = 'Specify alternate cgt root folder', default="")
+	opt.add_option('--with-ti-biosutils', type='string', dest='ti-biosutils-dir', help = 'Specify alternate biosutils folder', default="")
+	opt.add_option('--with-ti-dspbios', type='string', dest='ti-dspbios-dir', help = 'Specify alternate dspbios folder', default="")
+	opt.add_option('--with-ti-dsplink', type='string', dest='ti-dsplink-dir', help = 'Specify alternate dsplink folder', default="")
+	opt.add_option('--with-ti-xdctools', type='string', dest='ti-xdctools-dir', help = 'Specify alternate xdctools folder', default="")
+
+class ti_cprogram(cprogram):
+	"""
+	Link object files into a c program
+	
+	Changes:
+
+	- the linked executable to have a relative path (because we can)
+	- put the LIBPATH first
+	"""
+	run_str = '${LINK_CC} ${LIBPATH_ST:LIBPATH} ${LIB_ST:LIB} ${LINKFLAGS} ${CCLNK_SRC_F}${SRC} ${CCLNK_TGT_F}${TGT[0].bldpath()} ${RPATH_ST:RPATH} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${FRAMEWORK_ST:FRAMEWORK} ${ARCH_ST:ARCH} ${STLIB_MARKER} ${STLIBPATH_ST:STLIBPATH} ${STLIB_ST:STLIB} ${SHLIB_MARKER} '
+
+@feature("c")
+@before_method('apply_link')
+def use_ti_cprogram(self):
+	"""
+	Automatically uses ti_cprogram link process
+	"""
+	if 'cprogram' in self.features and self.env.CC_NAME == 'ticc':
+		self.features.insert(0, "ti_cprogram")
+
+class ti_c(Task.Task):
+	"""
+	Compile task for the TI codegen compiler
+
+	This compiler does not allow specifying the output file name, only the output path.
+
+	"""
+	"Compile C files into object files"
+	run_str = '${CC} ${ARCH_ST:ARCH} ${CFLAGS} ${FRAMEWORKPATH_ST:FRAMEWORKPATH} ${CPPPATH_ST:INCPATHS} ${DEFINES_ST:DEFINES} ${SRC} -c ${OUT} ${CPPFLAGS}'
+	vars    = ['CCDEPS'] # unused variable to depend on, just in case
+	ext_in  = ['.h'] # set the build order easily by using ext_out=['.h']
+	scan    = c_preproc.scan
+
+def create_compiled_task(self, name, node):
+	"""
+	Overrides ccroot.create_compiled_task to support ti_c
+	"""
+	out = '%s' % (node.change_ext('.obj').name)
+	if self.env.CC_NAME == 'ticc':
+		name = 'ti_c'
+	task = self.create_task(name, node, node.parent.find_or_declare(out))
+	self.env.OUT = '-fr%s' % (node.parent.get_bld().abspath())
+	try:
+		self.compiled_tasks.append(task)
+	except AttributeError:
+		self.compiled_tasks = [task]
+	return task
+
+@TaskGen.extension('.c')
+def c_hook(self, node):
+	"Bind the c file extension to the creation of a :py:class:`waflib.Tools.c.c` instance"
+	if self.env.CC_NAME == 'ticc':
+		return create_compiled_task(self, 'ti_c', node)
+	else:
+		return self.create_compiled_task('c', node)
+
+
+@feature("ti-tconf")
+@before_method('process_source')
+def apply_tconf(self):
+	sources = [x.get_src() for x in self.to_nodes(self.source, path=self.path.get_src())]
+	node = sources[0]
+	assert(sources[0].name.endswith(".tcf"))
+	if len(sources) > 1:
+		assert(sources[1].name.endswith(".cmd"))
+
+	target = getattr(self, 'target', self.source)
+	target_node = node.get_bld().parent.find_or_declare(node.name)
+	
+	procid = "%d" % int(getattr(self, 'procid', 0))
+
+	importpaths = []
+	includes = Utils.to_list(getattr(self, 'includes', []))
+	for x in includes + self.env.TCONF_INCLUDES:
+		if x == os.path.abspath(x):
+			importpaths.append(x)
+		else:
+			relpath = self.path.find_node(x).path_from(target_node.parent)
+			importpaths.append(relpath)
+
+	task = self.create_task('ti_tconf', sources, target_node.change_ext('.cdb'))
+	task.path = self.path
+	task.includes = includes
+	task.cwd = target_node.parent.abspath()
+	task.env = self.env.derive()
+	task.env["TCONFSRC"] = node.path_from(target_node.parent)
+	task.env["TCONFINC"] = '-Dconfig.importPath=%s' % ";".join(importpaths)
+	task.env['TCONFPROGNAME'] = '-Dconfig.programName=%s' % target
+	task.env['PROCID'] = procid
+	task.outputs = [
+	 target_node.change_ext("cfg_c.c"),
+	 target_node.change_ext("cfg.s62"),
+	 target_node.change_ext("cfg.cmd"),
+	]
+
+	create_compiled_task(self, 'ti_c', task.outputs[1])
+	ctask = create_compiled_task(self, 'ti_c', task.outputs[0])
+	ctask.env = self.env.derive()
+
+	self.add_those_o_files(target_node.change_ext("cfg.cmd"))
+	if len(sources) > 1:
+		self.add_those_o_files(sources[1])
+	self.source = []
+
+re_tconf_include = re.compile(r'(?P<type>utils\.importFile)\("(?P<file>.*)"\)',re.M)
+class ti_tconf(Task.Task):
+	run_str = '${TCONF} ${TCONFINC} ${TCONFPROGNAME} ${TCONFSRC} ${PROCID}'
+	color   = 'PINK'
+
+	def scan(self):
+		includes = Utils.to_list(getattr(self, 'includes', []))
+
+		def deps(node):
+			nodes, names = [], []
+			if node:
+				code = Utils.readf(node.abspath())
+				for match in re_tconf_include.finditer(code):
+					path = match.group('file')
+					if path:
+						for x in includes:
+							filename = opj(x, path)
+							fi = self.path.find_resource(filename)
+							if fi:
+								subnodes, subnames = deps(fi)
+								nodes += subnodes
+								names += subnames
+								nodes.append(fi)
+								names.append(path)
+								break
+			return nodes, names
+		return deps(self.inputs[0])
+
diff --git a/waflib/extras/unity.py b/waflib/extras/unity.py
new file mode 100644
index 00000000..78128ed3
--- /dev/null
+++ b/waflib/extras/unity.py
@@ -0,0 +1,108 @@
+#! /usr/bin/env python
+# encoding: utf-8
+
+"""
+Compile whole groups of C/C++ files at once
+(C and C++ files are processed independently though).
+
+To enable globally::
+
+	def options(opt):
+		opt.load('compiler_cxx')
+	def build(bld):
+		bld.load('compiler_cxx unity')
+
+To enable for specific task generators only::
+
+	def build(bld):
+		bld(features='c cprogram unity', source='main.c', ...)
+
+The file order is often significant in such builds, so it can be
+necessary to adjust the order of source files and the batch sizes.
+To control the amount of files processed in a batch per target
+(the default is 50)::
+
+	def build(bld):
+		bld(features='c cprogram', unity_size=20)
+
+"""
+
+from waflib import Task, Options
+from waflib.Tools import c_preproc
+from waflib import TaskGen
+
+MAX_BATCH = 50
+
+EXTS_C = ('.c',)
+EXTS_CXX = ('.cpp','.cc','.cxx','.C','.c++')
+
+def options(opt):
+	global MAX_BATCH
+	opt.add_option('--batchsize', action='store', dest='batchsize', type='int', default=MAX_BATCH,
+		help='default unity batch size (0 disables unity builds)')
+
+@TaskGen.taskgen_method
+def batch_size(self):
+	default = getattr(Options.options, 'batchsize', MAX_BATCH)
+	if default < 1:
+		return 0
+	return getattr(self, 'unity_size', default)
+
+
+class unity(Task.Task):
+	color = 'BLUE'
+	scan = c_preproc.scan
+	def to_include(self, node):
+		ret = node.path_from(self.outputs[0].parent)
+		ret = ret.replace('\\', '\\\\').replace('"', '\\"')
+		return ret
+	def run(self):
+		lst = ['#include "%s"\n' % self.to_include(node) for node in self.inputs]
+		txt = ''.join(lst)
+		self.outputs[0].write(txt)
+	def __str__(self):
+		node = self.outputs[0]
+		return node.path_from(node.ctx.launch_node())
+
+def bind_unity(obj, cls_name, exts):
+	if not 'mappings' in obj.__dict__:
+		obj.mappings = dict(obj.mappings)
+
+	for j in exts:
+		fun = obj.mappings[j]
+		if fun.__name__ == 'unity_fun':
+			raise ValueError('Attempt to bind unity mappings multiple times %r' % j)
+
+		def unity_fun(self, node):
+			cnt = self.batch_size()
+			if cnt <= 1:
+				return fun(self, node)
+			x = getattr(self, 'master_%s' % cls_name, None)
+			if not x or len(x.inputs) >= cnt:
+				x = self.create_task('unity')
+				setattr(self, 'master_%s' % cls_name, x)
+
+				cnt_cur = getattr(self, 'cnt_%s' % cls_name, 0)
+				c_node = node.parent.find_or_declare('unity_%s_%d_%d.%s' % (self.idx, cnt_cur, cnt, cls_name))
+				x.outputs = [c_node]
+				setattr(self, 'cnt_%s' % cls_name, cnt_cur + 1)
+				fun(self, c_node)
+			x.inputs.append(node)
+
+		obj.mappings[j] = unity_fun
+
+@TaskGen.feature('unity')
+@TaskGen.before('process_source')
+def single_unity(self):
+	lst = self.to_list(self.features)
+	if 'c' in lst:
+		bind_unity(self, 'c', EXTS_C)
+	if 'cxx' in lst:
+		bind_unity(self, 'cxx', EXTS_CXX)
+
+def build(bld):
+	if bld.env.CC_NAME:
+		bind_unity(TaskGen.task_gen, 'c', EXTS_C)
+	if bld.env.CXX_NAME:
+		bind_unity(TaskGen.task_gen, 'cxx', EXTS_CXX)
+
diff --git a/waflib/extras/use_config.py b/waflib/extras/use_config.py
new file mode 100644
index 00000000..ef5129f2
--- /dev/null
+++ b/waflib/extras/use_config.py
@@ -0,0 +1,185 @@
+#!/usr/bin/env python
+# coding=utf-8
+# Mathieu Courtois - EDF R&D, 2013 - http://www.code-aster.org
+
+"""
+When a project has a lot of options the 'waf configure' command line can be
+very long and it becomes a cause of error.
+This tool provides a convenient way to load a set of configuration parameters
+from a local file or from a remote url.
+
+The configuration parameters are stored in a Python file that is imported as
+an extra waf tool can be.
+
+Example:
+$ waf configure --use-config-dir=http://www.anywhere.org --use-config=myconf1 ...
+
+The file 'myconf1' will be downloaded from 'http://www.anywhere.org'
+(or 'http://www.anywhere.org/wafcfg').
+If the files are available locally, it could be:
+$ waf configure --use-config-dir=/somewhere/myconfigurations --use-config=myconf1 ...
+
+The configuration of 'myconf1.py' is automatically loaded by calling
+its 'configure' function. In this example, it defines environment variables and
+set options:
+
+def configure(self):
+	self.env['CC'] = 'gcc-4.8'
+	self.env.append_value('LIBPATH', [...])
+	self.options.perlbinary = '/usr/local/bin/perl'
+	self.options.pyc = False
+
+The corresponding command line should have been:
+$ CC=gcc-4.8 LIBPATH=... waf configure --nopyc --with-perl-binary=/usr/local/bin/perl
+
+
+This is an extra tool, not bundled with the default waf binary.
+To add the use_config tool to the waf file:
+$ ./waf-light --tools=use_config
+
+When using this tool, the wscript will look like:
+
+	def options(opt):
+		opt.load('use_config')
+
+	def configure(conf):
+		conf.load('use_config')
+"""
+
+import sys
+import os.path as osp
+import os
+
+local_repo = ''
+"""Local repository containing additional Waf tools (plugins)"""
+remote_repo = 'https://gitlab.com/ita1024/waf/raw/master/'
+"""
+Remote directory containing downloadable waf tools. The missing tools can be downloaded by using::
+
+	$ waf configure --download
+"""
+
+remote_locs = ['waflib/extras', 'waflib/Tools']
+"""
+Remote directories for use with :py:const:`waflib.extras.use_config.remote_repo`
+"""
+
+
+try:
+	from urllib import request
+except ImportError:
+	from urllib import urlopen
+else:
+	urlopen = request.urlopen
+
+
+from waflib import Errors, Context, Logs, Utils, Options, Configure
+
+try:
+	from urllib.parse import urlparse
+except ImportError:
+	from urlparse import urlparse
+
+
+
+
+DEFAULT_DIR = 'wafcfg'
+# add first the current wafcfg subdirectory
+sys.path.append(osp.abspath(DEFAULT_DIR))
+
+def options(self):
+	group = self.add_option_group('configure options')
+	group.add_option('--download', dest='download', default=False, action='store_true', help='try to download the tools if missing')
+
+	group.add_option('--use-config', action='store', default=None,
+					 metavar='CFG', dest='use_config',
+					 help='force the configuration parameters by importing '
+						  'CFG.py. Several modules may be provided (comma '
+						  'separated).')
+	group.add_option('--use-config-dir', action='store', default=DEFAULT_DIR,
+					 metavar='CFG_DIR', dest='use_config_dir',
+					 help='path or url where to find the configuration file')
+
+def download_check(node):
+	"""
+	Hook to check for the tools which are downloaded. Replace with your function if necessary.
+	"""
+	pass
+
+
+def download_tool(tool, force=False, ctx=None):
+	"""
+	Download a Waf tool from the remote repository defined in :py:const:`waflib.extras.use_config.remote_repo`::
+
+		$ waf configure --download
+	"""
+	for x in Utils.to_list(remote_repo):
+		for sub in Utils.to_list(remote_locs):
+			url = '/'.join((x, sub, tool + '.py'))
+			try:
+				web = urlopen(url)
+				try:
+					if web.getcode() != 200:
+						continue
+				except AttributeError:
+					pass
+			except Exception:
+				# on python3 urlopen throws an exception
+				# python 2.3 does not have getcode and throws an exception to fail
+				continue
+			else:
+				tmp = ctx.root.make_node(os.sep.join((Context.waf_dir, 'waflib', 'extras', tool + '.py')))
+				tmp.write(web.read(), 'wb')
+				Logs.warn('Downloaded %s from %s', tool, url)
+				download_check(tmp)
+				try:
+					module = Context.load_tool(tool)
+				except Exception:
+					Logs.warn('The tool %s from %s is unusable', tool, url)
+					try:
+						tmp.delete()
+					except Exception:
+						pass
+					continue
+				return module
+
+	raise Errors.WafError('Could not load the Waf tool')
+
+def load_tool(tool, tooldir=None, ctx=None, with_sys_path=True):
+	try:
+		module = Context.load_tool_default(tool, tooldir, ctx, with_sys_path)
+	except ImportError as e:
+		if not ctx or not hasattr(Options.options, 'download'):
+			Logs.error('Could not load %r during options phase (download unavailable at this point)' % tool)
+			raise
+		if Options.options.download:
+			module = download_tool(tool, ctx=ctx)
+			if not module:
+				ctx.fatal('Could not load the Waf tool %r or download a suitable replacement from the repository (sys.path %r)\n%s' % (tool, sys.path, e))
+		else:
+			ctx.fatal('Could not load the Waf tool %r from %r (try the --download option?):\n%s' % (tool, sys.path, e))
+	return module
+
+Context.load_tool_default = Context.load_tool
+Context.load_tool = load_tool
+Configure.download_tool = download_tool
+
+def configure(self):
+	opts = self.options
+	use_cfg = opts.use_config
+	if use_cfg is None:
+		return
+	url = urlparse(opts.use_config_dir)
+	kwargs = {}
+	if url.scheme:
+		kwargs['download'] = True
+		kwargs['remote_url'] = url.geturl()
+		# search first with the exact url, else try with +'/wafcfg'
+		kwargs['remote_locs'] = ['', DEFAULT_DIR]
+	tooldir = url.geturl() + ' ' + DEFAULT_DIR
+	for cfg in use_cfg.split(','):
+		Logs.pprint('NORMAL', "Searching configuration '%s'..." % cfg)
+		self.load(cfg, tooldir=tooldir, **kwargs)
+	self.start_msg('Checking for configuration')
+	self.end_msg(use_cfg)
+
diff --git a/waflib/extras/valadoc.py b/waflib/extras/valadoc.py
new file mode 100644
index 00000000..c50f69e7
--- /dev/null
+++ b/waflib/extras/valadoc.py
@@ -0,0 +1,140 @@
+#! /usr/bin/env python
+# encoding: UTF-8
+# Nicolas Joseph 2009
+
+"""
+ported from waf 1.5:
+TODO: tabs vs spaces
+"""
+
+from waflib import Task, Utils, Errors, Logs
+from waflib.TaskGen import feature
+
+VALADOC_STR = '${VALADOC}'
+
+class valadoc(Task.Task):
+	vars  = ['VALADOC', 'VALADOCFLAGS']
+	color = 'BLUE'
+	after = ['cprogram', 'cstlib', 'cshlib', 'cxxprogram', 'cxxstlib', 'cxxshlib']
+	quiet = True # no outputs .. this is weird
+
+	def __init__(self, *k, **kw):
+		Task.Task.__init__(self, *k, **kw)
+		self.output_dir = ''
+		self.doclet = ''
+		self.package_name = ''
+		self.package_version = ''
+		self.files = []
+		self.vapi_dirs = []
+		self.protected = True
+		self.private = False
+		self.inherit = False
+		self.deps = False
+		self.vala_defines = []
+		self.vala_target_glib = None
+		self.enable_non_null_experimental = False
+		self.force = False
+
+	def run(self):
+		if not self.env['VALADOCFLAGS']:
+			self.env['VALADOCFLAGS'] = ''
+		cmd = [Utils.subst_vars(VALADOC_STR, self.env)]
+		cmd.append ('-o %s' % self.output_dir)
+		if getattr(self, 'doclet', None):
+			cmd.append ('--doclet %s' % self.doclet)
+		cmd.append ('--package-name %s' % self.package_name)
+		if getattr(self, 'package_version', None):
+			cmd.append ('--package-version %s' % self.package_version)
+		if getattr(self, 'packages', None):
+			for package in self.packages:
+				cmd.append ('--pkg %s' % package)
+		if getattr(self, 'vapi_dirs', None):
+			for vapi_dir in self.vapi_dirs:
+				cmd.append ('--vapidir %s' % vapi_dir)
+		if not getattr(self, 'protected', None):
+			cmd.append ('--no-protected')
+		if getattr(self, 'private', None):
+			cmd.append ('--private')
+		if getattr(self, 'inherit', None):
+			cmd.append ('--inherit')
+		if getattr(self, 'deps', None):
+			cmd.append ('--deps')
+		if getattr(self, 'vala_defines', None):
+			for define in self.vala_defines:
+				cmd.append ('--define %s' % define)
+		if getattr(self, 'vala_target_glib', None):
+			cmd.append ('--target-glib=%s' % self.vala_target_glib)
+		if getattr(self, 'enable_non_null_experimental', None):
+			cmd.append ('--enable-non-null-experimental')
+		if getattr(self, 'force', None):
+			cmd.append ('--force')
+		cmd.append (' '.join ([x.abspath() for x in self.files]))
+		return self.generator.bld.exec_command(' '.join(cmd))
+
+@feature('valadoc')
+def process_valadoc(self):
+	"""
+	Generate API documentation from Vala source code with valadoc
+
+	doc = bld(
+		features = 'valadoc',
+		output_dir = '../doc/html',
+		package_name = 'vala-gtk-example',
+		package_version = '1.0.0',
+		packages = 'gtk+-2.0',
+		vapi_dirs = '../vapi',
+		force = True
+	)
+
+	path = bld.path.find_dir ('../src')
+	doc.files = path.ant_glob (incl='**/*.vala')
+	"""
+
+	task = self.create_task('valadoc')
+	if getattr(self, 'output_dir', None):
+		task.output_dir = self.path.find_or_declare(self.output_dir).abspath()
+	else:
+		Errors.WafError('no output directory')
+	if getattr(self, 'doclet', None):
+		task.doclet = self.doclet
+	else:
+		Errors.WafError('no doclet directory')
+	if getattr(self, 'package_name', None):
+		task.package_name = self.package_name
+	else:
+		Errors.WafError('no package name')
+	if getattr(self, 'package_version', None):
+		task.package_version = self.package_version
+	if getattr(self, 'packages', None):
+		task.packages = Utils.to_list(self.packages)
+	if getattr(self, 'vapi_dirs', None):
+		vapi_dirs = Utils.to_list(self.vapi_dirs)
+		for vapi_dir in vapi_dirs:
+			try:
+				task.vapi_dirs.append(self.path.find_dir(vapi_dir).abspath())
+			except AttributeError:
+				Logs.warn('Unable to locate Vala API directory: %r', vapi_dir)
+	if getattr(self, 'files', None):
+		task.files = self.files
+	else:
+		Errors.WafError('no input file')
+	if getattr(self, 'protected', None):
+		task.protected = self.protected
+	if getattr(self, 'private', None):
+		task.private = self.private
+	if getattr(self, 'inherit', None):
+		task.inherit = self.inherit
+	if getattr(self, 'deps', None):
+		task.deps = self.deps
+	if getattr(self, 'vala_defines', None):
+		task.vala_defines = Utils.to_list(self.vala_defines)
+	if getattr(self, 'vala_target_glib', None):
+		task.vala_target_glib = self.vala_target_glib
+	if getattr(self, 'enable_non_null_experimental', None):
+		task.enable_non_null_experimental = self.enable_non_null_experimental
+	if getattr(self, 'force', None):
+		task.force = self.force
+
+def configure(conf):
+	conf.find_program('valadoc', errmsg='You must install valadoc <http://live.gnome.org/Valadoc> for generate the API documentation')
+
diff --git a/waflib/extras/waf_xattr.py b/waflib/extras/waf_xattr.py
new file mode 100644
index 00000000..351dd63a
--- /dev/null
+++ b/waflib/extras/waf_xattr.py
@@ -0,0 +1,150 @@
+#! /usr/bin/env python
+# encoding: utf-8
+
+"""
+Use extended attributes instead of database files
+
+1. Input files will be made writable
+2. This is only for systems providing extended filesystem attributes
+3. By default, hashes are calculated only if timestamp/size change (HASH_CACHE below)
+4. The module enables "deep_inputs" on all tasks by propagating task signatures
+5. This module also skips task signature comparisons for task code changes due to point 4.
+6. This module is for Python3/Linux only, but it could be extended to Python2/other systems
+   using the xattr library
+7. For projects in which tasks always declare output files, it should be possible to
+   store the rest of build context attributes on output files (imp_sigs, raw_deps and node_deps)
+   but this is not done here
+
+On a simple C++ project benchmark, the variations before and after adding waf_xattr.py were observed:
+total build time: 20s -> 22s
+no-op build time: 2.4s -> 1.8s
+pickle file size: 2.9MB -> 2.6MB
+"""
+
+import os
+from waflib import Logs, Node, Task, Utils, Errors
+from waflib.Task import SKIP_ME, RUN_ME, CANCEL_ME, ASK_LATER, SKIPPED, MISSING
+
+HASH_CACHE = True
+SIG_VAR = 'user.waf.sig'
+SEP = ','.encode()
+TEMPLATE = '%b%d,%d'.encode()
+
+try:
+	PermissionError
+except NameError:
+	PermissionError = IOError
+
+def getxattr(self):
+	return os.getxattr(self.abspath(), SIG_VAR)
+
+def setxattr(self, val):
+	os.setxattr(self.abspath(), SIG_VAR, val)
+
+def h_file(self):
+	try:
+		ret = getxattr(self)
+	except OSError:
+		if HASH_CACHE:
+			st = os.stat(self.abspath())
+			mtime = st.st_mtime
+			size = st.st_size
+	else:
+		if len(ret) == 16:
+			# for build directory files
+			return ret
+
+		if HASH_CACHE:
+			# check if timestamp and mtime match to avoid re-hashing
+			st = os.stat(self.abspath())
+			mtime, size = ret[16:].split(SEP)
+			if int(1000 * st.st_mtime) == int(mtime) and st.st_size == int(size):
+				return ret[:16]
+
+	ret = Utils.h_file(self.abspath())
+	if HASH_CACHE:
+		val = TEMPLATE % (ret, int(1000 * st.st_mtime), int(st.st_size))
+		try:
+			setxattr(self, val)
+		except PermissionError:
+			os.chmod(self.abspath(), st.st_mode | 128)
+			setxattr(self, val)
+	return ret
+
+def runnable_status(self):
+	bld = self.generator.bld
+	if bld.is_install < 0:
+		return SKIP_ME
+
+	for t in self.run_after:
+		if not t.hasrun:
+			return ASK_LATER
+		elif t.hasrun < SKIPPED:
+			# a dependency has an error
+			return CANCEL_ME
+
+	# first compute the signature
+	try:
+		new_sig = self.signature()
+	except Errors.TaskNotReady:
+		return ASK_LATER
+
+	if not self.outputs:
+		# compare the signature to a signature computed previously
+		# this part is only for tasks with no output files
+		key = self.uid()
+		try:
+			prev_sig = bld.task_sigs[key]
+		except KeyError:
+			Logs.debug('task: task %r must run: it was never run before or the task code changed', self)
+			return RUN_ME
+		if new_sig != prev_sig:
+			Logs.debug('task: task %r must run: the task signature changed', self)
+			return RUN_ME
+
+	# compare the signatures of the outputs to make a decision
+	for node in self.outputs:
+		try:
+			sig = node.h_file()
+		except EnvironmentError:
+			Logs.debug('task: task %r must run: an output node does not exist', self)
+			return RUN_ME
+		if sig != new_sig:
+			Logs.debug('task: task %r must run: an output node is stale', self)
+			return RUN_ME
+
+	return (self.always_run and RUN_ME) or SKIP_ME
+
+def post_run(self):
+	bld = self.generator.bld
+	sig = self.signature()
+	for node in self.outputs:
+		if not node.exists():
+			self.hasrun = MISSING
+			self.err_msg = '-> missing file: %r' % node.abspath()
+			raise Errors.WafError(self.err_msg)
+		os.setxattr(node.abspath(), 'user.waf.sig', sig)
+	if not self.outputs:
+		# only for task with no outputs
+		bld.task_sigs[self.uid()] = sig
+	if not self.keep_last_cmd:
+		try:
+			del self.last_cmd
+		except AttributeError:
+			pass
+
+try:
+	os.getxattr
+except AttributeError:
+	pass
+else:
+	h_file.__doc__ = Node.Node.h_file.__doc__
+
+	# keep file hashes as file attributes
+	Node.Node.h_file = h_file
+
+	# enable "deep_inputs" on all tasks
+	Task.Task.runnable_status = runnable_status
+	Task.Task.post_run = post_run
+	Task.Task.sig_deep_inputs = Utils.nada
+
diff --git a/waflib/extras/why.py b/waflib/extras/why.py
new file mode 100644
index 00000000..1bb941f6
--- /dev/null
+++ b/waflib/extras/why.py
@@ -0,0 +1,78 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2010 (ita)
+
+"""
+This tool modifies the task signature scheme to store and obtain
+information about the task execution (why it must run, etc)::
+
+	def configure(conf):
+		conf.load('why')
+
+After adding the tool, a full rebuild is necessary:
+waf clean build --zones=task
+"""
+
+from waflib import Task, Utils, Logs, Errors
+
+def signature(self):
+	# compute the result one time, and suppose the scan_signature will give the good result
+	try:
+		return self.cache_sig
+	except AttributeError:
+		pass
+
+	self.m = Utils.md5()
+	self.m.update(self.hcode)
+	id_sig = self.m.digest()
+
+	# explicit deps
+	self.m = Utils.md5()
+	self.sig_explicit_deps()
+	exp_sig = self.m.digest()
+
+	# env vars
+	self.m = Utils.md5()
+	self.sig_vars()
+	var_sig = self.m.digest()
+
+	# implicit deps / scanner results
+	self.m = Utils.md5()
+	if self.scan:
+		try:
+			self.sig_implicit_deps()
+		except Errors.TaskRescan:
+			return self.signature()
+	impl_sig = self.m.digest()
+
+	ret = self.cache_sig = impl_sig + id_sig + exp_sig + var_sig
+	return ret
+
+
+Task.Task.signature = signature
+
+old = Task.Task.runnable_status
+def runnable_status(self):
+	ret = old(self)
+	if ret == Task.RUN_ME:
+		try:
+			old_sigs = self.generator.bld.task_sigs[self.uid()]
+		except (KeyError, AttributeError):
+			Logs.debug("task: task must run as no previous signature exists")
+		else:
+			new_sigs = self.cache_sig
+			def v(x):
+				return Utils.to_hex(x)
+
+			Logs.debug('Task %r', self)
+			msgs = ['* Implicit or scanner dependency', '* Task code', '* Source file, explicit or manual dependency', '* Configuration data variable']
+			tmp = 'task: -> %s: %s %s'
+			for x in range(len(msgs)):
+				l = len(Utils.SIG_NIL)
+				a = new_sigs[x*l : (x+1)*l]
+				b = old_sigs[x*l : (x+1)*l]
+				if (a != b):
+					Logs.debug(tmp, msgs[x].ljust(35), v(a), v(b))
+	return ret
+Task.Task.runnable_status = runnable_status
+
diff --git a/waflib/extras/win32_opts.py b/waflib/extras/win32_opts.py
new file mode 100644
index 00000000..9f7443c3
--- /dev/null
+++ b/waflib/extras/win32_opts.py
@@ -0,0 +1,170 @@
+#! /usr/bin/env python
+# encoding: utf-8
+
+"""
+Windows-specific optimizations
+
+This module can help reducing the overhead of listing files on windows
+(more than 10000 files). Python 3.5 already provides the listdir
+optimization though.
+"""
+
+import os
+from waflib import Utils, Build, Node, Logs
+
+try:
+	TP = '%s\\*'.decode('ascii')
+except AttributeError:
+	TP = '%s\\*'
+
+if Utils.is_win32:
+	from waflib.Tools import md5_tstamp
+	import ctypes, ctypes.wintypes
+
+	FindFirstFile        = ctypes.windll.kernel32.FindFirstFileW
+	FindNextFile         = ctypes.windll.kernel32.FindNextFileW
+	FindClose            = ctypes.windll.kernel32.FindClose
+	FILE_ATTRIBUTE_DIRECTORY = 0x10
+	INVALID_HANDLE_VALUE = -1
+	UPPER_FOLDERS = ('.', '..')
+	try:
+		UPPER_FOLDERS = [unicode(x) for x in UPPER_FOLDERS]
+	except NameError:
+		pass
+
+	def cached_hash_file(self):
+		try:
+			cache = self.ctx.cache_listdir_cache_hash_file
+		except AttributeError:
+			cache = self.ctx.cache_listdir_cache_hash_file = {}
+
+		if id(self.parent) in cache:
+			try:
+				t = cache[id(self.parent)][self.name]
+			except KeyError:
+				raise IOError('Not a file')
+		else:
+			# an opportunity to list the files and the timestamps at once
+			findData = ctypes.wintypes.WIN32_FIND_DATAW()
+			find     = FindFirstFile(TP % self.parent.abspath(), ctypes.byref(findData))
+
+			if find == INVALID_HANDLE_VALUE:
+				cache[id(self.parent)] = {}
+				raise IOError('Not a file')
+
+			cache[id(self.parent)] = lst_files = {}
+			try:
+				while True:
+					if findData.cFileName not in UPPER_FOLDERS:
+						thatsadir = findData.dwFileAttributes & FILE_ATTRIBUTE_DIRECTORY
+						if not thatsadir:
+							ts = findData.ftLastWriteTime
+							d = (ts.dwLowDateTime << 32) | ts.dwHighDateTime
+							lst_files[str(findData.cFileName)] = d
+					if not FindNextFile(find, ctypes.byref(findData)):
+						break
+			except Exception:
+				cache[id(self.parent)] = {}
+				raise IOError('Not a file')
+			finally:
+				FindClose(find)
+			t = lst_files[self.name]
+
+		fname = self.abspath()
+		if fname in Build.hashes_md5_tstamp:
+			if Build.hashes_md5_tstamp[fname][0] == t:
+				return Build.hashes_md5_tstamp[fname][1]
+
+		try:
+			fd = os.open(fname, os.O_BINARY | os.O_RDONLY | os.O_NOINHERIT)
+		except OSError:
+			raise IOError('Cannot read from %r' % fname)
+		f = os.fdopen(fd, 'rb')
+		m = Utils.md5()
+		rb = 1
+		try:
+			while rb:
+				rb = f.read(200000)
+				m.update(rb)
+		finally:
+			f.close()
+
+		# ensure that the cache is overwritten
+		Build.hashes_md5_tstamp[fname] = (t, m.digest())
+		return m.digest()
+	Node.Node.cached_hash_file = cached_hash_file
+
+	def get_bld_sig_win32(self):
+		try:
+			return self.ctx.hash_cache[id(self)]
+		except KeyError:
+			pass
+		except AttributeError:
+			self.ctx.hash_cache = {}
+		self.ctx.hash_cache[id(self)] = ret = Utils.h_file(self.abspath())
+		return ret
+	Node.Node.get_bld_sig = get_bld_sig_win32
+
+	def isfile_cached(self):
+		# optimize for nt.stat calls, assuming there are many files for few folders
+		try:
+			cache = self.__class__.cache_isfile_cache
+		except AttributeError:
+			cache = self.__class__.cache_isfile_cache = {}
+
+		try:
+			c1 = cache[id(self.parent)]
+		except KeyError:
+			c1 = cache[id(self.parent)] = []
+
+			curpath = self.parent.abspath()
+			findData = ctypes.wintypes.WIN32_FIND_DATAW()
+			find     = FindFirstFile(TP % curpath, ctypes.byref(findData))
+
+			if find == INVALID_HANDLE_VALUE:
+				Logs.error("invalid win32 handle isfile_cached %r", self.abspath())
+				return os.path.isfile(self.abspath())
+
+			try:
+				while True:
+					if findData.cFileName not in UPPER_FOLDERS:
+						thatsadir = findData.dwFileAttributes & FILE_ATTRIBUTE_DIRECTORY
+						if not thatsadir:
+							c1.append(str(findData.cFileName))
+					if not FindNextFile(find, ctypes.byref(findData)):
+						break
+			except Exception as e:
+				Logs.error('exception while listing a folder %r %r', self.abspath(), e)
+				return os.path.isfile(self.abspath())
+			finally:
+				FindClose(find)
+		return self.name in c1
+	Node.Node.isfile_cached = isfile_cached
+
+	def find_or_declare_win32(self, lst):
+		# assuming that "find_or_declare" is called before the build starts, remove the calls to os.path.isfile
+		if isinstance(lst, str):
+			lst = [x for x in Utils.split_path(lst) if x and x != '.']
+
+		node = self.get_bld().search_node(lst)
+		if node:
+			if not node.isfile_cached():
+				try:
+					node.parent.mkdir()
+				except OSError:
+					pass
+			return node
+		self = self.get_src()
+		node = self.find_node(lst)
+		if node:
+			if not node.isfile_cached():
+				try:
+					node.parent.mkdir()
+				except OSError:
+					pass
+			return node
+		node = self.get_bld().make_node(lst)
+		node.parent.mkdir()
+		return node
+	Node.Node.find_or_declare = find_or_declare_win32
+
diff --git a/waflib/extras/wix.py b/waflib/extras/wix.py
new file mode 100644
index 00000000..d87bfbb1
--- /dev/null
+++ b/waflib/extras/wix.py
@@ -0,0 +1,87 @@
+#!/usr/bin/python
+# encoding: utf-8
+# vim: tabstop=4 noexpandtab
+
+"""
+Windows Installer XML Tool (WiX)
+
+.wxs --- candle ---> .wxobj --- light ---> .msi
+
+bld(features='wix', some.wxs, gen='some.msi', candleflags=[..], lightflags=[..])
+
+bld(features='wix', source=['bundle.wxs','WixBalExtension'], gen='setup.exe', candleflags=[..])
+"""
+
+import os, copy
+from waflib import TaskGen
+from waflib import Task
+from waflib.Utils import winreg
+
+class candle(Task.Task):
+	run_str = '${CANDLE} -nologo ${CANDLEFLAGS} -out ${TGT} ${SRC[0].abspath()}',
+
+class light(Task.Task):
+	run_str = "${LIGHT} -nologo -b ${SRC[0].parent.abspath()} ${LIGHTFLAGS} -out ${TGT} ${SRC[0].abspath()}"
+
+@TaskGen.feature('wix')
+@TaskGen.before_method('process_source')
+def wix(self):
+	#X.wxs -> ${SRC} for CANDLE
+	#X.wxobj -> ${SRC} for LIGHT
+	#X.dll -> -ext X in ${LIGHTFLAGS}
+	#X.wxl -> wixui.wixlib -loc X.wxl in ${LIGHTFLAGS}
+	wxobj = []
+	wxs = []
+	exts = []
+	wxl = []
+	rest = []
+	for x in self.source:
+		if x.endswith('.wxobj'):
+			wxobj.append(x)
+		elif x.endswith('.wxs'):
+			wxobj.append(self.path.find_or_declare(x[:-4]+'.wxobj'))
+			wxs.append(x)
+		elif x.endswith('.dll'):
+			exts.append(x[:-4])
+		elif '.' not in x:
+			exts.append(x)
+		elif x.endswith('.wxl'):
+			wxl.append(x)
+		else:
+			rest.append(x)
+	self.source = self.to_nodes(rest) #.wxs
+
+	cndl = self.create_task('candle', self.to_nodes(wxs), self.to_nodes(wxobj))
+	lght = self.create_task('light', self.to_nodes(wxobj), self.path.find_or_declare(self.gen))
+
+	cndl.env.CANDLEFLAGS = copy.copy(getattr(self,'candleflags',[]))
+	lght.env.LIGHTFLAGS = copy.copy(getattr(self,'lightflags',[]))
+
+	for x in wxl:
+		lght.env.append_value('LIGHTFLAGS','wixui.wixlib')
+		lght.env.append_value('LIGHTFLAGS','-loc')
+		lght.env.append_value('LIGHTFLAGS',x)
+	for x in exts:
+		cndl.env.append_value('CANDLEFLAGS','-ext')
+		cndl.env.append_value('CANDLEFLAGS',x)
+		lght.env.append_value('LIGHTFLAGS','-ext')
+		lght.env.append_value('LIGHTFLAGS',x)
+
+#wix_bin_path()
+def wix_bin_path():
+	basekey = r"SOFTWARE\Microsoft\.NETFramework\AssemblyFolders"
+	query = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, basekey)
+	cnt=winreg.QueryInfoKey(query)[0]
+	thiskey = r'C:\Program Files (x86)\WiX Toolset v3.10\SDK'
+	for i in range(cnt-1,-1,-1):
+		thiskey = winreg.EnumKey(query,i)
+		if 'WiX' in thiskey:
+			break
+	winreg.CloseKey(query)
+	return os.path.normpath(winreg.QueryValue(winreg.HKEY_LOCAL_MACHINE, basekey+r'\\'+thiskey)+'..\\bin')
+
+def configure(ctx):
+	path_list=[wix_bin_path()]
+	ctx.find_program('candle', var='CANDLE', mandatory=True, path_list = path_list)
+	ctx.find_program('light', var='LIGHT', mandatory=True, path_list = path_list)
+
diff --git a/waflib/extras/xcode.py b/waflib/extras/xcode.py
deleted file mode 100644
index 49a16919..00000000
--- a/waflib/extras/xcode.py
+++ /dev/null
@@ -1,312 +0,0 @@
-#! /usr/bin/env python
-# encoding: utf-8
-# XCode 3/XCode 4 generator for Waf
-# Nicolas Mercier 2011
-
-"""
-Usage:
-
-def options(opt):
-	opt.load('xcode')
-
-$ waf configure xcode
-"""
-
-# TODO: support iOS projects
-
-from waflib import Context, TaskGen, Build, Utils
-import os, sys
-
-HEADERS_GLOB = '**/(*.h|*.hpp|*.H|*.inl)'
-
-MAP_EXT = {
-	'.h' :  "sourcecode.c.h",
-
-	'.hh':  "sourcecode.cpp.h",
-	'.inl': "sourcecode.cpp.h",
-	'.hpp': "sourcecode.cpp.h",
-
-	'.c':   "sourcecode.c.c",
-
-	'.m':   "sourcecode.c.objc",
-
-	'.mm':  "sourcecode.cpp.objcpp",
-
-	'.cc':  "sourcecode.cpp.cpp",
-
-	'.cpp': "sourcecode.cpp.cpp",
-	'.C':   "sourcecode.cpp.cpp",
-	'.cxx': "sourcecode.cpp.cpp",
-	'.c++': "sourcecode.cpp.cpp",
-
-	'.l':   "sourcecode.lex", # luthor
-	'.ll':  "sourcecode.lex",
-
-	'.y':   "sourcecode.yacc",
-	'.yy':  "sourcecode.yacc",
-
-	'.plist': "text.plist.xml",
-	".nib":   "wrapper.nib",
-	".xib":   "text.xib",
-}
-
-
-part1 = 0
-part2 = 10000
-part3 = 0
-id = 562000999
-def newid():
-	global id
-	id = id + 1
-	return "%04X%04X%04X%012d" % (0, 10000, 0, id)
-
-class XCodeNode:
-	def __init__(self):
-		self._id = newid()
-
-	def tostring(self, value):
-		if isinstance(value, dict):
-			result = "{\n"
-			for k,v in value.items():
-				result = result + "\t\t\t%s = %s;\n" % (k, self.tostring(v))
-			result = result + "\t\t}"
-			return result
-		elif isinstance(value, str):
-			return "\"%s\"" % value
-		elif isinstance(value, list):
-			result = "(\n"
-			for i in value:
-				result = result + "\t\t\t%s,\n" % self.tostring(i)
-			result = result + "\t\t)"
-			return result
-		elif isinstance(value, XCodeNode):
-			return value._id
-		else:
-			return str(value)
-
-	def write_recursive(self, value, file):
-		if isinstance(value, dict):
-			for k,v in value.items():
-				self.write_recursive(v, file)
-		elif isinstance(value, list):
-			for i in value:
-				self.write_recursive(i, file)
-		elif isinstance(value, XCodeNode):
-			value.write(file)
-
-	def write(self, file):
-		for attribute,value in self.__dict__.items():
-			if attribute[0] != '_':
-				self.write_recursive(value, file)
-
-		w = file.write
-		w("\t%s = {\n" % self._id)
-		w("\t\tisa = %s;\n" % self.__class__.__name__)
-		for attribute,value in self.__dict__.items():
-			if attribute[0] != '_':
-				w("\t\t%s = %s;\n" % (attribute, self.tostring(value)))
-		w("\t};\n\n")
-
-
-
-# Configurations
-class XCBuildConfiguration(XCodeNode):
-	def __init__(self, name, settings = {}, env=None):
-		XCodeNode.__init__(self)
-		self.baseConfigurationReference = ""
-		self.buildSettings = settings
-		self.name = name
-		if env and env.ARCH:
-			settings['ARCHS'] = " ".join(env.ARCH)
-
-
-class XCConfigurationList(XCodeNode):
-	def __init__(self, settings):
-		XCodeNode.__init__(self)
-		self.buildConfigurations = settings
-		self.defaultConfigurationIsVisible = 0
-		self.defaultConfigurationName = settings and settings[0].name or ""
-
-# Group/Files
-class PBXFileReference(XCodeNode):
-	def __init__(self, name, path, filetype = '', sourcetree = "SOURCE_ROOT"):
-		XCodeNode.__init__(self)
-		self.fileEncoding = 4
-		if not filetype:
-			_, ext = os.path.splitext(name)
-			filetype = MAP_EXT.get(ext, 'text')
-		self.lastKnownFileType = filetype
-		self.name = name
-		self.path = path
-		self.sourceTree = sourcetree
-
-class PBXGroup(XCodeNode):
-	def __init__(self, name, sourcetree = "<group>"):
-		XCodeNode.__init__(self)
-		self.children = []
-		self.name = name
-		self.sourceTree = sourcetree
-
-	def add(self, root, sources):
-		folders = {}
-		def folder(n):
-			if not n.is_child_of(root):
-				return self
-			try:
-				return folders[n]
-			except KeyError:
-				f = PBXGroup(n.name)
-				p = folder(n.parent)
-				folders[n] = f
-				p.children.append(f)
-				return f
-		for s in sources:
-			f = folder(s.parent)
-			source = PBXFileReference(s.name, s.abspath())
-			f.children.append(source)
-
-
-# Targets
-class PBXLegacyTarget(XCodeNode):
-	def __init__(self, action, target=''):
-		XCodeNode.__init__(self)
-		self.buildConfigurationList = XCConfigurationList([XCBuildConfiguration('waf', {})])
-		if not target:
-			self.buildArgumentsString = "%s %s" % (sys.argv[0], action)
-		else:
-			self.buildArgumentsString = "%s %s --targets=%s" % (sys.argv[0], action, target)
-		self.buildPhases = []
-		self.buildToolPath = sys.executable
-		self.buildWorkingDirectory = ""
-		self.dependencies = []
-		self.name = target or action
-		self.productName = target or action
-		self.passBuildSettingsInEnvironment = 0
-
-class PBXShellScriptBuildPhase(XCodeNode):
-	def __init__(self, action, target):
-		XCodeNode.__init__(self)
-		self.buildActionMask = 2147483647
-		self.files = []
-		self.inputPaths = []
-		self.outputPaths = []
-		self.runOnlyForDeploymentPostProcessing = 0
-		self.shellPath = "/bin/sh"
-		self.shellScript = "%s %s %s --targets=%s" % (sys.executable, sys.argv[0], action, target)
-
-class PBXNativeTarget(XCodeNode):
-	def __init__(self, action, target, node, env):
-		XCodeNode.__init__(self)
-		conf = XCBuildConfiguration('waf', {'PRODUCT_NAME':target, 'CONFIGURATION_BUILD_DIR':node.parent.abspath()}, env)
-		self.buildConfigurationList = XCConfigurationList([conf])
-		self.buildPhases = [PBXShellScriptBuildPhase(action, target)]
-		self.buildRules = []
-		self.dependencies = []
-		self.name = target
-		self.productName = target
-		self.productType = "com.apple.product-type.application"
-		self.productReference = PBXFileReference(target, node.abspath(), 'wrapper.application', 'BUILT_PRODUCTS_DIR')
-
-# Root project object
-class PBXProject(XCodeNode):
-	def __init__(self, name, version):
-		XCodeNode.__init__(self)
-		self.buildConfigurationList = XCConfigurationList([XCBuildConfiguration('waf', {})])
-		self.compatibilityVersion = version[0]
-		self.hasScannedForEncodings = 1;
-		self.mainGroup = PBXGroup(name)
-		self.projectRoot = ""
-		self.projectDirPath = ""
-		self.targets = []
-		self._objectVersion = version[1]
-		self._output = PBXGroup('out')
-		self.mainGroup.children.append(self._output)
-
-	def write(self, file):
-		w = file.write
-		w("// !$*UTF8*$!\n")
-		w("{\n")
-		w("\tarchiveVersion = 1;\n")
-		w("\tclasses = {\n")
-		w("\t};\n")
-		w("\tobjectVersion = %d;\n" % self._objectVersion)
-		w("\tobjects = {\n\n")
-
-		XCodeNode.write(self, file)
-
-		w("\t};\n")
-		w("\trootObject = %s;\n" % self._id)
-		w("}\n")
-
-	def add_task_gen(self, tg):
-		if not getattr(tg, 'mac_app', False):
-			self.targets.append(PBXLegacyTarget('build', tg.name))
-		else:
-			target = PBXNativeTarget('build', tg.name, tg.link_task.outputs[0].change_ext('.app'), tg.env)
-			self.targets.append(target)
-			self._output.children.append(target.productReference)
-
-class xcode(Build.BuildContext):
-	cmd = 'xcode'
-	fun = 'build'
-
-	def collect_source(self, tg):
-		source_files = tg.to_nodes(getattr(tg, 'source', []))
-		plist_files = tg.to_nodes(getattr(tg, 'mac_plist', []))
-		resource_files = [tg.path.find_node(i) for i in Utils.to_list(getattr(tg, 'mac_resources', []))]
-		include_dirs = Utils.to_list(getattr(tg, 'includes', [])) + Utils.to_list(getattr(tg, 'export_dirs', []))
-		include_files = []
-		for x in include_dirs:
-			if not isinstance(x, str):
-				include_files.append(x)
-				continue
-			d = tg.path.find_node(x)
-			if d:
-				lst = [y for y in d.ant_glob(HEADERS_GLOB, flat=False)]
-				include_files.extend(lst)
-
-		# remove duplicates
-		source = list(set(source_files + plist_files + resource_files + include_files))
-		source.sort(key=lambda x: x.abspath())
-		return source
-
-	def execute(self):
-		"""
-		Entry point
-		"""
-		self.restore()
-		if not self.all_envs:
-			self.load_envs()
-		self.recurse([self.run_dir])
-
-		appname = getattr(Context.g_module, Context.APPNAME, os.path.basename(self.srcnode.abspath()))
-		p = PBXProject(appname, ('Xcode 3.2', 46))
-
-		for g in self.groups:
-			for tg in g:
-				if not isinstance(tg, TaskGen.task_gen):
-					continue
-
-				tg.post()
-
-				features = Utils.to_list(getattr(tg, 'features', ''))
-
-				group = PBXGroup(tg.name)
-				group.add(tg.path, self.collect_source(tg))
-				p.mainGroup.children.append(group)
-
-				if 'cprogram' or 'cxxprogram' in features:
-					p.add_task_gen(tg)
-
-
-		# targets that don't produce the executable but that you might want to run
-		p.targets.append(PBXLegacyTarget('configure'))
-		p.targets.append(PBXLegacyTarget('dist'))
-		p.targets.append(PBXLegacyTarget('install'))
-		p.targets.append(PBXLegacyTarget('check'))
-		node = self.srcnode.make_node('%s.xcodeproj' % appname)
-		node.mkdir()
-		node = node.make_node('project.pbxproj')
-		p.write(open(node.abspath(), 'w'))
-
-
diff --git a/waflib/extras/xcode6.py b/waflib/extras/xcode6.py
index ea299ab9..91bbff18 100644
--- a/waflib/extras/xcode6.py
+++ b/waflib/extras/xcode6.py
@@ -1,61 +1,20 @@
 #! /usr/bin/env python
 # encoding: utf-8
-# XCode 3/XCode 4 generator for Waf
+# XCode 3/XCode 4/XCode 6/Xcode 7 generator for Waf
 # Based on work by Nicolas Mercier 2011
 # Extended by Simon Warg 2015, https://github.com/mimon
 # XCode project file format based on http://www.monobjc.net/xcode-project-file-format.html
 
 """
-Usage:
+See playground/xcode6/ for usage examples.
 
-See also demos/xcode6/ folder
-
-def options(opt):
-	opt.load('xcode6')
-
-def configure(cnf):
-	# <do your stuff>
-
-	# For example
-	cnf.env.SDKROOT = 'macosx10.9'
-
-	# Use cnf.PROJ_CONFIGURATION to completely set/override
-	# global project settings
-	# cnf.env.PROJ_CONFIGURATION = {
-	# 	'Debug': {
-	# 		'SDKROOT': 'macosx10.9'
-	# 	}
-	# 	'MyCustomReleaseConfig': {
-	# 		'SDKROOT': 'macosx10.10'
-	# 	}
-	# }
-
-	# In the end of configure() do
-	cnf.load('xcode6')
-
-def build(bld):
-
-	# Make a Framework target
-	bld.framework(
-		source_files={
-			'Include': bld.path.ant_glob('include/MyLib/*.h'),
-			'Source': bld.path.ant_glob('src/MyLib/*.cpp')
-		},
-		includes='include',
-		export_headers=bld.path.ant_glob('include/MyLib/*.h'),
-		target='MyLib',
-	)
-
-	# You can also make bld.dylib, bld.app, bld.stlib ...
-
-$ waf configure xcode6
 """
 
-# TODO: support iOS projects
+from waflib import Context, TaskGen, Build, Utils, Errors, Logs
+import os, sys
 
-from waflib import Context, TaskGen, Build, Utils, ConfigSet, Configure, Errors
-from waflib.Build import BuildContext
-import os, sys, random, time
+# FIXME too few extensions
+XCODE_EXTS = ['.c', '.cpp', '.m', '.mm']
 
 HEADERS_GLOB = '**/(*.h|*.hpp|*.H|*.inl)'
 
@@ -123,6 +82,13 @@ def build(bld):
 	'exe' :TARGET_TYPE_EXECUTABLE,
 }
 
+def delete_invalid_values(dct):
+	""" Deletes entries that are dictionaries or sets """
+	for k, v in list(dct.items()):
+		if isinstance(v, dict) or isinstance(v, set):
+			del dct[k]
+	return dct
+
 """
 Configuration of the global project settings. Sets an environment variable 'PROJ_CONFIGURATION'
 which is a dictionary of configuration name and buildsettings pair.
@@ -150,15 +116,16 @@ def configure(self):
 
 	# Create default project configuration?
 	if 'PROJ_CONFIGURATION' not in self.env:
+		defaults = delete_invalid_values(self.env.get_merged_dict())
 		self.env.PROJ_CONFIGURATION = {
-			"Debug": self.env.get_merged_dict(),
-			"Release": self.env.get_merged_dict(),
+			"Debug": defaults,
+			"Release": defaults,
 		}
 
 	# Some build settings are required to be present by XCode. We will supply default values
 	# if user hasn't defined any.
 	defaults_required = [('PRODUCT_NAME', '$(TARGET_NAME)')]
-	for cfgname,settings in self.env.PROJ_CONFIGURATION.iteritems():
+	for cfgname,settings in self.env.PROJ_CONFIGURATION.items():
 		for default_var, default_val in defaults_required:
 			if default_var not in settings:
 				settings[default_var] = default_val
@@ -173,10 +140,17 @@ def configure(self):
 id = 562000999
 def newid():
 	global id
-	id = id + 1
+	id += 1
 	return "%04X%04X%04X%012d" % (0, 10000, 0, id)
 
-class XCodeNode:
+"""
+Represents a tree node in the XCode project plist file format.
+When written to a file, all attributes of XCodeNode are stringified together with
+its value. However, attributes starting with an underscore _ are ignored
+during that process and allows you to store arbitrary values that are not supposed
+to be written out.
+"""
+class XCodeNode(object):
 	def __init__(self):
 		self._id = newid()
 		self._been_written = False
@@ -247,12 +221,14 @@ def __init__(self, configlst):
 # Group/Files
 class PBXFileReference(XCodeNode):
 	def __init__(self, name, path, filetype = '', sourcetree = "SOURCE_ROOT"):
+
 		XCodeNode.__init__(self)
 		self.fileEncoding = 4
 		if not filetype:
 			_, ext = os.path.splitext(name)
 			filetype = MAP_EXT.get(ext, 'text')
 		self.lastKnownFileType = filetype
+		self.explicitFileType = filetype
 		self.name = name
 		self.path = path
 		self.sourceTree = sourcetree
@@ -267,11 +243,11 @@ class PBXBuildFile(XCodeNode):
 	""" This element indicate a file reference that is used in a PBXBuildPhase (either as an include or resource). """
 	def __init__(self, fileRef, settings={}):
 		XCodeNode.__init__(self)
-		
+
 		# fileRef is a reference to a PBXFileReference object
 		self.fileRef = fileRef
 
-		# A map of key/value pairs for additionnal settings.
+		# A map of key/value pairs for additional settings.
 		self.settings = settings
 
 	def __hash__(self):
@@ -281,16 +257,49 @@ def __eq__(self, other):
 		return self.fileRef == other.fileRef
 
 class PBXGroup(XCodeNode):
-	def __init__(self, name, sourcetree = "<group>"):
+	def __init__(self, name, sourcetree = 'SOURCE_TREE'):
 		XCodeNode.__init__(self)
 		self.children = []
 		self.name = name
 		self.sourceTree = sourcetree
 
+		# Maintain a lookup table for all PBXFileReferences
+		# that are contained in this group.
+		self._filerefs = {}
+
 	def add(self, sources):
-		""" sources param should be a list of PBXFileReference objects """
+		"""
+		Add a list of PBXFileReferences to this group
+
+		:param sources: list of PBXFileReferences objects
+		"""
+		self._filerefs.update(dict(zip(sources, sources)))
 		self.children.extend(sources)
 
+	def get_sub_groups(self):
+		"""
+		Returns all child PBXGroup objects contained in this group
+		"""
+		return list(filter(lambda x: isinstance(x, PBXGroup), self.children))
+
+	def find_fileref(self, fileref):
+		"""
+		Recursively search this group for an existing PBXFileReference. Returns None
+		if none were found.
+
+		The reason you'd want to reuse existing PBXFileReferences from a PBXGroup is that XCode doesn't like PBXFileReferences that aren't part of a PBXGroup hierarchy.
+		If it isn't, the consequence is that certain UI features like 'Reveal in Finder'
+		stops working.
+		"""
+		if fileref in self._filerefs:
+			return self._filerefs[fileref]
+		elif self.children:
+			for childgroup in self.get_sub_groups():
+				f = childgroup.find_fileref(fileref)
+				if f:
+					return f
+		return None
+
 class PBXContainerItemProxy(XCodeNode):
 	""" This is the element for to decorate a target item. """
 	def __init__(self, containerPortal, remoteGlobalIDString, remoteInfo='', proxyType=1):
@@ -299,7 +308,6 @@ def __init__(self, containerPortal, remoteGlobalIDString, remoteInfo='', proxyTy
 		self.remoteGlobalIDString = remoteGlobalIDString # PBXNativeTarget
 		self.remoteInfo = remoteInfo # Target name
 		self.proxyType = proxyType
-		
 
 class PBXTargetDependency(XCodeNode):
 	""" This is the element for referencing other target through content proxies. """
@@ -307,7 +315,7 @@ def __init__(self, native_target, proxy):
 		XCodeNode.__init__(self)
 		self.target = native_target
 		self.targetProxy = proxy
-		
+
 class PBXFrameworksBuildPhase(XCodeNode):
 	""" This is the element for the framework link build phase, i.e. linking to frameworks """
 	def __init__(self, pbxbuildfiles):
@@ -409,7 +417,7 @@ def __init__(self, name, version, env):
 		if not isinstance(env.PROJ_CONFIGURATION, dict):
 			raise Errors.WafError("Error: env.PROJ_CONFIGURATION must be a dictionary. This is done for you if you do not define one yourself. However, did you load the xcode module at the end of your wscript configure() ?")
 
-		# Retreive project configuration
+		# Retrieve project configuration
 		configurations = []
 		for config_name, settings in env.PROJ_CONFIGURATION.items():
 			cf = XCBuildConfiguration(config_name, settings)
@@ -417,7 +425,7 @@ def __init__(self, name, version, env):
 
 		self.buildConfigurationList = XCConfigurationList(configurations)
 		self.compatibilityVersion = version[0]
-		self.hasScannedForEncodings = 1;
+		self.hasScannedForEncodings = 1
 		self.mainGroup = PBXGroup(name)
 		self.projectRoot = ""
 		self.projectDirPath = ""
@@ -427,8 +435,8 @@ def __init__(self, name, version, env):
 	def create_target_dependency(self, target, name):
 		""" : param target : PXBNativeTarget """
 		proxy = PBXContainerItemProxy(self, target, name)
-		dependecy = PBXTargetDependency(target, proxy)
-		return dependecy
+		dependency = PBXTargetDependency(target, proxy)
+		return dependency
 
 	def write(self, file):
 
@@ -461,13 +469,145 @@ def get_target(self, name):
 				return t
 		return None
 
+@TaskGen.feature('c', 'cxx')
+@TaskGen.after('propagate_uselib_vars', 'apply_incpaths')
+def process_xcode(self):
+	bld = self.bld
+	try:
+		p = bld.project
+	except AttributeError:
+		return
+
+	if not hasattr(self, 'target_type'):
+		return
+
+	products_group = bld.products_group
+
+	target_group = PBXGroup(self.name)
+	p.mainGroup.children.append(target_group)
+
+	# Determine what type to build - framework, app bundle etc.
+	target_type = getattr(self, 'target_type', 'app')
+	if target_type not in TARGET_TYPES:
+		raise Errors.WafError("Target type '%s' does not exists. Available options are '%s'. In target '%s'" % (target_type, "', '".join(TARGET_TYPES.keys()), self.name))
+	else:
+		target_type = TARGET_TYPES[target_type]
+	file_ext = target_type[2]
+
+	# Create the output node
+	target_node = self.path.find_or_declare(self.name+file_ext)
+	target = PBXNativeTarget(self.name, target_node, target_type, [], [])
+
+	products_group.children.append(target.productReference)
+
+	# Pull source files from the 'source' attribute and assign them to a UI group.
+	# Use a default UI group named 'Source' unless the user
+	# provides a 'group_files' dictionary to customize the UI grouping.
+	sources = getattr(self, 'source', [])
+	if hasattr(self, 'group_files'):
+		group_files = getattr(self, 'group_files', [])
+		for grpname,files in group_files.items():
+			group = bld.create_group(grpname, files)
+			target_group.children.append(group)
+	else:
+		group = bld.create_group('Source', sources)
+		target_group.children.append(group)
+
+	# Create a PBXFileReference for each source file.
+	# If the source file already exists as a PBXFileReference in any of the UI groups, then
+	# reuse that PBXFileReference object (XCode does not like it if we don't reuse)
+	for idx, path in enumerate(sources):
+		fileref = PBXFileReference(path.name, path.abspath())
+		existing_fileref = target_group.find_fileref(fileref)
+		if existing_fileref:
+			sources[idx] = existing_fileref
+		else:
+			sources[idx] = fileref
+
+	# If the 'source' attribute contains any file extension that XCode can't work with,
+	# then remove it. The allowed file extensions are defined in XCODE_EXTS.
+	is_valid_file_extension = lambda file: os.path.splitext(file.path)[1] in XCODE_EXTS
+	sources = list(filter(is_valid_file_extension, sources))
+
+	buildfiles = [bld.unique_buildfile(PBXBuildFile(x)) for x in sources]
+	target.add_build_phase(PBXSourcesBuildPhase(buildfiles))
+
+	# Check if any framework to link against is some other target we've made
+	libs = getattr(self, 'tmp_use_seen', [])
+	for lib in libs:
+		use_target = p.get_target(lib)
+		if use_target:
+			# Create an XCode dependency so that XCode knows to build the other target before this target
+			dependency = p.create_target_dependency(use_target, use_target.name)
+			target.add_dependency(dependency)
+
+			buildphase = PBXFrameworksBuildPhase([PBXBuildFile(use_target.productReference)])
+			target.add_build_phase(buildphase)
+			if lib in self.env.LIB:
+				self.env.LIB = list(filter(lambda x: x != lib, self.env.LIB))
+
+	# If 'export_headers' is present, add files to the Headers build phase in xcode.
+	# These are files that'll get packed into the Framework for instance.
+	exp_hdrs = getattr(self, 'export_headers', [])
+	hdrs = bld.as_nodes(Utils.to_list(exp_hdrs))
+	files = [p.mainGroup.find_fileref(PBXFileReference(n.name, n.abspath())) for n in hdrs]
+	files = [PBXBuildFile(f, {'ATTRIBUTES': ('Public',)}) for f in files]
+	buildphase = PBXHeadersBuildPhase(files)
+	target.add_build_phase(buildphase)
+
+	# Merge frameworks and libs into one list, and prefix the frameworks
+	frameworks = Utils.to_list(self.env.FRAMEWORK)
+	frameworks = ' '.join(['-framework %s' % (f.split('.framework')[0]) for f in frameworks])
+
+	libs = Utils.to_list(self.env.STLIB) + Utils.to_list(self.env.LIB)
+	libs = ' '.join(bld.env['STLIB_ST'] % t for t in libs)
+
+	# Override target specific build settings
+	bldsettings = {
+		'HEADER_SEARCH_PATHS': ['$(inherited)'] + self.env['INCPATHS'],
+		'LIBRARY_SEARCH_PATHS': ['$(inherited)'] + Utils.to_list(self.env.LIBPATH) + Utils.to_list(self.env.STLIBPATH) + Utils.to_list(self.env.LIBDIR) ,
+		'FRAMEWORK_SEARCH_PATHS': ['$(inherited)'] + Utils.to_list(self.env.FRAMEWORKPATH),
+		'OTHER_LDFLAGS': libs + ' ' + frameworks,
+		'OTHER_LIBTOOLFLAGS': bld.env['LINKFLAGS'],
+		'OTHER_CPLUSPLUSFLAGS': Utils.to_list(self.env['CXXFLAGS']),
+		'OTHER_CFLAGS': Utils.to_list(self.env['CFLAGS']),
+		'INSTALL_PATH': []
+	}
+
+	# Install path
+	installpaths = Utils.to_list(getattr(self, 'install', []))
+	prodbuildfile = PBXBuildFile(target.productReference)
+	for instpath in installpaths:
+		bldsettings['INSTALL_PATH'].append(instpath)
+		target.add_build_phase(PBXCopyFilesBuildPhase([prodbuildfile], instpath))
+
+	if not bldsettings['INSTALL_PATH']:
+		del bldsettings['INSTALL_PATH']
+
+	# Create build settings which can override the project settings. Defaults to none if user
+	# did not pass argument. This will be filled up with target specific
+	# search paths, libs to link etc.
+	settings = getattr(self, 'settings', {})
+
+	# The keys represents different build configuration, e.g. Debug, Release and so on..
+	# Insert our generated build settings to all configuration names
+	keys = set(settings.keys() + bld.env.PROJ_CONFIGURATION.keys())
+	for k in keys:
+		if k in settings:
+			settings[k].update(bldsettings)
+		else:
+			settings[k] = bldsettings
+
+	for k,v in settings.items():
+		target.add_configuration(XCBuildConfiguration(k, v))
+
+	p.add_target(target)
+
+
 class xcode(Build.BuildContext):
 	cmd = 'xcode6'
 	fun = 'build'
 
-	file_refs = dict()
-	build_files = dict()
-
 	def as_nodes(self, files):
 		""" Returns a list of waflib.Nodes from a list of string of file paths """
 		nodes = []
@@ -476,42 +616,42 @@ def as_nodes(self, files):
 				d = x
 			else:
 				d = self.srcnode.find_node(x)
+				if not d:
+					raise Errors.WafError('File \'%s\' was not found' % x)
 			nodes.append(d)
 		return nodes
 
 	def create_group(self, name, files):
 		"""
-			Returns a new PBXGroup containing the files (paths) passed in the files arg 
-			:type files: string
+		Returns a new PBXGroup containing the files (paths) passed in the files arg
+		:type files: string
 		"""
 		group = PBXGroup(name)
 		"""
 		Do not use unique file reference here, since XCode seem to allow only one file reference
 		to be referenced by a group.
 		"""
-		files = [(PBXFileReference(d.name, d.abspath())) for d in self.as_nodes(files)]
-		group.add(files)
+		files_ = []
+		for d in self.as_nodes(Utils.to_list(files)):
+			fileref = PBXFileReference(d.name, d.abspath())
+			files_.append(fileref)
+		group.add(files_)
 		return group
 
-	def unique_filereference(self, fileref):
-		"""
-		Returns a unique fileref, possibly an existing one if the paths are the same.
-		Use this after you've constructed a PBXFileReference to make sure there is
-		only one PBXFileReference for the same file in the same project.
-		"""
-		if fileref not in self.file_refs:
-			self.file_refs[fileref] = fileref
-		return self.file_refs[fileref]
-
 	def unique_buildfile(self, buildfile):
 		"""
 		Returns a unique buildfile, possibly an existing one.
 		Use this after you've constructed a PBXBuildFile to make sure there is
 		only one PBXBuildFile for the same file in the same project.
 		"""
-		if buildfile not in self.build_files:
-			self.build_files[buildfile] = buildfile
-		return self.build_files[buildfile]
+		try:
+			build_files = self.build_files
+		except AttributeError:
+			build_files = self.build_files = {}
+
+		if buildfile not in build_files:
+			build_files[buildfile] = buildfile
+		return build_files[buildfile]
 
 	def execute(self):
 		"""
@@ -525,7 +665,7 @@ def execute(self):
 		appname = getattr(Context.g_module, Context.APPNAME, os.path.basename(self.srcnode.abspath()))
 
 		p = PBXProject(appname, ('Xcode 3.2', 46), self.env)
-		
+
 		# If we don't create a Products group, then
 		# XCode will create one, which entails that
 		# we'll start to see duplicate files in the UI
@@ -533,124 +673,55 @@ def execute(self):
 		products_group = PBXGroup('Products')
 		p.mainGroup.children.append(products_group)
 
-		for g in self.groups:
-			for tg in g:
-				if not isinstance(tg, TaskGen.task_gen):
-					continue
-
-				tg.post()
-
-				target_group = PBXGroup(tg.name)
-				p.mainGroup.children.append(target_group)
-
-				# Determine what type to build - framework, app bundle etc.
-				target_type = getattr(tg, 'target_type', 'app')
-				if target_type not in TARGET_TYPES:
-					raise Errors.WafError("Target type '%s' does not exists. Available options are '%s'. In target '%s'" % (target_type, "', '".join(TARGET_TYPES.keys()), tg.name))
-				else:
-					target_type = TARGET_TYPES[target_type]
-				file_ext = target_type[2]
-
-				# Create the output node
-				target_node = tg.path.find_or_declare(tg.name+file_ext)
-				
-				target = PBXNativeTarget(tg.name, target_node, target_type, [], [])
-
-				products_group.children.append(target.productReference)
-
-				if hasattr(tg, 'source_files'):
-					# Create list of PBXFileReferences
-					sources = []
-					if isinstance(tg.source_files, dict):
-						for grpname,files in tg.source_files.items():
-							group = self.create_group(grpname, files)
-							target_group.children.append(group)
-							sources.extend(group.children)
-					elif isinstance(tg.source_files, list):
-						group = self.create_group("Source", tg.source_files)
-						target_group.children.append(group)
-						sources.extend(group.children)
-					else:
-						self.to_log("Argument 'source_files' passed to target '%s' was not a dictionary. Hence, some source files may not be included. Please provide a dictionary of source files, with group name as key and list of source files as value.\n" % tg.name)
-
-					supported_extensions = ['.c', '.cpp', '.m', '.mm']
-					sources = filter(lambda fileref: os.path.splitext(fileref.path)[1] in supported_extensions, sources)
-					buildfiles = [self.unique_buildfile(PBXBuildFile(fileref)) for fileref in sources]
-					target.add_build_phase(PBXSourcesBuildPhase(buildfiles))
-
-				# Create build settings which can override the project settings. Defaults to none if user
-				# did not pass argument. However, this will be filled up further below with target specfic
-				# search paths, libs to link etc.
-				settings = getattr(tg, 'settings', {})
-
-				# Check if any framework to link against is some other target we've made
-				libs = getattr(tg, 'tmp_use_seen', [])
-				for lib in libs:
-					use_target = p.get_target(lib)
-					if use_target:
-						# Create an XCode dependency so that XCode knows to build the other target before this target
-						target.add_dependency(p.create_target_dependency(use_target, use_target.name))
-						target.add_build_phase(PBXFrameworksBuildPhase([PBXBuildFile(use_target.productReference)]))
-						if lib in tg.env.LIB:
-							tg.env.LIB = list(filter(lambda x: x != lib, tg.env.LIB))
-
-				# If 'export_headers' is present, add files to the Headers build phase in xcode.
-				# These are files that'll get packed into the Framework for instance.
-				exp_hdrs = getattr(tg, 'export_headers', [])
-				hdrs = self.as_nodes(Utils.to_list(exp_hdrs))
-				files = [self.unique_filereference(PBXFileReference(n.name, n.abspath())) for n in hdrs]
-				target.add_build_phase(PBXHeadersBuildPhase([PBXBuildFile(f, {'ATTRIBUTES': ('Public',)}) for f in files]))
-
-				# Install path
-				installpaths = Utils.to_list(getattr(tg, 'install', []))
-				prodbuildfile = PBXBuildFile(target.productReference)
-				for instpath in installpaths:
-					target.add_build_phase(PBXCopyFilesBuildPhase([prodbuildfile], instpath))
-
-				# Merge frameworks and libs into one list, and prefix the frameworks
-				ld_flags = ['-framework %s' % lib.split('.framework')[0] for lib in Utils.to_list(tg.env.FRAMEWORK)]
-				ld_flags.extend(Utils.to_list(tg.env.STLIB) + Utils.to_list(tg.env.LIB))
-
-				# Override target specfic build settings
-				bldsettings = {
-					'HEADER_SEARCH_PATHS': ['$(inherited)'] + tg.env['INCPATHS'],
-					'LIBRARY_SEARCH_PATHS': ['$(inherited)'] + Utils.to_list(tg.env.LIBPATH) + Utils.to_list(tg.env.STLIBPATH),
-					'FRAMEWORK_SEARCH_PATHS': ['$(inherited)'] + Utils.to_list(tg.env.FRAMEWORKPATH),
-					'OTHER_LDFLAGS': r'\n'.join(ld_flags)
-				}
-
-				# The keys represents different build configuration, e.g. Debug, Release and so on..
-				# Insert our generated build settings to all configuration names
-				keys = set(settings.keys() + self.env.PROJ_CONFIGURATION.keys())
-				for k in keys:
-					if k in settings:
-						settings[k].update(bldsettings)
-					else:
-						settings[k] = bldsettings
-
-				for k,v in settings.items():
-					target.add_configuration(XCBuildConfiguration(k, v))
-
-				p.add_target(target)
-				
+		self.project = p
+		self.products_group = products_group
+
+		# post all task generators
+		# the process_xcode method above will be called for each target
+		if self.targets and self.targets != '*':
+			(self._min_grp, self._exact_tg) = self.get_targets()
+
+		self.current_group = 0
+		while self.current_group < len(self.groups):
+			self.post_group()
+			self.current_group += 1
+
 		node = self.bldnode.make_node('%s.xcodeproj' % appname)
 		node.mkdir()
 		node = node.make_node('project.pbxproj')
-		p.write(open(node.abspath(), 'w'))
-	
-	def build_target(self, tgtype, *k, **kw):
-		"""
-		Provide user-friendly methods to build different target types
-		E.g. bld.framework(source='..', ...) to build a Framework target.
-		E.g. bld.dylib(source='..', ...) to build a Dynamic library target. etc...
-		"""
-		self.load('ccroot')
-		kw['features'] = 'cxx cxxprogram'
+		with open(node.abspath(), 'w') as f:
+			p.write(f)
+		Logs.pprint('GREEN', 'Wrote %r' % node.abspath())
+
+def bind_fun(tgtype):
+	def fun(self, *k, **kw):
+		tgtype = fun.__name__
+		if tgtype == 'shlib' or tgtype == 'dylib':
+			features = 'cxx cxxshlib'
+			tgtype = 'dylib'
+		elif tgtype == 'framework':
+			features = 'cxx cxxshlib'
+			tgtype = 'framework'
+		elif tgtype == 'program':
+			features = 'cxx cxxprogram'
+			tgtype = 'exe'
+		elif tgtype == 'app':
+			features = 'cxx cxxprogram'
+			tgtype = 'app'
+		elif tgtype == 'stlib':
+			features = 'cxx cxxstlib'
+			tgtype = 'stlib'
+		lst = kw['features'] = Utils.to_list(kw.get('features', []))
+		for x in features.split():
+			if not x in kw['features']:
+				lst.append(x)
+
 		kw['target_type'] = tgtype
 		return self(*k, **kw)
+	fun.__name__ = tgtype
+	setattr(Build.BuildContext, tgtype, fun)
+	return fun
+
+for xx in 'app framework dylib shlib stlib program'.split():
+	bind_fun(xx)
 
-	def app(self, *k, **kw): return self.build_target('app', *k, **kw)
-	def framework(self, *k, **kw): return self.build_target('framework', *k, **kw)
-	def dylib(self, *k, **kw): return self.build_target('dylib', *k, **kw)
-	def stlib(self, *k, **kw): return self.build_target('stlib', *k, **kw)
-	def exe(self, *k, **kw): return self.build_target('exe', *k, **kw)
diff --git a/waflib/fixpy2.py b/waflib/fixpy2.py
index 1adc9e0c..24176e06 100644
--- a/waflib/fixpy2.py
+++ b/waflib/fixpy2.py
@@ -1,24 +1,21 @@
 #!/usr/bin/env python
 # encoding: utf-8
-# Thomas Nagy, 2010-2015 (ita)
+# Thomas Nagy, 2010-2018 (ita)
 
-"""
-burn a book, save a tree
-"""
+from __future__ import with_statement
 
 import os
 
 all_modifs = {}
 
 def fixdir(dir):
-	"""call all the substitution functions on the waf folders"""
-	global all_modifs
+	"""Call all substitution functions on Waf folders"""
 	for k in all_modifs:
 		for v in all_modifs[k]:
 			modif(os.path.join(dir, 'waflib'), k, v)
 
 def modif(dir, name, fun):
-	"""execute a substitution function"""
+	"""Call a substitution function"""
 	if name == '*':
 		lst = []
 		for y in '. Tools extras'.split():
@@ -30,24 +27,17 @@ def modif(dir, name, fun):
 		return
 
 	filename = os.path.join(dir, name)
-	f = open(filename, 'r')
-	try:
+	with open(filename, 'r') as f:
 		txt = f.read()
-	finally:
-		f.close()
 
 	txt = fun(txt)
 
-	f = open(filename, 'w')
-	try:
+	with open(filename, 'w') as f:
 		f.write(txt)
-	finally:
-		f.close()
 
 def subst(*k):
 	"""register a substitution function"""
 	def do_subst(fun):
-		global all_modifs
 		for x in k:
 			try:
 				all_modifs[x].append(fun)
@@ -60,13 +50,15 @@ def do_subst(fun):
 def r1(code):
 	"utf-8 fixes for python < 2.6"
 	code = code.replace('as e:', ',e:')
-	code = code.replace(".decode(sys.stdout.encoding or 'iso8859-1')", '')
-	code = code.replace('.encode()', '')
-	return code
+	code = code.replace(".decode(sys.stdout.encoding or'latin-1',errors='replace')", '')
+	return code.replace('.encode()', '')
 
 @subst('Runner.py')
 def r4(code):
 	"generator syntax"
-	code = code.replace('next(self.biter)', 'self.biter.next()')
-	return code
+	return code.replace('next(self.biter)', 'self.biter.next()')
+
+@subst('Context.py')
+def r5(code):
+	return code.replace("('Execution failure: %s'%str(e),ex=e)", "('Execution failure: %s'%str(e),ex=e),None,sys.exc_info()[2]")
 
diff --git a/waflib/processor.py b/waflib/processor.py
new file mode 100755
index 00000000..2eecf3bd
--- /dev/null
+++ b/waflib/processor.py
@@ -0,0 +1,64 @@
+#! /usr/bin/env python
+# encoding: utf-8
+# Thomas Nagy, 2016-2018 (ita)
+
+import os, sys, traceback, base64, signal
+try:
+	import cPickle
+except ImportError:
+	import pickle as cPickle
+
+try:
+	import subprocess32 as subprocess
+except ImportError:
+	import subprocess
+
+try:
+	TimeoutExpired = subprocess.TimeoutExpired
+except AttributeError:
+	class TimeoutExpired(Exception):
+		pass
+
+def run():
+	txt = sys.stdin.readline().strip()
+	if not txt:
+		# parent process probably ended
+		sys.exit(1)
+	[cmd, kwargs, cargs] = cPickle.loads(base64.b64decode(txt))
+	cargs = cargs or {}
+
+	ret = 1
+	out, err, ex, trace = (None, None, None, None)
+	try:
+		proc = subprocess.Popen(cmd, **kwargs)
+		try:
+			out, err = proc.communicate(**cargs)
+		except TimeoutExpired:
+			if kwargs.get('start_new_session') and hasattr(os, 'killpg'):
+				os.killpg(proc.pid, signal.SIGKILL)
+			else:
+				proc.kill()
+			out, err = proc.communicate()
+			exc = TimeoutExpired(proc.args, timeout=cargs['timeout'], output=out)
+			exc.stderr = err
+			raise exc
+		ret = proc.returncode
+	except Exception as e:
+		exc_type, exc_value, tb = sys.exc_info()
+		exc_lines = traceback.format_exception(exc_type, exc_value, tb)
+		trace = str(cmd) + '\n' + ''.join(exc_lines)
+		ex = e.__class__.__name__
+
+	# it is just text so maybe we do not need to pickle()
+	tmp = [ret, out, err, ex, trace]
+	obj = base64.b64encode(cPickle.dumps(tmp))
+	sys.stdout.write(obj.decode())
+	sys.stdout.write('\n')
+	sys.stdout.flush()
+
+while 1:
+	try:
+		run()
+	except KeyboardInterrupt:
+		break
+
diff --git a/wscript b/wscript
index 70462cc8..aa7be4cb 100644
--- a/wscript
+++ b/wscript
@@ -387,9 +387,6 @@ def options(opt):
     opt.load('compiler_cxx')
     opt.load('compiler_c')
 
-    opt.load('xcode')
-    opt.load('xcode6')
-
     # install directories
     opt.add_option('--htmldir', type='string', default=None, help='HTML documentation directory [Default: <prefix>/share/jack-audio-connection-kit/reference/html/')
     opt.add_option('--libdir', type='string', help='Library directory [Default: <prefix>/lib]')
