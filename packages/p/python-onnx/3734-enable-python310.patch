From 09e7853f9012f7f236d28016f63b553b0eaae113 Mon Sep 17 00:00:00 2001
From: Chun-Wei Chen <jacky82226@gmail.com>
Date: Wed, 22 Sep 2021 20:57:25 -0400
Subject: [PATCH 01/13] enable py3.10

Signed-off-by: Chun-Wei Chen <jacky82226@gmail.com>
---
 .azure-pipelines/Linux-CI.yml               |   11 -
 .azure-pipelines/MacOS-CI.yml               |   17 -
 .github/workflows/release_linux_aarch64.yml |    4 
 .github/workflows/weekly_mac_ci.yml         |   23 --
 .github/workflows/win_no_exception_ci.yml   |   19 -
 docs/Changelog.md                           |    4 
 onnx/__init__.py                            |   88 ++++----
 onnx/checker.py                             |    6 
 onnx/defs/controlflow/defs.cc               |    1 
 onnx/defs/controlflow/old.cc                |    1 
 onnx/external_data_helper.py                |   66 +++---
 onnx/helper.py                              |   63 +++++
 onnx/numpy_helper.py                        |   17 -
 onnx/parser.py                              |   14 +
 onnx/shape_inference.py                     |   36 +--
 onnx/version_converter.py                   |  297 +++++++++++++---------------
 requirements-release.txt                    |    2 
 17 files changed, 343 insertions(+), 326 deletions(-)

--- a/.azure-pipelines/Linux-CI.yml
+++ b/.azure-pipelines/Linux-CI.yml
@@ -45,16 +45,7 @@ jobs:
       python -m virtualenv py$(python.version)
       source py$(python.version)/bin/activate
 
-      git clone https://github.com/protocolbuffers/protobuf.git
-      cd protobuf
-      git checkout v3.16.0
-      git submodule update --init --recursive
-      mkdir build_source && cd build_source
-
-      cmake ../cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_INSTALL_PREFIX=/usr -DCMAKE_INSTALL_SYSCONFDIR=/etc -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release
-      make -j$(nproc)
-      sudo make install
-      cd ../..
+      source workflow_scripts/protobuf/build_protobuf_unix.sh $(nproc)
 
       python -m pip install -q --upgrade pip
       python -m pip install -q -r requirements-release.txt
--- a/.azure-pipelines/MacOS-CI.yml
+++ b/.azure-pipelines/MacOS-CI.yml
@@ -31,21 +31,8 @@ jobs:
 
   - script: |
       # Install protobuf 3.16.0 from source
-      export NUM_CORES=`sysctl -n hw.ncpu`
-      echo Using $NUM_CORES cores
-      brew update
-      brew install autoconf && brew install automake
-      export ONNX_PATH=$(pwd)
-      cd ..
-      wget https://github.com/protocolbuffers/protobuf/releases/download/v3.16.0/protobuf-cpp-3.16.0.tar.gz
-      tar -xvf protobuf-cpp-3.16.0.tar.gz
-      cd protobuf-3.16.0
-      mkdir build_source && cd build_source
-      cmake ../cmake -DBUILD_SHARED_LIBS=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release
-      make -j${NUM_CORES}
-      make install
-      export PATH=$(pwd)/bin:$PATH
-      cd $ONNX_PATH
+      export NUM_CORES=`sysctl -n hw.logicalcpu`
+      source workflow_scripts/protobuf/build_protobuf_unix.sh $NUM_CORES $(pwd)/protobuf/protobuf_install
 
       git submodule update --init --recursive
       python -m pip install -q --upgrade pip
--- a/.github/workflows/release_linux_aarch64.yml
+++ b/.github/workflows/release_linux_aarch64.yml
@@ -17,7 +17,7 @@ jobs:
     strategy:
       matrix:
         # the different python versions for building wheels
-        python-version: [cp36-cp36m, cp37-cp37m, cp38-cp38, cp39-cp39]
+        python-version: [cp36-cp36m, cp37-cp37m, cp38-cp38, cp39-cp39, cp310-cp310]
     env:
       # setting up python and docker image
       py: /opt/python/${{ matrix.python-version }}/bin/python
@@ -125,7 +125,7 @@ jobs:
           deactivate'
 
     - name: Verify ONNX with ort-nightly
-      if: ${{ always() }}
+      if: matrix.python-version != 'cp310-cp310' # TODO update once ort-nightly has supported Python 3.10
       run: |
          docker run --rm -v ${{ github.workspace }}:/ws:rw --workdir=/ws \
           ${{ env.img }} \
--- a/.github/workflows/weekly_mac_ci.yml
+++ b/.github/workflows/weekly_mac_ci.yml
@@ -32,25 +32,6 @@ jobs:
         python-version: ${{ matrix.python-version }}
         architecture: ${{ matrix.architecture }}
 
-    - name: Install protobuf dependencies
-      run: |
-        # Install protobuf 3.16.0 due to compatibility
-        export NUM_CORES=`sysctl -n hw.ncpu`
-        echo Using $NUM_CORES cores
-        brew update
-        brew install autoconf && brew install automake
-        export ONNX_PATH=$(pwd)
-        cd ..
-        wget https://github.com/protocolbuffers/protobuf/releases/download/v3.16.0/protobuf-cpp-3.16.0.tar.gz
-        tar -xvf protobuf-cpp-3.16.0.tar.gz
-        cd protobuf-3.16.0
-        mkdir build_source && cd build_source
-        cmake ../cmake -Dprotobuf_BUILD_SHARED_LIBS=OFF -DCMAKE_POSITION_INDEPENDENT_CODE=ON -Dprotobuf_BUILD_TESTS=OFF -DCMAKE_BUILD_TYPE=Release
-        make -j${NUM_CORES}
-        make install
-        export PATH=$(pwd)/bin:$PATH
-        cd $ONNX_PATH
-
     - name: Install dependencies
       shell: bash
       run: |
@@ -61,6 +42,10 @@ jobs:
     - name: Build, install and test ONNX
       shell: bash
       run: |
+        # Install protobuf from source
+        export NUM_CORES=`sysctl -n hw.logicalcpu`
+        source workflow_scripts/protobuf/build_protobuf_unix.sh $NUM_CORES $(pwd)/protobuf/protobuf_install
+        # Build ONNX
         export CC=clang
         export CXX=clang++
         export ONNX_ML=1
--- a/.github/workflows/win_no_exception_ci.yml
+++ b/.github/workflows/win_no_exception_ci.yml
@@ -10,24 +10,9 @@ jobs:
     runs-on: windows-latest
     strategy:
       matrix:
-        python-version: [3.6, 3.7, 3.8, 3.9]
+        python-version: ['3.6', '3.7', '3.8', '3.9', '3.10']
         architecture: ['x64']
     steps:
-    - name: Checkout Protobuf 
-      uses: actions/checkout@master
-      with:
-        repository: protocolbuffers/protobuf
-        ref: refs/tags/v3.16.0
-        path: ./protobuf_root/protobuf
-
-    - name: Checkout Protobuf submodules
-      shell: bash
-      run: |
-         cd ./protobuf_root/protobuf
-         auth_header="$(git config --local --get http.https://github.com/.extraheader)"
-         git submodule sync --recursive
-         git -c "http.extraheader=$auth_header" -c protocol.version=2 submodule update --init --force --recursive --depth=1
-
     - name: Checkout ONNX
       uses: actions/checkout@v2
       with:
@@ -45,7 +30,7 @@ jobs:
       uses: microsoft/setup-msbuild@v1.0.2
 
     - name: Set up Python ${{ matrix.python-version }}
-      uses: actions/setup-python@v1
+      uses: actions/setup-python@v2
       with:
         python-version: ${{ matrix.python-version }}
         architecture: ${{ matrix.architecture }}
--- a/docs/Changelog.md
+++ b/docs/Changelog.md
@@ -8924,8 +8924,6 @@ This version of the operator has been av
 #### Type Constraints
 
 <dl>
-<dt><tt>I</tt> : tensor(int64)</dt>
-<dd>Int64 tensor</dd>
 <dt><tt>V</tt> : tensor(uint8), tensor(uint16), tensor(uint32), tensor(uint64), tensor(int8), tensor(int16), tensor(int32), tensor(int64), tensor(float16), tensor(float), tensor(double), tensor(string), tensor(bool), tensor(complex64), tensor(complex128)</dt>
 <dd>All Tensor types</dd>
 </dl>
@@ -12928,8 +12926,6 @@ This version of the operator has been av
 #### Type Constraints
 
 <dl>
-<dt><tt>I</tt> : tensor(int64)</dt>
-<dd>Int64 tensor</dd>
 <dt><tt>V</tt> : tensor(uint8), tensor(uint16), tensor(uint32), tensor(uint64), tensor(int8), tensor(int16), tensor(int32), tensor(int64), tensor(float16), tensor(float), tensor(double), tensor(string), tensor(bool), tensor(complex64), tensor(complex128)</dt>
 <dd>All Tensor types</dd>
 </dl>
--- a/onnx/__init__.py
+++ b/onnx/__init__.py
@@ -59,11 +59,11 @@ def _serialize(proto):  # type: (Union[b
     '''
     Serialize a in-memory proto to bytes
 
-    @params
-    proto is a in-memory proto, such as a ModelProto, TensorProto, etc
+    Arguments:
+        proto: a in-memory proto, such as a ModelProto, TensorProto, etc
 
-    @return
-    Serialized proto in bytes
+    Returns:
+        Serialized proto in bytes
     '''
     if isinstance(proto, bytes):
         return proto
@@ -82,12 +82,12 @@ def _deserialize(s, proto):  # type: (by
     '''
     Parse bytes into a in-memory proto
 
-    @params
-    s is bytes containing serialized proto
-    proto is a in-memory proto object
+    Arguments:
+        s: bytes containing serialized proto
+        proto: a in-memory proto object
 
-    @return
-    The proto instance filled in by s
+    Returns:
+        The proto instance filled in by s
     '''
     if not isinstance(s, bytes):
         raise ValueError('Parameter s must be bytes, but got type: {}'.format(type(s)))
@@ -110,12 +110,12 @@ def load_model(f, format=None, load_exte
     load_external_data is true if the external data under the same directory of the model and load the external data
     If not, users need to call load_external_data_for_model with directory to load
 
-    @params
-    f can be a file-like object (has "read" function) or a string containing a file name
-    format is for future use
+    Arguments:
+        f: can be a file-like object (has "read" function) or a string containing a file name
+        format: for future use
 
-    @return
-    Loaded in-memory ModelProto
+    Returns:
+        Loaded in-memory ModelProto
     '''
     s = _load_bytes(f)
     model = load_model_from_string(s, format=format)
@@ -133,12 +133,12 @@ def load_tensor(f, format=None):  # type
     '''
     Loads a serialized TensorProto into memory
 
-    @params
-    f can be a file-like object (has "read" function) or a string containing a file name
-    format is for future use
+    Arguments:
+        f: can be a file-like object (has "read" function) or a string containing a file name
+        format: for future use
 
-    @return
-    Loaded in-memory TensorProto
+    Returns:
+        Loaded in-memory TensorProto
     '''
     s = _load_bytes(f)
     return load_tensor_from_string(s, format=format)
@@ -148,12 +148,12 @@ def load_model_from_string(s, format=Non
     '''
     Loads a binary string (bytes) that contains serialized ModelProto
 
-    @params
-    s is a string, which contains serialized ModelProto
-    format is for future use
+    Arguments:
+        s: a string, which contains serialized ModelProto
+        format: for future use
 
-    @return
-    Loaded in-memory ModelProto
+    Returns:
+        Loaded in-memory ModelProto
     '''
     return _deserialize(s, ModelProto())
 
@@ -162,12 +162,12 @@ def load_tensor_from_string(s, format=No
     '''
     Loads a binary string (bytes) that contains serialized TensorProto
 
-    @params
-    s is a string, which contains serialized TensorProto
-    format is for future use
+    Arguments:
+        s: a string, which contains serialized TensorProto
+        format: for future use
 
-    @return
-    Loaded in-memory TensorProto
+    Returns:
+        Loaded in-memory TensorProto
     '''
     return _deserialize(s, TensorProto())
 
@@ -177,17 +177,17 @@ def save_model(proto, f, format=None, sa
     '''
     Saves the ModelProto to the specified path and optionally, serialize tensors with raw data as external data before saving.
 
-    @params
-    proto: should be a in-memory ModelProto
-    f: can be a file-like object (has "write" function) or a string containing a file name format for future use
-    all_tensors_to_one_file: If true, save all tensors to one external file specified by location.
-                             If false, save each tensor to a file named with the tensor name.
-    location: specify the external file that all tensors to save to.
-              If not specified, will use the model name.
-    size_threshold: Threshold for size of data. Only when tensor's data is >= the size_threshold it will be converted
-                    to external data. To convert every tensor with raw data to external data set size_threshold=0.
-    convert_attribute: If true, convert all tensors to external data
-                       If false, convert only non-attribute tensors to external data
+    Arguments:
+        proto: should be a in-memory ModelProto
+        f: can be a file-like object (has "write" function) or a string containing a file name format for future use
+        all_tensors_to_one_file: If true, save all tensors to one external file specified by location.
+            If false, save each tensor to a file named with the tensor name.
+        location: specify the external file that all tensors to save to.
+            If not specified, will use the model name.
+        size_threshold: Threshold for size of data. Only when tensor's data is >= the size_threshold it will be converted
+            to external data. To convert every tensor with raw data to external data set size_threshold=0.
+        convert_attribute: If true, convert all tensors to external data
+            If false, convert only non-attribute tensors to external data
     '''
     if isinstance(proto, bytes):
         proto = _deserialize(proto, ModelProto())
@@ -208,10 +208,10 @@ def save_tensor(proto, f):  # type: (Ten
     '''
     Saves the TensorProto to the specified path.
 
-    @params
-    proto should be a in-memory TensorProto
-    f can be a file-like object (has "write" function) or a string containing a file name
-    format is for future use
+    Arguments:
+        proto: should be a in-memory TensorProto
+        f: can be a file-like object (has "write" function) or a string containing a file name
+        format: for future use
     '''
     s = _serialize(proto)
     _save_bytes(s, f)
--- a/onnx/checker.py
+++ b/onnx/checker.py
@@ -92,6 +92,12 @@ def check_sparse_tensor(sparse, ctx=DEFA
 
 
 def check_model(model, full_check=False):  # type: (Union[ModelProto, Text, bytes], bool) -> None
+    """Check the consistency of a model. An exception is raised if the test fails.
+
+    Arguments:
+        model (ModelProto): model to check
+        full_check (bool): if True, the function checks shapes can be inferred
+    """
     # If model is a path instead of ModelProto
     if isinstance(model, string_types):
         C.check_model_path(model)
--- a/onnx/defs/controlflow/defs.cc
+++ b/onnx/defs/controlflow/defs.cc
@@ -829,7 +829,6 @@ ONNX_OPERATOR_SET_SCHEMA(
             "range is [-r, r-1].",
             AttributeProto::INTS,
             false)
-        .TypeConstraint("I", {"tensor(int64)"}, "Int64 tensor")
         .TypeConstraint("V", OpSchema::all_tensor_types(), "All Tensor types")
         .TypeAndShapeInferenceFunction(ScanInferenceFunction));
 
--- a/onnx/defs/controlflow/old.cc
+++ b/onnx/defs/controlflow/old.cc
@@ -1299,7 +1299,6 @@ ONNX_OPERATOR_SET_SCHEMA(
             "axis. If omitted, 0 will be used as the scan axis for every scan_output.",
             AttributeProto::INTS,
             false)
-        .TypeConstraint("I", {"tensor(int64)"}, "Int64 tensor")
         .TypeConstraint("V", OpSchema::all_tensor_types(), "All Tensor types")
         .TypeAndShapeInferenceFunction(ScanInferenceFunctionOpset9));
 
--- a/onnx/external_data_helper.py
+++ b/onnx/external_data_helper.py
@@ -35,11 +35,11 @@ class ExternalDataInfo(object):
 
 def load_external_data_for_tensor(tensor, base_dir):  # type: (TensorProto, Text) -> None
     """
-    Load data from an external file for tensor.
+    Loads data from an external file for tensor.
 
-    @params
-    tensor: a TensorProto object.
-    base_dir: directory that contains the external data.
+    Arguments:
+        tensor: a TensorProto object.
+        base_dir: directory that contains the external data.
     """
     if tensor.HasField("raw_data"):  # already loaded
         return
@@ -62,9 +62,9 @@ def load_external_data_for_model(model,
     """
     Loads external tensors into model
 
-    @params
-    model: ModelProto to load external data to
-    base_dir: directory that contains external data
+    Arguments:
+        model: ModelProto to load external data to
+        base_dir: directory that contains external data
     """
     for tensor in _get_all_tensors(model):
         if uses_external_data(tensor):
@@ -105,15 +105,16 @@ def convert_model_to_external_data(model
     """
     Call to set all tensors with raw data as external data. This call should preceed 'save_model'.
     'save_model' saves all the tensors data as external data after calling this function.
-    @params
-    model: ModelProto to be converted.
-    all_tensors_to_one_file: If true, save all tensors to one external file specified by location.
-                             If false, save each tensor to a file named with the tensor name.
-    location: specify the external file that all tensors to save to.
-              If not specified, will use the model name.
-    size_threshold: Threshold for size of data. Only when tensor's data is >= the size_threshold
-    it will be converted to external data. To convert every tensor with raw data to external data set size_threshold=0.
-    convert_attribute: If true, convert all tensors to external data
+
+    Arguments:
+        model (ModelProto): Model to be converted.
+        all_tensors_to_one_file (bool): If true, save all tensors to one external file specified by location.
+            If false, save each tensor to a file named with the tensor name.
+        location: specify the external file that all tensors to save to.
+            If not specified, will use the model name.
+        size_threshold: Threshold for size of data. Only when tensor's data is >= the size_threshold
+            it will be converted to external data. To convert every tensor with raw data to external data set size_threshold=0.
+        convert_attribute (bool): If true, convert all tensors to external data
                        If false, convert only non-attribute tensors to external data
     """
     tensors = _get_initializer_tensors(model)
@@ -139,8 +140,9 @@ def convert_model_to_external_data(model
 def convert_model_from_external_data(model):  # type: (ModelProto) -> None
     """
     Call to set all tensors which use external data as embedded data. save_model saves all the tensors data as embedded data after calling this function.
-    @params
-    model: ModelProto to be converted.
+
+    Arguments:
+        model (ModelProto): Model to be converted.
     """
     for tensor in _get_all_tensors(model):
         if uses_external_data(tensor):
@@ -152,11 +154,11 @@ def convert_model_from_external_data(mod
 
 def save_external_data(tensor, base_path):  # type: (TensorProto, Text) -> None
     """
-    Write tensor data to an external file according to information in the `external_data` field.
+    Writes tensor data to an external file according to information in the `external_data` field.
 
-    @params
-    tensor: Tensor object to be serialized
-    base_path: System path of a folder where tensor data is to be stored
+    Arguments:
+        tensor (TensorProto): Tensor object to be serialized
+        base_path: System path of a folder where tensor data is to be stored
     """
     info = ExternalDataInfo(tensor)
     external_data_file_path = os.path.join(base_path, info.location)
@@ -225,19 +227,19 @@ def _is_valid_filename(filename):  # typ
 
 
 def uses_external_data(tensor):  # type: (TensorProto) -> bool
-    """Return true if the tensor stores data in an external location."""
+    """Returns true if the tensor stores data in an external location."""
     return tensor.HasField("data_location") and tensor.data_location == TensorProto.EXTERNAL
 
 
 def remove_external_data_field(tensor, field_key):  # type: (TensorProto, Text) -> None
     """
-    Remove a field from a Tensor's external_data key-value store.
+    Removes a field from a Tensor's external_data key-value store.
 
     Modifies tensor object in place.
 
-    @params
-    tensor: Tensor object from which value will be removed
-    field_key: The key of the field to be removed
+    Arguments:
+        tensor: Tensor object from which value will be removed
+        field_key: The key of the field to be removed
     """
     for (i, field) in enumerate(tensor.external_data):
         if field.key == field_key:
@@ -250,12 +252,12 @@ def write_external_data_tensors(model, f
 
     Note: This function also strips basepath information from all tensors' external_data fields.
 
-    @params
-    model: Model object which is the source of tensors to serialize.
-    filepath: System path to the directory which should be treated as base path for external data.
+    Arguments:
+        model (ModelProto): Model object which is the source of tensors to serialize.
+        filepath: System path to the directory which should be treated as base path for external data.
 
-    @return
-    The modified model object.
+    Returns:
+        ModelProto: The modified model object.
     """
     for tensor in _get_all_tensors(model):
         if uses_external_data(tensor):
--- a/onnx/helper.py
+++ b/onnx/helper.py
@@ -5,7 +5,7 @@ from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 
-import collections
+import collections.abc  # type: ignore
 import numbers
 from six import text_type, integer_types, binary_type
 
@@ -48,8 +48,8 @@ VERSION_TABLE = [
 VersionMapType = Dict[Tuple[Text, int], int]
 
 
-# create a map from (opset-domain, opset-version) to ir-version from above table
 def create_op_set_id_version_map(table):  # type: (VersionTableType) -> VersionMapType
+    """create a map from (opset-domain, opset-version) to ir-version from above table"""
     result = dict()  # type: VersionMapType
 
     def process(release_version, ir_version, *args):  # type: (Text, int, Any) -> None
@@ -64,8 +64,8 @@ def create_op_set_id_version_map(table):
 OP_SET_ID_VERSION_MAP = create_op_set_id_version_map(VERSION_TABLE)
 
 
-# Given list of opset ids, determine minimum IR version required
 def find_min_ir_version_for(opsetidlist):  # type: (List[OperatorSetIdProto]) -> int
+    """Given list of opset ids, determine minimum IR version required"""
     default_min_version = 3
 
     def find_min(domain, version):  # type: (Union[Text, None], int) -> int
@@ -100,6 +100,8 @@ def make_node(
             If it's None, we will just use default domain (which is empty)
         **kwargs (dict): the attributes of the node.  The acceptable values
             are documented in :func:`make_attribute`.
+    Returns:
+        NodeProto
     """
 
     node = NodeProto()
@@ -129,6 +131,8 @@ def make_operatorsetid(
     Arguments:
         domain (string): The domain of the operator set id
         version (integer): Version of operator set id
+    Returns:
+        OperatorSetIdProto
     """
     operatorsetid = OperatorSetIdProto()
     operatorsetid.domain = domain
@@ -146,6 +150,20 @@ def make_graph(
     value_info=[],  # type: Sequence[ValueInfoProto]
     sparse_initializer=None,  # type: Optional[Sequence[SparseTensorProto]]
 ):  # type: (...) -> GraphProto
+    """Construct a GraphProto
+
+    Arguments:
+        nodes: list of NodeProto
+        name (string): graph name
+        inputs: list of ValueInfoProto
+        outputs: list of ValueInfoProto
+        initializer: list of TensorProto
+        doc_string (string): graph documentation
+        value_info: list of ValueInfoProto
+        sparse_initializer: list of SparseTensorProto
+    Returns:
+        GraphProto
+    """
     if initializer is None:
         initializer = []
     if sparse_initializer is None:
@@ -166,6 +184,14 @@ def make_graph(
 
 
 def make_opsetid(domain, version):  # type: (Text, int) -> OperatorSetIdProto
+    """Construct an OperatorSetIdProto.
+
+    Arguments:
+        domain (string): The domain of the operator set id
+        version (integer): Version of operator set id
+    Returns:
+        OperatorSetIdProto
+    """
     opsetid = OperatorSetIdProto()
     opsetid.domain = domain
     opsetid.version = version
@@ -173,6 +199,14 @@ def make_opsetid(domain, version):  # ty
 
 
 def make_model(graph, **kwargs):  # type: (GraphProto, **Any) -> ModelProto
+    """Construct a ModelProto
+
+    Arguments:
+        graph (GraphProto): *make_graph* returns
+        **kwargs: any attribute to add to the returned instance
+    Returns:
+        ModelProto
+    """
     model = ModelProto()
     # Touch model.ir_version so it is stored as the version from which it is
     # generated.
@@ -232,6 +266,17 @@ def make_tensor(
     values based on data_type. If raw is True, use "raw_data" proto
     field to store the values, and values should be of type bytes in
     this case.
+
+    Arguments:
+        name (string): tensor name
+        data_type (int): a value such as onnx.TensorProto.FLOAT
+        dims (List[int]): shape
+        vals: values
+        raw (bool): if True, vals contains the seralized content of the tensor,
+            otherwise, vals should be a list of values of the type defined by *data_type*
+
+    Returns:
+        TensorProto
     '''
     tensor = TensorProto()
     tensor.data_type = data_type
@@ -362,7 +407,7 @@ def make_attribute(
     if doc_string:
         attr.doc_string = doc_string
 
-    is_iterable = isinstance(value, collections.Iterable)
+    is_iterable = isinstance(value, collections.abc.Iterable)
     bytes_or_false = _to_bytes_or_false(value)
     # First, singular cases
     # float
@@ -811,6 +856,16 @@ def printable_node(node, prefix='', subg
 
 
 def printable_graph(graph, prefix=''):  # type: (GraphProto, Text) -> Text
+    """
+    Display a GraphProto as a string.
+
+    Arguments:
+        graph (GraphProto): the graph to display
+        prefix (string): prefix of every line
+
+    Returns:
+        string
+    """
     content = []
     indent = prefix + '  '
     # header
--- a/onnx/numpy_helper.py
+++ b/onnx/numpy_helper.py
@@ -83,7 +83,7 @@ def from_array(arr, name=None):  # type:
         arr: a numpy array.
         name: (optional) the name of the tensor.
     Returns:
-        tensor_def: the converted tensor def.
+        TensorProto: the converted tensor def.
     """
     tensor = TensorProto()
     tensor.dims.extend(arr.shape)
@@ -140,7 +140,7 @@ def to_list(sequence):  # type: (Sequenc
     Inputs:
         sequence: a SequenceProto object.
     Returns:
-        lst: the converted list.
+        list: the converted list.
     """
     lst = []  # type: List[Any]
     elem_type = sequence.elem_type
@@ -167,7 +167,7 @@ def from_list(lst, name=None, dtype=None
         dtype: (optional) type of element in the input list, used for specifying
                           sequence values when converting an empty list.
     Returns:
-        sequence: the converted sequence def.
+        SequenceProto: the converted sequence def.
     """
     sequence = SequenceProto()
     if name:
@@ -208,7 +208,7 @@ def from_list(lst, name=None, dtype=None
     return sequence
 
 
-def to_dict(map):  # type: (MapProto) -> np.ndarray[Any]
+def to_dict(map):  # type: (MapProto) -> Dict[Any, Any]
     """Converts a map def to a Python dictionary.
 
     Inputs:
@@ -238,7 +238,7 @@ def from_dict(dict, name=None):  # type:
         dict: Python dictionary
         name: (optional) the name of the map.
     Returns:
-        map: the converted map def.
+        MapProto: the converted map def.
     """
     map = MapProto()
     if name:
@@ -346,9 +346,10 @@ def from_optional(
 
 def convert_endian(tensor):  # type: (TensorProto) -> None
     """
-    call to convert endianess of raw data in tensor.
-    @params
-    TensorProto: TensorProto to be converted.
+    Call to convert endianess of raw data in tensor.
+
+    Arguments:
+        tensor (TensorProto): TensorProto to be converted.
     """
     tensor_dtype = tensor.data_type
     np_dtype = mapping.TENSOR_TYPE_TO_NP_TYPE[tensor_dtype]
--- a/onnx/parser.py
+++ b/onnx/parser.py
@@ -10,6 +10,13 @@ class ParseError(Exception):
 
 
 def parse_model(model_text):  # type: (Text) -> onnx.ModelProto
+    """Parse a string to build a ModelProto.
+
+    Arguments:
+        model_text (string): formatted string
+    Returns:
+        ModelProto
+    """
     (success, msg, model_proto_str) = C.parse_model(model_text)
     if success:
         return onnx.load_from_string(model_proto_str)
@@ -18,6 +25,13 @@ def parse_model(model_text):  # type: (T
 
 
 def parse_graph(graph_text):  # type: (Text) -> onnx.GraphProto
+    """Parse a string to build a GraphProto.
+
+    Arguments:
+        graph_text (string): formatted string
+    Returns:
+        GraphProto
+    """
     (success, msg, graph_proto_str) = C.parse_graph(graph_text)
     if success:
         G = onnx.GraphProto()
--- a/onnx/shape_inference.py
+++ b/onnx/shape_inference.py
@@ -15,28 +15,26 @@ from onnx import ModelProto
 from six import string_types, binary_type
 from typing import Text, Union
 
-"""Apply shape inference to the provided ModelProto.
 
-Inferred shapes are added to the value_info field of the graph.
-
-If the inferred values conflict with values already provided in the
-graph, that means that the provided values are invalid (or there is a
-bug in shape inference), and the result is unspecified.
-
-bool check_type: Checks the type-equality for input and output
-bool strict_mode: Stricter shape inference, it will throw errors if any;
-    Otherwise, simply stop if any error
-bool data_prop: Enables data propagation for limited operators to perform shape computation
-
-Arguments:
-    input (Union[ModelProto, Text, bytes], bool, bool, bool) -> ModelProto
-
-Return:
-    return (ModelProto) model with inferred shape information
-"""
+def infer_shapes(model, check_type=False, strict_mode=False, data_prop=False):  # type: (Union[ModelProto, bytes], bool, bool, bool) -> ModelProto
+    """Apply shape inference to the provided ModelProto.
 
+    Inferred shapes are added to the value_info field of the graph.
 
-def infer_shapes(model, check_type=False, strict_mode=False, data_prop=False):  # type: (Union[ModelProto, bytes], bool, bool, bool) -> ModelProto
+    If the inferred values conflict with values already provided in the
+    graph, that means that the provided values are invalid (or there is a
+    bug in shape inference), and the result is unspecified.
+
+    Arguments:
+        model (Union[ModelProto, Text, bytes], bool, bool, bool) -> ModelProto
+        check_type (bool): Checks the type-equality for input and output
+        strict_mode (bool): Stricter shape inference, it will throw errors if any;
+            Otherwise, simply stop if any error
+        data_prop (bool): Enables data propagation for limited operators to perform shape computation
+
+    Returns:
+        (ModelProto) model with inferred shape information
+    """
     if isinstance(model, (ModelProto, binary_type)):
         model_str = model if isinstance(model, binary_type) else model.SerializeToString()
         inferred_model_str = C.infer_shapes(model_str, check_type, strict_mode, data_prop)
--- a/onnx/version_converter.py
+++ b/onnx/version_converter.py
@@ -15,157 +15,156 @@ import onnx.onnx_cpp2py_export.version_c
 from onnx import ModelProto
 from typing import Text, Sequence
 
-"""Apply the version conversion on the serialized ModelProto.
-
-Arguments:
-    input (ModelProto): model
-    target_version (int): target opset version
-
-Return:
-    return (ModelProto) converted model
-
-Raises Exceptions:
-    RuntimeError when some necessary conversion is not supported
-
-Supported adapters:
-    --Add from Opset 7 to Opset 6
-    --Add from Opset 6 to Opset 5
-    --Add from Opset 6 to Opset 7
-    --Add from Opset 5 to Opset 6
-    --Mul from Opset 6 to Opset 7
-    --Mul from Opset 7 to Opset 6
-    --Mul from Opset 6 to Opset 5
-    --Mul from Opset 5 to Opset 6
-    --Gemm from Opset 7 to Opset 6
-    --Gemm from Opset 6 to Opset 5
-    --Gemm from Opset 6 to Opset 7
-    --Gemm from Opset 5 to Opset 6
-    --Relu from Opset 6 to Opset 5
-    --Relu from Opset 5 to Opset 6
-    --BatchNorm from Opset 7 to Opset 6
-    --BatchNorm from Opset 6 to Opset 7
-    --BatchNorm from Opset 6 to Opset 5
-    --BatchNorm from Opset 5 to Opset 6
-    --Concat from Opset 4 to Opset 3
-    --Concat from Opset 3 to Opset 4
-    --Reshape from Opset 5 to Opset 4
-    --Reshape from Opset 4 to Opset 5
-    --Sum from Opset 7 to Opset 8
-    --Sum from Opset 8 to Opset 7
-    --Sum from Opset 6 to Opset 5
-    --Sum from Opset 5 to Opset 6
-    --MaxPool from Opset 8 to Opset 7
-    --MaxPool from Opset 7 to Opset 8
-    --AveragePool from Opset 7 to Opset 6
-    --AveragePool from Opset 6 to Opset 7
-    --Dropout from Opset 7 to Opset 6
-    --Dropout from Opset 6 to Opset 5
-    --Dropout from Opset 6 to Opset 7
-    --Dropout from Opset 5 to Opset 6
-    --RNN from Opset 13 to Opset 14
-    --RNN from Opset 14 to Opset 13
-    --GRU from Opset 13 to Opset 14
-    --GRU from Opset 14 to Opset 13
-    --LSTM from Opset 13 to Opset 14
-    --LSTM from Opset 14 to Opset 13
-
-Unsupported adapters:
-    --Min from Opset 8 to Opset 7
-    --Min from Opset 7 to Opset 8
-    --Min from Opset 6 to Opset 5
-    --Min from Opset 5 to Opset 6
-    --Mean from Opset 8 to Opset 7
-    --Mean from Opset 7 to Opset 8
-    --Mean from Opset 6 to Opset 5
-    --Mean from Opset 5 to Opset 6
-    --Max from Opset 8 to Opset 7
-    --Max from Opset 7 to Opset 8
-    --Max from Opset 6 to Opset 5
-    --Max from Opset 5 to Opset 6
-    --Xor from Opset 6 to Opset 7
-    --Xor from Opset 7 to Opset 6
-    --Upsample from Opset 6 to Opset 7
-    --Upsample from Opset 7 to Opset 6
-    --Sub from Opset 6 to Opset 7
-    --Sub from Opset 7 to Opset 6
-    --Sub from Opset 6 to Opset 5
-    --Sub from Opset 5 to Opset 6
-    --RNN from Opset 6 to Opset 7
-    --RNN from Opset 7 to Opset 6
-    --Pow from Opset 6 to Opset 7
-    --Pow from Opset 7 to Opset 6
-    --PRelu from Opset 6 to Opset 7
-    --PRelu from Opset 7 to Opset 6
-    --PRelu from Opset 6 to Opset 5
-    --PRelu from Opset 5 to Opset 6
-    --Or from Opset 6 to Opset 7
-    --Or from Opset 7 to Opset 6
-    --Less from Opset 6 to Opset 7
-    --Less from Opset 7 to Opset 6
-    --LSTM from Opset 6 to Opset 7
-    --LSTM from Opset 7 to Opset 6
-    --Greater from Opset 6 to Opset 7
-    --Greater from Opset 7 to Opset 6
-    --GRU from Opset 6 to Opset 7
-    --GRU from Opset 7 to Opset 6
-    --GRU from Opset 3 to Opset 2
-    --GRU from Opset 2 to Opset 3
-    --Equal from Opset 6 to Opset 7
-    --Equal from Opset 7 to Opset 6
-    --Div from Opset 6 to Opset 7
-    --Div from Opset 7 to Opset 6
-    --Div from Opset 6 to Opset 5
-    --Div from Opset 5 to Opset 6
-    --And from Opset 6 to Opset 7
-    --And from Opset 7 to Opset 6
-    --And from Opset 6 to Opset 5
-    --And from Opset 5 to Opset 6
-    --Tile from Opset 6 to Opset 5
-    --Tile from Opset 5 to Opset 6
-    --Sqrt from Opset 6 to Opset 5
-    --Sqrt from Opset 5 to Opset 6
-    --Sigmoid from opset 6 to opset 5
-    --Sigmoid from opset 5 to opset 6
-    --Selu from opset 6 to opset 5
-    --Selu from opset 5 to opset 6
-    --Reciprocal from opset 6 to opset 5
-    --Reciprocal from opset 5 to opset 6
-    --Neg from opset 6 to opset 5
-    --Neg from opset 5 to opset 6
-    --Log from opset 6 to opset 5
-    --Log from opset 5 to opset 6
-    --LeakyRelu from opset 6 to opset 5
-    --LeakyRelu from opset 5 to opset 6
-    --InstanceNormalization from opset 6 to opset 5
-    --InstanceNormalization from opset 5 to opset 6
-    --HardSigmoid from opset 6 to opset 5
-    --HardSigmoid from opset 5 to opset 6
-    --Floor from opset 6 to opset 5
-    --Floor from opset 5 to opset 6
-    --Exp from opset 6 to opset 5
-    --Exp from opset 5 to opset 6
-    --Elu from opset 6 to opset 5
-    --Elu from opset 5 to opset 6
-    --Clip from opset 6 to opset 5
-    --Clip from opset 5 to opset 6
-    --Ceil from opset 6 to opset 5
-    --Ceil from opset 5 to opset 6
-    --Cast from opset 6 to opset 5
-    --Cast from opset 5 to opset 6
-    --Abs from opset 6 to opset 5
-    --Abs from opset 5 to opset 6
-    --Split from opset 2 to opset 1
-    --Split from opset 1 to opset 2
-    --Pad from opset 2 to opset 1
-    --Pad from opset 1 to opset 2
-    --LpPool from opset 2 to opset 1
-    --LpPool from opset 1 to opset 2
-    --GlobalLpPool from opset 2 to opset 1
-    --GlobalLpPool from opset 1 to opset 2
-"""
-
 
 def convert_version(model, target_version):  # type: (ModelProto, int) -> ModelProto
+    """Apply the version conversion on the serialized ModelProto.
+
+    Arguments:
+        input (ModelProto): model
+        target_version (int): target opset version
+
+    Return:
+        return (ModelProto) converted model
+
+    Raises Exceptions:
+        RuntimeError when some necessary conversion is not supported
+
+    Supported adapters:
+        --Add from Opset 7 to Opset 6
+        --Add from Opset 6 to Opset 5
+        --Add from Opset 6 to Opset 7
+        --Add from Opset 5 to Opset 6
+        --Mul from Opset 6 to Opset 7
+        --Mul from Opset 7 to Opset 6
+        --Mul from Opset 6 to Opset 5
+        --Mul from Opset 5 to Opset 6
+        --Gemm from Opset 7 to Opset 6
+        --Gemm from Opset 6 to Opset 5
+        --Gemm from Opset 6 to Opset 7
+        --Gemm from Opset 5 to Opset 6
+        --Relu from Opset 6 to Opset 5
+        --Relu from Opset 5 to Opset 6
+        --BatchNorm from Opset 7 to Opset 6
+        --BatchNorm from Opset 6 to Opset 7
+        --BatchNorm from Opset 6 to Opset 5
+        --BatchNorm from Opset 5 to Opset 6
+        --Concat from Opset 4 to Opset 3
+        --Concat from Opset 3 to Opset 4
+        --Reshape from Opset 5 to Opset 4
+        --Reshape from Opset 4 to Opset 5
+        --Sum from Opset 7 to Opset 8
+        --Sum from Opset 8 to Opset 7
+        --Sum from Opset 6 to Opset 5
+        --Sum from Opset 5 to Opset 6
+        --MaxPool from Opset 8 to Opset 7
+        --MaxPool from Opset 7 to Opset 8
+        --AveragePool from Opset 7 to Opset 6
+        --AveragePool from Opset 6 to Opset 7
+        --Dropout from Opset 7 to Opset 6
+        --Dropout from Opset 6 to Opset 5
+        --Dropout from Opset 6 to Opset 7
+        --Dropout from Opset 5 to Opset 6
+        --RNN from Opset 13 to Opset 14
+        --RNN from Opset 14 to Opset 13
+        --GRU from Opset 13 to Opset 14
+        --GRU from Opset 14 to Opset 13
+        --LSTM from Opset 13 to Opset 14
+        --LSTM from Opset 14 to Opset 13
+
+    Unsupported adapters:
+        --Min from Opset 8 to Opset 7
+        --Min from Opset 7 to Opset 8
+        --Min from Opset 6 to Opset 5
+        --Min from Opset 5 to Opset 6
+        --Mean from Opset 8 to Opset 7
+        --Mean from Opset 7 to Opset 8
+        --Mean from Opset 6 to Opset 5
+        --Mean from Opset 5 to Opset 6
+        --Max from Opset 8 to Opset 7
+        --Max from Opset 7 to Opset 8
+        --Max from Opset 6 to Opset 5
+        --Max from Opset 5 to Opset 6
+        --Xor from Opset 6 to Opset 7
+        --Xor from Opset 7 to Opset 6
+        --Upsample from Opset 6 to Opset 7
+        --Upsample from Opset 7 to Opset 6
+        --Sub from Opset 6 to Opset 7
+        --Sub from Opset 7 to Opset 6
+        --Sub from Opset 6 to Opset 5
+        --Sub from Opset 5 to Opset 6
+        --RNN from Opset 6 to Opset 7
+        --RNN from Opset 7 to Opset 6
+        --Pow from Opset 6 to Opset 7
+        --Pow from Opset 7 to Opset 6
+        --PRelu from Opset 6 to Opset 7
+        --PRelu from Opset 7 to Opset 6
+        --PRelu from Opset 6 to Opset 5
+        --PRelu from Opset 5 to Opset 6
+        --Or from Opset 6 to Opset 7
+        --Or from Opset 7 to Opset 6
+        --Less from Opset 6 to Opset 7
+        --Less from Opset 7 to Opset 6
+        --LSTM from Opset 6 to Opset 7
+        --LSTM from Opset 7 to Opset 6
+        --Greater from Opset 6 to Opset 7
+        --Greater from Opset 7 to Opset 6
+        --GRU from Opset 6 to Opset 7
+        --GRU from Opset 7 to Opset 6
+        --GRU from Opset 3 to Opset 2
+        --GRU from Opset 2 to Opset 3
+        --Equal from Opset 6 to Opset 7
+        --Equal from Opset 7 to Opset 6
+        --Div from Opset 6 to Opset 7
+        --Div from Opset 7 to Opset 6
+        --Div from Opset 6 to Opset 5
+        --Div from Opset 5 to Opset 6
+        --And from Opset 6 to Opset 7
+        --And from Opset 7 to Opset 6
+        --And from Opset 6 to Opset 5
+        --And from Opset 5 to Opset 6
+        --Tile from Opset 6 to Opset 5
+        --Tile from Opset 5 to Opset 6
+        --Sqrt from Opset 6 to Opset 5
+        --Sqrt from Opset 5 to Opset 6
+        --Sigmoid from opset 6 to opset 5
+        --Sigmoid from opset 5 to opset 6
+        --Selu from opset 6 to opset 5
+        --Selu from opset 5 to opset 6
+        --Reciprocal from opset 6 to opset 5
+        --Reciprocal from opset 5 to opset 6
+        --Neg from opset 6 to opset 5
+        --Neg from opset 5 to opset 6
+        --Log from opset 6 to opset 5
+        --Log from opset 5 to opset 6
+        --LeakyRelu from opset 6 to opset 5
+        --LeakyRelu from opset 5 to opset 6
+        --InstanceNormalization from opset 6 to opset 5
+        --InstanceNormalization from opset 5 to opset 6
+        --HardSigmoid from opset 6 to opset 5
+        --HardSigmoid from opset 5 to opset 6
+        --Floor from opset 6 to opset 5
+        --Floor from opset 5 to opset 6
+        --Exp from opset 6 to opset 5
+        --Exp from opset 5 to opset 6
+        --Elu from opset 6 to opset 5
+        --Elu from opset 5 to opset 6
+        --Clip from opset 6 to opset 5
+        --Clip from opset 5 to opset 6
+        --Ceil from opset 6 to opset 5
+        --Ceil from opset 5 to opset 6
+        --Cast from opset 6 to opset 5
+        --Cast from opset 5 to opset 6
+        --Abs from opset 6 to opset 5
+        --Abs from opset 5 to opset 6
+        --Split from opset 2 to opset 1
+        --Split from opset 1 to opset 2
+        --Pad from opset 2 to opset 1
+        --Pad from opset 1 to opset 2
+        --LpPool from opset 2 to opset 1
+        --LpPool from opset 1 to opset 2
+        --GlobalLpPool from opset 2 to opset 1
+        --GlobalLpPool from opset 1 to opset 2
+    """
     if not isinstance(model, ModelProto):
         raise ValueError('VersionConverter only accepts ModelProto as model, incorrect type: {}'.format(type(model)))
     if not isinstance(target_version, int):
--- a/requirements-release.txt
+++ b/requirements-release.txt
@@ -1,4 +1,4 @@
-numpy == 1.16.6
+numpy
 protobuf == 3.16.0
 pytest
 nbval
