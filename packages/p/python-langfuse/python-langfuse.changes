-------------------------------------------------------------------
Sat Jul  5 10:28:23 UTC 2025 - Dirk Müller <dmueller@suse.com>

- update to 3.1.2:
  * fix(langchain): do not stringify metadata unnecessarily
  * chore: add good .env defaults
  * fix(openai): add check for metadata NotGiven
  * fix(scores): skip sampling if OTEL sampler not available
  * fix(dataset-run-items): correctly typ datasetRunItems.list
- update to 3.1.1:
  * fix(client): do not escape url param with httpx > 0.28
- update to 3.1.0:
  * feat(prompts): chat message placeholders
- update to 3.0.8:
  * feat(langchain): add last trace id property to
    CallbackHandler
  * fix(openai): chat.completions.parse out of beta
- update to 2.60.9:
  * fix(openai): chat completions parse out of beta (sdk-v2)
- update to 3.0.7:
  * fix(client): auth check on missing credentials
  * fix(flushing): rely on OTEL default flush settings
- update to 3.0.6:
  * fix(prompts): escape json in get_langchain_prompt
- update to 3.0.5:
  * feat(client): allow filtering spans by instrumentation scope
  * fix(observe): default IO capture on decorated functions
- update to 3.0.4:
  * fix(client): use tracing_enabled in get_client
- update to 3.0.3:
  * fix(client): correctly scope client in multiproject setups
  * chore(deps): bump protobuf from 5.29.4 to 5.29.5
- update to 3.0.2:
  * fix: set timeout to 5 seconds
  * chore(deps): bump requests from 2.32.3 to 2.32.4
  * feat(langchain): langfuse trace attributes in run invocation
    config
  * fix(client): url encoding slashes
  * fix(resource-manager): do not register processor if tracing
- update to 3.0.1:
  * fix: treat `usage_metadata.total_token_count` of vertex ai as
    total
  * fix(client): scope mask function to resources singleton
- update to 3.0.0:
  * This is the new OTEL-based Langfuse Python SDK. See the our
    changelog post for more details and let us know your feedback
    in this GitHub Discussion.
- update to 2.60.8:
  * fix(langchain): deployment version in azure model
- update to 2.60.7:
  * fix(langchain): vertex token counts
- update to 2.60.6:
  * fix(langchain): anthropic usage parsing
- update to 2.60.5:
  * fix(openai): usage parsing for streamed responses
- update to 2.60.4:
  * feat: add env filter to fetch_traces() and
    fetch_observations()
- update to 2.60.3:
  * fix(langchain): capture tool_call_id

-------------------------------------------------------------------
Fri Apr 11 20:57:55 UTC 2025 - Dirk Müller <dmueller@suse.com>

- update to 2.60.2:
  * fix(cost-tracking): handle none values in OpenAI schema
- update to 2.60.1:
  * fix(openai): remove unused openai.resources import
- update to 2.60.0:
  * feat(openai): add Response API support
  * fix(ingestion_consumer): mask before multimodal handling
- update to 2.59.7:
  * feat(client): add native environments
- update to 2.59.6:
  * fix(openai): handle missing text property on streamed
    completion
- update to 2.59.5:
  * Resolve runtime error with openai extension when metadata is
    missing
  * fix(openai): apply langfuse_mask
- update to 2.59.4:
  * fix(langchain): cached token usage
- update to 2.59.3:
  * fix(openai): implement aclose on async stream responses
- update to 2.59.2:
  * fix(serializer): NaN handling
  * feat(prompts): add commit message to prompt creation
- update to 2.59.1:
  * perf(ingestion): make max event and batch size configurable
- update to 2.59.0:
  * feat(api): expose public api client
  * feat(client): add async api client
- update to 2.58.2:
  * fix(openai): handle usage object without mutation
  * fix(media): retry requests upload errors
- update to 2.58.1:
  * fix(llama-index): workflow integration
- update to 2.58.0:
  * feat: update prompts and invalidate cache
- update to 2.57.13:
  * fix(langchain): add run_id check on_llm_error
- update to 2.57.12:
  * fix(decorators): merge trace tags and metadata on updates
- update to 2.57.11:
  * fix(openai): allow non in token_details
- update to 2.57.10:
  * fix(langchain): mitigate missing run_id on_chain_error
  * fix(openai): allow for none values in usage details in
    https://github.com/langfuse/langfuse-
    python/commit/4d47959fcb5cc473ba8bf59c72ed8dc2c6429677
- update to 2.57.9:
  * feat(langchain): add debug level to langsmith:hidden tagged
    spans
- update to 2.57.8:
  * perf(decorator): remove get_caller_module
- update to 2.57.7:
  * fix(api-models): usage unit to optional
- update to 2.57.6:
  * chore(models): update fern api
- update to 2.57.5:
  * fix(serializer): remove unnecessary log statement by
- update to 2.57.4:
  * fix: add project_id to urls returned by
    langfuse_context.get_current_trace_url
- update to 2.57.3:
  * fix(dataset+decorator): item observer with top-level
    generation
- update to 2.57.2:
  * perf: memory usage on repeat instantiation
- update to 2.57.1:
  * chore(singleton): shutdown on reset
- update to 2.56.2:
  * feat(media): utility to resolve media references with media
    content
  * fix(media): add timeout to reference resolution
  * chore(deps): bump the langchain group with 2 updates
- update to 2.56.1:
  * fix(client): pass timeout to FernLangfuse
  * fix(openai): pass metadata when model distillation feat is
    used
  * chore(deps-dev): bump pytest from 8.3.3 to 8.3.4
  * chore(deps-dev): bump the langchain group with 5 updates
  * chore(deps): bump django from 5.0.9 to 5.0.10 in
    /examples/django_example
  * chore(deps-dev): bump boto3 from 1.35.76 to 1.35.77
- update to 2.56.0:
  * chore(deps): bump the langchain group across 1 directory with
    2 updates
  * chore(ci): update to v3 docker compose
  * feat: use projectid when generating trace urls

-------------------------------------------------------------------
Wed Nov 20 15:19:09 UTC 2024 - Dirk Müller <dmueller@suse.com>

- update to 2.54.1:
  * fix(media): allow setting IO media via decorator update
- update to 2.54.0:
  * feat(core): add multimodal support
  * fix(openai): pass parsed_n only if greater 1
- update to 2.53.9:
  * perf: move serialization to background threads
- update to 2.53.8:
  * fix(datasets): encoding
- update to 2.53.7:
  * fix(openai): revert default stream option setting
- update to 2.53.6:
  * fix(serializer): reduce log level to debug on failed
    serialization
- update to 2.53.5:
  * fix(serializer): pydantic compat v1 v2
- update to 2.53.4:
  * feat(openai): parse usage if stream_options has include_usage
- update to 2.53.3:
  * fix(datasets): url encode dataset name and run name
  * refactor(llama-index): send generation updates directly from
    event handler
- update to 2.53.2:
  * fix(llama-index): CompletionResponse Serialization by
    @hassiebp
- update to 2.53.1:
  * fix: 'NoneType' object has no attribute '__dict__'
- update to 2.53.0:
  * feat(client): allow masking event input and output by
    @shawnzhu and @hassiebp in
    https://github.com/langfuse/langfuse-python/pull/977
  * fix(decorator): improve stack access safety
- update to 2.52.2:
  * fix(openai): handle NoneType responses gracefully
  * docs(llama-index): deprecate CallbackHandler and promote
    Instrumentor
  * fix(langchain): DeepInfra model parsing
- update to 2.52.1:
  * fix(decorators): stack trace on failed auth_check
  * fix(api): list models parsing errors
  * fix(langchain): parse tool calls in input
- update to 2.52.0:
  * feat(llama-index): add LlamaIndexInstrumentor for use with
    instrumentation module instead of LlamaIndexSpanHandler
- update to 2.51.5:
  * fix(openai): structured output parsing with openai >= 1.50
  * chore(deps): bump django from 5.0.8 to 5.0.9 in
    /examples/django_example
- update to 2.51.4:
  * fix(serializer): Fix None serialization without langchain
    installed
  * fix(langchain): invalid user_id / session_id in TraceBody
  * chore(deps-dev): bump pytest from 8.2.0 to 8.3.3
  * chore: automerge dependabot patch PRs
  * chore(deps): bump pydantic from 2.7.4 to 2.9.2
  * chore: fix dependabot automerge
  * chore: fix auto merging
  * chore: fix auto merging
- update to 2.51.3:
  * fix(langchain): time to first token
- update to 2.51.2:
  * fix(prompts): remove traceback from fetch error logs
  * fix(langchain): parse model name for ChatOCIGenAIAgent
- update to 2.51.0:
  * feat(langchain): time to first token on streamed generations
  * feat(langchain): allow passing trace attributes on chain
    invocation
  * fix(langchain): python langchain retriever - parent run not
    found
  * fix handle_span_events exception
- update to 2.50.3:
  * fix(serializer): support numpy scalars
- update to 2.50.2:
  * feat(prompts): allow passing kwargs to precompile langchain
    prompt
- update to 2.50.1:
  * fix(langchain): link prompts to nested generations as well
- update to 2.50.0:
  * feat(decorator): allow setting parent trace or observation id
- update to 2.49.0:
  * feat(langchain): link langfuse prompts to langchain
    executions
- update to 2.48.1:
  * fix(openai): multiple streamed tool calls
  * fix(openai): time to first token
- update to 2.48.0:
  * feat(decorator): Enhance `observe` decorator to support usage
    without parentheses
  * fix(llama-index): initialize OpenAI model serializers
  * fix(langchain): ollama usage parsing
  * fix(langchain/AzureChatOpenai): unnecessary logs
  * fix(langchain): batch run to update trace outputs
- update to 2.47.1:
  * feat: improve error messages
- update to 2.47.0:
  * feat(prompts): optionally disable prompt caching when cache
    ttl is 0
  * Docs: https://langfuse.com/docs/prompts/get-started#disable-
    caching

-------------------------------------------------------------------
Thu Oct 31 19:10:21 UTC 2024 - Matej Cepl <mcepl@cepl.eu>

- Add missing Requires.

-------------------------------------------------------------------
Fri Sep 27 01:36:18 UTC 2024 - Yogalakshmi Arunachalam <yarunachalam@suse.com>

- version update to 2.44.0
  changes since v2.43.1
  - fix: rename careers file to match hackernews post (#2115)
  - fix: link of help link of eval log
  - docs: careers file extension (#2105)
  - docs: Update .env.prod.example (#2103)
  - fix(ui): parse chatml messages where content is an array (#2100)
  - fix(ui): spacing of user menu
  - fix(ui): padding of second-level menu
  - feat(ui): less padding in main menu (#2095)
  - fix(api): merge tags when upserting a trace (#2091)
  - fix: set cache control explicitly (#2088)
  - fix(cloud): remove custom cookie management (#2087)
  - chore: adjust sentry (#2086)
  - feat(cloud): include data region in project invite emails
  - feat(auth): improve auth error messages and logging
  - fix(ui): use `svh` for fullscreen pages on mobile (#2076)
  - feat(auth): capture unexpected credential sign-in errors
  - feat(cloud): data region select and info modal (#2085)
  - intercept OpenAI requests in worker testruns (#2020)
  - fix(api): align get/v2/prompts meta response with api spec (#2079)

-------------------------------------------------------------------
Wed Aug 21 20:09:26 UTC 2024 - Aline Werner <aline.werner@suse.com>

- Fix executable file conflict for Tumbleweed build, change description and add missing URL.

-------------------------------------------------------------------
Fri Aug  9 17:59:18 UTC 2024 - Aline Werner <aline.werner@suse.com>

- Initial package creation for python-langfuse version 2.43.1
