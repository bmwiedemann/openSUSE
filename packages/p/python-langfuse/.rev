<revisionlist>
  <revision rev="1" vrev="1">
    <srcmd5>9370f8dfb170e3d15758fa160aadd673</srcmd5>
    <version>2.43.1</version>
    <time>1725885826</time>
    <user>anag+factory</user>
    <comment>needed for AI project</comment>
    <requestid>1199287</requestid>
  </revision>
  <revision rev="2" vrev="1">
    <srcmd5>49f864fa7429ec79eec37b50665db2f1</srcmd5>
    <version>2.44.0</version>
    <time>1730491567</time>
    <user>dimstar_suse</user>
    <comment>- Add missing Requires.
- version update to 2.44.0 
  changes since v2.43.1
  - fix: rename careers file to match hackernews post (#2115)
  - fix: link of help link of eval log
  - docs: careers file extension (#2105)
  - docs: Update .env.prod.example (#2103)
  - fix(ui): parse chatml messages where content is an array (#2100)
  - fix(ui): spacing of user menu
  - fix(ui): padding of second-level menu
  - feat(ui): less padding in main menu (#2095)
  - fix(api): merge tags when upserting a trace (#2091)
  - fix: set cache control explicitly (#2088)
  - fix(cloud): remove custom cookie management (#2087)
  - chore: adjust sentry (#2086)
  - feat(cloud): include data region in project invite emails
  - feat(auth): improve auth error messages and logging
  - fix(ui): use `svh` for fullscreen pages on mobile (#2076)
  - feat(auth): capture unexpected credential sign-in errors
  - feat(cloud): data region select and info modal (#2085)
  - intercept OpenAI requests in worker testruns (#2020)
  - fix(api): align get/v2/prompts meta response with api spec (#2079)
</comment>
    <requestid>1219953</requestid>
  </revision>
  <revision rev="3" vrev="1">
    <srcmd5>189a2c09599ac8cbbf7980914749e351</srcmd5>
    <version>2.54.1</version>
    <time>1732121083</time>
    <user>anag+factory</user>
    <comment>- update to 2.54.1:
  * fix(media): allow setting IO media via decorator update
- update to 2.54.0:
  * feat(core): add multimodal support
  * fix(openai): pass parsed_n only if greater 1
- update to 2.53.9:
  * perf: move serialization to background threads
- update to 2.53.8:
  * fix(datasets): encoding
- update to 2.53.7:
  * fix(openai): revert default stream option setting
- update to 2.53.6:
  * fix(serializer): reduce log level to debug on failed
    serialization
- update to 2.53.5:
  * fix(serializer): pydantic compat v1 v2
- update to 2.53.4:
  * feat(openai): parse usage if stream_options has include_usage
- update to 2.53.3:
  * fix(datasets): url encode dataset name and run name
  * refactor(llama-index): send generation updates directly from
    event handler
- update to 2.53.2:
  * fix(llama-index): CompletionResponse Serialization by
    @hassiebp
- update to 2.53.1:
  * fix: 'NoneType' object has no attribute '__dict__'
- update to 2.53.0:
  * feat(client): allow masking event input and output by
    @shawnzhu and @hassiebp in</comment>
    <requestid>1225319</requestid>
  </revision>
  <revision rev="4" vrev="1">
    <srcmd5>d79d36d56e21a9f23e59d264d1301349</srcmd5>
    <version>2.60.2</version>
    <time>1744628160</time>
    <user>anag_factory</user>
    <comment>- update to 2.60.2:
  * fix(cost-tracking): handle none values in OpenAI schema
- update to 2.60.1:
  * fix(openai): remove unused openai.resources import
- update to 2.60.0:
  * feat(openai): add Response API support
  * fix(ingestion_consumer): mask before multimodal handling
- update to 2.59.7:
  * feat(client): add native environments
- update to 2.59.6:
  * fix(openai): handle missing text property on streamed
    completion
- update to 2.59.5:
  * Resolve runtime error with openai extension when metadata is
    missing
  * fix(openai): apply langfuse_mask
- update to 2.59.4:
  * fix(langchain): cached token usage
- update to 2.59.3:
  * fix(openai): implement aclose on async stream responses
- update to 2.59.2:
  * fix(serializer): NaN handling
  * feat(prompts): add commit message to prompt creation
- update to 2.59.1:
  * perf(ingestion): make max event and batch size configurable
- update to 2.59.0:
  * feat(api): expose public api client
  * feat(client): add async api client
- update to 2.58.2:
  * fix(openai): handle usage object without mutation</comment>
    <requestid>1268730</requestid>
  </revision>
  <revision rev="5" vrev="1">
    <srcmd5>a8547cf4758a1f8c82dc1252228d80a3</srcmd5>
    <version>3.1.2</version>
    <time>1751814910</time>
    <user>anag_factory</user>
    <comment>- update to 3.1.2:
  * fix(langchain): do not stringify metadata unnecessarily
  * chore: add good .env defaults
  * fix(openai): add check for metadata NotGiven
  * fix(scores): skip sampling if OTEL sampler not available
  * fix(dataset-run-items): correctly typ datasetRunItems.list
- update to 3.1.1:
  * fix(client): do not escape url param with httpx &gt; 0.28
- update to 3.1.0:
  * feat(prompts): chat message placeholders
- update to 3.0.8:
  * feat(langchain): add last trace id property to
    CallbackHandler
  * fix(openai): chat.completions.parse out of beta
- update to 2.60.9:
  * fix(openai): chat completions parse out of beta (sdk-v2)
- update to 3.0.7:
  * fix(client): auth check on missing credentials
  * fix(flushing): rely on OTEL default flush settings
- update to 3.0.6:
  * fix(prompts): escape json in get_langchain_prompt
- update to 3.0.5:
  * feat(client): allow filtering spans by instrumentation scope
  * fix(observe): default IO capture on decorated functions
- update to 3.0.4:
  * fix(client): use tracing_enabled in get_client
- update to 3.0.3:
  * fix(client): correctly scope client in multiproject setups
  * chore(deps): bump protobuf from 5.29.4 to 5.29.5
- update to 3.0.2:</comment>
    <requestid>1290772</requestid>
  </revision>
</revisionlist>
