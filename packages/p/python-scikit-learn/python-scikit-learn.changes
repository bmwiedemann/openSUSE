-------------------------------------------------------------------
Tue Aug  1 14:07:31 UTC 2023 - Markéta Machová <mmachova@suse.com>

- Python flavors shifted again, drop test-py38, add test-py311

-------------------------------------------------------------------
Tue Jul 25 08:39:12 UTC 2023 - Markéta Machová <mmachova@suse.com>

- Update to 1.3.0
  * We are in the process of introducing a new way to route metadata 
    such as sample_weight throughout the codebase, which would 
    affect how meta-estimators such as pipeline.Pipeline and 
    model_selection.GridSearchCV route metadata.
  * Originally hosted in the scikit-learn-contrib repository, 
    cluster.HDBSCAN has been adopted into scikit-learn.
  * A new category encoding strategy preprocessing.TargetEncoder 
    encodes the categories based on a shrunk estimate of the average 
    target values for observations belonging to that category.
  * The classes tree.DecisionTreeClassifier and tree.DecisionTreeRegressor 
    now support missing values.
  * model_selection.ValidationCurveDisplay is now available to plot 
    results from model_selection.validation_curve
  * The class ensemble.HistGradientBoostingRegressor supports the 
    Gamma deviance loss function via loss="gamma".
  * Similarly to preprocessing.OneHotEncoder, the class preprocessing.OrdinalEncoder 
    now supports aggregating infrequent categories into a single 
    output for each feature.
  * More changes, see https://scikit-learn.org/stable/whats_new/v1.3.html

-------------------------------------------------------------------
Sat Jun 10 17:37:09 UTC 2023 - ecsos <ecsos@opensuse.org>

- Add %{?sle15_python_module_pythons}

-------------------------------------------------------------------
Wed Feb  8 18:34:17 UTC 2023 - Arun Persaud <arun@gmx.de>

- update to version 1.2.1:
  * Changed models
    + The following estimators and functions, when fit with the same
      data and parameters, may produce different models from the
      previous version. This often occurs due to changes in the
      modelling logic (bug fixes or enhancements), or in random
      sampling procedures.
    + Fix The fitted components in MiniBatchDictionaryLearning might
      differ. The online updates of the sufficient statistics now
      properly take the sizes of the batches into account. #25354 by
      Jérémie du Boisberranger.
    + Fix The categories_ attribute of preprocessing.OneHotEncoder now
      always contains an array of object`s when using predefined
      categories that are strings. Predefined categories encoded as
      bytes will no longer work with `X encoded as strings. #25174 by
      Tim Head.
  * Changes impacting all modules
    + Fix Support pandas.Int64 dtyped y for classifiers and
      regressors. #25089 by Tim Head.
    + Fix Remove spurious warnings for estimators internally using
      neighbors search methods. #25129 by Julien Jerphanion.
    + Fix Fix a bug where the current configuration was ignored in
      estimators using n_jobs > 1. This bug was triggered for tasks
      dispatched by the auxillary thread of joblib as
      sklearn.get_config used to access an empty thread local
      configuration instead of the configuration visible from the
      thread where joblib.Parallel was first called. #25363 by
      Guillaume Lemaitre.
  * Changelog
    o sklearn.base
    + Fix Fix a regression in BaseEstimator.__getstate__ that would
      prevent certain estimators to be pickled when using Python
      3.11. #25188 by Benjamin Bossan.
    + Fix Inheriting from base.TransformerMixin will only wrap the
      transform method if the class defines transform itself. #25295
      by Thomas Fan.

    o sklearn.datasets
    + Fix Fix an inconsistency in datasets.fetch_openml between
      liac-arff and pandas parser when a leading space is introduced
      after the delimiter. The ARFF specs requires to ignore the
      leading space. #25312 by Guillaume Lemaitre.

    o sklearn.decomposition
    + Fix Fixed a bug in decomposition.MiniBatchDictionaryLearning
      where the online updates of the sufficient statistics where not
      correct when calling partial_fit on batches of different
      sizes. #25354 by Jérémie du Boisberranger.
    + Fix decomposition.DictionaryLearning better supports readonly
      NumPy arrays. In particular, it better supports large datasets
      which are memory-mapped when it is used with coordinate descent
      algorithms (i.e. when fit_algorithm='cd'). #25172 by Julien
      Jerphanion.

    o sklearn.ensemble
    + Fix ensemble.RandomForestClassifier,
      ensemble.RandomForestRegressor ensemble.ExtraTreesClassifier and
      ensemble.ExtraTreesRegressor now support sparse readonly
      datasets. #25341 by Julien Jerphanion
    + sklearn.feature_extraction
    + Fix feature_extraction.FeatureHasher raises an informative error
      when the input is a list of strings. #25094 by Thomas Fan.

    o sklearn.linear_model
    + Fix Fix a regression in linear_model.SGDClassifier and
      linear_model.SGDRegressor that makes them unusable with the
      verbose parameter set to a value greater than 0. #25250 by
      Jérémie Du Boisberranger.

    o sklearn.manifold
    + Fix manifold.TSNE now works correctly when output type is set to
      pandas #25370 by Tim Head.

    o sklearn.model_selection
    + Fix model_selection.cross_validate with multimetric scoring in
      case of some failing scorers the non-failing scorers now returns
      proper scores instead of error_score values. #23101 by András
      Simon and Thomas Fan.

    o sklearn.neural_network
    + Fix neural_network.MLPClassifier and neural_network.MLPRegressor
      no longer raise warnings when fitting data with feature
      names. #24873 by Tim Head.

    o sklearn.preprocessing
    + Fix preprocessing.FunctionTransformer.inverse_transform
      correctly supports DataFrames that are all numerical when
      check_inverse=True. #25274 by Thomas Fan.
    + Fix preprocessing.SplineTransformer.get_feature_names_out
      correctly returns feature names when
      extrapolations="periodic". #25296 by Thomas Fan.

    o sklearn.tree
    + Fix tree.DecisionTreeClassifier, tree.DecisionTreeRegressor
      tree.ExtraTreeClassifier and tree.ExtraTreeRegressor now support
      sparse readonly datasets. #25341 by Julien Jerphanion

    o sklearn.utils
    + Fix Restore utils.check_array’s behaviour for pandas Series of
      type boolean. The type is maintained, instead of converting to
      float64. #25147 by Tim Head.
  * API Change utils.fixes.delayed is deprecated in 1.2.1 and will be
    removed in 1.5. Instead, import utils.parallel.delayed and use it
    in conjunction with the newly introduced utils.parallel.Parallel
    to ensure proper propagation of the scikit-learn configuration to
    the workers. #25363 by Guillaume Lemaitre.

-------------------------------------------------------------------
Sun Jan 15 13:04:19 UTC 2023 - Ben Greiner <code@bnavigator.de>

- Update to version 1.2.0
  * Pandas output with set_output API
  * Interaction constraints in Histogram-based Gradient Boosting
    Trees
  * New and enhanced displays
  * Faster parser in fetch_openml
  * Experimental Array API support in LinearDiscriminantAnalysis
  * Improved efficiency of many estimators
- Drop sklearn-pr24283-gradient-segfault.patch
- PEP517 build

-------------------------------------------------------------------
Thu Oct 27 18:40:17 UTC 2022 - Ben Greiner <code@bnavigator.de>

- Update to version 1.1.3
  * This bugfix release only includes fixes for compatibility with
    the latest SciPy release >= 1.9.2.
- Update sklearn-pr24283-gradient-segfault.patch

-------------------------------------------------------------------
Tue Oct 11 13:10:22 UTC 2022 - Ben Greiner <code@bnavigator.de>

- Update dependencies
- Add sklearn-pr24283-gradient-segfault.patch
  * gh#scikit-learn/scikit-learn#24283
- Update test suite setup.

-------------------------------------------------------------------
Sat Sep 10 13:32:25 UTC 2022 - Arun Persaud <arun@gmx.de>

- update to version 1.1.2:
  * Changes
    + Fix A default HTML representation is shown for meta-estimators
      with invalid parameters. #24015 by Thomas Fan.
    + Fix Add support for F-contiguous arrays for estimators and
      functions whose back-end have been changed in 1.1. #23990 by
      Julien Jerphanion.
    + Fix Wheels are now available for MacOS 10.9 and greater. #23833
      by Thomas Fan.
  * sklearn.base
    + Fix The get_params method of the BaseEstimator class now
      supports estimators with type-type params that have the
      get_params method. #24017 by Henry Sorsky.
  * sklearn.cluster
    + Fix Fixed a bug in cluster.Birch that could trigger an error
      when splitting a node if there are duplicates in the
      dataset. #23395 by Jérémie du Boisberranger.
  * sklearn.feature_selection
    + Fix feature_selection.SelectFromModel defaults to selection
      threshold 1e-5 when the estimator is either
      linear_model.ElasticNet or linear_model.ElasticNetCV with
      l1_ratio equals 1 or linear_model.LassoCV. #23636 by Hao Chun
      Chang.
  * sklearn.impute
    + Fix impute.SimpleImputer uses the dtype seen in fit for
      transform when the dtype is object. #22063 by Thomas Fan.
  * sklearn.linear_model
    + Fix Use dtype-aware tolerances for the validation of gram
      matrices (passed by users or precomputed). #22059 by Malte
      S. Kurz.
    + Fix Fixed an error in linear_model.LogisticRegression with
      solver="newton-cg", fit_intercept=True, and a single
      feature. #23608 by Tom Dupre la Tour.
  * sklearn.manifold
    + Fix manifold.TSNE now throws a ValueError when fit with
      perplexity>=n_samples to ensure mathematical correctness of the
      algorithm. #10805 by Mathias Andersen and #23471 by Meekail
      Zain.
  * sklearn.metrics
    + Fix Fixed error message of metrics.coverage_error for 1D array
      input. #23548 by Hao Chun Chang.
  * sklearn.preprocessing
    + Fix preprocessing.OrdinalEncoder.inverse_transform correctly
      handles use cases where unknown_value or encoded_missing_value
      is nan. #24087 by Thomas Fan.
  * sklearn.tree
    + Fix Fixed invalid memory access bug during fit in
      tree.DecisionTreeRegressor and
      tree.DecisionTreeClassifier. #23273 by Thomas Fan.

-------------------------------------------------------------------
Tue May 31 03:09:03 UTC 2022 - Arun Persaud <arun@gmx.de>

- specfile
  * updated numpy, scipy, and matplotlib requirements

- update to version 1.1.1:
  * Enhancement The error message is improved when importing
    model_selection.HalvingGridSearchCV,
    model_selection.HalvingRandomSearchCV, or impute.IterativeImputer
    without importing the experimental flag. #23194 by Thomas Fan.
  * Enhancement Added an extension in doc/conf.py to automatically
    generate the list of estimators that handle NaN values. #23198 by
    Lise Kleiber, Zhehao Liu and Chiara Marmo.
  * sklearn.datasets
    + Fix Avoid timeouts in datasets.fetch_openml by not passing a
      timeout argument, #23358 by Loïc Estève.
  * sklearn.decomposition
    + Fix Avoid spurious warning in decomposition.IncrementalPCA when
      n_samples == n_components. #23264 by Lucy Liu.
  * sklearn.feature_selection
    + Fix The partial_fit method of feature_selection.SelectFromModel
      now conducts validation for max_features and feature_names_in
      parameters. #23299 by Long Bao.
  * sklearn.metrics
    + Fix Fixes metrics.precision_recall_curve to compute
      precision-recall at 100% recall. The Precision-Recall curve now
      displays the last point corresponding to a classifier that
      always predicts the positive class: recall=100% and
      precision=class balance. #23214 by Stéphane Collot and Max Baak.
  * sklearn.preprocessing
    + Fix preprocessing.PolynomialFeatures with degree equal to 0 will
      raise error when include_bias is set to False, and outputs a
      single constant array when include_bias is set to True. #23370
      by Zhehao Liu.
  * sklearn.tree
    + Fix Fixes performance regression with low cardinality features
      for tree.DecisionTreeClassifier, tree.DecisionTreeRegressor,
      ensemble.RandomForestClassifier, ensemble.RandomForestRegressor,
      ensemble.GradientBoostingClassifier, and
      ensemble.GradientBoostingRegressor. #23410 by Loïc Estève.
  * sklearn.utils
    + Fix utils.class_weight.compute_sample_weight now works with
      sparse y. #23115 by kernc.

- changes from version 1.1.0:
long changelog, see https://scikit-learn.org/stable/whats_new/v1.1.html#version-1-1-0

-------------------------------------------------------------------
Mon May 30 10:40:34 UTC 2022 - Steve Kowalik <steven.kowalik@suse.com>

- Split up to using multibuild per Python version since the test suite
  may take a while. 

-------------------------------------------------------------------
Wed Feb  2 02:07:05 UTC 2022 - Steve Kowalik <steven.kowalik@suse.com>

- Update to 1.0.2: 
  * Fixed an infinite loop in cluster.SpectralClustering by moving an iteration counter from try to except. #21271 by Tyler Martin.
  * datasets.fetch_openml is now thread safe. Data is first downloaded to a temporary subfolder and then renamed. #21833 by Siavash Rezazadeh.
  * Fixed the constraint on the objective function of decomposition.DictionaryLearning, decomposition.MiniBatchDictionaryLearning, decomposition.SparsePCA and decomposition.MiniBatchSparsePCA to be convex and match the referenced article. #19210 by Jérémie du Boisberranger.
  * ensemble.RandomForestClassifier, ensemble.RandomForestRegressor, ensemble.ExtraTreesClassifier, ensemble.ExtraTreesRegressor, and ensemble.RandomTreesEmbedding now raise a ValueError when bootstrap=False and max_samples is not None. #21295 Haoyin Xu.
  * Solve a bug in ensemble.GradientBoostingClassifier where the exponential loss was computing the positive gradient instead of the negative one. #22050 by Guillaume Lemaitre.
  * Fixed feature_selection.SelectFromModel by improving support for base estimators that do not set feature_names_in_. #21991 by Thomas Fan.
  * Fix a bug in linear_model.RidgeClassifierCV where the method predict was performing an argmax on the scores obtained from decision_function instead of returning the multilabel indicator matrix. #19869 by Guillaume Lemaitre.
  * linear_model.LassoLarsIC now correctly computes AIC and BIC. An error is now raised when n_features > n_samples and when the noise variance is not provided. #21481 by Guillaume Lemaitre and Andrés Babino.
  * Fixed an unnecessary error when fitting manifold.Isomap with a precomputed dense distance matrix where the neighbors graph has multiple disconnected components. #21915 by Tom Dupre la Tour.
  * All sklearn.metrics.DistanceMetric subclasses now correctly support read-only buffer attributes. This fixes a regression introduced in 1.0.0 with respect to 0.24.2. #21694 by Julien Jerphanion.
  * neighbors.KDTree and neighbors.BallTree correctly supports read-only buffer attributes. #21845 by Thomas Fan.
  * Fixes compatibility bug with NumPy 1.22 in preprocessing.OneHotEncoder. #21517 by Thomas Fan.
  * Prevents tree.plot_tree from drawing out of the boundary of the figure. #21917 by Thomas Fan.
  * Support loading pickles of decision tree models when the pickle has been generated on a platform with a different bitness. A typical example is to train and pickle the model on 64 bit machine and load the model on a 32 bit machine for prediction. #21552 by Loïc Estève.
  * Non-fit methods in the following classes do not raise a UserWarning when fitted on DataFrames with valid feature names: covariance.EllipticEnvelope, ensemble.IsolationForest, ensemble.AdaBoostClassifier, neighbors.KNeighborsClassifier, neighbors.KNeighborsRegressor, neighbors.RadiusNeighborsClassifier, neighbors.RadiusNeighborsRegressor. #21199 by Thomas Fan.
  * Fixed calibration.CalibratedClassifierCV to take into account sample_weight when computing the base estimator prediction when ensemble=False. #20638 by Julien Bohné.
  * Fixed a bug in calibration.CalibratedClassifierCV with method="sigmoid" that was ignoring the sample_weight when computing the the Bayesian priors. #21179 by Guillaume Lemaitre.
  * Compute y_std properly with multi-target in sklearn.gaussian_process.GaussianProcessRegressor allowing proper normalization in multi-target scene. #20761 by Patrick de C. T. R. Ferreira.
  * Fixed a bug in feature_extraction.CountVectorizer and feature_extraction.TfidfVectorizer by raising an error when ‘min_idf’ or ‘max_idf’ are floating-point numbers greater than 1. #20752 by Alek Lefebvre.
  * linear_model.LogisticRegression now raises a better error message when the solver does not support sparse matrices with int64 indices. #21093 by Tom Dupre la Tour.
  * neighbors.KNeighborsClassifier, neighbors.KNeighborsRegressor, neighbors.RadiusNeighborsClassifier, neighbors.RadiusNeighborsRegressor with metric="precomputed" raises an error for bsr and dok sparse matrices in methods: fit, kneighbors and radius_neighbors, due to handling of explicit zeros in bsr and dok sparse graph formats. #21199 by Thomas Fan.
  * pipeline.Pipeline.get_feature_names_out correctly passes feature names out from one step of a pipeline to the next. #21351 by Thomas Fan.
  * svm.SVC and svm.SVR check for an inconsistency in its internal representation and raise an error instead of segfaulting. This fix also resolves CVE-2020-28975. #21336 by Thomas Fan.
  * manifold.TSNE now avoids numerical underflow issues during affinity matrix computation.
  * manifold.Isomap now connects disconnected components of the neighbors graph along some minimum distance pairs, instead of changing every infinite distances to zero.
  * Many others, see full changelog at https://scikit-learn.org/dev/whats_new/v1.0.html

-------------------------------------------------------------------
Sun Jun  6 13:27:15 UTC 2021 - Dirk Müller <dmueller@suse.com>

- update to 0.24.2:
  * a lot of bugfixes see https://scikit-learn.org/stable/whats_new/v0.24.html
- drop scikit-learn-pr19101-npfloat.patch: upstream

-------------------------------------------------------------------
Sat Feb 13 19:10:14 UTC 2021 - Ben Greiner <code@bnavigator.de>

- Add scikit-learn-pr19101-npfloat.patch in order to work with
  NumPy 1.20 

-------------------------------------------------------------------
Fri Jan 22 18:54:07 UTC 2021 - Benjamin Greiner <code@bnavigator.de>

- Skip python36 because SciPy 1.6.0 dropped it
- optionally enable more tests with matplotlib and pandas by
  --with extratests

-------------------------------------------------------------------
Fri Jan 22 08:11:53 UTC 2021 - andy great <andythe_great@pm.me>

- Skip test_convergence_dtype_consistency on 32 bit arch due to 
  precision-related errors on 32 bit arch.
  https://github.com/scikit-learn/scikit-learn/issues/19230
- Remove explicit dependecy python-matplotlib
  
-------------------------------------------------------------------
Wed Jan 20 19:43:33 UTC 2021 - andy great <andythe_great@pm.me>

- Remove assert_allclose-for-FP-comparison.patch, fixed.
- Update to version 0.24.1.
  * sklearn.metrics
    * Fix numerical stability bug that could happen in 
      metrics.adjusted_mutual_info_score and metrics.mutual_info_score
      with NumPy 1.20+.
  * sklearn.semi_supervised
    * Fix semi_supervised.SelfTrainingClassifier is now accepting 
      meta-estimator (e.g. ensemble.StackingClassifier). The 
      validation of this estimator is done on the fitted estimator,
      once we know the existence of the method predict_proba. 
- Updates for version 0.24.0.
  * sklearn.base
    * Fix base.BaseEstimator.get_params now will raise an 
      AttributeError if a parameter cannot be retrieved as an 
      instance attribute. Previously it would return None.
  * sklearn.calibration
    * Efficiency calibration.CalibratedClassifierCV.fit now supports
      parallelization via joblib.Parallel using argument n_jobs.
    * Enhancement Allow calibration.CalibratedClassifierCV use with 
      prefit pipeline.Pipeline where data is not X is not array-like,
      sparse matrix or dataframe at the start. #17546 by Lucy Liu.
    * Enhancement Add ensemble parameter to 
      calibration.CalibratedClassifierCV, which enables 
      implementation of calibration via an ensemble of calibrators 
      (current method) or just one calibrator using all the data 
      (similar to the built-in feature of sklearn.svm estimators with
      the probabilities=True parameter).
  * sklearn.cluster
    * Enhancement cluster.AgglomerativeClustering has a new 
      parameter compute_distances. When set to True, distances 
      between clusters are computed and stored in the distances_ 
      attribute even when the parameter distance_threshold is not 
      used. This new parameter is useful to produce dendrogram 
      visualizations, but introduces a computational and memory 
      overhead.
    * Enhancement cluster.SpectralClustering and 
      cluster.spectral_clustering have a new keyword argument verbose.
      When set to True, additional messages will be displayed which 
      can aid with debugging. #18052 by Sean O. Stalley.
    * Enhancement Added cluster.kmeans_plusplus as public function. 
      Initialization by KMeans++ can now be called separately to 
      generate initial cluster centroids.
    * API Change cluster.MiniBatchKMeans attributes, counts_ and 
      init_size_, are deprecated and will be removed in 1.1 
      (renaming of 0.26).
  * sklearn.compose
    * Fix compose.ColumnTransformer will skip transformers the 
      column selector is a list of bools that are False.
    * Fix compose.ColumnTransformer now displays the remainder in 
      the diagram display. #18167 by Thomas Fan.
    * Fix compose.ColumnTransformer enforces strict count and order
      of column names between fit and transform by raising an error 
      instead of a warning, following the deprecation cycle.
  * sklearn.covariance
    * API Change Deprecates cv_alphas_ in favor of 
      cv_results_['alphas'] and grid_scores_ in favor of split scores
      in cv_results_ in covariance.GraphicalLassoCV. cv_alphas_ and 
      grid_scores_ will be removed in version 1.1 (renaming of 0.26).
  * sklearn.cross_decomposition
    * Fixed a bug in cross_decomposition.PLSSVD which would sometimes
      return components in the reversed order of importance.
    * Fixed a bug in cross_decomposition.PLSSVD, 
      cross_decomposition.CCA, and cross_decomposition.PLSCanonical,
      which would lead to incorrect predictions for est.transform(Y) 
      when the training data is single-target.
    * Fix Increases the stability of cross_decomposition.CCA
    * API Change For cross_decomposition.NMF, the init value, when
      ‘init=None’ and n_components <= min(n_samples, n_features) 
      will be changed from 'nndsvd' to 'nndsvda' in 1.1 
      (renaming of 0.26).
    * API Change The bounds of the n_components parameter is now 
      restricted:
      * into [1, min(n_samples, n_features, n_targets)], for 
        cross_decomposition.PLSSVD, cross_decomposition.CCA, and 
        cross_decomposition.PLSCanonical.
      * into [1, n_features] or cross_decomposition.PLSRegression.
    * An error will be raised in 1.1 (renaming of 0.26).
    * API Change For cross_decomposition.PLSSVD, 
      cross_decomposition.CCA, and cross_decomposition.PLSCanonical,
      the x_scores_ and y_scores_ attributes were deprecated and will
      be removed in 1.1 (renaming of 0.26). They can be retrieved 
      by calling transform on the training data. The norm_y_weights
      attribute will also be removed. #17095 by Nicolas Hug.
    * API Change For cross_decomposition.PLSRegression, 
      cross_decomposition.PLSCanonical, cross_decomposition.CCA, 
      and cross_decomposition.PLSSVD, the x_mean_, y_mean_, x_std_,
      and y_std_ attributes were deprecated and will be removed in 
      1.1 (renaming of 0.26).
    * Fix decomposition.TruncatedSVD becomes deterministic by using
      the random_state. It controls the weights’ initialization of 
      the underlying ARPACK solver.
  * sklearn.datasets
    * Feature datasets.fetch_openml now validates md5 checksum of 
      arff files downloaded or cached to ensure data integrity. 
    * Feature datasets.fetch_openml now validates md5checksum of 
      arff files downloaded or cached to ensure data integrity. 
    * Enhancement datasets.fetch_openml now allows argument 
      as_frame to be ‘auto’, which tries to convert returned data 
      to pandas DataFrame unless data is sparse. #17396 by Jiaxiang.
    * Enhancement datasets.fetch_covtype now now supports the optional
      argument as_frame; when it is set to True, the returned Bunch 
      object’s data and frame members are pandas DataFrames, and the 
      target member is a pandas Series.
    * Enhancement datasets.fetch_kddcup99 now now supports the 
      optional argument as_frame; when it is set to True, the returned
      Bunch object’s data and frame members are pandas DataFrames, 
      and the target member is a pandas Series.
    * Enhancement datasets.fetch_20newsgroups_vectorized now supports 
      loading as a pandas DataFrame by setting as_frame=True.
    * API Change The default value of as_frame in 
      datasets.fetch_openml is changed from False to ‘auto’.
  * Many more updates and fixes.
- Skip tests for test_fetch_openml_verify_checksum[True] and
  test_fetch_openml_verify_checksum[False], not sure why it fail.
  
-------------------------------------------------------------------
Sun Dec 13 01:16:34 UTC 2020 - Benjamin Greiner <code@bnavigator.de>

- provide "sklearn" only for primary python3 flavor
  gh#openSUSE/python-rpm-macros#66
- clean test setup

-------------------------------------------------------------------
Tue Oct 13 22:49:47 UTC 2020 - Matej Cepl <mcepl@suse.com>

- Add assert_allclose-for-FP-comparison.patch to overcome
  equality comparison for FP numbers
  (gh#scikit-learn/scikit-learn#18031).

-------------------------------------------------------------------
Sat Aug 15 16:57:34 UTC 2020 - Arun Persaud <arun@gmx.de>

- specfile:
  * update numpy, scipy required versions
  * added requirement for threadpoolctl

- update to version 0.23.2:
  * sklearn.cluster
    + Fix Fixed a bug in cluster.KMeans where rounding errors could
      prevent convergence to be declared when tol=0. #17959 by Jérémie
      du Boisberranger.
    + Fix Fixed a bug in cluster.KMeans and cluster.MiniBatchKMeans
      where the reported inertia was incorrectly weighted by the
      sample weights. #17848 by Jérémie du Boisberranger.
    + Fix Fixed a bug in cluster.MeanShift with bin_seeding=True. When
      the estimated bandwidth is 0, the behavior is equivalent to
      bin_seeding=False. #17742 by Jeremie du Boisberranger.
    + Fix Fixed a bug in cluster.AffinityPropagation, that gives
      incorrect clusters when the array dtype is float32. #17995 by
      Thomaz Santana and Amanda Dsouza.
  * sklearn.decomposition
    + Fix Fixed a bug in
      decomposition.MiniBatchDictionaryLearning.partial_fit which
      should update the dictionary by iterating only once over a
      mini-batch. #17433 by Chiara Marmo.
    + Fix Avoid overflows on Windows in
      decomposition.IncrementalPCA.partial_fit for large batch_size
      and n_samples values. #17985 by Alan Butler and Amanda Dsouza.
  * sklearn.ensemble
    + Fix Fixed bug in ensemble.MultinomialDeviance where the average
      of logloss was incorrectly calculated as sum of logloss. #17694
      by Markus Rempfler and Tsutomu Kusanagi.
    + Fix Fixes ensemble.StackingClassifier and
      ensemble.StackingRegressor compatibility with estimators that do
      not define n_features_in_. #17357 by Thomas Fan.
  * sklearn.feature_extraction
    + Fix Fixes bug in feature_extraction.text.CountVectorizer where
      sample order invariance was broken when max_features was set and
      features had the same count. #18016 by Thomas Fan, Roman
      Yurchak, and Joel Nothman.
  * sklearn.linear_model
    + Fix linear_model.lars_path does not overwrite X when X_copy=True
      and Gram='auto'. #17914 by Thomas Fan.
  * sklearn.manifold
    + Fix Fixed a bug where metrics.pairwise_distances would raise an
      error if metric='seuclidean' and X is not type
      np.float64. #15730 by Forrest Koch.
  * sklearn.metrics
    + Fix Fixed a bug in metrics.mean_squared_error where the average
      of multiple RMSE values was incorrectly calculated as the root
      of the average of multiple MSE values. #17309 by Swier Heeres.
  * sklearn.pipeline
    + Fix pipeline.FeatureUnion raises a deprecation warning when None
      is included in transformer_list. #17360 by Thomas Fan.
  * sklearn.utils
    + Fix Fix utils.estimator_checks.check_estimator so that all test
      cases support the binary_only estimator tag. #17812 by Bruno
      Charron.

-------------------------------------------------------------------
Sat May 23 20:09:35 UTC 2020 - Arun Persaud <arun@gmx.de>

- update to version 0.23.1:
  * sklearn.cluster
    + Efficiency cluster.KMeans efficiency has been improved for very
      small datasets. In particular it cannot spawn idle threads any
      more. #17210 and #17235 by Jeremie du Boisberranger.
    + Fix Fixed a bug in cluster.KMeans where the sample weights
      provided by the user were modified in place. #17204 by Jeremie
      du Boisberranger.
  * Miscellaneous
    + Fix Fixed a bug in the repr of third-party estimators that use a
      **kwargs parameter in their constructor, when changed_only is
      True which is now the default. #17205 by Nicolas Hug.

- changes from version 0.23:
  * long list of changes, see
    https://scikit-learn.org/stable/whats_new/v0.23.html#version-0-23-0
    for details

- changes from version 0.22.2.post1:
  * sklearn.impute
    + Efficiency Reduce impute.KNNImputer asymptotic memory usage by
      chunking pairwise distance computation. #16397 by Joel Nothman.
  * sklearn.metrics
    + Fix Fixed a bug in metrics.plot_roc_curve where the name of the
      estimator was passed in the metrics.RocCurveDisplay instead of
      the parameter name. It results in a different plot when calling
      metrics.RocCurveDisplay.plot for the subsequent times. #16500 by
      Guillaume Lemaitre.
    + Fix Fixed a bug in metrics.plot_precision_recall_curve where the
      name of the estimator was passed in the
      metrics.PrecisionRecallDisplay instead of the parameter name. It
      results in a different plot when calling
      metrics.PrecisionRecallDisplay.plot for the subsequent
      times. #16505 by Guillaume Lemaitre.
  * sklearn.neighbors
    + Fix Fix a bug which converted a list of arrays into a 2-D object
      array instead of a 1-D array containing NumPy arrays. This bug

-------------------------------------------------------------------
Thu May  7 06:45:57 UTC 2020 - Christian Goll <cgoll@suse.com>

- added provide sklearn and python-sklearn as this is name most
  users will search for

-------------------------------------------------------------------
Tue Apr 28 11:13:42 UTC 2020 - Tomáš Chvátal <tchvatal@suse.com>

- Require at least pytest 4.x for testing

-------------------------------------------------------------------
Tue Jan  7 10:53:55 UTC 2020 - pgajdos@suse.com

- version update to 0.22.1
  * Clear definition of the public API
  * Deprecations: using FutureWarning from now on
  * Certain estimators and functions may produce different models
  * See
  https://scikit-learn.org/stable/whats_new/v0.22.html#version-0-22-0

-------------------------------------------------------------------
Tue Nov 19 20:10:24 UTC 2019 - Todd R <toddrme2178@gmail.com>

- Update to Version 0.21.3
  + sklearn.cluster
    * Fix: Fixed a bug in :class:`cluster.KMeans` where computation with
      `init='random'` was single threaded for `n_jobs > 1` or `n_jobs = -1`.
    * Fix: Fixed a bug in :class:`cluster.OPTICS` where users were unable to pass
      float `min_samples` and `min_cluster_size`.
    * Fix: Fixed a bug in :class:`cluster.KMeans` where KMeans++ initialisation
      could rarely result in an IndexError..
  + sklearn.compose
    * Fix: Fixed an issue in :class:`compose.ColumnTransformer` where using
      DataFrames whose column order differs between :func:``fit`` and
      :func:``transform`` could lead to silently passing incorrect columns to the
      ``remainder`` transformer.
  + sklearn.datasets
    * Fix: :func:`datasets.fetch_california_housing`,
      :func:`datasets.fetch_covtype`,
      :func:`datasets.fetch_kddcup99`, :func:`datasets.fetch_olivetti_faces`,
      :func:`datasets.fetch_rcv1`, and :func:`datasets.fetch_species_distributions`
      try to persist the previously cache using the new ``joblib`` if the cached
      data was persisted using the deprecated ``sklearn.externals.joblib``. This
      behavior is set to be deprecated and removed in v0.23.
.
  + sklearn.ensemble
    * Fix: Fix zero division error in :func:`HistGradientBoostingClassifier` and
      :func:`HistGradientBoostingRegressor`.
  + sklearn.impute
    * Fix: Fixed a bug in :class:`impute.SimpleImputer` and
      :class:`impute.IterativeImputer` so that no errors are thrown when there are
      missing values in training data.
  + sklearn.inspection
    * Fix: Fixed a bug in :func:`inspection.plot_partial_dependence` where 
      ``target`` parameter was not being taken into account for multiclass problems.
      :pr:`14393` by :user:`Guillem G. Subies <guillemgsubies>`.
  + sklearn.linear_model
    * Fix: Fixed a bug in :class:`linear_model.LogisticRegressionCV` where
      ``refit=False`` would fail depending on the ``'multiclass'`` and
      ``'penalty'`` parameters (regression introduced in 0.21).
    * Fix: Compatibility fix for :class:`linear_model.ARDRegression` and
      Scipy>=1.3.0. Adapts to upstream changes to the default `pinvh` cutoff
      threshold which otherwise results in poor accuracy in some cases.
  + sklearn.neighbors
    * Fix: Fixed a bug in :class:`neighbors.NeighborhoodComponentsAnalysis` where
      the validation of initial parameters ``n_components``, ``max_iter`` and
      ``tol`` required too strict types.
  + sklearn.tree
    * Fix: Fixed bug in :func:`tree.export_text` when the tree has one feature and 
      a single feature name is passed in.
    * Fix: Fixed an issue with :func:`plot_tree` where it displayed
      entropy calculations even for `gini` criterion in DecisionTreeClassifiers.

-------------------------------------------------------------------
Fri Jul 26 16:08:07 UTC 2019 - Todd R <toddrme2178@gmail.com>

- Update to Version 0.21.2
  + sklearn.decomposition
    * Fix:  Fixed a bug in cross_decomposition.CCA improving numerical
      stability when Y is close to zero..
  + sklearn.metrics
    * Fix:  Fixed a bug in metrics.euclidean_distances where a part of the
      distance matrix was left un-instanciated for suffiently large float32
      datasets (regression introduced in 0.21)..
  + sklearn.preprocessing
    * Fix:  Fixed a bug in preprocessing.OneHotEncoder where the new
      drop parameter was not reflected in get_feature_names..
  + sklearn.utils.sparsefuncs
    * Fix:  Fixed a bug where min_max_axis would fail on 32-bit systems
      for certain large inputs. This affects preprocessing.MaxAbsScaler,
      preprocessing.normalize and preprocessing.LabelBinarizer..
- Update to Version 0.21.1
  + sklearn.metrics
    * Fix:  Fixed a bug in metrics.pairwise_distances where it would raise
      AttributeError for boolean metrics when X had a boolean dtype and
      Y == None..
    * Fix:  Fixed two bugs in metrics.pairwise_distances when
      n_jobs > 1. First it used to return a distance matrix with same dtype as
      input, even for integer dtype. Then the diagonal was not zeros for euclidean
      metric when Y is X..
  + sklearn.neighbors
    * Fix:  Fixed a bug in neighbors.KernelDensity which could not be
      restored from a pickle if sample_weight had been used..
- Update to Version 0.21.0
  + Changed models
    The following estimators and functions, when fit with the same data and
    parameters, may produce different models from the previous version. This often
    occurs due to changes in the modelling logic (bug fixes or enhancements), or in
    random sampling procedures.
    * discriminant_analysis.LinearDiscriminantAnalysis for multiclass
      classification. |Fix|
    * discriminant_analysis.LinearDiscriminantAnalysis with 'eigen'
      solver. |Fix|
    * linear_model.BayesianRidge |Fix|
    * Decision trees and derived ensembles when both max_depth and
      max_leaf_nodes are set. |Fix|
    * linear_model.LogisticRegression and
      linear_model.LogisticRegressionCV with 'saga' solver. |Fix|
    * ensemble.GradientBoostingClassifier |Fix|
    * sklearn.feature_extraction.text.HashingVectorizer,
      sklearn.feature_extraction.text.TfidfVectorizer, and
      sklearn.feature_extraction.text.CountVectorizer |Fix|
    * neural_network.MLPClassifier |Fix|
    * svm.SVC.decision_function and
      multiclass.OneVsOneClassifier.decision_function. |Fix|
    * linear_model.SGDClassifier and any derived classifiers. |Fix|
    * Any model using the linear_model.sag.sag_solver function with a 0
      seed, including linear_model.LogisticRegression,
      linear_model.LogisticRegressionCV, linear_model.Ridge,
      and linear_model.RidgeCV with 'sag' solver. |Fix|
    * linear_model.RidgeCV when using generalized cross-validation
      with sparse inputs. |Fix|
    Details are listed in the changelog below.
    (While we are trying to better inform users by providing this information, we
    cannot assure that this list is complete.)
  + Known Major Bugs
    * The default max_iter for linear_model.LogisticRegression is too
      small for many solvers given the default tol. In particular, we
      accidentally changed the default max_iter for the liblinear solver from
      1000 to 100 iterations in released in version 0.16.
      In a future release we hope to choose better default max_iter and tol
      heuristically depending on the solver.
  + Support for Python 3.4 and below has been officially dropped.
  + sklearn.base
    * API:  The R2 score used when calling score on a regressor will use
      multioutput='uniform_average' from version 0.23 to keep consistent with
      metrics.r2_score. This will influence the score method of all
      the multioutput regressors (except for
      multioutput.MultiOutputRegressor)..
  + sklearn.calibration
    * Enhancement:  Added support to bin the data passed into
      calibration.calibration_curve by quantiles instead of uniformly
      between 0 and 1..
    * Enhancement:  Allow n-dimensional arrays as input for
      calibration.CalibratedClassifierCV..
  + sklearn.cluster
    * MajorFeature:  A new clustering algorithm: cluster.OPTICS: an
      algoritm related to cluster.DBSCAN, that has hyperparameters easier
      to set and that scales better,
    * Fix:  Fixed a bug where cluster.Birch could occasionally raise an
      AttributeError..
    * Fix:  Fixed a bug in cluster.KMeans where empty clusters weren't
      correctly relocated when using sample weights..
    * API:  The n_components_ attribute in cluster.AgglomerativeClustering
      and cluster.FeatureAgglomeration has been renamed to
      n_connected_components_..
    * Enhancement:  cluster.AgglomerativeClustering and
      cluster.FeatureAgglomeration now accept a distance_threshold
      parameter which can be used to find the clusters instead of n_clusters.
  + sklearn.compose
    * API:  compose.ColumnTransformer is no longer an experimental
      feature..
  + sklearn.datasets
    * Fix:  Added support for 64-bit group IDs and pointers in SVMLight files..
    * Fix:  datasets.load_sample_images returns images with a deterministic
      order..
  + sklearn.decomposition
    * Enhancement:  decomposition.KernelPCA now has deterministic output
      (resolved sign ambiguity in eigenvalue decomposition of the kernel matrix)..
    * Fix:  Fixed a bug in decomposition.KernelPCA, fit().transform()
      now produces the correct output (the same as fit_transform()) in case
      of non-removed zero eigenvalues (remove_zero_eig=False).
      fit_inverse_transform was also accelerated by using the same trick as
      fit_transform to compute the transform of X.
    * Fix:  Fixed a bug in decomposition.NMF where init = 'nndsvd',
      init = 'nndsvda', and init = 'nndsvdar' are allowed when
      n_components < n_features instead of
      n_components <= min(n_samples, n_features).
    * API:  The default value of the init argument in
      decomposition.non_negative_factorization will change from
      random to None in version 0.23 to make it consistent with
      decomposition.NMF. A FutureWarning is raised when
      the default value is used..
  + sklearn.discriminant_analysis
    * Enhancement:  discriminant_analysis.LinearDiscriminantAnalysis now
      preserves float32 and float64 dtypes.
    * Fix:  A ChangedBehaviourWarning is now raised when
      discriminant_analysis.LinearDiscriminantAnalysis is given as
      parameter n_components > min(n_features, n_classes - 1), and
      n_components is changed to min(n_features, n_classes - 1) if so.
      Previously the change was made, but silently..
    * Fix:  Fixed a bug in discriminant_analysis.LinearDiscriminantAnalysis
      where the predicted probabilities would be incorrectly computed in the
      multiclass case.
    * Fix:  Fixed a bug in discriminant_analysis.LinearDiscriminantAnalysis
      where the predicted probabilities would be incorrectly computed with eigen
      solver.
  + sklearn.dummy
    * Fix:  Fixed a bug in dummy.DummyClassifier where the
      predict_proba method was returning int32 array instead of
      float64 for the stratified strategy..
    * Fix:  Fixed a bug in dummy.DummyClassifier where it was throwing a
      dimension mismatch error in prediction time if a column vector y with
      shape=(n, 1) was given at fit time.
  + sklearn.ensemble
    * MajorFeature:  Add two new implementations of
      gradient boosting trees: ensemble.HistGradientBoostingClassifier
      and ensemble.HistGradientBoostingRegressor. The implementation of
      these estimators is inspired by
      LightGBM and can be orders of
      magnitude faster than ensemble.GradientBoostingRegressor and
      ensemble.GradientBoostingClassifier when the number of samples is
      larger than tens of thousands of samples. The API of these new estimators
      is slightly different, and some of the features from
      ensemble.GradientBoostingClassifier and
      ensemble.GradientBoostingRegressor are not yet supported.
      These new estimators are experimental, which means that their results or
      their API might change without any deprecation cycle. To use them, you
      need to explicitly import enable_hist_gradient_boosting::
        >>> # explicitly require this experimental feature
        >>> from sklearn.experimental import enable_hist_gradient_boosting  # noqa
        >>> # now you can import normally from sklearn.ensemble
        >>> from sklearn.ensemble import HistGradientBoostingClassifier.
    * Feature:  Add ensemble.VotingRegressor
      which provides an equivalent of ensemble.VotingClassifier
      for regression problems.
    * Efficiency:  Make ensemble.IsolationForest prefer threads over
      processes when running with n_jobs > 1 as the underlying decision tree
      fit calls do release the GIL. This changes reduces memory usage and
      communication overhead.
    * Efficiency:  Make ensemble.IsolationForest more memory efficient
      by avoiding keeping in memory each tree prediction..
    * Efficiency:  ensemble.IsolationForest now uses chunks of data at
      prediction step, thus capping the memory usage..
    * Efficiency:  sklearn.ensemble.GradientBoostingClassifier and
      sklearn.ensemble.GradientBoostingRegressor now keep the
      input y as float64 to avoid it being copied internally by trees..
    * Enhancement:  Minimized the validation of X in
      ensemble.AdaBoostClassifier and ensemble.AdaBoostRegressor.
    * Enhancement:  ensemble.IsolationForest now exposes warm_start
      parameter, allowing iterative addition of trees to an isolation
      forest..
    * Fix:  The values of feature_importances_ in all random forest based
      models (i.e.
      ensemble.RandomForestClassifier,
      ensemble.RandomForestRegressor,
      ensemble.ExtraTreesClassifier,
      ensemble.ExtraTreesRegressor,
      ensemble.RandomTreesEmbedding,
      ensemble.GradientBoostingClassifier, and
      ensemble.GradientBoostingRegressor) now:
      > sum up to 1
      > all the single node trees in feature importance calculation are ignored
      > in case all trees have only one single node (i.e. a root node),
        feature importances will be an array of all zeros.
    * Fix:  Fixed a bug in ensemble.GradientBoostingClassifier and
      ensemble.GradientBoostingRegressor, which didn't support
      scikit-learn estimators as the initial estimator. Also added support of
      initial estimator which does not support sample weights. and.
    * Fix:  Fixed the output of the average path length computed in
      ensemble.IsolationForest when the input is either 0, 1 or 2.
    * Fix:  Fixed a bug in ensemble.GradientBoostingClassifier where
      the gradients would be incorrectly computed in multiclass classification
      problems..
    * Fix:  Fixed a bug in ensemble.GradientBoostingClassifier where
      validation sets for early stopping were not sampled with stratification..
    * Fix:  Fixed a bug in ensemble.GradientBoostingClassifier where
      the default initial prediction of a multiclass classifier would predict the
      classes priors instead of the log of the priors..
    * Fix:  Fixed a bug in ensemble.RandomForestClassifier where the
      predict method would error for multiclass multioutput forests models
      if any targets were strings..
    * Fix:  Fixed a bug in ensemble.gradient_boosting.LossFunction and
      ensemble.gradient_boosting.LeastSquaresError where the default
      value of learning_rate in update_terminal_regions is not consistent
      with the document and the caller functions. Note however that directly using
      these loss functions is deprecated..
    * Fix:  ensemble.partial_dependence (and consequently the new
      version sklearn.inspection.partial_dependence) now takes sample
      weights into account for the partial dependence computation when the
      gradient boosting model has been trained with sample weights..
    * API:  ensemble.partial_dependence and
      ensemble.plot_partial_dependence are now deprecated in favor of
      inspection.partial_dependence
      and
      inspection.plot_partial_dependence. and
    * Fix:  ensemble.VotingClassifier and
      ensemble.VotingRegressor were failing during fit in one
      of the estimators was set to None and sample_weight was not None..
    * API:  ensemble.VotingClassifier and
      ensemble.VotingRegressor accept 'drop' to disable an estimator
      in addition to None to be consistent with other estimators (i.e.,
      pipeline.FeatureUnion and compose.ColumnTransformer)..
  + sklearn.externals
    * API:  Deprecated externals.six since we have dropped support for
      Python 2.7..
  + sklearn.feature_extraction
    * Fix:  If input='file' or input='filename', and a callable is given as
      the analyzer, sklearn.feature_extraction.text.HashingVectorizer,
      sklearn.feature_extraction.text.TfidfVectorizer, and
      sklearn.feature_extraction.text.CountVectorizer now read the data
      from the file(s) and then pass it to the given analyzer, instead of
      passing the file name(s) or the file object(s) to the analyzer..
  + sklearn.impute
    * MajorFeature:  Added impute.IterativeImputer, which is a strategy
      for imputing missing values by modeling each feature with missing values as a
      function of other features in a round-robin fashion.
      The API of IterativeImputer is experimental and subject to change without any
      deprecation cycle. To use them, you need to explicitly import
      enable_iterative_imputer::
        >>> from sklearn.experimental import enable_iterative_imputer  # noqa
        >>> # now you can import normally from sklearn.impute
        >>> from sklearn.impute import IterativeImputer
    * Feature:  The impute.SimpleImputer and
      impute.IterativeImputer have a new parameter 'add_indicator',
      which simply stacks a impute.MissingIndicator transform into the
      output of the imputer's transform. That allows a predictive estimator to
      account for missingness.
    * Fix:  In impute.MissingIndicator avoid implicit densification by
      raising an exception if input is sparse add missing_values property
      is set to 0..
    * Fix:  Fixed two bugs in impute.MissingIndicator. First, when
      X is sparse, all the non-zero non missing values used to become
      explicit False in the transformed data. Then, when
      features='missing-only', all features used to be kept if there were no
      missing values at all..
  + sklearn.inspection
    (new subpackage)
    * Feature:  Partial dependence plots
      (inspection.plot_partial_dependence) are now supported for
      any regressor or classifier (provided that they have a predict_proba
      method).
  + sklearn.isotonic
    * Feature:  Allow different dtypes (such as float32) in
      isotonic.IsotonicRegression.
  + sklearn.linear_model
    * Enhancement:  linear_model.Ridge now preserves float32 and
      float64 dtypes.
    * Feature:  linear_model.LogisticRegression and
      linear_model.LogisticRegressionCV now support Elastic-Net penalty,
      with the 'saga' solver..
    * Feature:  Added linear_model.lars_path_gram, which is
      linear_model.lars_path in the sufficient stats mode, allowing
      users to compute linear_model.lars_path without providing
      X and y..
    * Efficiency:  linear_model.make_dataset now preserves
      float32 and float64 dtypes, reducing memory consumption in stochastic
      gradient, SAG and SAGA solvers.
    * Enhancement:  linear_model.LogisticRegression now supports an
      unregularized objective when penalty='none' is passed. This is
      equivalent to setting C=np.inf with l2 regularization. Not supported
      by the liblinear solver..
    * Enhancement:  sparse_cg solver in linear_model.Ridge
      now supports fitting the intercept (i.e. fit_intercept=True) when
      inputs are sparse..
    * Enhancement:  The coordinate descent solver used in Lasso, ElasticNet,
      etc. now issues a ConvergenceWarning when it completes without meeting the
      desired toleranbce.
    * Fix:  Fixed a bug in linear_model.LogisticRegression and
      linear_model.LogisticRegressionCV with 'saga' solver, where the
      weights would not be correctly updated in some cases..
    * Fix:  Fixed the posterior mean, posterior covariance and returned
      regularization parameters in linear_model.BayesianRidge. The
      posterior mean and the posterior covariance were not the ones computed
      with the last update of the regularization parameters and the returned
      regularization parameters were not the final ones. Also fixed the formula of
      the log marginal likelihood used to compute the score when
      compute_score=True..
    * Fix:  Fixed a bug in linear_model.LassoLarsIC, where user input
      copy_X=False at instance creation would be overridden by default
      parameter value copy_X=True in fit.
    * Fix:  Fixed a bug in linear_model.LinearRegression that
      was not returning the same coeffecients and intercepts with
      fit_intercept=True in sparse and dense case.
    * Fix:  Fixed a bug in linear_model.HuberRegressor that was
      broken when X was of dtype bool..
    * Fix:  Fixed a performance issue of saga and sag solvers when called
      in a joblib.Parallel setting with n_jobs > 1 and
      backend="threading", causing them to perform worse than in the sequential
      case..
    * Fix:  Fixed a bug in
      linear_model.stochastic_gradient.BaseSGDClassifier that was not
      deterministic when trained in a multi-class setting on several threads..
    * Fix:  Fixed bug in linear_model.ridge_regression,
      linear_model.Ridge and
      linear_model.RidgeClassifier that
      caused unhandled exception for arguments return_intercept=True and
      solver=auto (default) or any other solver different from sag.
    * Fix:  linear_model.ridge_regression will now raise an exception
      if return_intercept=True and solver is different from sag. Previously,
      only warning was issued.
    * Fix:  linear_model.ridge_regression will choose sparse_cg
      solver for sparse inputs when solver=auto and sample_weight
      is provided (previously cholesky solver was selected).
    * API:   The use of linear_model.lars_path with X=None
      while passing Gram is deprecated in version 0.21 and will be removed
      in version 0.23. Use linear_model.lars_path_gram instead..
    * API:  linear_model.logistic_regression_path is deprecated
      in version 0.21 and will be removed in version 0.23..
    * Fix:  linear_model.RidgeCV with generalized cross-validation
      now correctly fits an intercept when fit_intercept=True and the design
      matrix is sparse.
  + sklearn.manifold
    * Efficiency:  Make manifold.tsne.trustworthiness use an inverted index
      instead of an np.where lookup to find the rank of neighbors in the input
      space. This improves efficiency in particular when computed with
      lots of neighbors and/or small datasets..
  + sklearn.metrics
    * Feature:  Added the metrics.max_error metric and a corresponding
      'max_error' scorer for single output regression..
    * Feature:  Add metrics.multilabel_confusion_matrix, which calculates a
      confusion matrix with true positive, false positive, false negative and true
      negative counts for each class. This facilitates the calculation of set-wise
      metrics such as recall, specificity, fall out and miss rate.
    * Feature:  metrics.jaccard_score has been added to calculate the
      Jaccard coefficient as an evaluation metric for binary, multilabel and
      multiclass tasks, with an interface analogous to metrics.f1_score.
    * Feature:  Added metrics.pairwise.haversine_distances which can be
      accessed with metric='pairwise' through metrics.pairwise_distances
      and estimators. (Haversine distance was previously available for nearest
      neighbors calculation.)
    * Efficiency:  Faster metrics.pairwise_distances with n_jobs
      > 1 by using a thread-based backend, instead of process-based backends.
    * Efficiency:  The pairwise manhattan distances with sparse input now uses the
      BLAS shipped with scipy instead of the bundled BLAS.
    * Enhancement:  Use label accuracy instead of micro-average on
      metrics.classification_report to avoid confusion. micro-average is
      only shown for multi-label or multi-class with a subset of classes because
      it is otherwise identical to accuracy.
    * Enhancement:  Added beta parameter to
      metrics.homogeneity_completeness_v_measure and
      metrics.v_measure_score to configure the
      tradeoff between homogeneity and completeness.
    * Fix:  The metric metrics.r2_score is degenerate with a single sample
      and now it returns NaN and raises exceptions.UndefinedMetricWarning..
    * Fix:  Fixed a bug where metrics.brier_score_loss will sometimes
      return incorrect result when there's only one class in y_true..
    * Fix:  Fixed a bug in metrics.label_ranking_average_precision_score
      where sample_weight wasn't taken into account for samples with degenerate
      labels..
    * API:  The parameter labels in metrics.hamming_loss is deprecated
      in version 0.21 and will be removed in version 0.23.
    * Fix:  The function metrics.pairwise.euclidean_distances, and
      therefore several estimators with metric='euclidean', suffered from
      numerical precision issues with float32 features. Precision has been
      increased at the cost of a small drop of performance.
    * API:  metrics.jaccard_similarity_score is deprecated in favour of
      the more consistent metrics.jaccard_score. The former behavior for
      binary and multiclass targets is broken..
  + sklearn.mixture
    * Fix:  Fixed a bug in mixture.BaseMixture and therefore on estimators
      based on it, i.e. mixture.GaussianMixture and
      mixture.BayesianGaussianMixture, where fit_predict and
      fit.predict were not equivalent..
  + sklearn.model_selection
    * Feature:  Classes ~model_selection.GridSearchCV and
      ~model_selection.RandomizedSearchCV now allow for refit=callable
      to add flexibility in identifying the best estimator.
      See sphx_glr_auto_examples_model_selection_plot_grid_search_refit_callable.py.
    * Enhancement:  Classes ~model_selection.GridSearchCV,
      ~model_selection.RandomizedSearchCV, and methods
      ~model_selection.cross_val_score,
      ~model_selection.cross_val_predict,
      ~model_selection.cross_validate, now print train scores when
      return_train_scores is True and verbose > 2. For
      ~model_selection.learning_curve, and
      ~model_selection.validation_curve only the latter is required.
    * Enhancement:  Some CV splitter classes and
      model_selection.train_test_split now raise ValueError when the
      resulting training set is empty..
    * Fix:  Fixed a bug where model_selection.StratifiedKFold
      shuffles each class's samples with the same random_state,
      making shuffle=True ineffective..
    * Fix:  Added ability for model_selection.cross_val_predict to handle
      multi-label (and multioutput-multiclass) targets with predict_proba-type
      methods..
    * Fix:  Fixed an issue in ~model_selection.cross_val_predict where
      method="predict_proba" returned always 0.0 when one of the classes was
      excluded in a cross-validation fold.
  + sklearn.multiclass
    * Fix:  Fixed an issue in multiclass.OneVsOneClassifier.decision_function
      where the decision_function value of a given sample was different depending on
      whether the decision_function was evaluated on the sample alone or on a batch
      containing this same sample due to the scaling used in decision_function..
  + sklearn.multioutput
    * Fix:  Fixed a bug in multioutput.MultiOutputClassifier where the
      predict_proba method incorrectly checked for predict_proba attribute in
      the estimator object.
  + sklearn.neighbors
    * MajorFeature:  Added neighbors.NeighborhoodComponentsAnalysis for
      metric learning, which implements the Neighborhood Components Analysis
      algorithm.
    * API:  Methods in neighbors.NearestNeighbors :
      ~neighbors.NearestNeighbors.kneighbors,
      ~neighbors.NearestNeighbors.radius_neighbors,
      ~neighbors.NearestNeighbors.kneighbors_graph,
      ~neighbors.NearestNeighbors.radius_neighbors_graph
      now raise NotFittedError, rather than AttributeError,
      when called before fit.
  + sklearn.neural_network
    * Fix:  Fixed a bug in neural_network.MLPClassifier and
      neural_network.MLPRegressor where the option shuffle=False
      was being ignored..
    * Fix:  Fixed a bug in neural_network.MLPClassifier where
      validation sets for early stopping were not sampled with stratification. In
      the multilabel case however, splits are still not stratified..
  + sklearn.pipeline
    * Feature:  pipeline.Pipeline can now use indexing notation (e.g.
      my_pipeline[0:-1]) to extract a subsequence of steps as another Pipeline
      instance.  A Pipeline can also be indexed directly to extract a particular
      step (e.g. my_pipeline['svc']), rather than accessing named_steps..
    * Feature:  Added optional parameter verbose in pipeline.Pipeline,
      compose.ColumnTransformer and pipeline.FeatureUnion
      and corresponding make_ helpers for showing progress and timing of
      each step.
    * Enhancement:  pipeline.Pipeline now supports using 'passthrough'
      as a transformer, with the same effect as None..
    * Enhancement:  pipeline.Pipeline  implements __len__ and
      therefore len(pipeline) returns the number of steps in the pipeline..
  + sklearn.preprocessing
    * Feature:  preprocessing.OneHotEncoder now supports dropping one
      feature per category with a new drop parameter..
    * Efficiency:  preprocessing.OneHotEncoder and
      preprocessing.OrdinalEncoder now handle pandas DataFrames more
      efficiently..
    * Efficiency:  Make preprocessing.MultiLabelBinarizer cache class
      mappings instead of calculating it every time on the fly.
    * Efficiency:  preprocessing.PolynomialFeatures now supports
      compressed sparse row (CSR) matrices as input for degrees 2 and 3. This is
      typically much faster than the dense case as it scales with matrix density
      and expansion degree (on the order of density^degree), and is much, much
      faster than the compressed sparse column (CSC) case..
    * Efficiency:  Speed improvement in preprocessing.PolynomialFeatures,
      in the dense case. Also added a new parameter order which controls output
      order for further speed performances..
    * Fix:  Fixed the calculation overflow when using a float16 dtype with
      preprocessing.StandardScaler.
    * Fix:  Fixed a bug in preprocessing.QuantileTransformer and
      preprocessing.quantile_transform to force n_quantiles to be at most
      equal to n_samples. Values of n_quantiles larger than n_samples were either
      useless or resulting in a wrong approximation of the cumulative distribution
      function estimator..
    * API:  The default value of copy in preprocessing.quantile_transform
      will change from False to True in 0.23 in order to make it more consistent
      with the default copy values of other functions in
      preprocessing and prevent unexpected side effects by modifying
      the value of X inplace..
  + sklearn.svm
    * Fix:  Fixed an issue in svm.SVC.decision_function when
      decision_function_shape='ovr'. The decision_function value of a given
      sample was different depending on whether the decision_function was evaluated
      on the sample alone or on a batch containing this same sample due to the
      scaling used in decision_function..
  + sklearn.tree
    * Feature:  Decision Trees can now be plotted with matplotlib using
      tree.plot_tree without relying on the dot library,
      removing a hard-to-install dependency..
    * Feature:  Decision Trees can now be exported in a human readable
      textual format using tree.export_text.
    * Feature:  get_n_leaves() and get_depth() have been added to
      tree.BaseDecisionTree and consequently all estimators based
      on it, including tree.DecisionTreeClassifier,
      tree.DecisionTreeRegressor, tree.ExtraTreeClassifier,
      and tree.ExtraTreeRegressor..
    * Fix:  Trees and forests did not previously predict multi-output
      classification targets with string labels, despite accepting them in fit..
    * Fix:  Fixed an issue with tree.BaseDecisionTree
      and consequently all estimators based
      on it, including tree.DecisionTreeClassifier,
      tree.DecisionTreeRegressor, tree.ExtraTreeClassifier,
      and tree.ExtraTreeRegressor, where they used to exceed the given
      max_depth by 1 while expanding the tree if max_leaf_nodes and
      max_depth were both specified by the user. Please note that this also
      affects all ensemble methods using decision trees..
  + sklearn.utils
    * Feature:  utils.resample now accepts a stratify parameter for
      sampling according to class distributions..
    * API:  Deprecated warn_on_dtype parameter from utils.check_array
      and utils.check_X_y. Added explicit warning for dtype conversion
      in check_pairwise_arrays if the metric being passed is a
      pairwise boolean metric..
  + Multiple modules
    * MajorFeature:  The __repr__() method of all estimators (used when calling
      print(estimator)) has been entirely re-written, building on Python's
      pretty printing standard library. All parameters are printed by default,
      but this can be altered with the print_changed_only option in
      sklearn.set_config..
    * MajorFeature:  Add estimators tags: these are annotations of estimators
      that allow programmatic inspection of their capabilities, such as sparse
      matrix support, supported output types and supported methods. Estimator
      tags also determine the tests that are run on an estimator when
      check_estimator is called.
    * Efficiency:  Memory copies are avoided when casting arrays to a different
      dtype in multiple estimators..
    * Fix:  Fixed a bug in the implementation of the our_rand_r
      helper function that was not behaving consistently across platforms.
  + Miscellaneous
    * Enhancement:  Joblib is no longer vendored in scikit-learn, and becomes a
      dependency. Minimal supported version is joblib 0.11, however using
      version >= 0.13 is strongly recommended..
  + Changes to estimator checks
    These changes mostly affect library developers.
    * Add check_fit_idempotent to
      ~utils.estimator_checks.check_estimator, which checks that
      when fit is called twice with the same data, the ouput of
      predict, predict_proba, transform, and decision_function does not
      change.
    * Many checks can now be disabled or configured with estimator_tags..

-------------------------------------------------------------------
Wed Jan 30 13:45:49 UTC 2019 - Matej Cepl <mcepl@suse.com>

- Switch off tests, gh#scikit-learn/scikit-learn#12369

-------------------------------------------------------------------
Tue Jan 29 17:06:46 CET 2019 - mcepl@suse.com

- Update to 0.20.2:
  * This is a bug-fix release with some minor documentation
    improvements and enhancements to features released in 0.20.0.
    Note that we also include some API changes in this release, so
    you might get some extra warnings after updating from 0.20.0.

-------------------------------------------------------------------
Wed Oct 24 18:40:50 UTC 2018 - Dirk Mueller <dmueller@suse.com>

- update to 0.20.0:
  * http://scikit-learn.org/stable/whats_new.html#version-0-20-0
  Support for Python 3.3 has been officially dropped
- drop scikit-learn-skip-test.patch (merged)

-------------------------------------------------------------------
Thu May 17 13:39:27 UTC 2018 - tchvatal@suse.com

- Skip test sklearn.linear_model.tests.test_logistic.test_max_iter
  * Upstream plans to fix it in next release
  * scikit-learn-skip-test.patch

-------------------------------------------------------------------
Thu May 17 12:25:53 UTC 2018 - tchvatal@suse.com

- Update package to properly state dependencies as in setup.py
- Install license file

-------------------------------------------------------------------
Mon Oct 30 06:03:24 UTC 2017 - arun@gmx.de

- update to version 0.19.1:
  * API changes
    + Reverted the addition of metrics.ndcg_score and
      metrics.dcg_score which had been merged into version 0.19.0 by
      error. The implementations were broken and undocumented.
    + return_train_score which was added to
      model_selection.GridSearchCV, model_selection.RandomizedSearchCV
      and model_selection.cross_validate in version 0.19.0 will be
      changing its default value from True to False in version
      0.21. We found that calculating training score could have a
      great effect on cross validation runtime in some cases. Users
      should explicitly set return_train_score to False if prediction
      or scoring functions are slow, resulting in a deleterious effect
      on CV runtime, or to True if they wish to use the calculated
      scores. #9677 by Kumar Ashutosh and Joel Nothman.
    + correlation_models and regression_models from the legacy
      gaussian processes implementation have been belatedly
      deprecated. #9717 by Kumar Ashutosh.
  * Bug fixes
    + Avoid integer overflows in metrics.matthews_corrcoef. #9693 by
      Sam Steingold.
    + Fix ValueError in preprocessing.LabelEncoder when using
      inverse_transform on unseen labels. #9816 by Charlie Newey.
    + Fixed a bug in the objective function for manifold.TSNE (both
      exact and with the Barnes-Hut approximation) when n_components
      >= 3. #9711 by @goncalo-rodrigues.
    + Fix regression in model_selection.cross_val_predict where it
      raised an error with method='predict_proba' for some
      probabilistic classifiers. #9641 by James Bourbeau.
    + Fixed a bug where datasets.make_classification modified its
      input weights. #9865 by Sachin Kelkar.
    + model_selection.StratifiedShuffleSplit now works with
      multioutput multiclass or multilabel data with more than 1000
      columns. #9922 by Charlie Brummitt.
    + Fixed a bug with nested and conditional parameter setting,
      e.g. setting a pipeline step and its parameter at the same
      time. #9945 by Andreas Müller and Joel Nothman.
  * Regressions in 0.19.0 fixed in 0.19.1:
    + Fixed a bug where parallelised prediction in random forests was
      not thread-safe and could (rarely) result in arbitrary
      errors. #9830 by Joel Nothman.
    + Fix regression in model_selection.cross_val_predict where it no
      longer accepted X as a list. #9600 by Rasul Kerimov.
    + Fixed handling of model_selection.cross_val_predict for binary
      classification with method='decision_function'. #9593 by
      Reiichiro Nakano and core devs.
    + Fix regression in pipeline.Pipeline where it no longer accepted
      steps as a tuple. #9604 by Joris Van den Bossche.
    + Fix bug where n_iter was not properly deprecated, leaving n_iter
      unavailable for interim use in linear_model.SGDClassifier,
      linear_model.SGDRegressor,
      linear_model.PassiveAggressiveClassifier,
      linear_model.PassiveAggressiveRegressor and
      linear_model.Perceptron. #9558 by Andreas Müller.
    + Dataset fetchers make sure temporary files are closed before
      removing them, which caused errors on Windows. #9847 by Joan
      Massich.
    + Fixed a regression in manifold.TSNE where it no longer supported
      metrics other than ‘euclidean’ and ‘precomputed’. #9623 by Oli
      Blum.
  * Enhancements
    + Our test suite and utils.estimator_checks.check_estimators can
      now be run without Nose installed. #9697 by Joan Massich.
    + To improve usability of version 0.19’s pipeline.Pipeline
      caching, memory now allows joblib.Memory instances. This make
      use of the new utils.validation.check_memory helper. #9584 by
      Kumar Ashutosh
    + Some fixes to examples: #9750, #9788, #9815
    + Made a FutureWarning in SGD-based estimators less verbose. #9802
      by Vrishank Bhardwaj.

-------------------------------------------------------------------
Sun Sep 24 21:50:13 UTC 2017 - arun@gmx.de

- update to version 0.19.0:
  * Highlights
    + We are excited to release a number of great new features
      including neighbors.LocalOutlierFactor for anomaly detection,
      preprocessing.QuantileTransformer for robust feature
      transformation, and the multioutput.ClassifierChain
      meta-estimator to simply account for dependencies between
      classes in multilabel problems. We have some new algorithms in
      existing estimators, such as multiplicative update in
      decomposition.NMF and multinomial
      linear_model.LogisticRegression with L1 loss (use
      solver='saga').
    + Cross validation is now able to return the results from multiple
      metric evaluations. The new model_selection.cross_validate can
      return many scores on the test data as well as training set
      performance and timings, and we have extended the scoring and
      refit parameters for grid/randomized search to handle multiple
      metrics.
    + You can also learn faster. For instance, the new option to cache
      transformations in pipeline.Pipeline makes grid search over
      pipelines including slow transformations much more
      efficient. And you can predict faster: if you’re sure you know
      what you’re doing, you can turn off validating that the input is
      finite using config_context.
    + We’ve made some important fixes too. We’ve fixed a longstanding
      implementation error in metrics.average_precision_score, so
      please be cautious with prior results reported from that
      function. A number of errors in the manifold.TSNE implementation
      have been fixed, particularly in the default Barnes-Hut
      approximation. semi_supervised.LabelSpreading and
      semi_supervised.LabelPropagation have had substantial
      fixes. LabelPropagation was previously broken. LabelSpreading
      should now correctly respect its alpha parameter.
  * Changed models
  * The following estimators and functions, when fit with the same
    data and parameters, may produce different models from the
    previous version. This often occurs due to changes in the
    modelling logic (bug fixes or enhancements), or in random sampling
    procedures.
    + cluster.KMeans with sparse X and initial centroids given (bug
      fix)
    + cross_decomposition.PLSRegression with scale=True (bug fix)
    + ensemble.GradientBoostingClassifier and
      ensemble.GradientBoostingRegressor where min_impurity_split is
      used (bug fix)
    + gradient boosting loss='quantile' (bug fix)
    + ensemble.IsolationForest (bug fix)
    + feature_selection.SelectFdr (bug fix)
    + linear_model.RANSACRegressor (bug fix)
    + linear_model.LassoLars (bug fix)
    + linear_model.LassoLarsIC (bug fix)
    + manifold.TSNE (bug fix)
    + neighbors.NearestCentroid (bug fix)
    + semi_supervised.LabelSpreading (bug fix)
    + semi_supervised.LabelPropagation (bug fix)
    + tree based models where min_weight_fraction_leaf is used
      (enhancement)
  * complete changelog at http://scikit-learn.org/stable/whats_new.html

-------------------------------------------------------------------
Sun Jun 11 19:37:48 UTC 2017 - toddrme2178@gmail.com

- Implement single-spec version
- Update source URL
- Update to version 0.18.1
  * Large number of changes. See:
    https://github.com/scikit-learn/scikit-learn/blob/0.18.1/doc/whats_new.rst

-------------------------------------------------------------------
Mon Jan 11 10:35:20 UTC 2016 - toddrme2178@gmail.com

- Switch to proper package name: python-scikit-learn

-------------------------------------------------------------------
Fri Nov 20 15:00:00 UTC 2015 - Angelos Tzotsos <tzotsos@opensuse.org>

- Update to version 0.17

-------------------------------------------------------------------
Thu Oct 24 12:40:24 UTC 2013 - toddrme2178@gmail.com

- Update to version 14.1
  * Minor bugfixes
- Update to version 14.0
  * Changelog
    - Missing values with sparse and dense matrices can be imputed with the
      transformer :class:`preprocessing.Imputer` by `Nicolas Trésegnie`_.
    - The core implementation of decisions trees has been rewritten from
      scratch, allowing for faster tree induction and lower memory
      consumption in all tree-based estimators. By `Gilles Louppe`_.
    - Added :class:`ensemble.AdaBoostClassifier` and
      :class:`ensemble.AdaBoostRegressor`, by `Noel Dawe`_  and
      `Gilles Louppe`_. See the :ref:`AdaBoost <adaboost>` section of the user
      guide for details and examples.
    - Added :class:`grid_search.RandomizedSearchCV` and
      :class:`grid_search.ParameterSampler` for randomized hyperparameter
      optimization. By `Andreas Müller`_.
    - Added :ref:`biclustering <biclustering>` algorithms
      (:class:`sklearn.cluster.bicluster.SpectralCoclustering` and
      :class:`sklearn.cluster.bicluster.SpectralBiclustering`), data
      generation methods (:func:`sklearn.datasets.make_biclusters` and
      :func:`sklearn.datasets.make_checkerboard`), and scoring metrics
      (:func:`sklearn.metrics.consensus_score`). By `Kemal Eren`_.
    - Added :ref:`Restricted Boltzmann Machines<rbm>`
      (:class:`neural_network.BernoulliRBM`). By `Yann Dauphin`_.
    - Python 3 support by `Justin Vincent`_, `Lars Buitinck`_,
      `Subhodeep Moitra`_ and `Olivier Grisel`_. All tests now pass under
      Python 3.3.
    - Ability to pass one penalty (alpha value) per target in
      :class:`linear_model.Ridge`, by @eickenberg and `Mathieu Blondel`_.
    - Fixed :mod:`sklearn.linear_model.stochastic_gradient.py` L2 regularization
      issue (minor practical significants).
      By `Norbert Crombach`_ and `Mathieu Blondel`_ .
    - Added an interactive version of `Andreas Müller`_'s
      `Machine Learning Cheat Sheet (for scikit-learn)
      <http://peekaboo-vision.blogspot.de/2013/01/machine-learning-cheat-sheet-for-scikit.html>`_
      to the documentation. See :ref:`Choosing the right estimator <ml_map>`.
      By `Jaques Grobler`_.
    - :class:`grid_search.GridSearchCV` and
      :func:`cross_validation.cross_val_score` now support the use of advanced
      scoring function such as area under the ROC curve and f-beta scores.
      See :ref:`scoring_parameter` for details. By `Andreas Müller`_
      and `Lars Buitinck`_.
      Passing a function from :mod:`sklearn.metrics` as ``score_func`` is
      deprecated.
    - Multi-label classification output is now supported by
      :func:`metrics.accuracy_score`, :func:`metrics.zero_one_loss`,
      :func:`metrics.f1_score`, :func:`metrics.fbeta_score`,
      :func:`metrics.classification_report`,
      :func:`metrics.precision_score` and :func:`metrics.recall_score`
      by `Arnaud Joly`_.
    - Two new metrics :func:`metrics.hamming_loss` and
      :func:`metrics.jaccard_similarity_score`
      are added with multi-label support by `Arnaud Joly`_.
    - Speed and memory usage improvements in
      :class:`feature_extraction.text.CountVectorizer` and
      :class:`feature_extraction.text.TfidfVectorizer`,
      by Jochen Wersdörfer and Roman Sinayev.
    - The ``min_df`` parameter in
      :class:`feature_extraction.text.CountVectorizer` and
      :class:`feature_extraction.text.TfidfVectorizer`, which used to be 2,
      has been reset to 1 to avoid unpleasant surprises (empty vocabularies)
      for novice users who try it out on tiny document collections.
      A value of at least 2 is still recommended for practical use.
    - :class:`svm.LinearSVC`, :class:`linear_model.SGDClassifier` and
      :class:`linear_model.SGDRegressor` now have a ``sparsify`` method that
      converts their ``coef_`` into a sparse matrix, meaning stored models
      trained using these estimators can be made much more compact.
    - :class:`linear_model.SGDClassifier` now produces multiclass probability
      estimates when trained under log loss or modified Huber loss.
    - Hyperlinks to documentation in example code on the website by
      `Martin Luessi`_.
    - Fixed bug in :class:`preprocessing.MinMaxScaler` causing incorrect scaling
      of the features for non-default ``feature_range`` settings. By `Andreas
      Müller`_.
    - ``max_features`` in :class:`tree.DecisionTreeClassifier`,
      :class:`tree.DecisionTreeRegressor` and all derived ensemble estimators
      now supports percentage values. By `Gilles Louppe`_.
    - Performance improvements in :class:`isotonic.IsotonicRegression` by
      `Nelle Varoquaux`_.
    - :func:`metrics.accuracy_score` has an option normalize to return
      the fraction or the number of correctly classified sample
      by `Arnaud Joly`_.
    - Added :func:`metrics.log_loss` that computes log loss, aka cross-entropy
      loss. By Jochen Wersdörfer and `Lars Buitinck`_.
    - A bug that caused :class:`ensemble.AdaBoostClassifier`'s to output
      incorrect probabilities has been fixed.
    - Feature selectors now share a mixin providing consistent `transform`,
      `inverse_transform` and `get_support` methods. By `Joel Nothman`_.
    - A fitted :class:`grid_search.GridSearchCV` or
      :class:`grid_search.RandomizedSearchCV` can now generally be pickled.
      By `Joel Nothman`_.
    - Refactored and vectorized implementation of :func:`metrics.roc_curve`
      and :func:`metrics.precision_recall_curve`. By `Joel Nothman`_.
    - The new estimator :class:`sklearn.decomposition.TruncatedSVD`
      performs dimensionality reduction using SVD on sparse matrices,
      and can be used for latent semantic analysis (LSA).
      By `Lars Buitinck`_.
    - Added self-contained example of out-of-core learning on text data
      :ref:`example_applications_plot_out_of_core_classification.py`.
      By `Eustache Diemert`_.
    - The default number of components for
      :class:`sklearn.decomposition.RandomizedPCA` is now correctly documented
      to be ``n_features``. This was the default behavior, so programs using it
      will continue to work as they did.
    - :class:`sklearn.cluster.KMeans` now fits several orders of magnitude
      faster on sparse data (the speedup depends on the sparsity). By
      `Lars Buitinck`_.
    - Reduce memory footprint of FastICA by `Denis Engemann`_ and
      `Alexandre Gramfort`_.
    - Verbose output in :mod:`sklearn.ensemble.gradient_boosting` now uses
      a column format and prints progress in decreasing frequency.
      It also shows the remaining time. By `Peter Prettenhofer`_.
    - :mod:`sklearn.ensemble.gradient_boosting` provides out-of-bag improvement
      :attr:`~sklearn.ensemble.GradientBoostingRegressor.oob_improvement_`
      rather than the OOB score for model selection. An example that shows
      how to use OOB estimates to select the number of trees was added.
      By `Peter Prettenhofer`_.
    - Most metrics now support string labels for multiclass classification
      by `Arnaud Joly`_ and `Lars Buitinck`_.
    - New OrthogonalMatchingPursuitCV class by `Alexandre Gramfort`_
      and `Vlad Niculae`_.
    - Fixed a bug in :class:`sklearn.covariance.GraphLassoCV`: the
      'alphas' parameter now works as expected when given a list of
      values. By Philippe Gervais.
    - Fixed an important bug in :class:`sklearn.covariance.GraphLassoCV`
      that prevented all folds provided by a CV object to be used (only
      the first 3 were used). When providing a CV object, execution
      time may thus increase significantly compared to the previous
      version (bug results are correct now). By Philippe Gervais.
    - :class:`cross_validation.cross_val_score` and the :mod:`grid_search`
      module is now tested with multi-output data by `Arnaud Joly`_.
    - :func:`datasets.make_multilabel_classification` can now return
      the output in label indicator multilabel format  by `Arnaud Joly`_.
    - K-nearest neighbors, :class:`neighbors.KNeighborsRegressor`
      and :class:`neighbors.RadiusNeighborsRegressor`,
      and radius neighbors, :class:`neighbors.RadiusNeighborsRegressor` and
      :class:`neighbors.RadiusNeighborsClassifier` support multioutput data
      by `Arnaud Joly`_.
    - Random state in LibSVM-based estimators (:class:`svm.SVC`, :class:`NuSVC`,
      :class:`OneClassSVM`, :class:`svm.SVR`, :class:`svm.NuSVR`) can now be
      controlled.  This is useful to ensure consistency in the probability
      estimates for the classifiers trained with ``probability=True``. By
      `Vlad Niculae`_.
    - Out-of-core learning support for discrete naive Bayes classifiers
      :class:`sklearn.naive_bayes.MultinomialNB` and
      :class:`sklearn.naive_bayes.BernoulliNB` by adding the ``partial_fit``
      method by `Olivier Grisel`_.
    - New website design and navigation by `Gilles Louppe`_, `Nelle Varoquaux`_,
      Vincent Michel and `Andreas Müller`_.
    - Improved documentation on :ref:`multi-class, multi-label and multi-output
      classification <multiclass>` by `Yannick Schwartz`_ and `Arnaud Joly`_.
    - Better input and error handling in the :mod:`metrics` module by
      `Arnaud Joly`_ and `Joel Nothman`_.
    - Speed optimization of the :mod:`hmm` module by `Mikhail Korobov`_
    - Significant speed improvements for :class:`sklearn.cluster.DBSCAN`_
      by `cleverless <https://github.com/cleverless>`_
  * API changes:
    - The :func:`auc_score` was renamed :func:`roc_auc_score`.
    - Testing scikit-learn with `sklearn.test()` is deprecated. Use
      `nosetest sklearn` from the command line.
    - Feature importances in :class:`tree.DecisionTreeClassifier`,
      :class:`tree.DecisionTreeRegressor` and all derived ensemble estimators
      are now computed on the fly when accessing  the ``feature_importances_``
      attribute. Setting ``compute_importances=True`` is no longer required.
      By `Gilles Louppe`_.
    - :class:`linear_model.lasso_path` and
      :class:`linear_model.enet_path` can return its results in the same
      format as that of :class:`linear_model.lars_path`. This is done by
      setting the `return_models` parameter to `False`. By
      `Jaques Grobler`_ and `Alexandre Gramfort`_
    - :class:`grid_search.IterGrid` was renamed to
      :class:`grid_search.ParameterGrid`.
    - Fixed bug in :class:`KFold` causing imperfect class balance in some
      cases. By `Alexandre Gramfort`_ and Tadej Janež.
    - :class:`sklearn.neighbors.BallTree` has been refactored, and a
      :class:`sklearn.neighbors.KDTree` has been
      added which shares the same interface.  The Ball Tree now works with
      a wide variety of distance metrics.  Both classes have many new
      methods, including single-tree and dual-tree queries, breadth-first
      and depth-first searching, and more advanced queries such as
      kernel density estimation and 2-point correlation functions.
      By `Jake Vanderplas`_
    - Support for scipy.spatial.cKDTree within neighbors queries has been
      removed, and the functionality replaced with the new :class:`KDTree`
      class.
    - :class:`sklearn.neighbors.KernelDensity` has been added, which performs
      efficient kernel density estimation with a variety of kernels.
    - :class:`sklearn.decomposition.KernelPCA` now always returns output with
      ``n_components`` components, unless the new parameter ``remove_zero_eig``
      is set to ``True``. This new behavior is consistent with the way
      kernel PCA was always documented; previously, the removal of components
      with zero eigenvalues was tacitly performed on all data.
    - ``gcv_mode="auto"`` no longer tries to perform SVD on a densified
      sparse matrix in :class:`sklearn.linear_model.RidgeCV`.
    - Sparse matrix support in :class:`sklearn.decomposition.RandomizedPCA`
      is now deprecated in favor of the new ``TruncatedSVD``.
    - :class:`cross_validation.KFold` and
      :class:`cross_validation.StratifiedKFold` now enforce `n_folds >= 2`
      otherwise a ``ValueError`` is raised. By `Olivier Grisel`_.
    - :func:`datasets.load_files`'s ``charset`` and ``charset_errors``
      parameters were renamed ``encoding`` and ``decode_errors``.
    - Attribute ``oob_score_`` in :class:`sklearn.ensemble.GradientBoostingRegressor`
      and :class:`sklearn.ensemble.GradientBoostingClassifier`
      is deprecated and has been replaced by ``oob_improvement_`` .
    - Attributes in OrthogonalMatchingPursuit have been deprecated
      (copy_X, Gram, ...) and precompute_gram renamed precompute
      for consistency. See #2224.
    - :class:`sklearn.preprocessing.StandardScaler` now converts integer input
      to float, and raises a warning. Previously it rounded for dense integer
      input.
    - Better input validation, warning on unexpected shapes for y.
- Fix building on 13.1+
- Update BuildRequires
- Cleanup spec file formatting

-------------------------------------------------------------------
Thu Oct 24 11:13:53 UTC 2013 - speilicke@suse.com

- Require python-setuptools instead of distribute (upstreams merged)

-------------------------------------------------------------------
Fri May  3 22:35:40 UTC 2013 - toddrme2178@gmail.com

- Update to version 0.13.1

-------------------------------------------------------------------
Sat Oct 13 13:57:04 UTC 2012 - Angelos Tzotsos <tzotsos@opensuse.org>

- Update to version 0.12.1

-------------------------------------------------------------------
Sun Jun  3 16:09:34 UTC 2012 - toddrme2178@gmail.com

- Clean up spec file
- Update to version 0.11

-------------------------------------------------------------------
Wed Mar  7 21:57:46 UTC 2012 - scorot@free.fr

- remove unneeded libatals3-devel dependency

-------------------------------------------------------------------
Mon Oct 10 00:00:00 UTC 2011 - scorot@gtt.fr

- fix python-Sphinx requirement

-------------------------------------------------------------------
Sat Oct 23 00:00:00 UTC 2010 - scorot@gtt.fr

- first package
- version 0.5
