Index: statsmodels-0.14.1/INSTALL.txt
===================================================================
--- statsmodels-0.14.1.orig/INSTALL.txt
+++ statsmodels-0.14.1/INSTALL.txt
@@ -17,7 +17,7 @@ pandas >= 1.0
 
     pandas.pydata.org
 
-patsy >= 0.5.2
+patsy >= 0.5.5
 
     patsy.readthedocs.org
 
Index: statsmodels-0.14.1/azure-pipelines.yml
===================================================================
--- statsmodels-0.14.1.orig/azure-pipelines.yml
+++ statsmodels-0.14.1/azure-pipelines.yml
@@ -20,17 +20,17 @@ variables:
 
 jobs:
 
-- template: tools/ci/azure/azure_template_windows.yml
+- template: tools/ci/azure/azure_template_posix.yml
   parameters:
-    name: Windows
-    vmImage: windows-latest
+    name: Linux
+    vmImage: ubuntu-latest
 
 - template: tools/ci/azure/azure_template_posix.yml
   parameters:
     name: macOS
     vmImage: macOS-latest
 
-- template: tools/ci/azure/azure_template_posix.yml
+- template: tools/ci/azure/azure_template_windows.yml
   parameters:
-    name: Linux
-    vmImage: ubuntu-latest
+    name: Windows
+    vmImage: windows-latest
Index: statsmodels-0.14.1/docs/source/optimization.rst
===================================================================
--- statsmodels-0.14.1.orig/docs/source/optimization.rst
+++ statsmodels-0.14.1/docs/source/optimization.rst
@@ -47,7 +47,7 @@ associated with that specific optimizer:
       gtol : float
           Stop when norm of gradient is less than gtol.
       norm : float
-          Order of norm (np.Inf is max, -np.Inf is min)
+          Order of norm (np.inf is max, -np.inf is min)
       epsilon
           If fprime is approximated, use this value for the step
           size. Only relevant if LikelihoodModel.score is None.
@@ -87,7 +87,7 @@ associated with that specific optimizer:
       gtol : float
           Stop when norm of gradient is less than gtol.
       norm : float
-          Order of norm (np.Inf is max, -np.Inf is min)
+          Order of norm (np.inf is max, -np.inf is min)
       epsilon : float
           If fprime is approximated, use this value for the step
           size. Can be scalar or vector.  Only relevant if
Index: statsmodels-0.14.1/pyproject.toml
===================================================================
--- statsmodels-0.14.1.orig/pyproject.toml
+++ statsmodels-0.14.1/pyproject.toml
@@ -7,8 +7,8 @@ requires = [
     "cython>=0.29.33,<4",  # Sync with CYTHON_MIN_VER in setup
     # Workaround for oldest supported numpy using 1.21.6, but SciPy 1.9.2+ requiring 1.22.3+
     "oldest-supported-numpy; python_version!='3.10' or platform_system!='Windows' or platform_python_implementation=='PyPy'",
-    "numpy>=1.22.3,<2; python_version=='3.10' and platform_system=='Windows' and platform_python_implementation != 'PyPy'",
-    "numpy<2; python_version>='3.12'",
+    "numpy>=1.22.3,<3; python_version=='3.10' and platform_system=='Windows' and platform_python_implementation != 'PyPy'",
+    "numpy<3; python_version>='3.12'",
     "scipy>=1.4",
     "setuptools_scm[toml]>=8,<9"
 ]
Index: statsmodels-0.14.1/requirements.txt
===================================================================
--- statsmodels-0.14.1.orig/requirements.txt
+++ statsmodels-0.14.1/requirements.txt
@@ -1,8 +1,8 @@
 # Workaround for scipy build requirement
-numpy>=1.22.3,<2; python_version=="3.10" and platform_system=="Windows" and platform_python_implementation != "PyPy"
-numpy >=1.18,<2  # released December 2019
+numpy>=1.22.3,<3; python_version=="3.10" and platform_system=="Windows" and platform_python_implementation != "PyPy"
+numpy >=1.18,<3  # released December 2019
 scipy>=1.4,!=1.9.2  # released December 2019
 scipy>=1.4,!=1.9.2; sys_platform == "win32"  # Blocked  1.9.2 due to Windows issues
 pandas>=1.0,!=2.1.0  # released January 2020, 2.1.0 blocked due to bug
-patsy>=0.5.4  # released January 2018
+patsy>=0.5.5  # released December 2023
 packaging>=21.3  # released Nov 2021
Index: statsmodels-0.14.1/setup.cfg
===================================================================
--- statsmodels-0.14.1.orig/setup.cfg
+++ statsmodels-0.14.1/setup.cfg
@@ -43,7 +43,6 @@ filterwarnings =
 	error:The value returned will change to a:FutureWarning
 	error:The default value of lags:FutureWarning
 	error:non-integer arg n is deprecated:DeprecationWarning
-	error:Creating an ndarray:numpy.VisibleDeprecationWarning
 	error:The default number of lags:FutureWarning:
 	error:fft=True will become the default:FutureWarning
 	error:The parameter names will change:FutureWarning
Index: statsmodels-0.14.1/statsmodels/base/_constraints.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/base/_constraints.py
+++ statsmodels-0.14.1/statsmodels/base/_constraints.py
@@ -162,7 +162,7 @@ class TransformRestriction:
             #self.constant = q.T.dot(np.linalg.inv(L.T.dot(R.T)).dot(L.T))
             try:
                 self.constant = q.T.dot(np.linalg.solve(L.T.dot(R.T), L.T))
-            except np.linalg.linalg.LinAlgError as e:
+            except np.linalg.LinAlgError as e:
                 raise ValueError('possibly inconsistent constraints. error '
                                  'generated by\n%r' % (e, ))
         else:
@@ -362,9 +362,9 @@ def fit_constrained_wrap(model, constrai
     #       patched version
     # TODO: decide whether to move the imports
     from patsy import DesignInfo
+
     # we need this import if we copy it to a different module
     #from statsmodels.base._constraints import fit_constrained
-
     # same pattern as in base.LikelihoodModel.t_test
     lc = DesignInfo(self.exog_names).linear_constraint(constraints)
     R, q = lc.coefs, lc.constants
Index: statsmodels-0.14.1/statsmodels/base/model.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/base/model.py
+++ statsmodels-0.14.1/statsmodels/base/model.py
@@ -427,7 +427,7 @@ class LikelihoodModel(Model):
                 gtol : float
                     Stop when norm of gradient is less than gtol.
                 norm : float
-                    Order of norm (np.Inf is max, -np.Inf is min)
+                    Order of norm (np.inf is max, -np.inf is min)
                 epsilon
                     If fprime is approximated, use this value for the step
                     size. Only relevant if LikelihoodModel.score is None.
@@ -452,7 +452,7 @@ class LikelihoodModel(Model):
                 gtol : float
                     Stop when norm of gradient is less than gtol.
                 norm : float
-                    Order of norm (np.Inf is max, -np.Inf is min)
+                    Order of norm (np.inf is max, -np.inf is min)
                 epsilon : float
                     If fprime is approximated, use this value for the step
                     size. Can be scalar or vector.  Only relevant if
Index: statsmodels-0.14.1/statsmodels/base/optimizer.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/base/optimizer.py
+++ statsmodels-0.14.1/statsmodels/base/optimizer.py
@@ -4,10 +4,12 @@ to untie these from LikelihoodModel so t
 """
 from __future__ import annotations
 
+from statsmodels.compat.scipy import SP_LT_15, SP_LT_17
+
 from typing import Any, Sequence
+
 import numpy as np
 from scipy import optimize
-from statsmodels.compat.scipy import SP_LT_15, SP_LT_17
 
 
 def check_kwargs(kwargs: dict[str, Any], allowed: Sequence[str], method: str):
@@ -117,7 +119,7 @@ class Optimizer:
                 gtol : float
                     Stop when norm of gradient is less than gtol.
                 norm : float
-                    Order of norm (np.Inf is max, -np.Inf is min)
+                    Order of norm (np.inf is max, -np.inf is min)
                 epsilon
                     If fprime is approximated, use this value for the step
                     size. Only relevant if LikelihoodModel.score is None.
@@ -152,7 +154,7 @@ class Optimizer:
                 gtol : float
                     Stop when norm of gradient is less than gtol.
                 norm : float
-                    Order of norm (np.Inf is max, -np.Inf is min)
+                    Order of norm (np.inf is max, -np.inf is min)
                 epsilon : float
                     If fprime is approximated, use this value for the step
                     size. Can be scalar or vector.  Only relevant if
@@ -532,7 +534,7 @@ def _fit_bfgs(f, score, start_params, fa
     """
     check_kwargs(kwargs, ("gtol", "norm", "epsilon"), "bfgs")
     gtol = kwargs.setdefault('gtol', 1.0000000000000001e-05)
-    norm = kwargs.setdefault('norm', np.Inf)
+    norm = kwargs.setdefault('norm', np.inf)
     epsilon = kwargs.setdefault('epsilon', 1.4901161193847656e-08)
     retvals = optimize.fmin_bfgs(f, start_params, score, args=fargs,
                                  gtol=gtol, norm=norm, epsilon=epsilon,
@@ -804,7 +806,7 @@ def _fit_cg(f, score, start_params, farg
     """
     check_kwargs(kwargs, ("gtol", "norm", "epsilon"), "cg")
     gtol = kwargs.setdefault('gtol', 1.0000000000000001e-05)
-    norm = kwargs.setdefault('norm', np.Inf)
+    norm = kwargs.setdefault('norm', np.inf)
     epsilon = kwargs.setdefault('epsilon', 1.4901161193847656e-08)
     retvals = optimize.fmin_cg(f, start_params, score, gtol=gtol, norm=norm,
                                epsilon=epsilon, maxiter=maxiter,
Index: statsmodels-0.14.1/statsmodels/compat/numpy.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/compat/numpy.py
+++ statsmodels-0.14.1/statsmodels/compat/numpy.py
@@ -38,11 +38,11 @@ THEORY OF LIABILITY, WHETHER IN CONTRACT
 (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
 OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.
 """
-from packaging.version import Version, parse
-
 import numpy as np
+from packaging.version import Version, parse
 
 __all__ = [
+    "NP_LT_2",
     "NP_LT_123",
     "NP_LT_114",
     "lstsq",
@@ -52,6 +52,7 @@ __all__ = [
 
 NP_LT_114 = parse(np.__version__) < Version("1.13.99")
 NP_LT_123 = parse(np.__version__) < Version("1.22.99")
+NP_LT_2 = parse(np.__version__) < Version("1.99.99")
 
 np_matrix_rank = np.linalg.matrix_rank
 np_new_unique = np.unique
Index: statsmodels-0.14.1/statsmodels/emplike/aft_el.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/emplike/aft_el.py
+++ statsmodels-0.14.1/statsmodels/emplike/aft_el.py
@@ -487,7 +487,7 @@ class AFTResults(OptAFT):
 
                 llr = res[1]
                 return llr, chi2.sf(llr, len(param_nums))
-            except np.linalg.linalg.LinAlgError:
+            except np.linalg.LinAlgError:
                 return np.inf, 0
 
     def ci_beta(self, param_num, beta_high, beta_low, sig=.05):
Index: statsmodels-0.14.1/statsmodels/emplike/elregress.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/emplike/elregress.py
+++ statsmodels-0.14.1/statsmodels/emplike/elregress.py
@@ -13,8 +13,8 @@ Owen, A.B.(2001). Empirical Likelihood.
 
 """
 import numpy as np
-from statsmodels.emplike.descriptive import _OptFuncts
 
+from statsmodels.emplike.descriptive import _OptFuncts
 
 
 class _ELRegOpts(_OptFuncts):
@@ -83,5 +83,5 @@ class _ELRegOpts(_OptFuncts):
             #    raise RuntimeError('weights do not sum to 1')
             llr = np.sum(np.log(nobs * self.new_weights))
             return -2 * llr
-        except np.linalg.linalg.LinAlgError:
+        except np.linalg.LinAlgError:
             return np.inf
Index: statsmodels-0.14.1/statsmodels/examples/ex_generic_mle_t.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/examples/ex_generic_mle_t.py
+++ statsmodels-0.14.1/statsmodels/examples/ex_generic_mle_t.py
@@ -7,8 +7,8 @@ Author: josef-pktd
 
 
 import numpy as np
-
 from scipy import special
+
 import statsmodels.api as sm
 from statsmodels.base.model import GenericLikelihoodModel
 from statsmodels.tools.numdiff import approx_hess
@@ -251,7 +251,7 @@ array([ 31.93524822,  22.0333515 ,
 Traceback (most recent call last):
   [...]
     raise LinAlgError, 'Singular matrix'
-numpy.linalg.linalg.LinAlgError: Singular matrix
+numpy.linalg.LinAlgError: Singular matrix
 >>> resp.params
 array([  1.58253308e-01,   1.73188603e-01,   1.77357447e-01,
          2.06707494e-02,  -1.31174789e-01,   8.79915580e-01,
Index: statsmodels-0.14.1/statsmodels/genmod/generalized_linear_model.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/genmod/generalized_linear_model.py
+++ statsmodels-0.14.1/statsmodels/genmod/generalized_linear_model.py
@@ -22,15 +22,13 @@ from statsmodels.compat.pandas import Ap
 import warnings
 
 import numpy as np
-from numpy.linalg.linalg import LinAlgError
-
-import statsmodels.base.model as base
-import statsmodels.base.wrapper as wrap
+from numpy.linalg import LinAlgError
 
 from statsmodels.base import _prediction_inference as pred
-from statsmodels.base._prediction_inference import PredictionResultsMean
 import statsmodels.base._parameter_inference as pinfer
-
+from statsmodels.base._prediction_inference import PredictionResultsMean
+import statsmodels.base.model as base
+import statsmodels.base.wrapper as wrap
 from statsmodels.graphics._regressionplots_doc import (
     _plot_added_variable_doc,
     _plot_ceres_residuals_doc,
@@ -54,7 +52,6 @@ from statsmodels.tools.validation import
 # need import in module instead of lazily to copy `__doc__`
 from . import families
 
-
 __all__ = ['GLM', 'PredictionResultsMean']
 
 
@@ -661,7 +658,7 @@ class GLM(base.LikelihoodModel):
         from statsmodels.discrete.discrete_margins import (
             _get_count_effects,
             _get_dummy_effects,
-            )
+        )
 
         if count_idx is not None:
             margeff = _get_count_effects(margeff, exog, count_idx, transform,
Index: statsmodels-0.14.1/statsmodels/genmod/tests/results/results_glm.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/genmod/tests/results/results_glm.py
+++ statsmodels-0.14.1/statsmodels/genmod/tests/results/results_glm.py
@@ -824,7 +824,7 @@ class Cancer:
     def __init__(self):
         filename = os.path.join(os.path.dirname(os.path.abspath(__file__)),
                                 "stata_cancer_glm.csv")
-        data = np.recfromcsv(open(filename, 'rb'))
+        data = pd.read_csv(filename)
         self.endog = data.studytime
         dummies = pd.get_dummies(pd.Series(data.drug, dtype="category"),
                                  drop_first=True)
Index: statsmodels-0.14.1/statsmodels/graphics/tests/test_boxplots.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/graphics/tests/test_boxplots.py
+++ statsmodels-0.14.1/statsmodels/graphics/tests/test_boxplots.py
@@ -24,12 +24,8 @@ def age_and_labels():
 
 
 # TODO: Remove once SciPy 1.5.0 is the minimum
-IGNORE_VISIBLE_DEPR = """\
-ignore:Creating an ndarray from ragged nested sequences:\
-numpy.VisibleDeprecationWarning:"""
 
 
-@pytest.mark.filterwarnings(IGNORE_VISIBLE_DEPR)
 @pytest.mark.matplotlib
 def test_violinplot(age_and_labels, close_figures):
     age, labels = age_and_labels
@@ -41,7 +37,6 @@ def test_violinplot(age_and_labels, clos
                           'label_rotation': 30})
 
 
-@pytest.mark.filterwarnings(IGNORE_VISIBLE_DEPR)
 @pytest.mark.matplotlib
 def test_violinplot_bw_factor(age_and_labels, close_figures):
     age, labels = age_and_labels
Index: statsmodels-0.14.1/statsmodels/graphics/tests/test_functional.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/graphics/tests/test_functional.py
+++ statsmodels-0.14.1/statsmodels/graphics/tests/test_functional.py
@@ -27,41 +27,41 @@ data = data.raw_data[:, 1:]
 def test_hdr_basic(close_figures):
     try:
         _, hdr = hdrboxplot(data, labels=labels, seed=12345)
-    except WindowsError:
-        pytest.xfail('Multiprocess randomly crashes in Windows testing')
 
-    assert len(hdr.extra_quantiles) == 0
+        assert len(hdr.extra_quantiles) == 0
 
-    median_t = [24.247, 25.625, 25.964, 24.999, 23.648, 22.302,
-                21.231, 20.366, 20.168, 20.434, 21.111, 22.299]
+        median_t = [24.247, 25.625, 25.964, 24.999, 23.648, 22.302,
+                    21.231, 20.366, 20.168, 20.434, 21.111, 22.299]
 
-    assert_almost_equal(hdr.median, median_t, decimal=2)
+        assert_almost_equal(hdr.median, median_t, decimal=2)
 
-    quant = np.vstack([hdr.outliers, hdr.hdr_90, hdr.hdr_50])
-    quant_t = np.vstack([[24.36, 25.42, 25.40, 24.96, 24.21, 23.35,
-                          22.50, 21.89, 22.04, 22.88, 24.57, 25.89],
-                         [27.25, 28.23, 28.85, 28.82, 28.37, 27.43,
-                          25.73, 23.88, 22.26, 22.22, 22.21, 23.19],
-                         [23.70, 26.08, 27.17, 26.74, 26.77, 26.15,
-                          25.59, 24.95, 24.69, 24.64, 25.85, 27.08],
-                         [28.12, 28.82, 29.24, 28.45, 27.36, 25.19,
-                          23.61, 22.27, 21.31, 21.37, 21.60, 22.81],
-                         [25.48, 26.99, 27.51, 27.04, 26.23, 24.94,
-                          23.69, 22.72, 22.26, 22.64, 23.33, 24.44],
-                         [23.11, 24.50, 24.66, 23.44, 21.74, 20.58,
-                          19.68, 18.84, 18.76, 18.99, 19.66, 20.86],
-                         [24.84, 26.23, 26.67, 25.93, 24.87, 23.57,
-                          22.46, 21.45, 21.26, 21.57, 22.14, 23.41],
-                         [23.62, 25.10, 25.34, 24.22, 22.74, 21.52,
-                          20.40, 19.56, 19.63, 19.67, 20.37, 21.76]])
-
-    assert_almost_equal(quant, quant_t, decimal=0)
-
-    labels_pos = np.all(np.in1d(data, hdr.outliers).reshape(data.shape),
-                        axis=1)
-    outliers = labels[labels_pos]
-    assert_equal([1982, 1983, 1997, 1998], outliers)
-    assert_equal(labels[hdr.outliers_idx], outliers)
+        quant = np.vstack([hdr.outliers, hdr.hdr_90, hdr.hdr_50])
+        quant_t = np.vstack([[24.36, 25.42, 25.40, 24.96, 24.21, 23.35,
+                              22.50, 21.89, 22.04, 22.88, 24.57, 25.89],
+                             [27.25, 28.23, 28.85, 28.82, 28.37, 27.43,
+                              25.73, 23.88, 22.26, 22.22, 22.21, 23.19],
+                             [23.70, 26.08, 27.17, 26.74, 26.77, 26.15,
+                              25.59, 24.95, 24.69, 24.64, 25.85, 27.08],
+                             [28.12, 28.82, 29.24, 28.45, 27.36, 25.19,
+                              23.61, 22.27, 21.31, 21.37, 21.60, 22.81],
+                             [25.48, 26.99, 27.51, 27.04, 26.23, 24.94,
+                              23.69, 22.72, 22.26, 22.64, 23.33, 24.44],
+                             [23.11, 24.50, 24.66, 23.44, 21.74, 20.58,
+                              19.68, 18.84, 18.76, 18.99, 19.66, 20.86],
+                             [24.84, 26.23, 26.67, 25.93, 24.87, 23.57,
+                              22.46, 21.45, 21.26, 21.57, 22.14, 23.41],
+                             [23.62, 25.10, 25.34, 24.22, 22.74, 21.52,
+                              20.40, 19.56, 19.63, 19.67, 20.37, 21.76]])
+
+        assert_almost_equal(quant, quant_t, decimal=0)
+
+        labels_pos = np.all(np.isin(data, hdr.outliers).reshape(data.shape),
+                            axis=1)
+        outliers = labels[labels_pos]
+        assert_equal([1982, 1983, 1997, 1998], outliers)
+        assert_equal(labels[hdr.outliers_idx], outliers)
+    except WindowsError:
+        pytest.xfail('Multiprocess randomly crashes in Windows testing')
 
 
 @pytest.mark.slow
@@ -69,16 +69,14 @@ def test_hdr_basic(close_figures):
 def test_hdr_basic_brute(close_figures, reset_randomstate):
     try:
         _, hdr = hdrboxplot(data, ncomp=2, labels=labels, use_brute=True)
+        assert len(hdr.extra_quantiles) == 0
+        median_t = [24.247, 25.625, 25.964, 24.999, 23.648, 22.302,
+                    21.231, 20.366, 20.168, 20.434, 21.111, 22.299]
+
+        assert_almost_equal(hdr.median, median_t, decimal=2)
     except WindowsError:
         pytest.xfail('Multiprocess randomly crashes in Windows testing')
 
-    assert len(hdr.extra_quantiles) == 0
-
-    median_t = [24.247, 25.625, 25.964, 24.999, 23.648, 22.302,
-                21.231, 20.366, 20.168, 20.434, 21.111, 22.299]
-
-    assert_almost_equal(hdr.median, median_t, decimal=2)
-
 
 @pytest.mark.slow
 @pytest.mark.matplotlib
@@ -89,54 +87,54 @@ def test_hdr_plot(close_figures):
     try:
         hdrboxplot(data, labels=labels.tolist(), ax=ax, threshold=1,
                    seed=12345)
+
+        ax.set_xlabel("Month of the year")
+        ax.set_ylabel("Sea surface temperature (C)")
+        ax.set_xticks(np.arange(13, step=3) - 1)
+        ax.set_xticklabels(["", "Mar", "Jun", "Sep", "Dec"])
+        ax.set_xlim([-0.2, 11.2])
     except WindowsError:
         pytest.xfail('Multiprocess randomly crashes in Windows testing')
 
-    ax.set_xlabel("Month of the year")
-    ax.set_ylabel("Sea surface temperature (C)")
-    ax.set_xticks(np.arange(13, step=3) - 1)
-    ax.set_xticklabels(["", "Mar", "Jun", "Sep", "Dec"])
-    ax.set_xlim([-0.2, 11.2])
-
 
 @pytest.mark.slow
 @pytest.mark.matplotlib
 def test_hdr_alpha(close_figures):
     try:
         _, hdr = hdrboxplot(data, alpha=[0.7], seed=12345)
+
+        extra_quant_t = np.vstack([[25.1, 26.5, 27.0, 26.4, 25.4, 24.1,
+                                    23.0, 22.0, 21.7, 22.1, 22.7, 23.8],
+                                   [23.4, 24.8, 25.0, 23.9, 22.4, 21.1,
+                                    20.0, 19.3, 19.2, 19.4, 20.1, 21.3]])
+        assert_almost_equal(hdr.extra_quantiles, extra_quant_t, decimal=0)
     except WindowsError:
         pytest.xfail('Multiprocess randomly crashes in Windows testing')
 
-    extra_quant_t = np.vstack([[25.1, 26.5, 27.0, 26.4, 25.4, 24.1,
-                                23.0, 22.0, 21.7, 22.1, 22.7, 23.8],
-                               [23.4, 24.8, 25.0, 23.9, 22.4, 21.1,
-                                20.0, 19.3, 19.2, 19.4, 20.1, 21.3]])
-    assert_almost_equal(hdr.extra_quantiles, extra_quant_t, decimal=0)
-
 
 @pytest.mark.slow
 @pytest.mark.matplotlib
 def test_hdr_multiple_alpha(close_figures):
     try:
         _, hdr = hdrboxplot(data, alpha=[0.4, 0.92], seed=12345)
+
+        extra_quant_t = [[25.712, 27.052, 27.711, 27.200,
+                          26.162, 24.833, 23.639, 22.378,
+                          22.250, 22.640, 23.472, 24.649],
+                         [22.973, 24.526, 24.608, 23.343,
+                          21.908, 20.655, 19.750, 19.046,
+                          18.812, 18.989, 19.520, 20.685],
+                         [24.667, 26.033, 26.416, 25.584,
+                          24.308, 22.849, 21.684, 20.948,
+                          20.483, 21.019, 21.751, 22.890],
+                         [23.873, 25.371, 25.667, 24.644,
+                          23.177, 21.923, 20.791, 20.015,
+                          19.697, 19.951, 20.622, 21.858]]
+        assert_almost_equal(hdr.extra_quantiles, np.vstack(extra_quant_t),
+                            decimal=0)
     except WindowsError:
         pytest.xfail('Multiprocess randomly crashes in Windows testing')
 
-    extra_quant_t = [[25.712, 27.052, 27.711, 27.200,
-                      26.162, 24.833, 23.639, 22.378,
-                      22.250, 22.640, 23.472, 24.649],
-                     [22.973, 24.526, 24.608, 23.343,
-                      21.908, 20.655, 19.750, 19.046,
-                      18.812, 18.989, 19.520, 20.685],
-                     [24.667, 26.033, 26.416, 25.584,
-                      24.308, 22.849, 21.684, 20.948,
-                      20.483, 21.019, 21.751, 22.890],
-                     [23.873, 25.371, 25.667, 24.644,
-                      23.177, 21.923, 20.791, 20.015,
-                      19.697, 19.951, 20.622, 21.858]]
-    assert_almost_equal(hdr.extra_quantiles, np.vstack(extra_quant_t),
-                        decimal=0)
-
 
 @pytest.mark.slow
 @pytest.mark.matplotlib
@@ -144,39 +142,38 @@ def test_hdr_threshold(close_figures):
     try:
         _, hdr = hdrboxplot(data, alpha=[0.8], threshold=0.93,
                             seed=12345)
+        labels_pos = np.all(np.isin(data, hdr.outliers).reshape(data.shape),
+                            axis=1)
+        outliers = labels[labels_pos]
+        assert_equal([1968, 1982, 1983, 1997, 1998], outliers)
     except WindowsError:
         pytest.xfail('Multiprocess randomly crashes in Windows testing')
 
-    labels_pos = np.all(np.in1d(data, hdr.outliers).reshape(data.shape),
-                        axis=1)
-    outliers = labels[labels_pos]
-    assert_equal([1968, 1982, 1983, 1997, 1998], outliers)
-
 
 @pytest.mark.matplotlib
 def test_hdr_bw(close_figures):
     try:
         _, hdr = hdrboxplot(data, bw='cv_ml', seed=12345)
+
+        median_t = [24.25, 25.64, 25.99, 25.04, 23.71, 22.38,
+
+                    21.31, 20.44, 20.24, 20.51, 21.19, 22.38]
+        assert_almost_equal(hdr.median, median_t, decimal=2)
     except WindowsError:
         pytest.xfail('Multiprocess randomly crashes in Windows testing')
 
-    median_t = [24.25, 25.64, 25.99, 25.04, 23.71, 22.38,
-                21.31, 20.44, 20.24, 20.51, 21.19, 22.38]
-    assert_almost_equal(hdr.median, median_t, decimal=2)
-
 
 @pytest.mark.slow
 @pytest.mark.matplotlib
 def test_hdr_ncomp(close_figures):
     try:
         _, hdr = hdrboxplot(data, ncomp=3, seed=12345)
+        median_t = [24.33, 25.71, 26.04, 25.08, 23.74, 22.40,
+                    21.32, 20.45, 20.25, 20.53, 21.20, 22.39]
+        assert_almost_equal(hdr.median, median_t, decimal=2)
     except WindowsError:
         pytest.xfail('Multiprocess randomly crashes in Windows testing')
 
-    median_t = [24.33, 25.71, 26.04, 25.08, 23.74, 22.40,
-                21.32, 20.45, 20.25, 20.53, 21.20, 22.39]
-    assert_almost_equal(hdr.median, median_t, decimal=2)
-
 
 def test_banddepth_BD2():
     xx = np.arange(500) / 150.
Index: statsmodels-0.14.1/statsmodels/nonparametric/linbin.pyx
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/nonparametric/linbin.pyx
+++ statsmodels-0.14.1/statsmodels/nonparametric/linbin.pyx
@@ -10,7 +10,7 @@ cimport numpy as np
 import numpy as np
 
 ctypedef np.float64_t DOUBLE
-ctypedef np.int_t INT
+ctypedef np.int64_t INT
 
 def fast_linbin(np.ndarray[DOUBLE] X, double a, double b, int M, int trunc=1):
     """
@@ -22,7 +22,7 @@ def fast_linbin(np.ndarray[DOUBLE] X, do
         double delta = (b - a)/(M - 1)
         np.ndarray[DOUBLE] gcnts = np.zeros(M, float)
         np.ndarray[DOUBLE] lxi = (X - a)/delta
-        np.ndarray[INT] li = lxi.astype(int)
+        np.ndarray[INT] li = lxi.astype(np.int64)
         np.ndarray[DOUBLE] rem = lxi - li
 
 
Index: statsmodels-0.14.1/statsmodels/nonparametric/tests/test_kernels.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/nonparametric/tests/test_kernels.py
+++ statsmodels-0.14.1/statsmodels/nonparametric/tests/test_kernels.py
@@ -6,21 +6,22 @@ Created on Sat Dec 14 17:23:25 2013
 Author: Josef Perktold
 """
 import os
+
 import numpy as np
 from numpy.testing import assert_allclose, assert_array_less
+import pandas as pd
 import pytest
 
 from statsmodels.sandbox.nonparametric import kernels
 
-
 DEBUG = 0
 
 curdir = os.path.dirname(os.path.abspath(__file__))
 fname = 'results/results_kernel_regression.csv'
-results = np.recfromcsv(os.path.join(curdir, fname))
+results = pd.read_csv(os.path.join(curdir, fname))
 
-y = results['accident']
-x = results['service']
+y = results['accident'].to_numpy(copy=True)
+x = results['service'].to_numpy(copy=True)
 positive = x >= 0
 x = np.log(x[positive])
 y = y[positive]
Index: statsmodels-0.14.1/statsmodels/regression/linear_model.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/regression/linear_model.py
+++ statsmodels-0.14.1/statsmodels/regression/linear_model.py
@@ -49,10 +49,7 @@ from statsmodels.emplike.elregress impor
 # need import in module instead of lazily to copy `__doc__`
 from statsmodels.regression._prediction import PredictionResults
 from statsmodels.tools.decorators import cache_readonly, cache_writable
-from statsmodels.tools.sm_exceptions import (
-    InvalidTestWarning,
-    ValueWarning,
-    )
+from statsmodels.tools.sm_exceptions import InvalidTestWarning, ValueWarning
 from statsmodels.tools.tools import pinv_extended
 from statsmodels.tools.typing import Float64Array
 from statsmodels.tools.validation import bool_like, float_like, string_like
@@ -1949,7 +1946,7 @@ class RegressionResults(base.LikelihoodM
             eigvals = self._wexog_singular_values ** 2
         else:
             wx = self.model.wexog
-            eigvals = np.linalg.linalg.eigvalsh(wx.T @ wx)
+            eigvals = np.linalg.eigvalsh(wx.T @ wx)
         return np.sort(eigvals)[::-1]
 
     @cache_readonly
Index: statsmodels-0.14.1/statsmodels/regression/tests/test_lme.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/regression/tests/test_lme.py
+++ statsmodels-0.14.1/statsmodels/regression/tests/test_lme.py
@@ -1008,12 +1008,12 @@ def test_handle_missing():
     re = np.kron(re, np.ones((2, 1)))
     df["y"] = re[:, 0] + re[:, 1] * df.z1 + re[:, 2] * df.c1
     df["y"] += re[:, 3] * df.c2 + np.random.normal(size=100)
-    df.loc[1, "y"] = np.NaN
-    df.loc[2, "g"] = np.NaN
-    df.loc[3, "x1"] = np.NaN
-    df.loc[4, "z1"] = np.NaN
-    df.loc[5, "c1"] = np.NaN
-    df.loc[6, "c2"] = np.NaN
+    df.loc[1, "y"] = np.nan
+    df.loc[2, "g"] = np.nan
+    df.loc[3, "x1"] = np.nan
+    df.loc[4, "z1"] = np.nan
+    df.loc[5, "c1"] = np.nan
+    df.loc[6, "c2"] = np.nan
 
     fml = "y ~ x1"
     re_formula = "1 + z1"
Index: statsmodels-0.14.1/statsmodels/sandbox/descstats.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/sandbox/descstats.py
+++ statsmodels-0.14.1/statsmodels/sandbox/descstats.py
@@ -131,7 +131,7 @@ def descstats(data, cols=None, axis=0):
 #    import os
 #    loc='http://eagle1.american.edu/~js2796a/data/handguns_data.csv'
 #    relpath=(load_dataset(loc))
-#    dta=np.recfromcsv(relpath)
+#    dta=np.genfromtxt(relpath, delimiter=",")
 #    descstats(dta,['stpop'])
 #    raw_input('Hit enter for multivariate test')
 #    descstats(dta,['stpop','avginc','vio'])
@@ -163,7 +163,7 @@ if __name__ == '__main__':
     sum1a = descstats(data.exog[:,:1])
 
 #    loc='http://eagle1.american.edu/~js2796a/data/handguns_data.csv'
-#    dta=np.recfromcsv(loc)
+#    dta=np.genfromtxt(loc, delimiter=",")
 #    summary2 = descstats(dta,['stpop'])
 #    summary3 =  descstats(dta,['stpop','avginc','vio'])
 #TODO: needs a by argument
@@ -176,7 +176,7 @@ if __name__ == '__main__':
 
 ### This is *really* slow ###
     if os.path.isfile('./Econ724_PS_I_Data.csv'):
-        data2 = np.recfromcsv('./Econ724_PS_I_Data.csv')
+        data2 = np.genfromtxt('./Econ724_PS_I_Data.csv', delimiter=",")
         sum2 = descstats(data2.ahe)
         sum3 = descstats(np.column_stack((data2.ahe,data2.yrseduc)))
         sum4 = descstats(np.column_stack(([data2[_] for \
Index: statsmodels-0.14.1/statsmodels/sandbox/regression/gmm.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/sandbox/regression/gmm.py
+++ statsmodels-0.14.1/statsmodels/sandbox/regression/gmm.py
@@ -53,13 +53,19 @@ from statsmodels.compat.python import lr
 import numpy as np
 from scipy import optimize, stats
 
-from statsmodels.tools.numdiff import approx_fprime
-from statsmodels.base.model import (Model,
-                                    LikelihoodModel, LikelihoodModelResults)
-from statsmodels.regression.linear_model import (OLS, RegressionResults,
-                                                 RegressionResultsWrapper)
+from statsmodels.base.model import (
+    LikelihoodModel,
+    LikelihoodModelResults,
+    Model,
+)
+from statsmodels.regression.linear_model import (
+    OLS,
+    RegressionResults,
+    RegressionResultsWrapper,
+)
 import statsmodels.stats.sandwich_covariance as smcov
 from statsmodels.tools.decorators import cache_readonly
+from statsmodels.tools.numdiff import approx_fprime
 from statsmodels.tools.tools import _ensure_2d
 
 DEBUG = 0
@@ -277,15 +283,18 @@ class IVRegressionResults(RegressionResu
         """
 
         #TODO: import where we need it (for now), add as cached attributes
-        from statsmodels.stats.stattools import (jarque_bera,
-                omni_normtest, durbin_watson)
+        from statsmodels.stats.stattools import (
+            durbin_watson,
+            jarque_bera,
+            omni_normtest,
+        )
         jb, jbpv, skew, kurtosis = jarque_bera(self.wresid)
         omni, omnipv = omni_normtest(self.wresid)
 
         #TODO: reuse condno from somewhere else ?
         #condno = np.linalg.cond(np.dot(self.wexog.T, self.wexog))
         wexog = self.model.wexog
-        eigvals = np.linalg.linalg.eigvalsh(np.dot(wexog.T, wexog))
+        eigvals = np.linalg.eigvalsh(np.dot(wexog.T, wexog))
         eigvals = np.sort(eigvals) #in increasing order
         condno = np.sqrt(eigvals[-1]/eigvals[0])
 
Index: statsmodels-0.14.1/statsmodels/tools/data.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tools/data.py
+++ statsmodels-0.14.1/statsmodels/tools/data.py
@@ -1,12 +1,14 @@
 """
 Compatibility tools for various data structure inputs
 """
+from statsmodels.compat.numpy import NP_LT_2
+
 import numpy as np
 import pandas as pd
 
 
 def _check_period_index(x, freq="M"):
-    from pandas import PeriodIndex, DatetimeIndex
+    from pandas import DatetimeIndex, PeriodIndex
     if not isinstance(x.index, (DatetimeIndex, PeriodIndex)):
         raise ValueError("The index must be a DatetimeIndex or PeriodIndex")
 
@@ -115,4 +117,7 @@ def _is_recarray(data):
     """
     Returns true if data is a recarray
     """
-    return isinstance(data, np.core.recarray)
+    if NP_LT_2:
+        return isinstance(data, np.core.recarray)
+    else:
+        return isinstance(data, np.rec.recarray)
Index: statsmodels-0.14.1/statsmodels/tools/tools.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tools/tools.py
+++ statsmodels-0.14.1/statsmodels/tools/tools.py
@@ -271,7 +271,7 @@ def pinv_extended(x, rcond=1e-15):
             s[i] = 1./s[i]
         else:
             s[i] = 0.
-    res = np.dot(np.transpose(vt), np.multiply(s[:, np.core.newaxis],
+    res = np.dot(np.transpose(vt), np.multiply(s[:, np.newaxis],
                                                np.transpose(u)))
     return res, s_orig
 
Index: statsmodels-0.14.1/statsmodels/tsa/arima/specification.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/arima/specification.py
+++ statsmodels-0.14.1/statsmodels/tsa/arima/specification.py
@@ -8,13 +8,16 @@ import numpy as np
 import pandas as pd
 
 from statsmodels.tools.data import _is_using_pandas
+from statsmodels.tsa.arima.tools import standardize_lag_order, validate_basic
 from statsmodels.tsa.base.tsa_model import TimeSeriesModel
 from statsmodels.tsa.statespace.tools import (
-    is_invertible, constrain_stationary_univariate as constrain,
+    constrain_stationary_univariate as constrain,
+    is_invertible,
+    prepare_exog,
+    prepare_trend_data,
+    prepare_trend_spec,
     unconstrain_stationary_univariate as unconstrain,
-    prepare_exog, prepare_trend_spec, prepare_trend_data)
-
-from statsmodels.tsa.arima.tools import standardize_lag_order, validate_basic
+)
 
 
 class SARIMAXSpecification:
@@ -739,7 +742,7 @@ class SARIMAXSpecification:
         params : array_like
             Array of model parameters.
         allow_infnan : bool, optional
-            Whether or not to allow `params` to contain -np.Inf, np.Inf, and
+            Whether or not to allow `params` to contain -np.inf, np.inf, and
             np.nan. Default is False.
 
         Returns
Index: statsmodels-0.14.1/statsmodels/tsa/arima/tools.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/arima/tools.py
+++ statsmodels-0.14.1/statsmodels/tsa/arima/tools.py
@@ -119,7 +119,7 @@ def validate_basic(params, length, allow
     length : int
         Expected length of the parameter vector.
     allow_infnan : bool, optional
-            Whether or not to allow `params` to contain -np.Inf, np.Inf, and
+            Whether or not to allow `params` to contain -np.inf, np.inf, and
             np.nan. Default is False.
     title : str, optional
         Description of the parameters (e.g. "autoregressive") to use in error
Index: statsmodels-0.14.1/statsmodels/tsa/arima_process.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/arima_process.py
+++ statsmodels-0.14.1/statsmodels/tsa/arima_process.py
@@ -16,16 +16,22 @@ Judge, ... (1985): The Theory and Practi
 Author: josefpktd
 License: BSD
 """
-import warnings
-
+from statsmodels.compat.numpy import NP_LT_2
 from statsmodels.compat.pandas import Appender
 
+import warnings
+
 import numpy as np
 from scipy import linalg, optimize, signal
 
 from statsmodels.tools.docstring import Docstring, remove_parameters
 from statsmodels.tools.validation import array_like
 
+if NP_LT_2:
+    ComplexWarning = np.ComplexWarning
+else:
+    ComplexWarning = np.exceptions.ComplexWarning
+
 __all__ = [
     "arma_acf",
     "arma_acovf",
@@ -500,7 +506,7 @@ def lpol2index(ar):
         index (lags) of lag polynomial with non-zero elements
     """
     with warnings.catch_warnings():
-        warnings.simplefilter("ignore", np.ComplexWarning)
+        warnings.simplefilter("ignore", ComplexWarning)
         ar = array_like(ar, "ar")
     index = np.nonzero(ar)[0]
     coeffs = ar[index]
@@ -734,7 +740,7 @@ class ArmaProcess:
         if ma is None:
             ma = np.array([1.0])
         with warnings.catch_warnings():
-            warnings.simplefilter("ignore", np.ComplexWarning)
+            warnings.simplefilter("ignore", ComplexWarning)
             self.ar = array_like(ar, "ar")
             self.ma = array_like(ma, "ma")
         self.arcoefs = -self.ar[1:]
Index: statsmodels-0.14.1/statsmodels/tsa/base/datetools.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/base/datetools.py
+++ statsmodels-0.14.1/statsmodels/tsa/base/datetools.py
@@ -138,9 +138,9 @@ def date_range_str(start, end=None, leng
     # tack on last year
     years = years + [(str(yr2))] * offset2
     if split != 'a':
-        offset = np.tile(np.arange(1, annual_freq + 1), yr2 - yr1 - 1).astype("a2")
-        offset = np.r_[np.arange(offset1, annual_freq + 1).astype('a2'), offset]
-        offset = np.r_[offset, np.arange(1, offset2 + 1).astype('a2')]
+        offset = np.tile(np.arange(1, annual_freq + 1), yr2 - yr1 - 1).astype("S2")
+        offset = np.r_[np.arange(offset1, annual_freq + 1).astype('S2'), offset]
+        offset = np.r_[offset, np.arange(1, offset2 + 1).astype('S2')]
         date_arr_range = [''.join([i, split, asstr(j)])
                           for i, j in zip(years, offset)]
     else:
Index: statsmodels-0.14.1/statsmodels/tsa/base/tests/test_tsa_indexes.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/base/tests/test_tsa_indexes.py
+++ statsmodels-0.14.1/statsmodels/tsa/base/tests/test_tsa_indexes.py
@@ -341,8 +341,13 @@ def test_instantiation_valid():
             warnings.simplefilter("error")
 
             for ix, freq in supported_date_indexes:
-                endog = base_endog.copy()
-                endog.index = ix
+                # Avoid warnings due to Series with object dtype
+                if isinstance(ix, pd.Series) and ix.dtype == object:
+                    with warnings.catch_warnings():
+                        warnings.simplefilter("ignore")
+                        endog = pd.DataFrame(base_endog, index=ix)
+                else:
+                    endog = pd.DataFrame(base_endog, index=ix)
 
                 mod = tsa_model.TimeSeriesModel(endog, freq=freq)
                 if freq is None:
Index: statsmodels-0.14.1/statsmodels/tsa/holtwinters/tests/test_holtwinters.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/holtwinters/tests/test_holtwinters.py
+++ statsmodels-0.14.1/statsmodels/tsa/holtwinters/tests/test_holtwinters.py
@@ -490,10 +490,10 @@ class TestHoltWinters:
         fit5 = mod5.fit()
         # We accept the below values as we getting a better SSE than text book
         assert_almost_equal(fit1.params["smoothing_level"], 1.00, 2)
-        assert_almost_equal(fit1.params["smoothing_trend"], np.NaN, 2)
-        assert_almost_equal(fit1.params["damping_trend"], np.NaN, 2)
+        assert_almost_equal(fit1.params["smoothing_trend"], np.nan, 2)
+        assert_almost_equal(fit1.params["damping_trend"], np.nan, 2)
         assert_almost_equal(fit1.params["initial_level"], 263.96, 1)
-        assert_almost_equal(fit1.params["initial_trend"], np.NaN, 2)
+        assert_almost_equal(fit1.params["initial_trend"], np.nan, 2)
         assert_almost_equal(fit1.sse, 6761.35, 2)  # 6080.26
         assert isinstance(fit1.summary().as_text(), str)
 
Index: statsmodels-0.14.1/statsmodels/tsa/statespace/tests/test_dynamic_factor_mq_frbny_nowcast.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/statespace/tests/test_dynamic_factor_mq_frbny_nowcast.py
+++ statsmodels-0.14.1/statsmodels/tsa/statespace/tests/test_dynamic_factor_mq_frbny_nowcast.py
@@ -1,12 +1,14 @@
+from statsmodels.compat.pandas import QUARTER_END
+
 import os
+
 import numpy as np
+from numpy.testing import assert_allclose
 import pandas as pd
-from scipy.io import matlab
-
 import pytest
-from numpy.testing import assert_allclose
+from scipy.io import matlab
 
-from statsmodels.tsa.statespace import initialization, dynamic_factor_mq
+from statsmodels.tsa.statespace import dynamic_factor_mq, initialization
 
 # Load dataset
 current_path = os.path.dirname(os.path.abspath(__file__))
@@ -197,7 +199,10 @@ def matlab_results():
     def get_data(us_data, mean_M=None, std_M=None, mean_Q=None, std_Q=None):
         dta_M = us_data[['CPIAUCSL', 'UNRATE', 'PAYEMS', 'RSAFS', 'TTLCONS',
                          'TCU']].copy()
-        dta_Q = us_data[['GDPC1', 'ULCNFB']].copy().resample('Q').last()
+        dta_Q = us_data[['GDPC1', 'ULCNFB']].copy()
+        dta_Q.index = dta_Q.index.to_timestamp()
+        dta_Q = dta_Q.resample(QUARTER_END).last()
+        dta_Q.index = dta_Q.index.to_period()
 
         dta_M['CPIAUCSL'] = (dta_M['CPIAUCSL'] /
                              dta_M['CPIAUCSL'].shift(1) - 1) * 100
Index: statsmodels-0.14.1/statsmodels/tsa/statespace/tests/test_dynamic_factor_mq_monte_carlo.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/statespace/tests/test_dynamic_factor_mq_monte_carlo.py
+++ statsmodels-0.14.1/statsmodels/tsa/statespace/tests/test_dynamic_factor_mq_monte_carlo.py
@@ -1,21 +1,25 @@
 """
 Monte Carlo-type tests for the BM model
 
-Note that that the actual tests that run are just regression tests against
+Note that the actual tests that run are just regression tests against
 previously estimated values with small sample sizes that can be run quickly
 for continuous integration. However, this file can be used to re-run (slow)
 large-sample Monte Carlo tests.
 """
+from statsmodels.compat.pandas import QUARTER_END
+
 import numpy as np
+from numpy.testing import assert_allclose
 import pandas as pd
-
 import pytest
-
-from numpy.testing import assert_allclose
 from scipy.signal import lfilter
 
 from statsmodels.tsa.statespace import (
-    dynamic_factor_mq, sarimax, varmax, dynamic_factor)
+    dynamic_factor,
+    dynamic_factor_mq,
+    sarimax,
+    varmax,
+)
 
 
 def simulate_k_factor1(nobs=1000):
@@ -38,8 +42,11 @@ def simulate_k_factor1(nobs=1000):
     levels_M.iloc[0] = 100
     levels_M = levels_M.cumprod()
     log_levels_M = np.log(levels_M) * 100
-    log_levels_Q = (np.log(levels_M).resample('Q', convention='e')
-                                    .sum().iloc[:-1] * 100)
+    # Recast to datetime and then back to period to resample
+    log_levels_Q = np.log(levels_M)
+    log_levels_Q.index = log_levels_Q.index.to_timestamp()
+    log_levels_Q = log_levels_Q.resample(QUARTER_END).sum().iloc[:-1] * 100
+    log_levels_Q.index = log_levels_Q.index.to_period()
 
     # This is an alternative way to compute the quarterly levels
     # endog_M = endog.iloc[:, :3]
@@ -121,8 +128,11 @@ def simulate_k_factors3_blocks2(nobs=100
     levels_M.iloc[0] = 100
     levels_M = levels_M.cumprod()
     log_levels_M = np.log(levels_M) * 100
-    log_levels_Q = (np.log(levels_M).resample('Q', convention='e')
-                                    .sum().iloc[:-1] * 100)
+
+    log_levels_Q = np.log(levels_M)
+    log_levels_Q.index = log_levels_Q.index.to_timestamp()
+    log_levels_Q = log_levels_Q.resample(QUARTER_END).sum().iloc[:-1] * 100
+    log_levels_Q.index = log_levels_Q.index.to_period()
 
     # Compute the growth rate series that we'll actually run the model on
     endog_M = log_levels_M.iloc[:, :7].diff().iloc[1:]
@@ -195,8 +205,11 @@ def gen_k_factor1_nonstationary(nobs=100
     levels_M.iloc[0] = 100
     levels_M = levels_M.cumprod()
     log_levels_M = np.log(levels_M) * 100
-    log_levels_Q = (np.log(levels_M).resample('Q', convention='e')
-                                    .sum().iloc[:-1] * 100)
+
+    log_levels_Q = np.log(levels_M)
+    log_levels_Q.index = log_levels_Q.index.to_timestamp()
+    log_levels_Q = log_levels_Q.resample(QUARTER_END).sum().iloc[:-1] * 100
+    log_levels_Q.index = log_levels_Q.index.to_period()
 
     # Compute the growth rate series that we'll actually run the model on
     endog_M = log_levels_M.diff().iloc[1:, :k]
@@ -258,8 +271,11 @@ def gen_k_factor1(nobs=10000, k=1, idios
     levels_M.iloc[0] = 100
     levels_M = levels_M.cumprod()
     log_levels_M = np.log(levels_M) * 100
-    log_levels_Q = (np.log(levels_M).resample('Q', convention='e')
-                                    .sum().iloc[:-1] * 100)
+
+    log_levels_Q = np.log(levels_M)
+    log_levels_Q.index = log_levels_Q.index.to_timestamp()
+    log_levels_Q = log_levels_Q.resample(QUARTER_END).sum().iloc[:-1] * 100
+    log_levels_Q.index = log_levels_Q.index.to_period()
 
     # Compute the growth rate series that we'll actually run the model on
     endog_M = log_levels_M.diff().iloc[1:, :k]
@@ -372,8 +388,10 @@ def gen_k_factor2(nobs=10000, k=2, idios
     levels_M.iloc[0] = 100
     levels_M = levels_M.cumprod()
     # log_levels_M = np.log(levels_M) * 100
-    log_levels_Q = (np.log(levels_M).resample('Q', convention='e')
-                                    .sum().iloc[:-1] * 100)
+    log_levels_Q = np.log(levels_M)
+    log_levels_Q.index = log_levels_Q.index.to_timestamp()
+    log_levels_Q = log_levels_Q.resample(QUARTER_END).sum().iloc[:-1] * 100
+    log_levels_Q.index = log_levels_Q.index.to_period()
 
     # Compute the quarterly growth rate series
     endog_Q = log_levels_Q.diff()
Index: statsmodels-0.14.1/statsmodels/tsa/statespace/tests/test_representation.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/statespace/tests/test_representation.py
+++ statsmodels-0.14.1/statsmodels/tsa/statespace/tests/test_representation.py
@@ -17,17 +17,25 @@ import os
 import warnings
 
 import numpy as np
+from numpy.testing import (
+    assert_allclose,
+    assert_almost_equal,
+    assert_equal,
+    assert_raises,
+)
 import pandas as pd
 import pytest
 
-from statsmodels.tsa.statespace.representation import Representation
+from statsmodels.tsa.statespace import sarimax, tools
 from statsmodels.tsa.statespace.kalman_filter import (
-    KalmanFilter, FilterResults, PredictionResults)
+    FilterResults,
+    KalmanFilter,
+    PredictionResults,
+)
+from statsmodels.tsa.statespace.representation import Representation
 from statsmodels.tsa.statespace.simulation_smoother import SimulationSmoother
-from statsmodels.tsa.statespace import tools, sarimax
+
 from .results import results_kalman_filter
-from numpy.testing import (
-    assert_equal, assert_almost_equal, assert_raises, assert_allclose)
 
 current_path = os.path.dirname(os.path.abspath(__file__))
 
@@ -526,7 +534,7 @@ class TestClark1989PartialMissing(Clark1
     def setup_class(cls):
         super(TestClark1989PartialMissing, cls).setup_class()
         endog = cls.model.endog
-        endog[1, -51:] = np.NaN
+        endog[1, -51:] = np.nan
         cls.model.bind(endog)
 
         cls.results = cls.run_filter()
Index: statsmodels-0.14.1/statsmodels/tsa/vector_ar/vecm.py
===================================================================
--- statsmodels-0.14.1.orig/statsmodels/tsa/vector_ar/vecm.py
+++ statsmodels-0.14.1/statsmodels/tsa/vector_ar/vecm.py
@@ -343,7 +343,7 @@ def _endog_matrices(
         y_lag1_stack.append(_linear_trend(T, p, coint=True))
     if exog_coint is not None:
         y_lag1_stack.append(exog_coint[-T - 1 : -1].T)
-    y_lag1 = np.row_stack(y_lag1_stack)
+    y_lag1 = np.vstack(y_lag1_stack)
 
     # p. 286:
     delta_x = np.zeros((diff_lags * K, T))
@@ -369,7 +369,7 @@ def _endog_matrices(
         delta_x_stack.append(_linear_trend(T, p))
     if exog is not None:
         delta_x_stack.append(exog[-T:].T)
-    delta_x = np.row_stack(delta_x_stack)
+    delta_x = np.vstack(delta_x_stack)
 
     return y_1_T, delta_y_1_T, y_lag1, delta_x
 
@@ -1916,7 +1916,7 @@ class VECMResults:
         # glueing all deterministics together
         exog = np.column_stack(exog) if exog != [] else None
         if trend_coefs != []:
-            trend_coefs = np.row_stack(trend_coefs)
+            trend_coefs = np.vstack(trend_coefs)
         else:
             trend_coefs = None
 
@@ -2087,7 +2087,7 @@ class VECMResults:
         x_min_p[-k:, :] = y[:, :-p]  # fill last rows of x_min_p
         x_min_p_components.append(x_min_p)
 
-        x_min_p = np.row_stack(x_min_p_components)
+        x_min_p = np.vstack(x_min_p_components)
         x_x = np.dot(x_min_p, x_min_p.T)  # k*k_ar x k*k_ar
         x_x_11 = inv(x_x)[
             : k * (p - 1) + num_det_terms, : k * (p - 1) + num_det_terms
