<revisionlist>
  <revision rev="1" vrev="1">
    <srcmd5>ef2426b5c93ada70c1e9d8bd74b694cd</srcmd5>
    <version>4.0.2</version>
    <time>1584744961</time>
    <user>dimstar_suse</user>
    <comment>Add openmpi4 package</comment>
    <requestid>786504</requestid>
  </revision>
  <revision rev="2" vrev="1">
    <srcmd5>a6576d5433286248d71397b3bd9eb652</srcmd5>
    <version>4.0.3</version>
    <time>1591743080</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>812864</requestid>
  </revision>
  <revision rev="3" vrev="1">
    <srcmd5>21657b7fd10071ff9fea5de3278d1956</srcmd5>
    <version>4.0.4</version>
    <time>1591990948</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>814028</requestid>
  </revision>
  <revision rev="4" vrev="1">
    <srcmd5>ab3c2dc41f5c36d6f4f864b95ffe6504</srcmd5>
    <version>4.0.5</version>
    <time>1601919747</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>839582</requestid>
  </revision>
  <revision rev="5" vrev="1">
    <srcmd5>9aedee9dfba3c7eee53c785030172286</srcmd5>
    <version>4.1.0</version>
    <time>1617279454</time>
    <user>RBrownSUSE</user>
    <comment>- Update to version 4.1.0
  * collectives: Add HAN and ADAPT adaptive collectives components.
    Both components are off by default and can be enabled by specifying
    &quot;mpirun --mca coll_adapt_priority 100 --mca coll_han_priority 100 ...&quot;.
    We intend to enable both by default in Open MPI 5.0.
  * OMPIO is now the default for MPI-IO on all filesystems, including
    Lustre (prior to this, ROMIO was the default for Lustre).  Many
    thanks to Mark Dixon for identifying MPI I/O issues and providing
    access to Lustre systems for testing.
  * Minor MPI one-sided RDMA performance improvements.
  * Fix hcoll MPI_SCATTERV with MPI_IN_PLACE.
  * Add AVX support for MPI collectives.
  * Updates to mpirun(1) about &quot;slots&quot; and PE=x values.
  * Fix buffer allocation for large environment variables.  Thanks to
    @zrss for reporting the issue.
  * Upgrade the embedded OpenPMIx to v3.2.2.
  * Fix issue with extra-long values in MCA files.  Thanks to GitHub
    user @zrss for bringing the issue to our attention.
  * UCX: Fix zero-sized datatype transfers.
  * Fix --cpu-list for non-uniform modes.
  * Fix issue in PMIx callback caused by missing memory barrier on Arm platforms.
  * OFI MTL: Various bug fixes.
  * Fixed issue where MPI_TYPE_CREATE_RESIZED would create a datatype
    with unexpected extent on oddly-aligned datatypes.
  * collectives: Adjust default tuning thresholds for many collective
    algorithms
  * runtime: fix situation where rank-by argument does not work
  * Portals4: Clean up error handling corner cases
  * runtime: Remove --enable-install-libpmix option, which has not
    worked since it was added
  * UCX: Allow UCX 1.8 to be used with the btl uct
  * UCX: Replace usage of the deprecated NB API of UCX with NBX
  * OMPIO: Add support for the IME file system
  * OFI/libfabric: Added support for multiple NICs
  * OFI/libfabric: Added support for Scalable Endpoints
  * OFI/libfabric: Added btl for one-sided support
  * OFI/libfabric: Multiple small bugfixes
  * libnbc: Adding numerous performance-improving algorithms
- Removed: reproducible.patch - replaced by spec file settings.</comment>
    <requestid>882294</requestid>
  </revision>
  <revision rev="6" vrev="1">
    <srcmd5>1decaf43f54baa5ad58908a1a6149d56</srcmd5>
    <version>4.1.1</version>
    <time>1619973356</time>
    <user>dimstar_suse</user>
    <comment>Change default openmpi to openmpi4
- openmpi4 is now the default openmpi for releases &gt; 15.3
- Add orted-mpir-add-version-to-shared-library.patch to fix unversionned library
- Change RPM macros install path to %{_rpmmacrodir}

- Update to version 4.1.1
  - Fix a number of datatype issues, including an issue with
    improper handling of partial datatypes that could lead to
    an unexpected application failure.
  - Change UCX PML to not warn about MPI_Request leaks during
    MPI_FINALIZE by default.  The old behavior can be restored with
    the mca_pml_ucx_request_leak_check MCA parameter.
  - Reverted temporary solution that worked around launch issues in
    SLURM v20.11.{0,1,2}. SchedMD encourages users to avoid these
    versions and to upgrade to v20.11.3 or newer.
  - Updated PMIx to v3.2.2.
  - Disabled gcc built-in atomics by default on aarch64 platforms.
  - Disabled UCX PML when UCX v1.8.0 is detected. UCX version 1.8.0 has a bug that
    may cause data corruption when its TCP transport is used in conjunction with
    the shared memory transport. UCX versions prior to v1.8.0 are not affected by
    this issue. Thanks to @ksiazekm for reporting the issue.
  - Fixed detection of available UCX transports/devices to better inform PML
    prioritization.
  - Fixed SLURM support to mark ORTE daemons as non-MPI tasks.
  - Improved AVX detection to more accurately detect supported
    platforms.  Also improved the generated AVX code, and switched to
    using word-based MCA params for the op/avx component (vs. numeric
    big flags).
  - Improved OFI compatibility support and fixed memory leaks in error
    handling paths.
  - Improved HAN collectives with support for Barrier and Scatter. Thanks</comment>
    <requestid>889565</requestid>
  </revision>
  <revision rev="7" vrev="1">
    <srcmd5>e46c6b2240782322603b59365c2aa5e4</srcmd5>
    <version>4.1.4</version>
    <time>1662462049</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>1001218</requestid>
  </revision>
  <revision rev="8" vrev="2">
    <srcmd5>3b7c5225fdd230b67eab63337ad89c23</srcmd5>
    <version>4.1.4</version>
    <time>1664638931</time>
    <user>RBrownFactory</user>
    <comment></comment>
    <requestid>1006489</requestid>
  </revision>
  <revision rev="9" vrev="1">
    <srcmd5>c1239789103c3af4ed93538191d736a4</srcmd5>
    <version>4.1.5</version>
    <time>1691154154</time>
    <user>dimstar_suse</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1101767</requestid>
  </revision>
  <revision rev="10" vrev="2">
    <srcmd5>527cbedeee453ee2acdd446f2a647fe5</srcmd5>
    <version>4.1.5</version>
    <time>1691594636</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>1102781</requestid>
  </revision>
  <revision rev="11" vrev="1">
    <srcmd5>d6986bac56ec2ff0c027d180be0ce7d2</srcmd5>
    <version>4.1.6</version>
    <time>1696451429</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1114904</requestid>
  </revision>
  <revision rev="12" vrev="2">
    <srcmd5>f54e92c54c5d5e0862c06af30580ecf9</srcmd5>
    <version>4.1.6</version>
    <time>1697061247</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1116699</requestid>
  </revision>
  <revision rev="13" vrev="3">
    <srcmd5>fa82b916cf02e030a17d295a9fabb486</srcmd5>
    <version>4.1.6</version>
    <time>1709070258</time>
    <user>anag+factory</user>
    <comment>Prepare for RPM 4.20 (forwarded request 1151411 from dimstar)</comment>
    <requestid>1151418</requestid>
  </revision>
  <revision rev="14" vrev="4">
    <srcmd5>cfa02576e7b62d07ef6799310ac0fb30</srcmd5>
    <version>4.1.6</version>
    <time>1719582398</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1183210</requestid>
  </revision>
</revisionlist>
