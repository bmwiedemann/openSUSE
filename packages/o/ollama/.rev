<revisionlist>
  <revision rev="1" vrev="1">
    <srcmd5>ecad936eaf52ca8a806407726100bc14</srcmd5>
    <version>0.1.27</version>
    <time>1709070585</time>
    <user>anag+factory</user>
    <comment>New package (see https://lists.opensuse.org/archives/list/factory@lists.opensuse.org/thread/IPU5KGC3JNIDJ5QMKSQITJSCSQ34HLG6/)</comment>
    <requestid>1152310</requestid>
  </revision>
  <revision rev="2" vrev="1">
    <srcmd5>4f3177ee185c1e58cd11573ef8da036e</srcmd5>
    <version>0.1.31</version>
    <time>1713357950</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>1168439</requestid>
  </revision>
  <revision rev="3" vrev="1">
    <srcmd5>0877b843f6fca11cf3bc0df8fb22201e</srcmd5>
    <version>0.1.32</version>
    <time>1713891440</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1169871</requestid>
  </revision>
  <revision rev="4" vrev="1">
    <srcmd5>20d67f3e88e45aef720fff6ff13b38b1</srcmd5>
    <version>0.1.36</version>
    <time>1715547251</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1173462</requestid>
  </revision>
  <revision rev="5" vrev="1">
    <srcmd5>aca5aa787d9bc929652490663bc020c9</srcmd5>
    <version>0.1.37</version>
    <time>1715615879</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1173543</requestid>
  </revision>
  <revision rev="6" vrev="1">
    <srcmd5>1d84ef8c4cdc49d136cad96aa3f44733</srcmd5>
    <version>0.1.38</version>
    <time>1715969130</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1174685</requestid>
  </revision>
  <revision rev="7" vrev="2">
    <srcmd5>7cdcd43aea6fa4e4e5f9b2ad4481c2f3</srcmd5>
    <version>0.1.38</version>
    <time>1716471311</time>
    <user>anag+factory</user>
    <comment>- Added 15.6 build</comment>
    <requestid>1175956</requestid>
  </revision>
  <revision rev="8" vrev="1">
    <srcmd5>55f08c1bf30f1f3b7a44279fc2f09e63</srcmd5>
    <version>0.1.40</version>
    <time>1717429381</time>
    <user>anag+factory</user>
    <comment>- Update to version 0.1.40:

- Update to version 0.1.39:</comment>
    <requestid>1178089</requestid>
  </revision>
  <revision rev="9" vrev="1">
    <srcmd5>c0316def402e52dc58101771d5b8aa9f</srcmd5>
    <version>0.1.44</version>
    <time>1718743936</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1181491</requestid>
  </revision>
  <revision rev="10" vrev="1">
    <srcmd5>c5b26ec3669102fbf471a5fa72deefb7</srcmd5>
    <version>0.1.45</version>
    <time>1719825574</time>
    <user>dimstar_suse</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1183991</requestid>
  </revision>
  <revision rev="11" vrev="1">
    <srcmd5>8da3e47d0c0b4de3c6090975c8e6380c</srcmd5>
    <version>0.1.48</version>
    <time>1720458505</time>
    <user>anag+factory</user>
    <comment>- Update to version 0.1.48:
  * Fixed issue where Gemma 2 would continuously output when 
    reaching context limits
  * Fixed out of memory and core dump errors when running Gemma 2
  * /show info will now show additional model information in
    ollama run
  * Fixed issue where ollama show would result in an error on 
    certain vision models
- Update to version 0.1.48:
  * Added support for Google Gemma 2 models (9B and 27B)
  * Fixed issues with ollama create when importing from Safetensors
  
- Update to version 0.1.46:
  * Docs (#5149)
  * fix: quantization with template
  * Fix use_mmap parsing for modelfiles
  * Refine mmap default logic on linux
  * Bump latest fedora cuda repo to 39
</comment>
    <requestid>1186033</requestid>
  </revision>
  <revision rev="12" vrev="1">
    <srcmd5>3240dfd45012373263d128e89c602b04</srcmd5>
    <version>0.2.5</version>
    <time>1721065747</time>
    <user>anag+factory</user>
    <comment>- Update to version 0.2.5:
- Update to version 0.2.4:
- Update to version 0.2.3:
- Update to version 0.2.2:
- Update to version 0.2.1:
- Update to version 0.2.0:</comment>
    <requestid>1187407</requestid>
  </revision>
  <revision rev="13" vrev="1">
    <srcmd5>3e96d9d7e53a6024ae40fcdd7f0c8f39</srcmd5>
    <version>0.2.6</version>
    <time>1721395671</time>
    <user>anag+factory</user>
    <comment>- Fixed issue with shared libraries 

- Added %check section
- Use -v when building 
- Update to version 0.2.6:
  * New models: MathÎ£tral is a 7B model designed for math 
    reasoning and scientific discovery by Mistral AI.
  * Fixed issue where uppercase roles such as USER would no longer
    work in the chat endpoints
  * Fixed issue where empty system message would be included in the
    prompt</comment>
    <requestid>1188404</requestid>
  </revision>
  <revision rev="14" vrev="1">
    <srcmd5>c785f08f2a05d36e89afb54eb9285098</srcmd5>
    <version>0.2.8</version>
    <time>1722003322</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>1189591</requestid>
  </revision>
  <revision rev="15" vrev="1">
    <srcmd5>ccd31114e7ab10648c96270f5c2d63b4</srcmd5>
    <version>0.3.0</version>
    <time>1722180022</time>
    <user>dimstar_suse</user>
    <comment>- Update to version 0.3.0:
  * Ollama now supports tool calling with popular models such
    as Llama 3.1. This enables a model to answer a given prompt
    using tool(s) it knows about, making it possible for models to
    perform more complex tasks or interact with the outside world.
  * New models:
    ~ Llama 3.1
    ~ Mistral Large 2
    ~ Firefunction v2
    ~ Llama-3-Groq-Tool-Use
  * Fixed duplicate error message when running ollama create</comment>
    <requestid>1189982</requestid>
  </revision>
  <revision rev="16" vrev="1">
    <srcmd5>c5a64b16c4973dd44cd2d4a3ca0f3493</srcmd5>
    <version>0.3.1</version>
    <time>1722542687</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>1190824</requestid>
  </revision>
  <revision rev="17" vrev="1">
    <srcmd5>a53a6b587b797f8b660aeec85032e96c</srcmd5>
    <version>0.3.3</version>
    <time>1722708415</time>
    <user>dimstar_suse</user>
    <comment>- Update to version 0.3.3:
  * The /api/embed endpoint now returns statistics: total_duration,
    load_duration, and prompt_eval_count
  * Added usage metrics to the /v1/embeddings OpenAI compatibility
    API
  * Fixed issue where /api/generate would respond with an empty 
    string if provided a context
  * Fixed issue where /api/generate would return an incorrect 
    value for context
  * /show modefile will now render MESSAGE commands correctly
- Update to version 0.3.2:
  * Fixed issue where ollama pull would not resume download 
    progress
  * Fixed issue where phi3 would report an error on older versions</comment>
    <requestid>1191409</requestid>
  </revision>
  <revision rev="18" vrev="1">
    <srcmd5>b6bdd31c3c210dd407bc2e2432b88e19</srcmd5>
    <version>0.3.6</version>
    <time>1723891290</time>
    <user>dimstar_suse</user>
    <comment>- Update to version 0.3.6:
  * Fixed issue where /api/embed would return an error instead of
    loading the model when the input field was not provided.
  * ollama create can now import Phi-3 models from Safetensors
  * Added progress information to ollama create when importing GGUF
    files
  * Ollama will now import GGUF files faster by minimizing file
    copies
- Update to version 0.3.6:
  * Fixed issue where temporary files would not be cleaned up
  * Fix rare error when Ollama would start up due to invalid model
    data
- Update to version 0.3.4:
 * New embedding models
  - BGE-M3: a large embedding model from BAAI distinguished for 
    its versatility in Multi-Functionality, Multi-Linguality, and 
    Multi-Granularity.
  - BGE-Large: a large embedding model trained in english.
  - Paraphrase-Multilingual: A multilingual embedding model 
    trained on parallel data for 50+ languages.
 * New embedding API with batch support
   - Ollama now supports a new API endpoint /api/embed for 
     embedding generation:
 * This API endpoint supports new features:
   - Batches: generate embeddings for several documents in 
     one request
   - Normalized embeddings: embeddings are now normalized, 
     improving similarity results
   - Truncation: a new truncate parameter that will error if 
     set to false
   - Metrics: responses include load_duration, total_duration and 
     prompt_eval_count metrics
</comment>
    <requestid>1194354</requestid>
  </revision>
  <revision rev="19" vrev="1">
    <srcmd5>530b1dd76c6a4b985ed4e90a057bf50f</srcmd5>
    <version>0.3.10</version>
    <time>1726773464</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1201962</requestid>
  </revision>
  <revision rev="20" vrev="1">
    <srcmd5>6a24117f180c9368896f0cf9dde25632</srcmd5>
    <version>0.3.11</version>
    <time>1726995969</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1202264</requestid>
  </revision>
  <revision rev="21" vrev="1">
    <srcmd5>178e0ebd8c4502f6193fd687e9ccd8d0</srcmd5>
    <version>0.3.12</version>
    <time>1727703627</time>
    <user>anag+factory</user>
    <comment>- Update to version 0.3.12:
  * Llama 3.2: Meta's Llama 3.2 goes small with 1B and 3B 
    models.
  * Qwen 2.5 Coder: The latest series of Code-Specific Qwen 
    models, with significant improvements in code generation, 
    code reasoning, and code fixing.
  * Ollama now supports ARM Windows machines
  * Fixed rare issue where Ollama would report a missing .dll
    file on Windows
  * Fixed performance issue for Windows without GPUs (forwarded request 1204394 from cabelo)</comment>
    <requestid>1204591</requestid>
  </revision>
  <revision rev="22" vrev="1">
    <srcmd5>544c9f36d6b626196a855ce5ef4ff8f7</srcmd5>
    <version>0.3.13</version>
    <time>1728904074</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>1207827</requestid>
  </revision>
  <revision rev="23" vrev="1">
    <srcmd5>5ace3b839912c7465e114e3ed41d006e</srcmd5>
    <version>0.3.14</version>
    <time>1730387385</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>1219752</requestid>
  </revision>
  <revision rev="24" vrev="1">
    <srcmd5>0f65c1541dc689f22caa383d9ceab492</srcmd5>
    <version>0.4.0</version>
    <time>1730999825</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>1222485</requestid>
  </revision>
  <revision rev="25" vrev="1">
    <srcmd5>0095926818f254f31fe619afca8f62d2</srcmd5>
    <version>0.4.2</version>
    <time>1732442691</time>
    <user>anag+factory</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1225993</requestid>
  </revision>
  <revision rev="26" vrev="1">
    <srcmd5>1123bff86c0bccfcada4e8f9af2b4651</srcmd5>
    <version>0.5.1</version>
    <time>1734034695</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1230609</requestid>
  </revision>
  <revision rev="27" vrev="1">
    <srcmd5>406a66bd1c77f7913c612b1b01f3d703</srcmd5>
    <version>0.5.7</version>
    <time>1738163409</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1240594</requestid>
  </revision>
  <revision rev="28" vrev="1">
    <srcmd5>334d419bdf2db0a48afd7bc77d64c8e9</srcmd5>
    <version>0.5.11</version>
    <time>1740247512</time>
    <user>dimstar_suse</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1247774</requestid>
  </revision>
  <revision rev="29" vrev="1">
    <srcmd5>14ecd1f1530512abeb789343447b61dc</srcmd5>
    <version>0.5.12</version>
    <time>1740760722</time>
    <user>dimstar_suse</user>
    <comment></comment>
    <requestid>1249021</requestid>
  </revision>
  <revision rev="30" vrev="1">
    <srcmd5>f49782fc5474c65d7221d0c356ebd43e</srcmd5>
    <version>0.6.0</version>
    <time>1741992728</time>
    <user>anag+factory</user>
    <comment>- Update BuildRequires to go1.24 
- Update to version 0.6.0:
  * New model: Gemma 3
  * Fixed error that would occur when running
    snowflake-arctic-embed and snowflake-arctic-embed2 models
  * Various performance improvements and bug fixes

- Update to version 0.5.13:</comment>
    <requestid>1252927</requestid>
  </revision>
  <revision rev="31" vrev="2">
    <srcmd5>788e8717957d1d9dfcb1a6d101e44e90</srcmd5>
    <version>0.6.0</version>
    <time>1742420006</time>
    <user>anag+factory</user>
    <comment>Only require git-core
because we don't need git-web and the other stuff here (forwarded request 1254170 from bmwiedemann)</comment>
    <requestid>1254230</requestid>
  </revision>
  <revision rev="32" vrev="1">
    <srcmd5>0ad59922613e212b6730acf5c8d49220</srcmd5>
    <version>0.6.2</version>
    <time>1743111113</time>
    <user>anag+factory</user>
    <comment>- Update to version 0.6.2:
  * Multiple images are now supported in Gemma 3
  * Fixed issue where running Gemma 3 would consume a large amount
    of system memory
  * ollama create --quantize now works when converting Gemma 3
    from safetensors
  * Fixed issue where /save would not work if running a model
    with / in the name
  * Add support for AMD Strix Halo GPUs</comment>
    <requestid>1256309</requestid>
  </revision>
  <revision rev="33" vrev="1">
    <srcmd5>a910cc08485188625ecd554b6298922c</srcmd5>
    <version>0.6.5</version>
    <time>1745134588</time>
    <user>dimstar_suse</user>
    <comment>- Add ollama to the video group
- Update to version 0.6.5:
  * Add support for mistral-small
  * Fix issues with spm tokenizer for Gemma 3 models
  * Add checks for values falling out of sliding window cache
  * Improve file descriptor management for tensors and
    Pull operations
  * Add gfx1200 &amp; gfx1201 GPU support on Linux
  * Optimize sliding window attention and KV cache implementations
  * Implement loading tensors in 32KiB chunks for better performance
  * Add autotemplate for gemma3 models
  * Add benchmarking for ollama server performance
  * Fix file handling in /proc/cpuinfo discovery
  * Support heterogeneous KV cache layer sizes in memory estimation
  * Fix debug logging for memory estimates
  * Improve error handling for empty logits and tensor data reading
  * Return model capabilities from the show endpoint</comment>
    <requestid>1271013</requestid>
  </revision>
  <revision rev="34" vrev="1">
    <srcmd5>b6140f6c98c96f456ed6cb627be31984</srcmd5>
    <version>0.6.6</version>
    <time>1745612343</time>
    <user>anag_factory</user>
    <comment>- Update to version 0.6.6:
  * New model: IBM Granite 3.3
  * New model: DeepCoder
  * New, faster model downloading: OLLAMA_EXPERIMENT=client2
    ollama serve will run Ollama using a new downloader with
    improved performance and reliability when running ollama pull
  * Fixed memory leak issues when running Gemma 3, Mistral Small
    3.1 and other models on Ollama
  * Improved performance of ollama create when importing models
    from Safetensors
  * Ollama will now allow tool function parameters with either a
    single type or an array of types
  * Fixed certain out-of-memory issues caused by not reserving
    enough memory at startup
  * Fixed nondeterministic model unload order
  * Included the items and $defs fields to properly handle array
    types in the API
  * OpenAI-Beta headers are now included in the CORS safelist
  * Fixed issue where model tensor data would be corrupted when
    importing models from Safetensors</comment>
    <requestid>1272498</requestid>
  </revision>
  <revision rev="35" vrev="1">
    <srcmd5>cbe2024a9205d47c4030ae3723365516</srcmd5>
    <version>0.6.8</version>
    <time>1747234870</time>
    <user>anag_factory</user>
    <comment>- Update to version 0.6.8
- Update to version 0.6.7
- Use source url (https://en.opensuse.org/SourceUrls)</comment>
    <requestid>1277233</requestid>
  </revision>
  <revision rev="36" vrev="1">
    <srcmd5>64d0c21c4792097d7034eba51cfe52ae</srcmd5>
    <version>0.7.0</version>
    <time>1747726601</time>
    <user>anag_factory</user>
    <comment>- Update to version 0.7.0:
  * Ollama now supports multimodal models via Ollamaâs new engine,
    starting with new vision multimodal models:
    ~ Meta Llama 4
    ~ Google Gemma 3
    ~ Qwen 2.5 VL
    ~ Qwen 2.5 VL
  * Ollama now supports providing WebP images as input to
    multimodal models
  * Improved performance of importing safetensors models via
    ollama create
  * Various bug fixes and performance enhancements</comment>
    <requestid>1278142</requestid>
  </revision>
  <revision rev="37" vrev="2">
    <srcmd5>e71a6525161995b875cd9766a7babd55</srcmd5>
    <version>0.7.0</version>
    <time>1748277157</time>
    <user>anag_factory</user>
    <comment>- Cleanup part in spec file where build for SLE-15-SP6 and above
  is defined to make if condition more robust

- Allow to build for Package Hub for SLE-15-SP7 
  (openSUSE:Backports:SLE-15-SP7) with g++-12/gcc-12
  by checking for sle_version &gt;= 150600 in spec file (bsc#1243438)</comment>
    <requestid>1279778</requestid>
  </revision>
</revisionlist>
