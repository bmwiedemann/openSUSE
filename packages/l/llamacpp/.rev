<revisionlist>
  <revision rev="1" vrev="1">
    <srcmd5>d266a27abcb6daacd90de0ab83d4074b</srcmd5>
    <version>4501</version>
    <time>1737202691</time>
    <user>dimstar_suse</user>
    <comment>Useful for LLM inference on consumer HW</comment>
    <requestid>1238533</requestid>
  </revision>
  <revision rev="2" vrev="1">
    <srcmd5>8df1e728c373d02e2819b5c3e12ef77a</srcmd5>
    <version>4589</version>
    <time>1738335891</time>
    <user>anag+factory</user>
    <comment>- Update to version 4589:
- Package ggml cmake scripts
</comment>
    <requestid>1241557</requestid>
  </revision>
  <revision rev="3" vrev="2">
    <srcmd5>2d7f1feab7584a6e440d31f9dd0b2b21</srcmd5>
    <version>4589</version>
    <time>1738615488</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1241683</requestid>
  </revision>
  <revision rev="4" vrev="1">
    <srcmd5>3ef8523ee440bdd5f9adba9036dcfdb7</srcmd5>
    <version>4719</version>
    <time>1739742049</time>
    <user>dimstar_suse</user>
    <comment>- Update to version 4719:</comment>
    <requestid>1246038</requestid>
  </revision>
  <revision rev="5" vrev="1">
    <srcmd5>dbd531ec07c12f555fbe1c5ae2b6fa03</srcmd5>
    <version>4889</version>
    <time>1742246243</time>
    <user>anag+factory</user>
    <comment>- Update to version 4889:</comment>
    <requestid>1253529</requestid>
  </revision>
  <revision rev="6" vrev="1">
    <srcmd5>5c8b6f15606f8e03185f682bbd40d225</srcmd5>
    <version>5158</version>
    <time>1745134581</time>
    <user>dimstar_suse</user>
    <comment>- Remove convert_hf_to_gguf.py
- Update to version 5158:</comment>
    <requestid>1271007</requestid>
  </revision>
  <revision rev="7" vrev="1">
    <srcmd5>8ccee6bc79ea54c3d0a1e9be8b88c3fc</srcmd5>
    <version>5321</version>
    <time>1746809520</time>
    <user>anag_factory</user>
    <comment>- Use source urls instead of obs_scm
- Add libllava and libmtmd libraries
- Update to version 5327:
  * A new binary llama-mtmd-cli is introduced to replace llava-cli,
    minicpmv-cli, gemma3-cli (#13012) and qwen2vl-cli (#13141),
    libllava will be deprecated
  * Full changes here:
    https://github.com/ggml-org/llama.cpp/compare/b5158...b5321
- Delete patch 0002-build-main-cli.patch: build system changed
  upstream</comment>
    <requestid>1276203</requestid>
  </revision>
  <revision rev="8" vrev="1">
    <srcmd5>56e5d3b4cce4f37d47e2fbd4be21ce3c</srcmd5>
    <version>5426</version>
    <time>1747736392</time>
    <user>anag_factory</user>
    <comment>- Update to 5426:
  * print hint when loading a model when no backends are loaded
  * vulkan: use scalar FA rather than coopmat2 when N==1
  * mtmd : add vision support for llama 4
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b5402...b5426
- Update to 5402
  * removed llava subpackage (#13460)
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b5158...b5321
- Update to version 5332:
  * server : vision support via libmtmd
</comment>
    <requestid>1278459</requestid>
  </revision>
  <revision rev="9" vrev="1">
    <srcmd5>9b4bfdfd7d9a76e5dfb6a648cde12490</srcmd5>
    <version>5516</version>
    <time>1748608343</time>
    <user>dimstar_suse</user>
    <comment>- Update to 5516:
  * llama : remove llama_kv_cache_view API
  * model : disable SWA for Phi models
  * kv-cache : simplify the interface
  * server : Add the endpoints /api/tags and /api/chat
  * ggml : add ggml_gelu_erf()
  * hparams : support models for which all layers use SWA
  * opencl: fix couple crashes
  * opencl: Add support for multiple devices
  * mtmd : add ultravox audio input
  * server : support audio input
  * server: streaming of tool calls and thoughts when jinja is on
  * mtmd : support Qwen 2.5 Omni
  * ggml : riscv: add xtheadvector support
  * opencl : various optimizations
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b5426...b5516
</comment>
    <requestid>1280718</requestid>
  </revision>
  <revision rev="10" vrev="1">
    <srcmd5>4c2d107a563add14866cc2438f02b426</srcmd5>
    <version>5556</version>
    <time>1749539125</time>
    <user>anag_factory</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1283892</requestid>
  </revision>
  <revision rev="11" vrev="1">
    <srcmd5>49b3736039a76a6f14ccda46e94b35bf</srcmd5>
    <version>5699</version>
    <time>1750430936</time>
    <user>anag_factory</user>
    <comment>- Update to 5699:
  * vocab : prevent integer overflow during load
    (bsc#1244714) (CVE-2025-49847)
  ...
- Update to 5657:
  ...</comment>
    <requestid>1286807</requestid>
  </revision>
  <revision rev="12" vrev="1">
    <srcmd5>0511e0eebf924b9d28365e9282449ecc</srcmd5>
    <version>5812</version>
    <time>1751814473</time>
    <user>anag_factory</user>
    <comment>- Update to version 5812:
  * Mamba-2 Support: Initial integration of Mamba-2 architecture.
  * Added support for ERNIE 4.5 0.3B, NeoBERT, Arcee AI's AFM,
    Gemma3n text-only, and dots.llm1 architectures
  * Vulkan Improvements: Support for softmax/FlashAttention
    batch/broadcast, fused RMS_NORM+MUL, and better memory handling
  * GGML Backend: Added REGLU/GEGLU/SWIGLU ops, ggml_set_rows, and
    improved SYCL/OpenCL/Metal support
  * Server Improvements: Jinja template kwargs, draft model cache
    params, and Unix socket support
  * Quantization: User-defined layer pruning and KV override fixes
  * Optimizations: Batched Vulkan mul_mat_id splitting
    and ARM hsum reduction
  * Added GGML version function
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b5699...b5812</comment>
    <requestid>1290235</requestid>
  </revision>
  <revision rev="13" vrev="1">
    <srcmd5>3aedd043e095f6d55fd8a757c6428134</srcmd5>
    <version>5889</version>
    <time>1752590598</time>
    <user>anag_factory</user>
    <comment>- Add GGML_NATIVE=OFF build flag
- Update to version 5889:
  * Remove Kompute support
  * Prevent integer overflow in gguf tensor size calculation
    (bsc#1246377) (CVE-2025-53630) (GHSA-vgg9-87g3-85w8)
  * Improved build-time messaging for ggml_set_rows.
  * Enhanced test coverage for LFM2 and added LFM2 to
    documentation.
  * Synchronized ggml updates and improved Vulkan backend
    (bilinear interpolation, ggml_roll, SET_ROWS, optimizations).
  * Fixed pooled embedding output in server and improved prompt
    processing.
  * Added support for LiquidAI LFM2 hybrid family and Falcon-H1
    models.
  * Improved HIP, OpenCL, and SYCL backend compatibility
    and features.
  * Added new vocabularies and model support
    (midm-2.0, skt/A.X-4.0, SmolLM3, hunyuan moe, Granite Four).
  * Various bug fixes, optimizations, and documentation improvements
    across backends and models.
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b5812...b5889</comment>
    <requestid>1292534</requestid>
  </revision>
  <revision rev="14" vrev="1">
    <srcmd5>daa369d0904f3ea2afb52f672bfb97d0</srcmd5>
    <version>5970</version>
    <time>1753976788</time>
    <user>dimstar_suse</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1296595</requestid>
  </revision>
  <revision rev="15" vrev="1">
    <srcmd5>1913fb648089dd0cc6dcdef6032efe2c</srcmd5>
    <version>6100</version>
    <time>1754578127</time>
    <user>dimstar_suse</user>
    <comment>- Drop 0001-dl-load-path.patch: use GGML_BACKEND_DIR instead
- Enable loading backends dynamically
- Update to version 6100:
  * llama : add gpt-oss (#15091)
  * llama : add --n-cpu-moe option (#15077)
  * llama : enable LLAMA_SET_ROWS=1 by default (#14959)
  * server : add openai-style logit_bias support (#14946)
  * server : implement universal assisted decoding (#12635)
  * mtmd : support MiniCPM-V 4.0 (#14983)
  * opencl: add f16 for `add`, `sub`, `mul`, `div` (#14984)
  * model : add hunyuan dense (#14878)
  * model : add text-only support for Kimi-VL
  * model: support GLM 4.5 family of models (#14939)
  * model : support Qwen3-Embedding (#15023)
  * graph : Optimize graph operations
  * vulkan: various bug fixes and optimizations
  * Various bug fixes

- Update to version 6038:
  * chat : fix kimi-k2 chat template (#14852)
  * common : avoid logging partial messages (which can contain
    broken UTF-8 sequences) (#14937)
  * context : perform output reorder lazily upon access after sync
    (#14853)
  * context : restore preemptive sched reset when LLAMA_SET_ROWS=0
    (#14870)
  * convert : text-only support for GLM-4.1V-9B-Thinking (#14823)
  * embeddings: fix extraction of CLS pooling results (#14927)
  * ggml-cpu : deduplicate scalar implementations (#14897)
  * ggml-cpu : disable GGML_NNPA by default due to instability</comment>
    <requestid>1298007</requestid>
  </revision>
  <revision rev="16" vrev="1">
    <srcmd5>38d7bbd796418272794ae26cce7fe25c</srcmd5>
    <version>6139</version>
    <time>1755095452</time>
    <user>dimstar_suse</user>
    <comment>- Update to version 6139:
  * opencl: allow mixed f16/f32 `add` (#15140)
  * mtmd : Fix MinicpmV model converter and clip to avoid using
    hardcode. (#14750)
  * chat : hotfix gpt-oss jinja raising an exception (#15243)
  * server : allow specifying reasoning_format in HTTP request
    (#15238)
  * kv-cache : fix seq_rm with seq_id == -1 (#15226)
  * kv-cache : log (debug) all streams in find_slot (#15176)
  * convert : improve Mistral models integration (#14737)
  * kleidiai: fix unsigned overflow bug (#15150)

- Add LLAMA_BUILD_NUMBER and LLAMA_VERSION to the build 

- Update to version 6121:
  * Support intern-s1
  * opencl: add swiglu_oai and add_id
  * vulkan: support fattn sinks
  * vulkan: Add env var to disable host visible vidmem
  * ggml: Skip backend library linking code when GGML_BACKEND_DL=ON
  * ggml : fix fallback to CPU for ununsupported ops
  * Various bug fixes
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b6100...b6121</comment>
    <requestid>1299150</requestid>
  </revision>
  <revision rev="17" vrev="1">
    <srcmd5>b129af1a917d9c5c19ac5fff9c0f155f</srcmd5>
    <version>6188</version>
    <time>1756147138</time>
    <user>anag_factory</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1301212</requestid>
  </revision>
  <revision rev="18" vrev="1">
    <srcmd5>fcc9e5038b9db796d408ad292be89c03</srcmd5>
    <version>6269</version>
    <time>1756828704</time>
    <user>anag_factory</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1302234</requestid>
  </revision>
  <revision rev="19" vrev="1">
    <srcmd5>907fd0174f12825fb85122b0a83a4ebd</srcmd5>
    <version>6428</version>
    <time>1758039603</time>
    <user>anag_factory</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1305191</requestid>
  </revision>
  <revision rev="20" vrev="1">
    <srcmd5>36ead546acce59a982f0ee159d1820d6</srcmd5>
    <version>6605</version>
    <time>1759156333</time>
    <user>anag_factory</user>
    <comment></comment>
    <requestid>1307499</requestid>
  </revision>
  <revision rev="21" vrev="1">
    <srcmd5>bdd03a89f45a73bb7d6969f665d73ead</srcmd5>
    <version>6690</version>
    <time>1759679476</time>
    <user>dimstar_suse</user>
    <comment>- Update to version 6690:
  * Full commit log:
    https://github.com/ggml-org/llama.cpp/compare/b6605...b6690</comment>
    <requestid>1309029</requestid>
  </revision>
  <revision rev="22" vrev="1">
    <srcmd5>385adddc750758e9518adcf4d7b6f92b</srcmd5>
    <version>6937</version>
    <time>1762449167</time>
    <user>anag_factory</user>
    <comment>- Update to version 6937:
  * New model: Janus Pro
  * New model: Minimax M2
  * New model: Granite Hybrid nano types
  * New model: support for qwen3vl series
  * New model: support for CogVLM model
  * New model: LightOnOCR-1B model
  * New model: BailingMoeV2 support
  * New model: Granite Hybrid types
  * New model: Support home-cooked Mistral Small Omni
  * New model: Support LiquidAI LFM2-MoE hybrid model
  * New model: Granite docling + Idefics3 preprocessing (SmolVLM)
  * New model: EmbeddingGemma Adding Support for
    SentenceTransformers Dense Modules
  * Server improvements, OpenAI API compatibility, optimizations,
    and bug fixes
  * Vulkan backend improvements, optimizations, and bug fixes
  * OpenCL backend fixes
  * CPU backend optimizations
  * Multimodal (mtmd) improvements
  * WebUI enhancements
  * Architecture-specific improvements
  * llama core improvements
  * Memory management improvements
  * Conversion and quantization tools enhancements
  * Grammar and sampling improvements
  * Chat and prompts enhancements
  * General fixes and improvements
  * RPC improvements and bug fixes
  * Full commit log:</comment>
    <requestid>1315691</requestid>
  </revision>
</revisionlist>
