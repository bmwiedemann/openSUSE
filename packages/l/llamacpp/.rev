<revisionlist>
  <revision rev="1" vrev="1">
    <srcmd5>d266a27abcb6daacd90de0ab83d4074b</srcmd5>
    <version>4501</version>
    <time>1737202691</time>
    <user>dimstar_suse</user>
    <comment>Useful for LLM inference on consumer HW</comment>
    <requestid>1238533</requestid>
  </revision>
  <revision rev="2" vrev="1">
    <srcmd5>8df1e728c373d02e2819b5c3e12ef77a</srcmd5>
    <version>4589</version>
    <time>1738335891</time>
    <user>anag+factory</user>
    <comment>- Update to version 4589:
- Package ggml cmake scripts
</comment>
    <requestid>1241557</requestid>
  </revision>
  <revision rev="3" vrev="2">
    <srcmd5>2d7f1feab7584a6e440d31f9dd0b2b21</srcmd5>
    <version>4589</version>
    <time>1738615488</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1241683</requestid>
  </revision>
  <revision rev="4" vrev="1">
    <srcmd5>3ef8523ee440bdd5f9adba9036dcfdb7</srcmd5>
    <version>4719</version>
    <time>1739742049</time>
    <user>dimstar_suse</user>
    <comment>- Update to version 4719:</comment>
    <requestid>1246038</requestid>
  </revision>
  <revision rev="5" vrev="1">
    <srcmd5>dbd531ec07c12f555fbe1c5ae2b6fa03</srcmd5>
    <version>4889</version>
    <time>1742246243</time>
    <user>anag+factory</user>
    <comment>- Update to version 4889:</comment>
    <requestid>1253529</requestid>
  </revision>
  <revision rev="6" vrev="1">
    <srcmd5>5c8b6f15606f8e03185f682bbd40d225</srcmd5>
    <version>5158</version>
    <time>1745134581</time>
    <user>dimstar_suse</user>
    <comment>- Remove convert_hf_to_gguf.py
- Update to version 5158:</comment>
    <requestid>1271007</requestid>
  </revision>
  <revision rev="7" vrev="1">
    <srcmd5>8ccee6bc79ea54c3d0a1e9be8b88c3fc</srcmd5>
    <version>5321</version>
    <time>1746809520</time>
    <user>anag_factory</user>
    <comment>- Use source urls instead of obs_scm
- Add libllava and libmtmd libraries
- Update to version 5327:
  * A new binary llama-mtmd-cli is introduced to replace llava-cli,
    minicpmv-cli, gemma3-cli (#13012) and qwen2vl-cli (#13141),
    libllava will be deprecated
  * Full changes here:
    https://github.com/ggml-org/llama.cpp/compare/b5158...b5321
- Delete patch 0002-build-main-cli.patch: build system changed
  upstream</comment>
    <requestid>1276203</requestid>
  </revision>
  <revision rev="8" vrev="1">
    <srcmd5>56e5d3b4cce4f37d47e2fbd4be21ce3c</srcmd5>
    <version>5426</version>
    <time>1747736392</time>
    <user>anag_factory</user>
    <comment>- Update to 5426:
  * print hint when loading a model when no backends are loaded
  * vulkan: use scalar FA rather than coopmat2 when N==1
  * mtmd : add vision support for llama 4
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b5402...b5426
- Update to 5402
  * removed llava subpackage (#13460)
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b5158...b5321
- Update to version 5332:
  * server : vision support via libmtmd
</comment>
    <requestid>1278459</requestid>
  </revision>
  <revision rev="9" vrev="1">
    <srcmd5>9b4bfdfd7d9a76e5dfb6a648cde12490</srcmd5>
    <version>5516</version>
    <time>1748608343</time>
    <user>dimstar_suse</user>
    <comment>- Update to 5516:
  * llama : remove llama_kv_cache_view API
  * model : disable SWA for Phi models
  * kv-cache : simplify the interface
  * server : Add the endpoints /api/tags and /api/chat
  * ggml : add ggml_gelu_erf()
  * hparams : support models for which all layers use SWA
  * opencl: fix couple crashes
  * opencl: Add support for multiple devices
  * mtmd : add ultravox audio input
  * server : support audio input
  * server: streaming of tool calls and thoughts when jinja is on
  * mtmd : support Qwen 2.5 Omni
  * ggml : riscv: add xtheadvector support
  * opencl : various optimizations
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b5426...b5516
</comment>
    <requestid>1280718</requestid>
  </revision>
  <revision rev="10" vrev="1">
    <srcmd5>4c2d107a563add14866cc2438f02b426</srcmd5>
    <version>5556</version>
    <time>1749539125</time>
    <user>anag_factory</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1283892</requestid>
  </revision>
  <revision rev="11" vrev="1">
    <srcmd5>49b3736039a76a6f14ccda46e94b35bf</srcmd5>
    <version>5699</version>
    <time>1750430936</time>
    <user>anag_factory</user>
    <comment>- Update to 5699:
  * vocab : prevent integer overflow during load
    (bsc#1244714) (CVE-2025-49847)
  ...
- Update to 5657:
  ...</comment>
    <requestid>1286807</requestid>
  </revision>
  <revision rev="12" vrev="1">
    <srcmd5>0511e0eebf924b9d28365e9282449ecc</srcmd5>
    <version>5812</version>
    <time>1751814473</time>
    <user>anag_factory</user>
    <comment>- Update to version 5812:
  * Mamba-2 Support: Initial integration of Mamba-2 architecture.
  * Added support for ERNIE 4.5 0.3B, NeoBERT, Arcee AI's AFM,
    Gemma3n text-only, and dots.llm1 architectures
  * Vulkan Improvements: Support for softmax/FlashAttention
    batch/broadcast, fused RMS_NORM+MUL, and better memory handling
  * GGML Backend: Added REGLU/GEGLU/SWIGLU ops, ggml_set_rows, and
    improved SYCL/OpenCL/Metal support
  * Server Improvements: Jinja template kwargs, draft model cache
    params, and Unix socket support
  * Quantization: User-defined layer pruning and KV override fixes
  * Optimizations: Batched Vulkan mul_mat_id splitting
    and ARM hsum reduction
  * Added GGML version function
  * Full changelog:
    https://github.com/ggml-org/llama.cpp/compare/b5699...b5812</comment>
    <requestid>1290235</requestid>
  </revision>
</revisionlist>
