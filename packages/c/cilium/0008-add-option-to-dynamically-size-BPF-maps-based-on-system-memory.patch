diff -crB --new-file cilium-3-patch-update/daemon/daemon_main.go cilium-1.7.0-original-test/daemon/daemon_main.go
*** cilium-3-patch-update/daemon/daemon_main.go	2020-04-25 02:35:35.310104034 +0000
--- cilium-1.7.0-original-test/daemon/daemon_main.go	2020-04-25 02:52:29.358670265 +0000
***************
*** 50,56 ****
--- 50,59 ----
  	"github.com/cilium/cilium/pkg/loadinfo"
  	"github.com/cilium/cilium/pkg/logging"
  	"github.com/cilium/cilium/pkg/logging/logfields"
+ 	"github.com/cilium/cilium/pkg/maps/ctmap"
  	"github.com/cilium/cilium/pkg/maps/ctmap/gc"
+ 	"github.com/cilium/cilium/pkg/maps/nat"
+ 	"github.com/cilium/cilium/pkg/maps/policymap"
  	"github.com/cilium/cilium/pkg/metrics"
  	monitorAPI "github.com/cilium/cilium/pkg/monitor/api"
  	"github.com/cilium/cilium/pkg/node"
***************
*** 661,666 ****
--- 664,672 ----
  	flags.Int(option.PolicyMapEntriesName, defaults.PolicyMapEntries, "Maximum number of entries in endpoint policy map (per endpoint)")
  	option.BindEnv(option.PolicyMapEntriesName)
  
+ 	flags.Float64(option.MapEntriesGlobalDynamicSizeRatioName, 0.0, "Ratio (0.0-1.0) of total system memory to use for dynamic sizing of CT, NAT and policy BPF maps. Set to 0.0 to disable dynamic BPF map sizing (default: 0.0)")
+ 	option.BindEnv(option.MapEntriesGlobalDynamicSizeRatioName)
+ 
  	flags.String(option.CMDRef, "", "Path to cmdref output directory")
  	flags.MarkHidden(option.CMDRef)
  	option.BindEnv(option.CMDRef)
***************
*** 794,799 ****
--- 800,812 ----
  }
  
  func initEnv(cmd *cobra.Command) {
+ 	option.Config.SetMapElementSizes(
+ 		// for the conntrack and NAT element size we assume the largest possible
+ 		// key size, i.e. IPv6 keys
+ 		ctmap.SizeofCtKey6Global+ctmap.SizeofCtEntry,
+ 		nat.SizeofNatKey6+nat.SizeofNatEntry6,
+ 		policymap.SizeofPolicyKey+policymap.SizeofPolicyEntry)
+ 
  	// Prepopulate option.Config with options from CLI.
  	option.Config.Populate()
  
diff -crB --new-file cilium-3-patch-update/Documentation/architecture.rst cilium-1.7.0-original-test/Documentation/architecture.rst
*** cilium-3-patch-update/Documentation/architecture.rst	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/Documentation/architecture.rst	2020-04-24 21:38:15.314530171 +0000
***************
*** 45,51 ****
    tc ingress hook can be coupled with above XDP hook. When this is done it
    is reasonable to assume that the majority of the traffic at this
    point is legitimate and destined for the host.
!   
    Containers typically use a virtual device called a veth pair which acts
    as a virtual wire connecting the container to the host. By attaching to
    the TC ingress hook of the host side of this veth pair Cilium can monitor
--- 45,51 ----
    tc ingress hook can be coupled with above XDP hook. When this is done it
    is reasonable to assume that the majority of the traffic at this
    point is legitimate and destined for the host.
! 
    Containers typically use a virtual device called a veth pair which acts
    as a virtual wire connecting the container to the host. By attaching to
    the TC ingress hook of the host side of this veth pair Cilium can monitor
***************
*** 54,60 ****
    network traffic to the host side virtual devices with another BPF program
    attached to the tc ingress hook as well Cilium can monitor and enforce
    policy on all traffic entering or exiting the node.
!   
    Depending on the use case, containers may also be connected through ipvlan
    devices instead of a veth pair. In this mode, the physical device in the
    host is the ipvlan master where virtual ipvlan devices in slave mode are
--- 54,60 ----
    network traffic to the host side virtual devices with another BPF program
    attached to the tc ingress hook as well Cilium can monitor and enforce
    policy on all traffic entering or exiting the node.
! 
    Depending on the use case, containers may also be connected through ipvlan
    devices instead of a veth pair. In this mode, the physical device in the
    host is the ipvlan master where virtual ipvlan devices in slave mode are
***************
*** 257,262 ****
--- 257,273 ----
  Tunnel                   node             64k             Max 32k nodes (IPv4+IPv6) or 64k nodes (IPv4 or IPv6) across all clusters
  ======================== ================ =============== =====================================================
  
+ For some BPF maps, the upper capacity limit can be overridden using command
+ line options for ``cilium-agent``. A given capacity can be set using
+ ``--bpf-ct-global-tcp-max``, ``--bpf-ct-global-any-max``,
+ ``--bpf-nat-global-max``, ``--bpf-policy-map-max``, and
+ ``--bpf-fragments-map-max``.
+ 
+ Using ``--bpf-map-dynamic-size-ratio`` the upper capacity limits of the
+ connection tracking, NAT, and policy maps are determined at agent startup based
+ on the given ratio of the total system memory. For example a given ratio of 0.03
+ leads to 3% of the total system memory to be used for these maps.
+ 
  Kubernetes Integration
  ======================
  
diff -crB --new-file cilium-3-patch-update/Documentation/cmdref/cilium-agent.md cilium-1.7.0-original-test/Documentation/cmdref/cilium-agent.md
*** cilium-3-patch-update/Documentation/cmdref/cilium-agent.md	2020-04-25 02:35:35.322104100 +0000
--- cilium-1.7.0-original-test/Documentation/cmdref/cilium-agent.md	2020-04-24 21:35:12.017619615 +0000
***************
*** 32,37 ****
--- 32,38 ----
        --bpf-ct-timeout-regular-tcp-syn duration               Establishment timeout for entries in TCP CT table (default 1m0s)
        --bpf-ct-timeout-service-any duration                   Timeout for service entries in non-TCP CT table (default 1m0s)
        --bpf-ct-timeout-service-tcp duration                   Timeout for established service entries in TCP CT table (default 6h0m0s)
+       --bpf-map-dynamic-size-ratio float                      Ratio (0.0-1.0) of total system memory to use for dynamic sizing of CT, NAT and policy BPF maps. Set to 0.0 to disable dynamic BPF map sizing (default: 0.0)
        --bpf-nat-global-max int                                Maximum number of entries for the global BPF NAT table (default 524288)
        --bpf-policy-map-max int                                Maximum number of entries in endpoint policy map (per endpoint) (default 16384)
        --bpf-root string                                       Path to BPF filesystem
diff -crB --new-file cilium-3-patch-update/Documentation/install/upgrade.rst cilium-1.7.0-original-test/Documentation/install/upgrade.rst
*** cilium-3-patch-update/Documentation/install/upgrade.rst	2020-04-25 02:35:35.322104100 +0000
--- cilium-1.7.0-original-test/Documentation/install/upgrade.rst	2020-04-24 21:32:23.000779998 +0000
***************
*** 291,390 ****
     upgrade. Connections should successfully re-establish without requiring
     clients to reconnect.
  
- Upgrading from >=1.7.0 to 1.8.y
- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
- 
- * Since Cilium 1.5, the TCP connection tracking table size parameter
-   ``bpf-ct-global-tcp-max`` in the daemon was set to the default value
-   ``1000000`` to retain backwards compatibility with previous versions. In
-   Cilium 1.8 the default value is set to 512K by default in order to reduce the
-   agent memory consumption.
- 
-   If Cilium was deployed using Helm, the new default value of 512K was already
-   effective in Cilium 1.6 or later, unless it was manually configured to a
-   different value.
- 
-   If the table size was configured to a value different from 512K in the
-   previous installation, ongoing connections will be disrupted during the
-   upgrade. To avoid connection breakage, ``bpf-ct-global-tcp-max`` needs to be
-   manually adjusted.
- 
-   To check whether any action is required the following command can be used to
-   check the currently configured maximum number of TCP conntrack entries:
- 
-   .. code:: bash
- 
-      sudo grep -R CT_MAP_SIZE_TCP /var/run/cilium/state/templates/
- 
-   If the maximum number is 524288, no action is required. If the number is
-   different, ``bpf-ct-global-tcp-max`` needs to be adjusted in the `ConfigMap`
-   to the value shown by the command above (100000 in the example below):
- 
- .. tabs::
-   .. group-tab:: kubectl
- 
-     .. parsed-literal::
- 
-       helm template cilium \\
-       --namespace=kube-system \\
-       ...
-       --set global.bpf.ctTcpMax=100000
-       ...
-       > cilium.yaml
-       kubectl apply -f cilium.yaml
- 
-   .. group-tab:: Helm
- 
-     .. parsed-literal::
- 
-       helm upgrade cilium --namespace=kube-system \\
-       --set global.bpf.ctTcpMax=100000
- 
- * The default value for the NAT table size parameter ``bpf-nat-global-max`` in
-   the daemon is derived from the default value of the conntrack table size
-   parameter ``bpf-ct-global-tcp-max``. Since the latter was changed (see
-   above), the default NAT table size decreased from ~820K to 512K.
- 
-   The NAT table is only used if either BPF NodePort (``enable-node-port``
-   parameter) or masquerading (``masquerade`` parameter) are enabled. No action
-   is required if neither of the parameters is enabled.
- 
-   If either of the parameters is enabled, ongoing connections will be disrupted
-   during the upgrade. In order to avoid connection breakage,
-   ``bpf-nat-global-max`` needs to be manually adjusted.
- 
-   To check whether any adjustment is required the following command can be used
-   to check the currently configured maximum number of NAT table entries:
- 
-   .. code:: bash
- 
-      sudo grep -R SNAT_MAPPING_IPV[46]_SIZE /var/run/cilium/state/globals/
- 
-   If the command does not return any value or if the returned maximum number is
-   524288, no action is required. If the number is different,
-   ``bpf-nat-global-max`` needs to be adjusted in the `ConfigMap` to the value
-   shown by the command above (841429 in the example below):
- 
- .. tabs::
-   .. group-tab:: kubectl
- 
-     .. parsed-literal::
- 
-       helm template cilium \\
-       --namespace=kube-system \\
-       ...
-       --set global.bpf.natMax=841429
-       ...
-       > cilium.yaml
-       kubectl apply -f cilium.yaml
- 
-   .. group-tab:: Helm
- 
-     .. parsed-literal::
- 
-       helm upgrade cilium --namespace=kube-system \\
-       --set global.bpf.natMax=841429
- 
  .. _1.7_upgrade_notes:
  
  1.7 Upgrade Notes
--- 291,296 ----
***************
*** 490,495 ****
--- 396,416 ----
  New ConfigMap Options
  ~~~~~~~~~~~~~~~~~~~~~
  
+   * ``bpf-map-dynamic-size-ratio`` has been added to allow sizing of the TCP CT,
+     non-TCP CT, NAT and policy BPF maps based on the total system memory. This
+     option allows to specify a ratio (0.0-1.0) of total system memory to use for
+     these maps. On new installations, this ratio is set to 0.03 by default,
+     leading to 3% of the total system memory to be allocated for these maps. On
+     a node with 4 GiB of total system memory this ratio corresponds
+     approximately to the default BPF map sizes. A value of 0.0 will disable
+     sizing of the BPF maps based on system memory. Any BPF map sizes configured
+     manually using the ``ctTcpMax``, ``ctAnyMax``, ``natMax`` options will take
+     precedence over the dynamically determined value.
+ 
+     On upgrades of existing installations, this option is disable by default,
+     i.e. it is set to 0.0. Users wanting to use this feature need to enable it
+     explicitly in their `ConfigMap`, see section :ref:`upgrade_configmap`.
+ 
    * ``enable-well-known-identities`` has been added to control the
      initialization of the well-known identities. Well-known identities have
      initially been added to support the managed etcd concept to allow the etcd
diff -crB --new-file cilium-3-patch-update/Documentation/spelling_wordlist.txt cilium-1.7.0-original-test/Documentation/spelling_wordlist.txt
*** cilium-3-patch-update/Documentation/spelling_wordlist.txt	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/Documentation/spelling_wordlist.txt	2020-04-24 21:27:55.703478692 +0000
***************
*** 38,43 ****
--- 38,44 ----
  GCE
  Gartrell
  Geneve
+ GiB
  Github
  Golang
  Gospodarek
diff -crB --new-file cilium-3-patch-update/install/kubernetes/cilium/charts/config/templates/configmap.yaml cilium-1.7.0-original-test/install/kubernetes/cilium/charts/config/templates/configmap.yaml
*** cilium-3-patch-update/install/kubernetes/cilium/charts/config/templates/configmap.yaml	2020-04-25 02:35:35.322104100 +0000
--- cilium-1.7.0-original-test/install/kubernetes/cilium/charts/config/templates/configmap.yaml	2020-04-24 21:25:16.214725004 +0000
***************
*** 132,137 ****
--- 132,141 ----
    # table.
    bpf-nat-global-max: "{{ .Values.global.bpf.natMax }}"
  
+   # Specifies the ratio (0.0-1.0) of total system memory to use for dynamic
+   # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.
+   bpf-map-dynamic-size-ratio: "{{ .Values.global.bpf.mapDynamicSizeRatio }}"
+ 
    # Pre-allocation of map entries allows per-packet latency to be reduced, at
    # the expense of up-front memory allocation for the entries in the maps. The
    # default value below will minimize memory usage in the default installation;
diff -crB --new-file cilium-3-patch-update/install/kubernetes/cilium/values.yaml cilium-1.7.0-original-test/install/kubernetes/cilium/values.yaml
*** cilium-3-patch-update/install/kubernetes/cilium/values.yaml	2020-04-25 02:35:35.322104100 +0000
--- cilium-1.7.0-original-test/install/kubernetes/cilium/values.yaml	2020-04-24 21:01:26.223967365 +0000
***************
*** 193,198 ****
--- 193,205 ----
      # natMax is the maximum number of entries for the NAT table
      natMax: 524288 
  
+     # mapDynamicSizeRatio is the ratio (0.0-1.0) of total system memory to use
+     # for dynamic sizing of CT, NAT and policy BPF maps. If set to 0.0, dynamic
+     # sizing of BPF maps is disabled. The default value of 0.03 (3%) leads to
+     # approximately the default BPF map sizes on a node with 4 GiB of total
+     # system memory.
+     mapDynamicSizeRatio: 0.03
+ 
      # montiorAggregation is the level of aggregation for datapath trace events
      monitorAggregation: medium
  
diff -crB --new-file cilium-3-patch-update/install/kubernetes/quick-install.yaml cilium-1.7.0-original-test/install/kubernetes/quick-install.yaml
*** cilium-3-patch-update/install/kubernetes/quick-install.yaml	2020-04-25 02:35:35.322104100 +0000
--- cilium-1.7.0-original-test/install/kubernetes/quick-install.yaml	2020-04-24 20:58:39.691180390 +0000
***************
*** 65,70 ****
--- 65,74 ----
    # table.
    bpf-nat-global-max: "524288"
  
+   # Specifies the ratio (0.0-1.0) of total system memory to use for dynamic
+   # sizing of the TCP CT, non-TCP CT, NAT and policy BPF maps.
+   bpf-map-dynamic-size-ratio: "0.03"
+ 
    # Pre-allocation of map entries allows per-packet latency to be reduced, at
    # the expense of up-front memory allocation for the entries in the maps. The
    # default value below will minimize memory usage in the default installation;
diff -crB --new-file cilium-3-patch-update/pkg/maps/ctmap/ctmap.go cilium-1.7.0-original-test/pkg/maps/ctmap/ctmap.go
*** cilium-3-patch-update/pkg/maps/ctmap/ctmap.go	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/pkg/maps/ctmap/ctmap.go	2020-04-24 20:56:12.958486982 +0000
***************
*** 119,125 ****
  		keySize:   keySize,
  		// the value type is CtEntry for all CT maps
  		mapValue:   &CtEntry{},
! 		valueSize:  int(unsafe.Sizeof(CtEntry{})),
  		maxEntries: maxEntries,
  		parser:     bpf.ConvertKeyValue,
  		natMap:     nat,
--- 119,125 ----
  		keySize:   keySize,
  		// the value type is CtEntry for all CT maps
  		mapValue:   &CtEntry{},
! 		valueSize:  SizeofCtEntry,
  		maxEntries: maxEntries,
  		parser:     bpf.ConvertKeyValue,
  		natMap:     nat,
diff -crB --new-file cilium-3-patch-update/pkg/maps/ctmap/types.go cilium-1.7.0-original-test/pkg/maps/ctmap/types.go
*** cilium-3-patch-update/pkg/maps/ctmap/types.go	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/pkg/maps/ctmap/types.go	2020-04-24 20:55:46.522362054 +0000
***************
*** 381,386 ****
--- 381,388 ----
  	tuple.TupleKey6Global
  }
  
+ const SizeofCtKey6Global = int(unsafe.Sizeof(CtKey6Global{}))
+ 
  // NewValue creates a new bpf.MapValue.
  func (k *CtKey6Global) NewValue() bpf.MapValue { return &CtEntry{} }
  
***************
*** 483,488 ****
--- 485,492 ----
  	LastRxReport     uint32 `align:"last_rx_report"`
  }
  
+ const SizeofCtEntry = int(unsafe.Sizeof(CtEntry{}))
+ 
  // GetValuePtr returns the unsafe.Pointer for s.
  func (c *CtEntry) GetValuePtr() unsafe.Pointer { return unsafe.Pointer(c) }
  
diff -crB --new-file cilium-3-patch-update/pkg/maps/nat/ipv4.go cilium-1.7.0-original-test/pkg/maps/nat/ipv4.go
*** cilium-3-patch-update/pkg/maps/nat/ipv4.go	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/pkg/maps/nat/ipv4.go	2020-04-24 20:55:04.330162669 +0000
***************
*** 35,40 ****
--- 35,43 ----
  	Port      uint16     `align:"to_sport"`
  }
  
+ // SizeofNatEntry4 is the size of the NatEntry4 type in bytes.
+ const SizeofNatEntry4 = int(unsafe.Sizeof(NatEntry4{}))
+ 
  // GetValuePtr returns the unsafe.Pointer for n.
  func (n *NatEntry4) GetValuePtr() unsafe.Pointer { return unsafe.Pointer(n) }
  
diff -crB --new-file cilium-3-patch-update/pkg/maps/nat/ipv6.go cilium-1.7.0-original-test/pkg/maps/nat/ipv6.go
*** cilium-3-patch-update/pkg/maps/nat/ipv6.go	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/pkg/maps/nat/ipv6.go	2020-04-24 20:53:55.801838828 +0000
***************
*** 35,40 ****
--- 35,43 ----
  	Port      uint16     `align:"to_sport"`
  }
  
+ // SizeofNatEntry6 is the size of the NatEntry6 type in bytes.
+ const SizeofNatEntry6 = int(unsafe.Sizeof(NatEntry6{}))
+ 
  // GetValuePtr returns the unsafe.Pointer for n.
  func (n *NatEntry6) GetValuePtr() unsafe.Pointer { return unsafe.Pointer(n) }
  
diff -crB --new-file cilium-3-patch-update/pkg/maps/nat/nat.go cilium-1.7.0-original-test/pkg/maps/nat/nat.go
*** cilium-3-patch-update/pkg/maps/nat/nat.go	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/pkg/maps/nat/nat.go	2020-04-24 20:52:56.429558255 +0000
***************
*** 77,90 ****
  
  	if v4 {
  		mapKey = &NatKey4{}
! 		sizeKey = int(unsafe.Sizeof(NatKey4{}))
  		mapValue = &NatEntry4{}
! 		sizeVal = int(unsafe.Sizeof(NatEntry4{}))
  	} else {
  		mapKey = &NatKey6{}
! 		sizeKey = int(unsafe.Sizeof(NatKey6{}))
  		mapValue = &NatEntry6{}
! 		sizeVal = int(unsafe.Sizeof(NatEntry6{}))
  	}
  	return &Map{
  		Map: *bpf.NewMap(
--- 77,90 ----
  
  	if v4 {
  		mapKey = &NatKey4{}
! 		sizeKey = SizeofNatKey4
  		mapValue = &NatEntry4{}
! 		sizeVal = SizeofNatEntry4
  	} else {
  		mapKey = &NatKey6{}
! 		sizeKey = SizeofNatKey6
  		mapValue = &NatEntry6{}
! 		sizeVal = SizeofNatEntry6
  	}
  	return &Map{
  		Map: *bpf.NewMap(
diff -crB --new-file cilium-3-patch-update/pkg/maps/nat/types.go cilium-1.7.0-original-test/pkg/maps/nat/types.go
*** cilium-3-patch-update/pkg/maps/nat/types.go	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/pkg/maps/nat/types.go	2020-04-24 20:52:13.817356884 +0000
***************
*** 45,50 ****
--- 45,53 ----
  	tuple.TupleKey4Global
  }
  
+ // SizeofNatKey4 is the size of the NatKey4 type in bytes.
+ const SizeofNatKey4 = int(unsafe.Sizeof(NatKey4{}))
+ 
  // NewValue creates a new bpf.MapValue.
  func (k *NatKey4) NewValue() bpf.MapValue { return &NatEntry4{} }
  
***************
*** 80,85 ****
--- 83,91 ----
  	tuple.TupleKey6Global
  }
  
+ // SizeofNatKey6 is the size of the NatKey6 type in bytes.
+ const SizeofNatKey6 = int(unsafe.Sizeof(NatKey6{}))
+ 
  // NewValue creates a new bpf.MapValue.
  func (k *NatKey6) NewValue() bpf.MapValue { return &NatEntry6{} }
  
diff -crB --new-file cilium-3-patch-update/pkg/maps/policymap/policymap.go cilium-1.7.0-original-test/pkg/maps/policymap/policymap.go
*** cilium-3-patch-update/pkg/maps/policymap/policymap.go	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/pkg/maps/policymap/policymap.go	2020-04-24 20:50:56.492991477 +0000
***************
*** 76,81 ****
--- 76,84 ----
  	TrafficDirection uint8  `align:"egress"`
  }
  
+ // SizeofPolicyKey is the size of type PolicyKey.
+ const SizeofPolicyKey = int(unsafe.Sizeof(PolicyKey{}))
+ 
  // PolicyEntry represents an entry in the BPF policy map for an endpoint. It must
  // match the layout of policy_entry in bpf/lib/common.h.
  // +k8s:deepcopy-gen=true
***************
*** 89,94 ****
--- 92,100 ----
  	Bytes     uint64 `align:"bytes"`
  }
  
+ // SizeofPolicyEntry is the size of type PolicyEntry.
+ const SizeofPolicyEntry = int(unsafe.Sizeof(PolicyEntry{}))
+ 
  func (pe *PolicyEntry) GetValuePtr() unsafe.Pointer { return unsafe.Pointer(pe) }
  func (pe *PolicyEntry) NewValue() bpf.MapValue      { return &PolicyEntry{} }
  
***************
*** 264,272 ****
  			path,
  			mapType,
  			&PolicyKey{},
! 			int(unsafe.Sizeof(PolicyKey{})),
  			&PolicyEntry{},
! 			int(unsafe.Sizeof(PolicyEntry{})),
  			MaxEntries,
  			flags, 0,
  			bpf.ConvertKeyValue,
--- 270,278 ----
  			path,
  			mapType,
  			&PolicyKey{},
! 			SizeofPolicyKey,
  			&PolicyEntry{},
! 			SizeofPolicyEntry,
  			MaxEntries,
  			flags, 0,
  			bpf.ConvertKeyValue,
diff -crB --new-file cilium-3-patch-update/pkg/option/config.go cilium-1.7.0-original-test/pkg/option/config.go
*** cilium-3-patch-update/pkg/option/config.go	2020-04-25 02:35:35.322104100 +0000
--- cilium-1.7.0-original-test/pkg/option/config.go	2020-04-25 02:50:59.610052286 +0000
***************
*** 38,43 ****
--- 38,44 ----
  	"github.com/cilium/cilium/pkg/metrics"
  
  	"github.com/prometheus/client_golang/prometheus"
+ 	"github.com/shirou/gopsutil/mem"
  	"github.com/sirupsen/logrus"
  	"github.com/spf13/viper"
  )
***************
*** 443,448 ****
--- 444,454 ----
  	// and is 2/3 of the full CT size as a heuristic
  	NATMapEntriesGlobalDefault = int((CTMapEntriesGlobalTCPDefault + CTMapEntriesGlobalAnyDefault) * 2 / 3)
  
+ 	// MapEntriesGlobalDynamicSizeRatioName is the name of the option to
+ 	// set the ratio of total system memory to use for dynamic sizing of the
+ 	// CT, NAT and policy BPF maps.
+ 	MapEntriesGlobalDynamicSizeRatioName = "bpf-map-dynamic-size-ratio"
+ 
  	// LimitTableMin defines the minimum CT or NAT table limit
  	LimitTableMin = 1 << 10 // 1Ki entries
  
***************
*** 1353,1358 ****
--- 1359,1374 ----
  
  	// EnableRemoteNodeIdentity enables use of the remote-node identity
  	EnableRemoteNodeIdentity bool
+ 
+ 	// sizeofCTElement is the size of an element (key + value) in the CT map.
+ 	sizeofCTElement int
+ 
+ 	// sizeofNATElement is the size of an element (key + value) in the NAT map.
+ 	sizeofNATElement int
+ 
+ 	// sizeofPolicyElement is the size of an element (key + value) in the
+ 	// policy map.
+ 	sizeofPolicyElement int
  }
  
  var (
***************
*** 1667,1675 ****
  	c.AnnotateK8sNode = viper.GetBool(AnnotateK8sNode)
  	c.AutoCreateCiliumNodeResource = viper.GetBool(AutoCreateCiliumNodeResource)
  	c.BPFCompilationDebug = viper.GetBool(BPFCompileDebugName)
- 	c.CTMapEntriesGlobalTCP = viper.GetInt(CTMapEntriesGlobalTCPName)
- 	c.CTMapEntriesGlobalAny = viper.GetInt(CTMapEntriesGlobalAnyName)
- 	c.NATMapEntriesGlobal = viper.GetInt(NATMapEntriesGlobalName)
  	c.BPFRoot = viper.GetString(BPFRoot)
  	c.CertDirectory = viper.GetString(CertsDirectory)
  	c.CGroupRoot = viper.GetString(CGroupRoot)
--- 1683,1688 ----
***************
*** 1761,1767 ****
  	c.NAT46Range = viper.GetString(NAT46Range)
  	c.FlannelMasterDevice = viper.GetString(FlannelMasterDevice)
  	c.FlannelUninstallOnExit = viper.GetBool(FlannelUninstallOnExit)
- 	c.PolicyMapEntries = viper.GetInt(PolicyMapEntriesName)
  	c.PProf = viper.GetBool(PProf)
  	c.PreAllocateMaps = viper.GetBool(PreAllocateMapsName)
  	c.PrependIptablesChains = viper.GetBool(PrependIptablesChainsName)
--- 1774,1779 ----
***************
*** 1791,1796 ****
--- 1803,1812 ----
  		c.ipv4NativeRoutingCIDR = cidr.MustParseCIDR(nativeCIDR)
  	}
  
+ 	if err := c.calculateBPFMapSizes(); err != nil {
+ 		log.Fatal(err)
+ 	}
+ 
  	// toFQDNs options
  	// When the poller is enabled, the default MinTTL is lowered. This is to
  	// avoid caching large sets of identities generated by a poller (it runs
***************
*** 1965,1970 ****
--- 1981,2098 ----
  	c.AwsReleaseExcessIps = viper.GetBool(AwsReleaseExcessIps)
  }
  
+ func (c *DaemonConfig) calculateBPFMapSizes() error {
+ 	// BPF map size options
+ 	// Any map size explicitly set via option will override the dynamic
+ 	// sizing.
+ 	c.CTMapEntriesGlobalTCP = viper.GetInt(CTMapEntriesGlobalTCPName)
+ 	c.CTMapEntriesGlobalAny = viper.GetInt(CTMapEntriesGlobalAnyName)
+ 	c.NATMapEntriesGlobal = viper.GetInt(NATMapEntriesGlobalName)
+ 	c.PolicyMapEntries = viper.GetInt(PolicyMapEntriesName)
+ 
+ 	// Don't attempt dynamic sizing if any of the sizeof members was not
+ 	// populated by the daemon (or any other caller).
+ 	if c.sizeofCTElement == 0 || c.sizeofNATElement == 0 || c.sizeofPolicyElement == 0 {
+ 		return nil
+ 	}
+ 
+ 	// Allow the range (0.0, 1.0] because the dynamic size will anyway be
+ 	// clamped to the table limits. Thus, a ratio of e.g. 0.98 will not lead
+ 	// to 98% of the total memory being allocated for BPF maps.
+ 	dynamicSizeRatio := viper.GetFloat64(MapEntriesGlobalDynamicSizeRatioName)
+ 	if 0.0 < dynamicSizeRatio && dynamicSizeRatio <= 1.0 {
+ 		vms, err := mem.VirtualMemory()
+ 		if err != nil || vms == nil {
+ 			log.WithError(err).Fatal("Failed to get system memory")
+ 		}
+ 		c.calculateDynamicBPFMapSizes(vms.Total, dynamicSizeRatio)
+ 	} else if dynamicSizeRatio < 0.0 {
+ 		return fmt.Errorf("specified dynamic map size ratio %f must be ≥ 0.0", dynamicSizeRatio)
+ 	} else if dynamicSizeRatio > 1.0 {
+ 		return fmt.Errorf("specified dynamic map size ratio %f must be ≤ 1.0", dynamicSizeRatio)
+ 	}
+ 	return nil
+ }
+ 
+ // SetMapElementSizes sets the BPF map element sizes (key + value) used for
+ // dynamic BPF map size calculations in calculateDynamicBPFMapSizes.
+ func (c *DaemonConfig) SetMapElementSizes(sizeofCTElement, sizeofNATElement, sizeofPolicyElement int) {
+ 	c.sizeofCTElement = sizeofCTElement
+ 	c.sizeofNATElement = sizeofNATElement
+ 	c.sizeofPolicyElement = sizeofPolicyElement
+ }
+ 
+ func (c *DaemonConfig) calculateDynamicBPFMapSizes(totalMemory uint64, dynamicSizeRatio float64) {
+ 	// Heuristic:
+ 	// Distribute relative to map default entries among the different maps.
+ 	// Cap each map size by the maximum. Map size provided by the user will
+ 	// override the calculated value and also the max. There will be a check
+ 	// for maximum size later on in DaemonConfig.Validate()
+ 	//
+ 	// Calculation examples:
+ 	//
+ 	// Memory   CT TCP  CT Any      NAT  Policy
+ 	//
+ 	//  512MB    33140   16570    33140    1035
+ 	//    1GB    66280   33140    66280    2071
+ 	//    4GB   265121  132560   265121    8285
+ 	//   16GB  1060485  530242  1060485   33140
+ 	memoryAvailableForMaps := int(float64(totalMemory) * dynamicSizeRatio)
+ 	log.Debugf("Memory available for map entries (%.3f of %d): %d", dynamicSizeRatio, totalMemory, memoryAvailableForMaps)
+ 	totalMapMemoryDefault := CTMapEntriesGlobalTCPDefault*c.sizeofCTElement +
+ 		CTMapEntriesGlobalAnyDefault*c.sizeofCTElement +
+ 		NATMapEntriesGlobalDefault*c.sizeofNATElement +
+ 		defaults.PolicyMapEntries*c.sizeofPolicyElement
+ 	log.Debugf("Total memory for default map entries: %d", totalMapMemoryDefault)
+ 
+ 	getEntries := func(entriesDefault, min, max int) int {
+ 		entries := (entriesDefault * memoryAvailableForMaps) / totalMapMemoryDefault
+ 		if entries < min {
+ 			entries = min
+ 		} else if entries > max {
+ 			log.Debugf("clamped from %d to %d", entries, max)
+ 			entries = max
+ 		}
+ 		return entries
+ 	}
+ 
+ 	// If value for a particular map was explicitly set by an
+ 	// option, disable dynamic sizing for this map and use the
+ 	// provided size.
+ 	if !viper.IsSet(CTMapEntriesGlobalTCPName) {
+ 		c.CTMapEntriesGlobalTCP =
+ 			getEntries(CTMapEntriesGlobalTCPDefault, LimitTableMin, LimitTableMax)
+ 		log.Debugf("option %s set by dynamic sizing to %v (default %v)",
+ 			CTMapEntriesGlobalTCPName, c.CTMapEntriesGlobalTCP, CTMapEntriesGlobalTCPDefault)
+ 	} else {
+ 		log.Debugf("option %s set by user to %v", CTMapEntriesGlobalTCPName, c.CTMapEntriesGlobalTCP)
+ 	}
+ 	if !viper.IsSet(CTMapEntriesGlobalAnyName) {
+ 		c.CTMapEntriesGlobalAny =
+ 			getEntries(CTMapEntriesGlobalAnyDefault, LimitTableMin, LimitTableMax)
+ 		log.Debugf("option %s set by dynamic sizing to %v (default %v)",
+ 			CTMapEntriesGlobalAnyName, c.CTMapEntriesGlobalAny, CTMapEntriesGlobalAnyDefault)
+ 	} else {
+ 		log.Debugf("option %s set by user to %v", CTMapEntriesGlobalAnyName, c.CTMapEntriesGlobalAny)
+ 	}
+ 	if !viper.IsSet(NATMapEntriesGlobalName) {
+ 		c.NATMapEntriesGlobal =
+ 			getEntries(NATMapEntriesGlobalDefault, LimitTableMin, LimitTableMax)
+ 		log.Debugf("option %s set by dynamic sizing to %v (default %v)",
+ 			NATMapEntriesGlobalName, c.NATMapEntriesGlobal, NATMapEntriesGlobalDefault)
+ 	} else {
+ 		log.Debugf("option %s set by user to %v", NATMapEntriesGlobalName, c.NATMapEntriesGlobal)
+ 	}
+ 	if !viper.IsSet(PolicyMapEntriesName) {
+ 		c.PolicyMapEntries =
+ 			getEntries(defaults.PolicyMapEntries, PolicyMapMin, PolicyMapMax)
+ 		log.Debugf("option %s set by dynamic sizing to %v (default %v)",
+ 			PolicyMapEntriesName, c.PolicyMapEntries, defaults.PolicyMapEntries)
+ 	} else {
+ 		log.Debugf("option %s set by user to %v", PolicyMapEntriesName, c.PolicyMapEntries)
+ 	}
+ }
+ 
  func sanitizeIntParam(paramName string, paramDefault int) int {
  	intParam := viper.GetInt(paramName)
  	if intParam <= 0 {
diff -crB --new-file cilium-3-patch-update/pkg/option/config_test.go cilium-1.7.0-original-test/pkg/option/config_test.go
*** cilium-3-patch-update/pkg/option/config_test.go	2020-02-18 22:32:45.000000000 +0000
--- cilium-1.7.0-original-test/pkg/option/config_test.go	2020-04-25 03:43:58.255528717 +0000
***************
*** 24,29 ****
--- 24,31 ----
  	"path/filepath"
  	"testing"
  
+ 	"github.com/cilium/cilium/pkg/defaults"
+ 	"github.com/google/go-cmp/cmp"
  	flag "github.com/spf13/pflag"
  	"github.com/spf13/viper"
  	. "gopkg.in/check.v1"
***************
*** 227,229 ****
--- 229,406 ----
  	c.Assert(d.IsExcludedLocalAddress(net.ParseIP("f00d::1")), Equals, true)
  	c.Assert(d.IsExcludedLocalAddress(net.ParseIP("f00d::2")), Equals, false)
  }
+ 
+ const (
+ 	_   = iota
+ 	KiB = 1 << (10 * iota)
+ 	MiB
+ 	GiB
+ )
+ 
+ func TestBPFMapSizeCalculation(t *testing.T) {
+ 	type sizes struct {
+ 		CTMapSizeTCP  int
+ 		CTMapSizeAny  int
+ 		NATMapSize    int
+ 		PolicyMapSize int
+ 	}
+ 	tests := []struct {
+ 		name        string
+ 		totalMemory uint64
+ 		ratio       float64
+ 		want        sizes
+ 		preTestRun  func()
+ 	}{
+ 		{
+ 			name: "static default sizes",
+ 			// zero memory and ratio: skip calculateDynamicBPFMapSizes
+ 			want: sizes{
+ 				CTMapSizeTCP:  CTMapEntriesGlobalTCPDefault,
+ 				CTMapSizeAny:  CTMapEntriesGlobalAnyDefault,
+ 				NATMapSize:    NATMapEntriesGlobalDefault,
+ 				PolicyMapSize: defaults.PolicyMapEntries,
+ 			},
+ 			preTestRun: func() {
+ 				viper.Set(CTMapEntriesGlobalTCPName, CTMapEntriesGlobalTCPDefault)
+ 				viper.Set(CTMapEntriesGlobalAnyName, CTMapEntriesGlobalAnyDefault)
+ 				viper.Set(NATMapEntriesGlobalName, NATMapEntriesGlobalDefault)
+ 				viper.Set(PolicyMapEntriesName, defaults.PolicyMapEntries)
+ 			},
+ 		},
+ 		{
+ 			name: "static, non-default sizes inside range",
+ 			// zero memory and ratio: skip calculateDynamicBPFMapSizes
+ 			want: sizes{
+ 				CTMapSizeTCP:  CTMapEntriesGlobalTCPDefault + 128,
+ 				CTMapSizeAny:  CTMapEntriesGlobalAnyDefault - 64,
+ 				NATMapSize:    NATMapEntriesGlobalDefault + 256,
+ 				PolicyMapSize: defaults.PolicyMapEntries - 32,
+ 			},
+ 			preTestRun: func() {
+ 				viper.Set(CTMapEntriesGlobalTCPName, CTMapEntriesGlobalTCPDefault+128)
+ 				viper.Set(CTMapEntriesGlobalAnyName, CTMapEntriesGlobalAnyDefault-64)
+ 				viper.Set(NATMapEntriesGlobalName, NATMapEntriesGlobalDefault+256)
+ 				viper.Set(PolicyMapEntriesName, defaults.PolicyMapEntries-32)
+ 			},
+ 		},
+ 		{
+ 			name:        "dynamic size without any static sizes (512MB, 3%)",
+ 			totalMemory: 512 * MiB,
+ 			ratio:       0.03,
+ 			want: sizes{
+ 				CTMapSizeTCP:  68246,
+ 				CTMapSizeAny:  34123,
+ 				NATMapSize:    68246,
+ 				PolicyMapSize: 2132,
+ 			},
+ 		},
+ 		{
+ 			name:        "dynamic size without any static sizes (1GiB, 3%)",
+ 			totalMemory: 1 * GiB,
+ 			ratio:       0.03,
+ 			want: sizes{
+ 				CTMapSizeTCP:  136492,
+ 				CTMapSizeAny:  68246,
+ 				NATMapSize:    136492,
+ 				PolicyMapSize: 4265,
+ 			},
+ 		},
+ 		{
+ 			name:        "dynamic size without any static sizes (2GiB, 3%)",
+ 			totalMemory: 2 * GiB,
+ 			ratio:       0.03,
+ 			want: sizes{
+ 				CTMapSizeTCP:  272985,
+ 				CTMapSizeAny:  136492,
+ 				NATMapSize:    272985,
+ 				PolicyMapSize: 8530,
+ 			},
+ 		},
+ 		{
+ 			name:        "dynamic size without any static sizes (4GiB, 3%)",
+ 			totalMemory: 4 * GiB,
+ 			ratio:       0.03,
+ 			want: sizes{
+ 				CTMapSizeTCP:  545970,
+ 				CTMapSizeAny:  272985,
+ 				NATMapSize:    545970,
+ 				PolicyMapSize: 17061,
+ 			},
+ 		},
+ 		{
+ 			name:        "dynamic size without any static sizes (16GiB, 3%)",
+ 			totalMemory: 16 * GiB,
+ 			ratio:       0.03,
+ 			want: sizes{
+ 				CTMapSizeTCP:  2183881,
+ 				CTMapSizeAny:  1091940,
+ 				NATMapSize:    2183881,
+ 				PolicyMapSize: PolicyMapMax,
+ 			},
+ 		},
+ 		{
+ 			name:        "dynamic size with static CT TCP size (4GiB, 2.5%)",
+ 			totalMemory: 4 * GiB,
+ 			ratio:       0.025,
+ 			want: sizes{
+ 				CTMapSizeTCP:  CTMapEntriesGlobalTCPDefault + 1024,
+ 				CTMapSizeAny:  227487,
+ 				NATMapSize:    454975,
+ 				PolicyMapSize: 14217,
+ 			},
+ 			preTestRun: func() {
+ 				viper.Set(CTMapEntriesGlobalTCPName, CTMapEntriesGlobalTCPDefault+1024)
+ 			},
+ 		},
+ 		{
+ 			name:        "huge dynamic size ratio gets clamped (8GiB, 98%)",
+ 			totalMemory: 16 * GiB,
+ 			ratio:       0.98,
+ 			want: sizes{
+ 				CTMapSizeTCP:  LimitTableMax,
+ 				CTMapSizeAny:  LimitTableMax,
+ 				NATMapSize:    LimitTableMax,
+ 				PolicyMapSize: PolicyMapMax,
+ 			},
+ 		},
+ 	}
+ 
+ 	for _, tt := range tests {
+ 		t.Run(tt.name, func(t *testing.T) {
+ 			viper.Reset()
+ 
+ 			if tt.preTestRun != nil {
+ 				tt.preTestRun()
+ 			}
+ 
+ 			d := &DaemonConfig{
+ 				CTMapEntriesGlobalTCP: viper.GetInt(CTMapEntriesGlobalTCPName),
+ 				CTMapEntriesGlobalAny: viper.GetInt(CTMapEntriesGlobalAnyName),
+ 				NATMapEntriesGlobal:   viper.GetInt(NATMapEntriesGlobalName),
+ 				PolicyMapEntries:      viper.GetInt(PolicyMapEntriesName),
+ 			}
+ 			// cannot set these from the Sizeof* consts from
+ 			// pkg/maps/* due to circular dependencies.
+ 			d.SetMapElementSizes(
+ 				94, // ctmap.SizeofCTKey + policymap.SizeofCTEntry
+ 				94, // nat.SizeofNATKey + nat.SizeofNATEntry
+ 				32, // policymap.SizeofPolicyKey + policymap.SizeofPolicyEntry
+ 			)
+ 
+ 			if tt.totalMemory > 0 && tt.ratio > 0.0 {
+ 				d.calculateDynamicBPFMapSizes(tt.totalMemory, tt.ratio)
+ 			}
+ 
+ 			got := sizes{
+ 				d.CTMapEntriesGlobalTCP,
+ 				d.CTMapEntriesGlobalAny,
+ 				d.NATMapEntriesGlobal,
+ 				d.PolicyMapEntries,
+ 			}
+ 
+ 			if diff := cmp.Diff(tt.want, got); diff != "" {
+ 				t.Errorf("DaemonConfig.calculateDynamicBPFMapSize (-want +got):\n%s", diff)
+ 			}
+ 		})
+ 	}
+ }
