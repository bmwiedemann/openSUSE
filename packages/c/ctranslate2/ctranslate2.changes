-------------------------------------------------------------------
Tue Sep 17 09:28:36 UTC 2024 - Adrian Schröter <adrian@suse.de>

- update to version 4.4.0

  **Removed**: Flash Attention support in the Python package due to
               significant package size increase with minimal
               performance gain.  

  ### New features
  * Support Llama3
  * Support Gemma2
  * Add log probs for all tokens in vocab
  * Grouped conv1d
  
  ### Fixes and improvements
  * Some improvements in flash attention
  * Fix crash when using return_alternative on CUDA
  * Quantization AWQ GEMM + GEMV

-------------------------------------------------------------------
Thu Jul 11 08:05:01 UTC 2024 - Adrian Schröter <adrian@suse.de>

- initial package of version 4.3.1

