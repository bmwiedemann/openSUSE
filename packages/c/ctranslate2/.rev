<revisionlist>
  <revision rev="1" vrev="1">
    <srcmd5>df5a14f8f62538dd07247c6b5a0e5963</srcmd5>
    <version>4.3.1</version>
    <time>1722333236</time>
    <user>dimstar_suse</user>
    <comment>Another attempt with fixed deps</comment>
    <requestid>1190392</requestid>
  </revision>
  <revision rev="2" vrev="1">
    <srcmd5>795f533d81db2186fc6a3068db457eb9</srcmd5>
    <version>4.4.0</version>
    <time>1726589941</time>
    <user>anag+factory</user>
    <comment>- update to version 4.4.0
  **Removed**: Flash Attention support in the Python package due to
               significant package size increase with minimal
               performance gain.  
  ### New features
  * Support Llama3
  * Support Gemma2
  * Add log probs for all tokens in vocab
  * Grouped conv1d
  
  ### Fixes and improvements
  * Some improvements in flash attention
  * Fix crash when using return_alternative on CUDA
  * Quantization AWQ GEMM + GEMV</comment>
    <requestid>1201559</requestid>
  </revision>
  <revision rev="3" vrev="2">
    <srcmd5>46efebde56e8f57f1f4d81354c380672</srcmd5>
    <version>4.4.0</version>
    <time>1727294049</time>
    <user>anag+factory</user>
    <comment></comment>
    <requestid>1203278</requestid>
  </revision>
  <revision rev="4" vrev="1">
    <srcmd5>abb79a048f4c34199425e8c6a1241d06</srcmd5>
    <version>4.5.0</version>
    <time>1731597742</time>
    <user>anag+factory</user>
    <comment>Automatic submission by obs-autosubmit</comment>
    <requestid>1224200</requestid>
  </revision>
</revisionlist>
